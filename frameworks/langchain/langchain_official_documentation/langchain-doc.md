<!--
Downloaded via https://llm.codes by @steipete on January 4, 2026 at 12:12 PM
Source URL: https://docs.langchain.com/
Total pages processed: 839
URLs filtered: Yes
Content de-duplicated: Yes
Availability strings filtered: Yes
Code blocks only: No
-->

# https://docs.langchain.com/

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

Home

Search...

Ctrl K

Navigation

# Documentation

LangChain is the platform for agent engineering. AI teams at Replit, Clay, Rippling, Cloudflare, Workday, and more trust LangChain’s products to engineer reliable agents.

## Open source agent frameworks

- Python

- TypeScript

**LangChain (Python)** \\
\\
Quickly get started building agents, with any model provider of your choice.\\
\\
Learn more **LangGraph (Python)** \\
\\
Control every step of your custom agent with low-level orchestration, memory, and human-in-the-loop support.\\
\\
Learn more **Deep Agents (Python)** \\
\\
Build agents that can tackle complex, multi-step tasks.\\
\\
Learn more

**LangChain (TypeScript)** \\
\\
Quickly get started building agents, with any model provider of your choice.\\
\\
Learn more **LangGraph (TypeScript)** \\
\\
Control every step of your custom agent with low-level orchestration, memory, and human-in-the-loop support.\\
\\
Learn more **Deep Agents (TypeScript)** \\
\\
Build agents that can tackle complex, multi-step tasks.\\
\\
Learn more

## LangSmith

**LangSmith** is a platform that helps AI teams use live production data for continuous testing and improvement. LangSmith provides:

**Observability** \\
\\
See exactly how your agent thinks and acts with detailed tracing and aggregate trend metrics.\\
\\
Learn more **Evaluation** \\
\\
Test and score agent behavior on production data or offline datasets to continuously improve performance.\\
\\
Learn more **Prompt Engineering** \\
\\
Iterate on prompts with version control, prompt optimization, and collaboration features.\\
\\
Learn more **Deployment** \\
\\
Ship your agent in one click, using scalable infrastructure built for long-running tasks.\\
\\
Learn more **Agent Builder (Beta)** \\
\\
Turn natural-language ideas into production agents with persistent memory and self-updating capabilities.\\
\\
Learn more

LangSmith meets the highest standards of data security and privacy with HIPAA, SOC 2 Type 2, and GDPR compliance. For more information, see the Trust Center.

## Get started

**Build your first agent with LangChain** \\
\\
Get started **Sign up for LangSmith** \\
\\
Try LangSmith **Build an advanced agent with LangGraph** \\
\\
Get started **Enroll in LangChain Academy** \\
\\
Get started

## Additional resources

**Community forum** \\
\\
Ask questions, share solutions, and discuss best practices.\\
\\
Join **Community Slack** \\
\\
Connect with other builders and get quick help.\\
\\
Join **Support portal** \\
\\
Submit tickets and track support requests.\\
\\
Visit **LangSmith status** \\
\\
Real-time status of LangSmith services and APIs.\\
\\
View

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangChain overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Create an agent
- Core benefits

LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.

## ​ Create an agent

Copy

# pip install -qU langchain "langchain[anthropic]"
from langchain.agents import create_agent

"""Get weather for a given city."""
return f"It's always sunny in {city}!"

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[get_weather],
system_prompt="You are a helpful assistant",
)

# Run the agent
agent.invoke(
{"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)

See the Installation instructions and Quickstart guide to get started building your own agents and applications with LangChain.

## ​ Core benefits

**Standard model interface** \\
\\
Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\\
\\
Learn more **Easy to use, highly flexible agent** \\
\\
LangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\\
\\
Learn more **Built on top of LangGraph** \\
\\
LangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more.\\
\\
Learn more **Debug with LangSmith** \\
\\
Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\
\\
Learn more

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Install LangChain\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangGraph overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Install
- Core benefits
- LangGraph ecosystem
- Acknowledgements

Trusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.We will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.

## ​ Install

pip

uv

Copy

pip install -U langgraph

Then, create a simple hello world example:

from langgraph.graph import StateGraph, MessagesState, START, END

def mock_llm(state: MessagesState):
return {"messages": [{"role": "ai", "content": "hello world"}]}

graph = StateGraph(MessagesState)
graph.add_node(mock_llm)
graph.add_edge(START, "mock_llm")
graph.add_edge("mock_llm", END)
graph = graph.compile()

graph.invoke({"messages": [{"role": "user", "content": "hi!"}]})

## ​ Core benefits

LangGraph provides low-level supporting infrastructure for _any_ long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:

- Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.
- Human-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.
- Comprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.
- Debugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.
- Production-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.

## ​ LangGraph ecosystem

While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:

**LangSmith** \\
\\
Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\\
\\
Learn more **LangGraph** \\
\\
Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.\\
\\
Learn more **LangChain** \\
\\
Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\\
\\
Learn more

## ​ Acknowledgements

LangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Install LangGraph\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Deep Agents overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

On this page

- When to use deep agents
- Core capabilities
- Relationship to the LangChain ecosystem
- Get started

`deepagents` is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.

## ​ When to use deep agents

Use deep agents when you need agents that can:

- **Handle complex, multi-step tasks** that require planning and decomposition
- **Manage large amounts of context** through file system tools
- **Delegate work** to specialized subagents for context isolation
- **Persist memory** across conversations and threads

For simpler use cases, consider using LangChain’s `create_agent` or building a custom LangGraph workflow.

## ​ Core capabilities

## Planning and task decomposition

Deep agents include a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.

## Context management

File system tools (`ls`, `read_file`, `write_file`, `edit_file`) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.

## Subagent spawning

A built-in `task` tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent’s context clean while still going deep on specific subtasks.

## Long-term memory

Extend agents with persistent memory across threads using LangGraph’s Store. Agents can save and retrieve information from previous conversations.

## ​ Relationship to the LangChain ecosystem

Deep agents is built on top of:

- LangGraph \- Provides the underlying graph execution and state management
- LangChain \- Tools and model integrations work seamlessly with deep agents
- LangSmith \- Observability, evaluation, and deployment

Deep agents applications can be deployed via LangSmith Deployment and monitored with LangSmith Observability.

## ​ Get started

**Quickstart** \\
\\
Build your first deep agent **Customization** \\
\\
Learn about customization options **Middleware** \\
\\
Understand the middleware architecture **Reference** \\
\\
See the `deepagents` API reference

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Quickstart\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangChain overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Create an agent
- Core benefits

LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.

## ​ Create an agent

Copy

import * as z from "zod";
// npm install @langchain/anthropic to call the model
import { createAgent, tool } from "langchain";

const getWeather = tool(

{
name: "get_weather",
description: "Get the weather for a given city",
schema: z.object({
city: z.string(),
}),
},
);

const agent = createAgent({
model: "claude-sonnet-4-5-20250929",
tools: [getWeather],
});

console.log(
await agent.invoke({
messages: [{ role: "user", content: "What's the weather in Tokyo?" }],
})
);

See the Installation instructions and Quickstart guide to get started building your own agents and applications with LangChain.

## ​ Core benefits

**Standard model interface** \\
\\
Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\\
\\
Learn more **Easy to use, highly flexible agent** \\
\\
LangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\\
\\
Learn more **Built on top of LangGraph** \\
\\
LangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more.\\
\\
Learn more **Debug with LangSmith** \\
\\
Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\
\\
Learn more

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Install LangChain\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangGraph overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Install
- Core benefits
- LangGraph ecosystem
- Acknowledgements

Trusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.LangGraph is very low-level, and focused entirely on agent **orchestration**. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.We will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.

## ​ Install

npm

pnpm

yarn

bun

Copy

npm install @langchain/langgraph @langchain/core

Then, create a simple hello world example:

import { MessagesAnnotation, StateGraph, START, END } from "@langchain/langgraph";

return { messages: [{ role: "ai", content: "hello world" }] };
};

const graph = new StateGraph(MessagesAnnotation)
.addNode("mock_llm", mockLlm)
.addEdge(START, "mock_llm")
.addEdge("mock_llm", END)
.compile();

await graph.invoke({ messages: [{ role: "user", content: "hi!" }] });

## ​ Core benefits

LangGraph provides low-level supporting infrastructure for _any_ long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:

- Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.
- Human-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.
- Comprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.
- Debugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.
- Production-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.

## ​ LangGraph ecosystem

While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:

**LangSmith** \\
\\
Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.\\
\\
Learn more **LangGraph** \\
\\
Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.\\
\\
Learn more **LangChain** \\
\\
Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\\
\\
Learn more

## ​ Acknowledgements

LangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Install LangGraph\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Deep Agents overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

On this page

- When to use deep agents
- Core capabilities
- Relationship to the LangChain ecosystem
- Get started

`deepagents` is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents.

## ​ When to use deep agents

Use deep agents when you need agents that can:

- **Handle complex, multi-step tasks** that require planning and decomposition
- **Manage large amounts of context** through file system tools
- **Delegate work** to specialized subagents for context isolation
- **Persist memory** across conversations and threads

For simpler use cases, consider using LangChain’s `createAgent` or building a custom LangGraph workflow.

## ​ Core capabilities

## Planning and task decomposition

Deep agents include a built-in `write_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.

## Context management

File system tools (`ls`, `read_file`, `write_file`, `edit_file`) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.

## Subagent spawning

A built-in `task` tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent’s context clean while still going deep on specific subtasks.

## Long-term memory

Extend agents with persistent memory across threads using LangGraph’s Store. Agents can save and retrieve information from previous conversations.

## ​ Relationship to the LangChain ecosystem

Deep agents is built on top of:

- LangGraph \- Provides the underlying graph execution and state management
- LangChain \- Tools and model integrations work seamlessly with deep agents
- LangSmith \- Observability, evaluation, and deployment

Deep agents applications can be deployed via LangSmith Deployment and monitored with LangSmith Observability.

## ​ Get started

**Quickstart** \\
\\
Build your first deep agent **Customization** \\
\\
Learn about customization options **Middleware** \\
\\
Understand the middleware architecture

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Quickstart\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/home

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

LangSmith docs

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

**LangSmith provides tools for developing, debugging, and deploying LLM applications.**
It helps you trace requests, evaluate outputs, test prompts, and manage deployments in one place.
LangSmith is framework agnostic, so you can use it with or without LangChain’s open-source libraries
`langchain` and `langgraph`.
Prototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.

LangGraph Platform is now LangSmith Deployment. For more information, check out the Changelog.

## ​ Get started

Create an account

Sign up at smith.langchain.com (no credit card required).
You can log in with **Google**, **GitHub**, or **email**.

Create an API key

Go to your Settings page → **API Keys** → **Create API Key**.
Copy the key and save it securely.

Once your account and API key are ready, choose a quickstart to begin building with LangSmith:

**Observability** \\
\\
Gain visibility into every step your application takes to debug faster and improve reliability.\\
\\
Start tracing **Evaluation** \\
\\
Measure and track quality over time to ensure your AI applications are consistent and trustworthy.\\
\\
Evaluate your app **Deployment** \\
\\
Deploy your agents as Agent Servers, ready to scale in production.\\
\\
Deploy your agents

### ​ More ways to build

**Platform setup** \\
\\
Use LangSmith in managed cloud, in a self-hosted environment, or hybrid to match your infrastructure and compliance needs.\\
\\
Choose how to set up LangSmith **Agent Builder (Beta)** \\
\\
Design and deploy AI agents visually with a no-code interface—perfect for rapid prototyping and getting started without writing code.\\
\\
Build an agent **Studio** \\
\\
Use a visual interface to design, test, and refine applications end-to-end.\\
\\
Develop with Studio **Prompt testing** \\
\\
Iterate on prompts with built-in versioning and collaboration to ship improvements faster.\\
\\
Test your prompts

LangSmith meets the highest standards of data security and privacy with HIPAA, SOC 2 Type 2, and GDPR compliance. For more information, see the Trust Center.

## ​ Workflow

LangSmith combines observability, evaluation, deployment, and platform setup in one integrated workflow—from local development to production.!Diagram showing how LangSmith integrates observability, evaluation, deployment, and platform setup in a single workflow from development to production.!Diagram showing how LangSmith integrates observability, evaluation, deployment, and platform setup in a single workflow from development to production.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Pricing plans\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

LangSmith Observability

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

The following sections help you set up and use tracing, monitoring, and observability features:

**Set up tracing** \\
\\
Configure tracing with basic options, framework integrations, or advanced settings for full control. **View traces** \\
\\
Access and manage traces via UI or API with filtering, exporting, sharing, and comparison tools. **Monitor performance** \\
\\
Create dashboards and set alerts to track performance and get notified when issues arise. **Configure automations** \\
\\
Use rules, webhooks, and online evaluations to streamline observability workflows. **Collect feedback** \\
\\
Gather and manage annotations on outputs using queues and inline annotation. **Trace a RAG app** \\
\\
Follow a step-by-step tutorial to trace a Retrieval-Augmented Generation application from start to finish.

For terminology definitions and core concepts, refer to Observability concepts.

Use **Polly**, LangSmith’s AI assistant, to analyze traces and get AI-powered insights into your application’s performance.

To set up a LangSmith instance, visit the Platform setup section to choose between cloud, hybrid, or self-hosted. All options include observability, evaluation, prompt engineering, and deployment.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Tracing quickstart\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluation

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

LangSmith Evaluation

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

LangSmith supports two types of evaluations based on when and where they run:

## Offline Evaluation

**Test before you ship**Run evaluations on curated datasets during development to compare versions, benchmark performance, and catch regressions.

## Online Evaluation

**Monitor in production**Evaluate real user interactions in real-time to detect issues and measure quality on live traffic.

## ​ Evaluation workflow

- Offline evaluation flow

- Online evaluation flow

1

Create a dataset

Create a dataset with examples from manually curated test cases, historical production traces, or synthetic data generation.

2

Define evaluators

Create evaluators to score performance:

- Human review
- Code rules
- LLM-as-judge
- Pairwise comparison

3

Run an experiment

Execute your application on the dataset to create an experiment. Configure repetitions, concurrency, and caching to optimize runs.

4

Analyze results

Compare experiments for benchmarking, unit tests, regression tests, or backtesting.

Deploy your application

Each interaction creates a run without reference outputs.

Configure online evaluators

Set up evaluators to run automatically on production traces: safety checks, format validation, quality heuristics, and reference-free LLM-as-judge. Apply filters and sampling rates to control costs.

Monitor in real-time

Evaluators run automatically on runs or threads, providing real-time monitoring, anomaly detection, and alerting.

Establish a feedback loop

Add failing production traces to your dataset, create targeted evaluators, validate fixes with offline experiments, and redeploy.

For more on the differences between offline and online evaluation, refer to the Evaluation concepts page.

## ​ Get started

**Evaluation quickstart** \\
\\
Get started with offline evaluation. **Manage datasets** \\
\\
Create and manage datasets for evaluation through the UI or SDK. **Run offline evaluations** \\
\\
Explore evaluation types, techniques, and frameworks for comprehensive testing. **Analyze results** \\
\\
View and analyze evaluation results, compare experiments, filter data, and export findings. **Run online evaluations** \\
\\
Monitor production quality in real-time from the Observability tab. **Follow tutorials** \\
\\
Learn by following step-by-step tutorials, from simple chatbots to complex agent evaluations.

To set up a LangSmith instance, visit the Platform setup section to choose between cloud, hybrid, or self-hosted. All options include observability, evaluation, prompt engineering, and deployment.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Evaluation quickstart\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/prompt-engineering

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Prompt engineering

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

The following sections help you create, manage, and optimize your prompts:

**Review core concepts** \\
\\
Read definitions and key terminology for prompt engineering in LangSmith. **Create and update prompts** \\
\\
Build prompts via the UI or SDK, configure settings, use tools, add multimodal inputs, and connect model providers. **Manage prompts** \\
\\
Organize with tags, commit changes, trigger webhooks, and share through the public prompt hub. **Explore the prompt hub** \\
\\
Browse and manage prompt tags and discover community prompts from the LangChain Hub. **Open the prompt playground** \\
\\
Test and experiment with prompts using custom endpoints and model configurations. **Follow tutorials** \\
\\
Learn step-by-step techniques, like optimizing classifiers and advanced prompt engineering.

Use **Polly** in the Prompt Playground to optimize prompts, generate tools, and create output schemas with AI-powered assistance.

To set up a LangSmith instance, visit the Platform setup section to choose between cloud, hybrid, or self-hosted. All options include observability, evaluation, prompt engineering, and deployment.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Prompt engineering quickstart\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deployments

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

LangSmith Deployment

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

**Start here if you’re building or operating agent applications.** This section is about deploying **your application**. If you need to set up LangSmith infrastructure, the Platform setup section covers infrastructure options (cloud, hybrid, self-hosted) and setup guides for hybrid and self-hosted deployments.

This section covers how to package, build, and deploy your _agents_ and applications as Agent Servers.A typical deployment workflow consists of the following steps:

1

Test locally

Run your application on a local server.

2

Configure app for deployment

Set up dependencies, project structure, and environment configuration.

3

Choose hosting

(Required for deployment) Select Cloud, Hybrid, or Self-hosted.

4

Deploy your app

- **Cloud**: Push code from a git repository
- **Hybrid or Self-hosted with control plane**: Build and push Docker images, deploy via UI
- **Standalone servers**: Deploy directly without control plane

5

Monitor & manage

Track traces, alerts, and dashboards.

## ​ What you’ll learn

- Configure your app for deployment (dependencies, project setup, and monorepo support).
- Build, deploy, and update Agent Servers.
- Secure your deployments with authentication and access control.
- Customize your server runtime ( lifespan hooks, middleware, and routes).
- Debug, observe, and troubleshoot deployed agents using the Studio UI.

**Get started with deployment** \\
\\
Package, build, and deploy your agents and graphs to Agent Server.\\
\\
Configure your app

### ​ Related

- Agent Server
- Application structure
- Local server testing

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Run a LangGraph app locally\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Agent Builder

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

Agent Builder is in Beta.

## ​ What you can do

- Automate everyday tasks like drafting emails, summarizing updates, and organizing information.
- Connect your favorite apps to bring context into your agent’s work.
- Use in chat or where you work (e.g., Slack) to get help in the flow.
- Stay in control with simple approvals for important actions.

## ​ Start building

**Create with a template** \\
\\
Pick a ready-made starter (e.g., email assistant or team updates) and customize.\\
\\
Browse templates

## Create with AI

Describe your goal in plain English and let AI draft your agent’s configuration. Review and edit before running.

## ​ Get started

Create an agent

Start from a ready-to-use template, or describe your goal and let AI draft your agent’s instructions. You can edit details before running. Browse templates.

Connect your accounts

Securely sign in to the services you want the agent to use.

Try it out

Run the agent and iterate on its instructions in a few clicks.

## ​ Learn more

- Essentials: connections, automation, memory, approvals
- Create from a template
- Set up your workspace
- Connect apps and services and use remote connections
- Choose between workspace and private agents
- Authorize accounts when prompted
- Call agents from your app

Agent Builder is free for Plus and Enterprise users during the beta period. It will become a paid product when it reaches general availability.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Essentials\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/quickstart

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Quickstart

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Requirements
- Build a basic agent
- Build a real-world agent

This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.

**LangChain Docs MCP server**If you’re using an AI coding assistant or IDE (e.g. Claude Code or Cursor), you should install the LangChain Docs MCP server to get the most out of it. This ensures your agent has access to up-to-date LangChain documentation and examples.

## ​ Requirements

For these examples, you will need to:

- Install the LangChain package
- Set up a Claude (Anthropic) account and get an API key
- Set the `ANTHROPIC_API_KEY` environment variable in your terminal

Although these examples use Claude, you can use any supported model by changing the model name in the code and setting up the appropriate API key.

## ​ Build a basic agent

Start by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.

Copy

from langchain.agents import create_agent

"""Get weather for a given city."""
return f"It's always sunny in {city}!"

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[get_weather],
system_prompt="You are a helpful assistant",
)

# Run the agent
agent.invoke(
{"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)

To learn how to trace your agent with LangSmith, see the LangSmith documentation.

## ​ Build a real-world agent

Next, build a practical weather forecasting agent that demonstrates key production concepts:

1. **Detailed system prompts** for better agent behavior
2. **Create tools** that integrate with external data
3. **Model configuration** for consistent responses
4. **Structured output** for predictable results
5. **Conversational memory** for chat-like interactions
6. **Create and run the agent** create a fully functional agent

Let’s walk through each step:

1

Define the system prompt

The system prompt defines your agent’s role and behavior. Keep it specific and actionable:

SYSTEM_PROMPT = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location."""

2

Create tools

Tools let a model interact with external systems by calling functions you define.
Tools can depend on runtime context and also interact with agent memory.Notice below how the `get_user_location` tool uses runtime context:

from dataclasses import dataclass
from langchain.tools import tool, ToolRuntime

@tool

@dataclass
class Context:
"""Custom runtime context schema."""
user_id: str

"""Retrieve user information based on user ID."""
user_id = runtime.context.user_id
return "Florida" if user_id == "1" else "SF"

Tools should be well-documented: their name, description, and argument names become part of the model’s prompt.
LangChain’s `@tool` decorator adds metadata and enables runtime injection via the `ToolRuntime` parameter.

3

Configure your model

Set up your language model with the right parameters for your use case:

from langchain.chat_models import init_chat_model

model = init_chat_model(
"claude-sonnet-4-5-20250929",
temperature=0.5,
timeout=10,
max_tokens=1000
)

Depending on the model and provider chosen, initialization parameters may vary; refer to their reference pages for details.

4

Define response format

Optionally, define a structured response format if you need the agent responses to match
a specific schema.

from dataclasses import dataclass

# We use a dataclass here, but Pydantic models are also supported.
@dataclass
class ResponseFormat:
"""Response schema for the agent."""
# A punny response (always required)
punny_response: str
# Any interesting information about the weather if available
weather_conditions: str | None = None

5

Add memory

Add memory to your agent to maintain state across interactions. This allows
the agent to remember previous conversations and context.

from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()

In production, use a persistent checkpointer that saves to a database.
See Add and manage memory for more details.

6

Create and run the agent

Now assemble your agent with all the components and run it!

from langchain.agents.structured_output import ToolStrategy

agent = create_agent(
model=model,
system_prompt=SYSTEM_PROMPT,
tools=[get_user_location, get_weather_for_location],
context_schema=Context,
response_format=ToolStrategy(ResponseFormat),
checkpointer=checkpointer
)

# `thread_id` is a unique identifier for a given conversation.
config = {"configurable": {"thread_id": "1"}}

response = agent.invoke(
{"messages": [{"role": "user", "content": "what is the weather outside?"}]},
config=config,
context=Context(user_id="1")
)

print(response['structured_response'])
# ResponseFormat(
# punny_response="Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!",
# weather_conditions="It's always sunny in Florida!"
# )

# Note that we can continue the conversation using the same `thread_id`.
response = agent.invoke(
{"messages": [{"role": "user", "content": "thank you!"}]},
config=config,
context=Context(user_id="1")
)

# punny_response="You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!",
# weather_conditions=None

Show Full example code

from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.tools import tool, ToolRuntime
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents.structured_output import ToolStrategy

# Define system prompt

# Define context schema

# Define tools

# Configure model
model = init_chat_model(
"claude-sonnet-4-5-20250929",
temperature=0
)

# Define response format
@dataclass
class ResponseFormat:
"""Response schema for the agent."""
punny_response: str

# Set up memory

# Create agent

# Run agent

Congratulations! You now have an AI agent that can:

- **Understand context** and remember conversations
- **Use multiple tools** intelligently
- **Provide structured responses** in a consistent format
- **Handle user-specific information** through context
- **Maintain conversation state** across interactions

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Install LangChain\\
\\
Previous Changelog\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/quickstart

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Quickstart

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- 1\. Define tools and model
- 2\. Define state
- 3\. Define model node
- 4\. Define tool node
- 5\. Define end logic
- 6\. Build and compile the agent

This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.

- Use the Graph API if you prefer to define your agent as a graph of nodes and edges.
- Use the Functional API if you prefer to define your agent as a single function.

For conceptual information, see Graph API overview and Functional API overview.

For this example, you will need to set up a Claude (Anthropic) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.

- Use the Graph API

- Use the Functional API

## ​ 1\. Define tools and model

In this example, we’ll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.

Copy

from langchain.tools import tool
from langchain.chat_models import init_chat_model

model = init_chat_model(
"claude-sonnet-4-5-20250929",
temperature=0
)

# Define tools
@tool

"""Multiply `a` and `b`.

Args:
a: First int
b: Second int
"""
return a * b

"""Adds `a` and `b`.

Args:
a: First int
b: Second int
"""
return a + b

"""Divide `a` and `b`.

Args:
a: First int
b: Second int
"""
return a / b

# Augment the LLM with tools
tools = [add, multiply, divide]
tools_by_name = {tool.name: tool for tool in tools}
model_with_tools = model.bind_tools(tools)

## ​ 2\. Define state

The graph’s state is used to store the messages and the number of LLM calls.

State in LangGraph persists throughout the agent’s execution.The `Annotated` type with `operator.add` ensures that new messages are appended to the existing list rather than replacing it.

from langchain.messages import AnyMessage
from typing_extensions import TypedDict, Annotated
import operator

class MessagesState(TypedDict):
messages: Annotated[list[AnyMessage], operator.add]
llm_calls: int

## ​ 3\. Define model node

The model node is used to call the LLM and decide whether to call a tool or not.

from langchain.messages import SystemMessage

def llm_call(state: dict):
"""LLM decides whether to call a tool or not"""

return {
"messages": [\
model_with_tools.invoke(\
[\
SystemMessage(\
content="You are a helpful assistant tasked with performing arithmetic on a set of inputs."\
)\
]\
+ state["messages"]\
)\
],
"llm_calls": state.get('llm_calls', 0) + 1
}

## ​ 4\. Define tool node

The tool node is used to call the tools and return the results.

from langchain.messages import ToolMessage

def tool_node(state: dict):
"""Performs the tool call"""

result = []
for tool_call in state["messages"][-1].tool_calls:
tool = tools_by_name[tool_call["name"]]
observation = tool.invoke(tool_call["args"])
result.append(ToolMessage(content=observation, tool_call_id=tool_call["id"]))
return {"messages": result}

## ​ 5\. Define end logic

The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.

from typing import Literal
from langgraph.graph import StateGraph, START, END

"""Decide if we should continue the loop or stop based upon whether the LLM made a tool call"""

messages = state["messages"]
last_message = messages[-1]

# If the LLM makes a tool call, then perform an action
if last_message.tool_calls:
return "tool_node"

# Otherwise, we stop (reply to the user)
return END

## ​ 6\. Build and compile the agent

The agent is built using the `StateGraph` class and compiled using the `compile` method.

# Build workflow
agent_builder = StateGraph(MessagesState)

# Add nodes
agent_builder.add_node("llm_call", llm_call)
agent_builder.add_node("tool_node", tool_node)

# Add edges to connect nodes
agent_builder.add_edge(START, "llm_call")
agent_builder.add_conditional_edges(
"llm_call",
should_continue,
["tool_node", END]
)
agent_builder.add_edge("tool_node", "llm_call")

# Compile the agent
agent = agent_builder.compile()

# Show the agent
from IPython.display import Image, display
display(Image(agent.get_graph(xray=True).draw_mermaid_png()))

# Invoke
from langchain.messages import HumanMessage
messages = [HumanMessage(content="Add 3 and 4.")]
messages = agent.invoke({"messages": messages})
for m in messages["messages"]:
m.pretty_print()

To learn how to trace your agent with LangSmith, see the LangSmith documentation.

Congratulations! You’ve built your first agent using the LangGraph Graph API.

Full code example

# Step 1: Define tools and model

# Step 2: Define state

# Step 3: Define model node

# Step 4: Define tool node

# Step 5: Define logic to determine whether to end

# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call

# Step 6: Build agent

from IPython.display import Image, display
display(Image(agent.get_graph(xray=True).draw_mermaid_png()))

from langgraph.graph import add_messages
from langchain.messages import (
SystemMessage,
HumanMessage,
ToolCall,
)
from langchain_core.messages import BaseMessage
from langgraph.func import entrypoint, task

## ​ 2\. Define model node

The `@task` decorator marks a function as a task that can be executed as part of the agent. Tasks can be called synchronously or asynchronously within your entrypoint function.

@task
def call_llm(messages: list[BaseMessage]):
"""LLM decides whether to call a tool or not"""
return model_with_tools.invoke(
[\
SystemMessage(\
content="You are a helpful assistant tasked with performing arithmetic on a set of inputs."\
)\
]
+ messages
)

## ​ 3\. Define tool node

@task
def call_tool(tool_call: ToolCall):
"""Performs the tool call"""
tool = tools_by_name[tool_call["name"]]
return tool.invoke(tool_call)

## ​ 4\. Define agent

The agent is built using the `@entrypoint` function.

In the Functional API, instead of defining nodes and edges explicitly, you write standard control flow logic (loops, conditionals) within a single function.

@entrypoint()
def agent(messages: list[BaseMessage]):
model_response = call_llm(messages).result()

while True:
if not model_response.tool_calls:
break

# Execute tools
tool_result_futures = [\
call_tool(tool_call) for tool_call in model_response.tool_calls\
]
tool_results = [fut.result() for fut in tool_result_futures]
messages = add_messages(messages, [model_response, *tool_results])
model_response = call_llm(messages).result()

messages = add_messages(messages, model_response)
return messages

messages = [HumanMessage(content="Add 3 and 4.")]
for chunk in agent.stream(messages, stream_mode="updates"):
print(chunk)
print("\n")

Congratulations! You’ve built your first agent using the LangGraph Functional API.

# Step 2: Define model node

# Step 3: Define tool node

# Step 4: Define agent

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Install LangGraph\\
\\
Previous Run a local server\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/use-these-docs

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Use docs programmatically

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Quick access options
- Use our MCP server
- Connect with Claude Code
- Connect with Claude Desktop
- Connect with Codex CLI
- Connect with Cursor or VS Code
- Connect with Antigravity
- Learn more

We want to make our documentation as accessible as possible. We’ve included several ways for you to use these docs programmatically through AI assistants, code editors, and direct integrations, such as Model Context Protocol (MCP).

## ​ Quick access options

On any page in our documentation, you’ll find a contextual menu dropdown in the top right corner:!Copy page light mode!Copy page dark modeThis includes our `llms.txt`, MCP server connection, and other quick access options such as ChatGPT and Claude.

## ​ Use our MCP server

Our documentation includes a built-in **Model Context Protocol (MCP) server** that lets AI applications query the latest docs in real-time.The LangChain docs MCP server is available at:

Copy

Once connected, you can ask your AI assistant questions about LangChain, LangGraph, and LangSmith, and it will search our documentation to provide accurate, current answers.

### ​ Connect with Claude Code

If you’re using Claude Code, run this command in your terminal to add the server to your current project:

claude mcp add --transport http docs-langchain

**Project (local) scoped**The command above adds the MCP server only to your current project/working directory. To add the MCP server globally and access it in all projects, add the user scope by adding `--scope user` to the command:

claude mcp add --transport http docs-langchain --scope user

### ​ Connect with Claude Desktop

1. Open Claude Desktop

3. Add our MCP server URL: `https://docs.langchain.com/mcp`

### ​ Connect with Codex CLI

If you’re using OpenAI Codex CLI, run this command in your terminal to add the server globally:

codex mcp add langchain-docs --url

### ​ Connect with Cursor or VS Code

Add the following to your MCP settings configuration file:

{
"mcpServers": {
"docs-langchain": {
"url": "https://docs.langchain.com/mcp"
}
}
}

### ​ Connect with Antigravity

{
"mcpServers": {
"docs-langchain": {
"serverUrl": "https://docs.langchain.com/mcp"
}
}
}

## ​ Learn more

For more information about using Mintlify’s MCP servers, see the official Mintlify documentation.Have questions or feedback? Let us know in our community forum.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Human-in-the-loop Interrupt concurrent Configure LangSmith Agent Server for scale

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK LangGraph overview LangSmith reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Deep Agents overview Quickstart Deep Agents Middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph overview LangGraph CLI

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Deep Agents overview Deep Agents Middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/home)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith docs LangSmith Deployment Self-hosted LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluation)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Evaluation LangSmith docs Implement a CI/CD pipeline using LangSmith Deployment and Evaluation

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/prompt-engineering)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Prompt engineering quickstart Prompt engineering Prompt engineering concepts

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deployments)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment App development in LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Agent Builder App for Slack LangSmith Tool Server Configure LangSmith Agent Server for scale

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/quickstart)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Quickstart Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/quickstart)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK Tracing quickstart LangSmith reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/use-these-docs)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith docs Build a custom RAG agent with LangGraph Build a RAG agent with LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangChain integrations packages

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

LangChain offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more.

**Chat models** **Embedding models** **Tools and toolkits**

To see a full list of integrations by component type, refer to the categories in the sidebar.

## ​ Popular providers

| Provider | Package | Downloads | Latest version | JS/TS support |
| --- | --- | --- | --- | --- |
| OpenAI | `langchain-openai` | ![Downloads per month](https://pypi.org/project/langchain-openai/) | ![PyPI - Latest version](https://pypi.org/project/langchain-openai/) | ✅ |
| Google (Vertex AI) | `langchain-google-vertexai` | ![Downloads per month](https://pypi.org/project/langchain-google-vertexai/) | ![PyPI - Latest version](https://pypi.org/project/langchain-google-vertexai/) | ✅ |
| Anthropic (Claude) | `langchain-anthropic` | ![Downloads per month](https://pypi.org/project/langchain-anthropic/) | ![PyPI - Latest version](https://pypi.org/project/langchain-anthropic/) | ✅ |
| AWS | `langchain-aws` | ![Downloads per month](https://pypi.org/project/langchain-aws/) | ![PyPI - Latest version](https://pypi.org/project/langchain-aws/) | ✅ |
| Google (GenAI) | `langchain-google-genai` | ![Downloads per month](https://pypi.org/project/langchain-google-genai/) | ![PyPI - Latest version](https://pypi.org/project/langchain-google-genai/) | ✅ |
| Ollama | `langchain-ollama` | ![Downloads per month](https://pypi.org/project/langchain-ollama/) | ![PyPI - Latest version](https://pypi.org/project/langchain-ollama/) | ✅ |
| Chroma | `langchain-chroma` | ![Downloads per month](https://pypi.org/project/langchain-chroma/) | ![PyPI - Latest version](https://pypi.org/project/langchain-chroma/) | ✅ |
| Groq | `langchain-groq` | ![Downloads per month](https://pypi.org/project/langchain-groq/) | ![PyPI - Latest version](https://pypi.org/project/langchain-groq/) | ✅ |
| Huggingface | `langchain-huggingface` | ![Downloads per month](https://pypi.org/project/langchain-huggingface/) | ![PyPI - Latest version](https://pypi.org/project/langchain-huggingface/) | ✅ |
| Pinecone | `langchain-pinecone` | ![Downloads per month](https://pypi.org/project/langchain-pinecone/) | ![PyPI - Latest version](https://pypi.org/project/langchain-pinecone/) | ✅ |
| Postgres | `langchain-postgres` | ![Downloads per month](https://pypi.org/project/langchain-postgres/) | ![PyPI - Latest version](https://pypi.org/project/langchain-postgres/) | ✅ |
| Cohere | `langchain-cohere` | ![Downloads per month](https://pypi.org/project/langchain-cohere/) | ![PyPI - Latest version](https://pypi.org/project/langchain-cohere/) | ✅ |
| MistralAI | `langchain-mistralai` | ![Downloads per month](https://pypi.org/project/langchain-mistralai/) | ![PyPI - Latest version](https://pypi.org/project/langchain-mistralai/) | ✅ |
| Fireworks | `langchain-fireworks` | ![Downloads per month](https://pypi.org/project/langchain-fireworks/) | ![PyPI - Latest version](https://pypi.org/project/langchain-fireworks/) | ✅ |
| Databricks | `databricks-langchain` | ![Downloads per month](https://pypi.org/project/databricks-langchain/) | ![PyPI - Latest version](https://pypi.org/project/databricks-langchain/) | ✅ |
| Deepseek | `langchain-deepseek` | ![Downloads per month](https://pypi.org/project/langchain-deepseek/) | ![PyPI - Latest version](https://pypi.org/project/langchain-deepseek/) | ✅ |
| MongoDB | `langchain-mongodb` | ![Downloads per month](https://pypi.org/project/langchain-mongodb/) | ![PyPI - Latest version](https://pypi.org/project/langchain-mongodb/) | ✅ |
| xAI (Grok) | `langchain-xai` | ![Downloads per month](https://pypi.org/project/langchain-xai/) | ![PyPI - Latest version](https://pypi.org/project/langchain-xai/) | ✅ |
| IBM | `langchain-ibm` | ![Downloads per month](https://pypi.org/project/langchain-ibm/) | ![PyPI - Latest version](https://pypi.org/project/langchain-ibm/) | ✅ |
| Perplexity | `langchain-perplexity` | ![Downloads per month](https://pypi.org/project/langchain-perplexity/) | ![PyPI - Latest version](https://pypi.org/project/langchain-perplexity/) | ✅ |
| Nvidia AI Endpoints | `langchain-nvidia-ai-endpoints` | ![Downloads per month](https://pypi.org/project/langchain-nvidia-ai-endpoints/) | ![PyPI - Latest version](https://pypi.org/project/langchain-nvidia-ai-endpoints/) | ❌ |
| Qdrant | `langchain-qdrant` | ![Downloads per month](https://pypi.org/project/langchain-qdrant/) | ![PyPI - Latest version](https://pypi.org/project/langchain-qdrant/) | ✅ |
| Tavily | `langchain-tavily` | ![Downloads per month](https://pypi.org/project/langchain-tavily/) | ![PyPI - Latest version](https://pypi.org/project/langchain-tavily/) | ✅ |
| Milvus | `langchain-milvus` | ![Downloads per month](https://pypi.org/project/langchain-milvus/) | ![PyPI - Latest version](https://pypi.org/project/langchain-milvus/) | ✅ |
| Elasticsearch | `langchain-elasticsearch` | ![Downloads per month](https://pypi.org/project/langchain-elasticsearch/) | ![PyPI - Latest version](https://pypi.org/project/langchain-elasticsearch/) | ✅ |
| Azure AI | `langchain-azure-ai` | ![Downloads per month](https://pypi.org/project/langchain-azure-ai/) | ![PyPI - Latest version](https://pypi.org/project/langchain-azure-ai/) | ✅ |
| LiteLLM | `langchain-litellm` | ![Downloads per month](https://pypi.org/project/langchain-litellm/) | ![PyPI - Latest version](https://pypi.org/project/langchain-litellm/) | N/A |
| DataStax Astra DB | `langchain-astradb` | ![Downloads per month](https://pypi.org/project/langchain-astradb/) | ![PyPI - Latest version](https://pypi.org/project/langchain-astradb/) | ✅ |
| Redis | `langchain-redis` | ![Downloads per month](https://pypi.org/project/langchain-redis/) | ![PyPI - Latest version](https://pypi.org/project/langchain-redis/) | ✅ |
| Together | `langchain-together` | ![Downloads per month](https://pypi.org/project/langchain-together/) | ![PyPI - Latest version](https://pypi.org/project/langchain-together/) | ✅ |
| MCP Toolbox (Google) | `toolbox-langchain` | ![Downloads per month](https://pypi.org/project/toolbox-langchain/) | ![PyPI - Latest version](https://pypi.org/project/toolbox-langchain/) | ❌ |
| Google (Community) | `langchain-google-community` | ![Downloads per month](https://pypi.org/project/langchain-google-community/) | ![PyPI - Latest version](https://pypi.org/project/langchain-google-community/) | ❌ |
| Unstructured | `langchain-unstructured` | ![Downloads per month](https://pypi.org/project/langchain-unstructured/) | ![PyPI - Latest version](https://pypi.org/project/langchain-unstructured/) | ✅ |
| Neo4J | `langchain-neo4j` | ![Downloads per month](https://pypi.org/project/langchain-neo4j/) | ![PyPI - Latest version](https://pypi.org/project/langchain-neo4j/) | ✅ |
| Graph RAG | `langchain-graph-retriever` | ![Downloads per month](https://pypi.org/project/langchain-graph-retriever/) | ![PyPI - Latest version](https://pypi.org/project/langchain-graph-retriever/) | ❌ |
| Sambanova | `langchain-sambanova` | ![Downloads per month](https://pypi.org/project/langchain-sambanova/) | ![PyPI - Latest version](https://pypi.org/project/langchain-sambanova/) | ❌ |

## ​ All providers

See all providers or search for a provider using the search field.Community integrations can be found in `langchain-community`.

If you’d like to contribute an integration, see the contributing guide.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

All integration providers\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/learn

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Learn

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Use Cases
- LangChain
- LangGraph
- Multi-agent
- Conceptual Overviews
- Additional Resources

In the **Learn** section of the documentation, you’ll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph.

## ​ Use Cases

Below are tutorials for common use cases, organized by framework.

### ​ LangChain

LangChain agent implementations make it easy to get started for most use cases. **Semantic Search** \\
\\
Build a semantic search engine over a PDF with LangChain components. **RAG Agent** \\
\\
Create a Retrieval Augmented Generation (RAG) agent. **SQL Agent** \\
\\
Build a SQL agent to interact with databases with human-in-the-loop review. **Voice Agent** \\
\\
Build an agent you can speak and listen to.

### ​ LangGraph

LangChain’s agent implementations use LangGraph primitives.
If deeper customization is required, agents can be implemented directly in LangGraph. **Custom RAG Agent** \\
\\
Build a RAG agent using LangGraph primitives for fine-grained control. **Custom SQL Agent** \\
\\
Implement a SQL agent directly in LangGraph for maximum flexibility.

### ​ Multi-agent

These tutorials demonstrate multi-agent patterns, blending LangChain agents with LangGraph workflows. **Subagents: Personal assistant** \\
\\
Build a personal assistant that delegates to sub-agents. **Handoffs: Customer support** \\
\\
Build a customer support workflow where a single agent transitions between different states. **Router: Knowledge base** \\
\\
Build a multi-source knowledge base that routes queries to specialized agents. **Skills: SQL assistant** \\
\\
Build an agent that loads specialized skills progressively using on-demand context loading.

## ​ Conceptual Overviews

These guides explain the core concepts and APIs underlying LangChain and LangGraph. **Memory** \\
\\
Understand persistence of interactions within and across threads. **Context engineering** \\
\\
Learn methods for providing AI applications the right information and tools to accomplish a task. **Graph API** \\
\\
Explore LangGraph’s declarative graph-building API. **Functional API** \\
\\
Build agents as a single function.

## ​ Additional Resources

**LangChain Academy** \\
\\
Courses and exercises to level up your LangChain skills. **Case Studies** \\
\\
See how teams are using LangChain and LangGraph in production.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a semantic search engine with LangChain\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/reference/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Reference

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

On this page

- Reference sites

Comprehensive API reference documentation for the LangChain and LangGraph Python and TypeScript libraries.

## ​ Reference sites

**LangChain** \\
\\
Complete API reference for LangChain Python, including chat models, tools, agents, and more. **LangGraph** \\
\\
Complete API reference for LangGraph Python, including graph APIs, state management, checkpointing, and more. **LangChain Integrations** \\
\\
LangChain packages to connect with popular LLM providers, vector stores, tools, and other services. **MCP Adapter** \\
\\
Use Model Context Protocol (MCP) tools within LangChain and LangGraph applications. **Deep Agents** \\
\\
Build agents that can plan, use subagents, and leverage file systems for complex tasks

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangChain SDK\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/contributing/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Contributing

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Contribute

- Documentation
- Code
- Integrations

On this page

- Ways to Contribute
- Acceptable uses of LLMs

**Welcome! Thank you for your interest in contributing.**LangChain has helped form the largest developer community in generative AI, and we’re always open to new contributors. Whether you’re fixing bugs, adding features, improving documentation, or sharing feedback, your involvement helps make LangChain and LangGraph better for everyone 🦜❤️

## ​ Ways to Contribute

Report bugs

Found a bug? Please help us fix it by following these steps:

1

Search

Check if the issue already exists in our GitHub Issues for the respective repo:

**LangChain** \\
\\
Issues **LangGraph** \\
\\
Issues

2

Create issue

If no issue exists, create a new one. When writing, be sure to follow the template provided and to include a minimal, reproducible, example. Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.

3

Wait

A project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.

If you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please link them rather than combining them. For example,

Copy

This issue is blocked by #123 and related to #456.

Suggest features

Have an idea for a new feature or enhancement?

Search the issues for the respective repository for existing feature requests:

Discuss

If no requests exist, start a new discussion under the relevant category so that project maintainers and the community can provide feedback.

Describe

Be sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.

Improve documentation

Documentation improvements are always welcome! We strive to keep our docs clear and comprehensive, and your perspective can make a big difference. **How to propose changes to the documentation** \\
\\
Guide

Contribute code

With a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help! **How to make your first Pull Request** \\
\\
Guide If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work.If you are looking for something to work on, check out the issues labeled “good first issue” or “help wanted” in our repos:

**LangChain** \\
\\
Labels **LangGraph** \\
\\
Labels

Add a new integration

**LangChain** \\
\\
Guide to adding a new LangChain integration

## ​ Acceptable uses of LLMs

Generative AI can be a useful tool for contributors, but like any tool should be used with critical thinking and good judgement.We struggle when contributors’ entire work (code changes, documentation update, pull request descriptions) are LLM-generated. These drive-by contributions often mean well but often miss the mark in terms of contextual relevance, accuracy, and quality.We will close those pull requests and issues that are unproductive, so we can focus our maintainer capacity elsewhere.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Contributing to documentation\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/install

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Install LangChain

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

To install the LangChain package:

pip

uv

Copy

pip install -U langchain
# Requires Python 3.10+

LangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages.

# Installing the OpenAI integration
pip install -U langchain-openai

# Installing the Anthropic integration
pip install -U langchain-anthropic

See the Integrations tab for a full list of available integrations.

Now that you have LangChain installed, you can get started by following the Quickstart guide.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangChain overview\\
\\
Previous Quickstart\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/releases/changelog

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Releases

Changelog

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

FiltersClear

langchainintegrationslanggraph

**Subscribe**: Our changelog includes an RSS feed that can integrate with Slack, email, Discord bots like Readybot or RSS Feeds to Discord Bot, and other subscription tools.

​

Dec 15, 2025

langchainintegrations

## ​ `langchain` v1.2.0

- `create_agent`: Simplified support for provider-specific tool parameters and definitions via a new `extras` attribute on tools. Examples:

- Provider-specific configuration such as Anthropic’s programmatic tool calling and tool search.
- Built-in tools that are executed client-side, as supported by Anthropic, OpenAI, and other providers.
- Support for strict schema-adherence in agent `response_format` (see `ProviderStrategy` docs).

Dec 8, 2025

## ​ `langchain-google-genai` v4.0.0

We’ve re-written the Google GenAI integration to use Google’s consolidated Generative AI SDK, which provides access to the Gemini API and Vertex AI Platform under the same interface. This includes minimal breaking changes as well as deprecated packages in `langchain-google-vertexai`.See the full release notes and migration guide for details.

Nov 25, 2025

langchain

## ​ `langchain` v1.1.0

- Model profiles: Chat models now expose supported features and capabilities through a `.profile` attribute. These data are derived from models.dev, an open source project providing model capability data.
- Summarization middleware: Updated to support flexible trigger points using model profiles for context-aware summarization.
- Structured output: `ProviderStrategy` support (native structured output) can now be inferred from model profiles.
- `SystemMessage` for `create_agent`: Support for passing `SystemMessage` instances directly to `create_agent`’s `system_prompt` parameter, enabling advanced features like cache control and structured content blocks.
- Model retry middleware: New middleware for automatically retrying failed model calls with configurable exponential backoff.
- Content moderation middleware: OpenAI content moderation middleware for detecting and handling unsafe content in agent interactions. Supports checking user input, model output, and tool results.

Oct 20, 2025

langchainlanggraph

## ​ v1.0.0

### ​ `langchain`

- Release notes
- Migration guide

### ​ `langgraph`

If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content and API reference.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Versioning\\
\\
Previous What's new in LangChain v1\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/philosophy

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Philosophy

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

LangChain is driven by a few core beliefs:

- Large Language Models (LLMs) are great, powerful new technology.
- LLMs are even better when you combine them with external sources of data.
- LLMs will transform what the applications of the future look like. Specifically, the applications of the future will look more and more agentic.
- It is still very early on in that transformation.
- While it’s easy to build a prototype of those agentic applications, it’s still really hard to build agents that are reliable enough to put into production.

With LangChain, we have two core focuses:

1

We want to enable developers to build with the best models.

Different providers expose different APIs, with different model parameters and different message formats.
Standardizing these model inputs and outputs is a core focus, making it easy for developer to easily change to the most recent state-of-the-art model, avoiding lock-in.

2

We want to make it easy to use models to orchestrate more complex flows that interact with other data and computation.

Models should be used for more than just _text generation_ \- they should also be used to orchestrate more complex flows that interact with other data. LangChain makes it easy to define tools that LLMs can use dynamically, as well as help with parsing of and access to unstructured data.

## ​ History

Given the constant rate of change in the field, LangChain has also evolved over time. Below is a brief timeline of how LangChain has changed over the years, evolving alongside what it means to build with LLMs:

​

2022-10-24

v0.0.1

A month before ChatGPT, **LangChain was launched as a Python package**. It consisted of two main components:

- LLM abstractions
- “Chains”, or predetermined steps of computation to run, for common use cases. For example - RAG: run a retrieval step, then run a generation step.

The name LangChain comes from “Language” (like Language models) and “Chains”.

2022-12

The first general purpose agents were added to LangChain.These general purpose agents were based on the ReAct paper (ReAct standing for Reasoning and Acting). They used LLMs to generate JSON that represented tool calls, and then parsed that JSON to determine what tools to call.

2023-01

OpenAI releases a ‘Chat Completion’ API.Previously, models took in strings and returned a string. In the ChatCompletions API, they evolved to take in a list of messages and return a message. Other model providers followed suit, and LangChain updated to work with lists of messages.

LangChain releases a JavaScript version.LLMs and agents will change how applications are built and JavaScript is the language of application developers.

2023-02

**LangChain Inc. was formed as a company** around the open source LangChain project.The main goal was to “make intelligent agents ubiquitous”. The team recognized that while LangChain was a key part (LangChain made it simple to get started with LLMs), there was also a need for other components.

2023-03

OpenAI releases ‘function calling’ in their API.This allowed the API to explicitly generate payloads that represented tool calls. Other model providers followed suit, and LangChain was updated to use this as the preferred method for tool calling (rather than parsing JSON).

2023-06

**LangSmith is released** as closed source platform by LangChain Inc., providing observability and evalsThe main issue with building agents is getting them to be reliable, and LangSmith, which provides observability and evals, was built to solve that need. LangChain was updated to integrate seamlessly with LangSmith.

2024-01

v0.1.0

**LangChain releases 0.1.0**, its first non-0.0.x.The industry matured from prototypes to production, and as such, LangChain increased its focus on stability.

2024-02

**LangGraph is released** as an open-source library.The original LangChain had two focuses: LLM abstractions, and high-level interfaces for getting started with common applications; however, it was missing a low-level orchestration layer that allowed developers to control the exact flow of their agent. Enter: LangGraph.When building LangGraph, we learned from lessons when building LangChain and added functionality we discovered was needed: streaming, durable execution, short-term memory, human-in-the-loop, and more.

2024-06

**LangChain has over 700 integrations.**Integrations were split out of the core LangChain package, and either moved into their own standalone packages (for the core integrations) or `langchain-community`.

2024-10

LangGraph becomes the preferred way to build any AI application that is more than a single LLM call.As developers tried to improve the reliability of their applications, they needed more control than the high-level interfaces provided. LangGraph provided that low-level flexibility. Most chains and agents were marked as deprecated in LangChain with guides on how to migrate them to LangGraph. There is still one high-level abstraction created in LangGraph: an agent abstraction. It is built on top of low-level LangGraph and has the same interface as the ReAct agents from LangChain.

2025-04

Model APIs become more multimodal.Models started to accept files, images, videos, and more. We updated the `langchain-core` message format accordingly to allow developers to specify these multimodal inputs in a standard way.

2025-10-20

v1.0.0

**LangChain releases 1.0** with two major changes:

1. Complete revamp of all chains and agents in `langchain`. All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain.For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the `langchain-classic` package.
2. A standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Changelog\\
\\
Previous Agents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/agents

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Agents

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Core components
- Model
- Static model
- Dynamic model
- Tools
- Defining tools
- Tool error handling
- Tool use in the ReAct loop
- System prompt
- Dynamic system prompt
- Invocation
- Advanced concepts
- Structured output
- ToolStrategy
- ProviderStrategy
- Memory
- Defining state via middleware
- Defining state via state\_schema
- Streaming
- Middleware

Agents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.`create_agent` provides a production-ready agent implementation.An LLM Agent runs tools in a loop to achieve a goal.
An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.

action

observation

finish

input

model

tools

output

`create_agent` builds a **graph**-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.

## ​ Core components

### ​ Model

The model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.

#### ​ Static model

Static models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.To initialize a static model from a model identifier string:

Copy

from langchain.agents import create_agent

agent = create_agent("gpt-5", tools=tools)

Model identifier strings support automatic inference (e.g., `"gpt-5"` will be inferred as `"openai:gpt-5"`). Refer to the reference to see a full list of model identifier string mappings.

For more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use `ChatOpenAI`. See Chat models for other available chat model classes.

from langchain.agents import create_agent
from langchain_openai import ChatOpenAI

model = ChatOpenAI(
model="gpt-5",
temperature=0.1,
max_tokens=1000,
timeout=30
# ... (other params)
)
agent = create_agent(model, tools=tools)

Model instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, `base_url`, and other provider-specific settings. Refer to the reference to see available params and methods on your model.

#### ​ Dynamic model

Dynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.To use a dynamic model, create middleware using the `@wrap_model_call` decorator that modifies the model in the request:

from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse

basic_model = ChatOpenAI(model="gpt-4o-mini")
advanced_model = ChatOpenAI(model="gpt-4o")

@wrap_model_call

"""Choose model based on conversation complexity."""
message_count = len(request.state["messages"])

# Use an advanced model for longer conversations
model = advanced_model
else:
model = basic_model

return handler(request.override(model=model))

agent = create_agent(
model=basic_model, # Default model
tools=tools,
middleware=[dynamic_model_selection]
)

Pre-bound models (models with `bind_tools` already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.

For model configuration details, see Models. For dynamic model selection patterns, see Dynamic model in middleware.

### ​ Tools

Tools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:

- Multiple tool calls in sequence (triggered by a single prompt)
- Parallel tool calls when appropriate
- Dynamic tool selection based on previous results
- Tool retry logic and error handling
- State persistence across tool calls

For more information, see Tools.

#### ​ Defining tools

Pass a list of tools to the agent.

Tools can be specified as plain Python functions or coroutines.The tool decorator can be used to customize tool names, descriptions, argument schemas, and other properties.

from langchain.tools import tool
from langchain.agents import create_agent

@tool

"""Search for information."""
return f"Results for: {query}"

"""Get weather information for a location."""
return f"Weather in {location}: Sunny, 72°F"

agent = create_agent(model, tools=[search, get_weather])

If an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.

#### ​ Tool error handling

To customize how tool errors are handled, use the `@wrap_tool_call` decorator to create middleware:

from langchain.agents import create_agent
from langchain.agents.middleware import wrap_tool_call
from langchain.messages import ToolMessage

@wrap_tool_call
def handle_tool_errors(request, handler):
"""Handle tool execution errors with custom messages."""
try:
return handler(request)
except Exception as e:
# Return a custom error message to the model
return ToolMessage(
content=f"Tool error: Please check your input and try again. ({str(e)})",
tool_call_id=request.tool_call["id"]
)

agent = create_agent(
model="gpt-4o",
tools=[search, get_weather],
middleware=[handle_tool_errors]
)

The agent will return a `ToolMessage` with the custom error message when a tool fails:

[\
...\
ToolMessage(\
content="Tool error: Please check your input and try again. (division by zero)",\
tool_call_id="..."\
),\
...\
]

#### ​ Tool use in the ReAct loop

Agents follow the ReAct (“Reasoning + Acting”) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.

Example of ReAct loop

**Prompt:** Identify the current most popular wireless headphones and verify availability.

================================ Human Message =================================

Find the most popular wireless headphones right now and check if they're in stock

- **Reasoning**: “Popularity is time-sensitive, I need to use the provided search tool.”
- **Acting**: Call `search_products("wireless headphones")`

================================== Ai Message ==================================
Tool Calls:
search_products (call_abc123)
Call ID: call_abc123
Args:
query: wireless headphones

================================= Tool Message =================================

Found 5 products matching "wireless headphones". Top 5 results: WH-1000XM5, ...

- **Reasoning**: “I need to confirm availability for the top-ranked item before answering.”
- **Acting**: Call `check_inventory("WH-1000XM5")`

================================== Ai Message ==================================
Tool Calls:
check_inventory (call_def456)
Call ID: call_def456
Args:
product_id: WH-1000XM5

Product WH-1000XM5: 10 units in stock

- **Reasoning**: “I have the most popular model and its stock status. I can now answer the user’s question.”
- **Acting**: Produce final answer

================================== Ai Message ==================================

I found wireless headphones (model WH-1000XM5) with 10 units in stock...

To learn more about tools, see Tools.

### ​ System prompt

You can shape how your agent approaches tasks by providing a prompt. The `system_prompt` parameter can be provided as a string:

agent = create_agent(
model,
tools,
system_prompt="You are a helpful assistant. Be concise and accurate."
)

When no `system_prompt` is provided, the agent will infer its task from the messages directly.The `system_prompt` parameter accepts either a `str` or a `SystemMessage`. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like Anthropic’s prompt caching:

from langchain.agents import create_agent
from langchain.messages import SystemMessage, HumanMessage

literary_agent = create_agent(
model="anthropic:claude-sonnet-4-5",
system_prompt=SystemMessage(
content=[\
{\
"type": "text",\
"text": "You are an AI assistant tasked with analyzing literary works.",\
},\
{\
"type": "text",\

"cache_control": {"type": "ephemeral"}\
}\
]
)
)

result = literary_agent.invoke(
{"messages": [HumanMessage("Analyze the major themes in 'Pride and Prejudice'.")]}
)

The `cache_control` field with `{"type": "ephemeral"}` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.

#### ​ Dynamic system prompt

For more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.The `@dynamic_prompt` decorator creates middleware that generates system prompts based on the model request:

from typing import TypedDict

from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest

class Context(TypedDict):
user_role: str

@dynamic_prompt

"""Generate system prompt based on user role."""
user_role = request.runtime.context.get("user_role", "user")
base_prompt = "You are a helpful assistant."

if user_role == "expert":
return f"{base_prompt} Provide detailed technical responses."
elif user_role == "beginner":
return f"{base_prompt} Explain concepts simply and avoid jargon."

return base_prompt

agent = create_agent(
model="gpt-4o",
tools=[web_search],
middleware=[user_role_prompt],
context_schema=Context
)

# The system prompt will be set dynamically based on context
result = agent.invoke(
{"messages": [{"role": "user", "content": "Explain machine learning"}]},
context={"user_role": "expert"}
)

For more details on message types and formatting, see Messages. For comprehensive middleware documentation, see Middleware.

## ​ Invocation

You can invoke an agent by passing an update to its `State`. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:

result = agent.invoke(
{"messages": [{"role": "user", "content": "What's the weather in San Francisco?"}]}
)

For streaming steps and / or tokens from the agent, refer to the streaming guide.Otherwise, the agent follows the LangGraph Graph API and supports all associated methods, such as `stream` and `invoke`.

## ​ Advanced concepts

### ​ Structured output

In some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the `response_format` parameter.

#### ​ ToolStrategy

`ToolStrategy` uses artificial tool calling to generate structured output. This works with any model that supports tool calling:

from pydantic import BaseModel
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy

class ContactInfo(BaseModel):
name: str
email: str
phone: str

agent = create_agent(
model="gpt-4o-mini",
tools=[search_tool],
response_format=ToolStrategy(ContactInfo)
)

result = agent.invoke({
"messages": [{"role": "user", "content": "Extract contact info from: John Doe, john@example.com, (555) 123-4567"}]
})

result["structured_response"]
# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')

#### ​ ProviderStrategy

`ProviderStrategy` uses the model provider’s native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):

from langchain.agents.structured_output import ProviderStrategy

agent = create_agent(
model="gpt-4o",
response_format=ProviderStrategy(ContactInfo)
)

As of `langchain 1.0`, simply passing a schema (e.g., `response_format=ContactInfo`) is no longer supported. You must explicitly use `ToolStrategy` or `ProviderStrategy`.

To learn about structured output, see Structured output.

### ​ Memory

Agents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.Information stored in the state can be thought of as the short-term memory of the agent:Custom state schemas must extend `AgentState` as a `TypedDict`.There are two ways to define custom state:

1. Via middleware (preferred)
2. Via `state_schema` on `create_agent`

#### ​ Defining state via middleware

Use middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.

from langchain.agents import AgentState
from langchain.agents.middleware import AgentMiddleware
from typing import Any

class CustomState(AgentState):
user_preferences: dict

class CustomMiddleware(AgentMiddleware):
state_schema = CustomState
tools = [tool1, tool2]

...

agent = create_agent(
model,
tools=tools,
middleware=[CustomMiddleware()]
)

# The agent can now track additional state beyond messages
result = agent.invoke({
"messages": [{"role": "user", "content": "I prefer technical explanations"}],
"user_preferences": {"style": "technical", "verbosity": "detailed"},
})

#### ​ Defining state via `state_schema`

Use the `state_schema` parameter as a shortcut to define custom state that is only used in tools.

from langchain.agents import AgentState

agent = create_agent(
model,
tools=[tool1, tool2],
state_schema=CustomState
)

As of `langchain 1.0`, custom state schemas **must** be `TypedDict` types. Pydantic models and dataclasses are no longer supported. See the v1 migration guide for more details.

Defining custom state via middleware is preferred over defining it via `state_schema` on `create_agent` because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.`state_schema` is still supported for backwards compatibility on `create_agent`.

To learn more about memory, see Memory. For information on implementing long-term memory that persists across sessions, see Long-term memory.

### ​ Streaming

We’ve seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.

for chunk in agent.stream({
"messages": [{"role": "user", "content": "Search for AI news and summarize the findings"}]
}, stream_mode="values"):
# Each chunk contains the full state at that point
latest_message = chunk["messages"][-1]
if latest_message.content:
print(f"Agent: {latest_message.content}")
elif latest_message.tool_calls:
print(f"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}")

For more details on streaming, see Streaming.

### ​ Middleware

Middleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:

- Process state before the model is called (e.g., message trimming, context injection)
- Modify or validate the model’s response (e.g., guardrails, content filtering)
- Handle tool execution errors with custom logic
- Implement dynamic model selection based on state or context
- Add custom logging, monitoring, or analytics

Middleware integrates seamlessly into the agent’s execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.

For comprehensive middleware documentation including decorators like `@before_model`, `@after_model`, and `@wrap_tool_call`, see Middleware.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Philosophy\\
\\
Previous Models\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/models

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Models

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Basic usage
- Initialize a model
- Key methods
- Parameters
- Invocation
- Invoke
- Stream
- Batch
- Tool calling
- Structured output
- Supported models
- Advanced topics
- Model profiles
- Multimodal
- Reasoning
- Local models
- Prompt caching
- Server-side tool use
- Rate limiting
- Base URL or proxy
- Log probabilities
- Token usage
- Invocation config
- Configurable models

LLMs are powerful AI tools that can interpret and generate text like humans. They’re versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.In addition to text generation, many models support:

- Tool calling \- calling external tools (like databases queries or API calls) and use results in their responses.
- Structured output \- where the model’s response is constrained to follow a defined format.
- Multimodality \- process and return data other than text, such as images, audio, and video.
- Reasoning \- models perform multi-step reasoning to arrive at a conclusion.

Models are the reasoning engine of agents. They drive the agent’s decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.The quality and capabilities of the model you choose directly impact your agent’s baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.LangChain’s standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.

For provider-specific integration information and capabilities, see the provider’s chat model page.

## ​ Basic usage

Models can be utilized in two ways:

1. **With agents** \- Models can be dynamically specified when creating an agent.
2. **Standalone** \- Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.

The same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.

### ​ Initialize a model

The easiest way to get started with a standalone model in LangChain is to use `init_chat_model` to initialize one from a chat model provider of your choice (examples below):

- OpenAI

- Anthropic

- Azure

- Google Gemini

- AWS Bedrock

- HuggingFace

👉 Read the OpenAI chat model integration docs

Copy

pip install -U "langchain[openai]"

init\_chat\_model

Model Class

import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")

👉 Read the Anthropic chat model integration docs

pip install -U "langchain[anthropic]"

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")

👉 Read the Azure chat model integration docs

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
"azure_openai:gpt-4.1",
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)

👉 Read the Google GenAI chat model integration docs

pip install -U "langchain[google-genai]"

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")

👉 Read the AWS Bedrock chat model integration docs

pip install -U "langchain[aws]"

from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
#

model = init_chat_model(
"anthropic.claude-3-5-sonnet-20240620-v1:0",
model_provider="bedrock_converse",
)

👉 Read the HuggingFace chat model integration docs

pip install -U "langchain[huggingface]"

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

model = init_chat_model(
"microsoft/Phi-3-mini-4k-instruct",
model_provider="huggingface",
temperature=0.7,
max_tokens=1024,
)

response = model.invoke("Why do parrots talk?")

See `init_chat_model` for more detail, including information on how to pass model parameters.

### ​ Key methods

**Invoke** \\
\\
The model takes messages as input and outputs messages after generating a complete response. **Stream** \\
\\
Invoke the model, but stream the output as it is generated in real-time. **Batch** \\
\\
Send multiple requests to a model in a batch for more efficient processing.

In addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the integrations page for details.

## ​ Parameters

A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:

​

model

string

required

The name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the ’:’ format, for example, ‘openai:o1’.

api\_key

The key required for authenticating with the model’s provider. This is usually issued when you sign up for access to the model. Often accessed by setting an environment variable.

temperature

number

Controls the randomness of the model’s output. A higher number makes responses more creative; lower ones make them more deterministic.

max\_tokens

Limits the total number of tokens in the response, effectively controlling how long the output can be.

timeout

The maximum time (in seconds) to wait for a response from the model before canceling the request.

max\_retries

The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.

Using `init_chat_model`, pass these parameters as inline `**kwargs`:

Initialize using model parameters

model = init_chat_model(
"claude-sonnet-4-5-20250929",
# Kwargs passed to the model:
temperature=0.7,
timeout=30,
max_tokens=1000,
)

Each chat model integration may have additional params used to control provider-specific functionality.For example, `ChatOpenAI` has `use_responses_api` to dictate whether to use the OpenAI Responses or Completions API.To find all the parameters supported by a given chat model, head to the chat model integrations page.

* * *

## ​ Invocation

A chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.

### ​ Invoke

The most straightforward way to call a model is to use `invoke()` with a single message or a list of messages.

Single message

response = model.invoke("Why do parrots have colorful feathers?")
print(response)

A list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.See the messages guide for more detail on roles, types, and content.

Dictionary format

conversation = [\
{"role": "system", "content": "You are a helpful assistant that translates English to French."},\
{"role": "user", "content": "Translate: I love programming."},\
{"role": "assistant", "content": "J'adore la programmation."},\
{"role": "user", "content": "Translate: I love building applications."}\
]

response = model.invoke(conversation)
print(response) # AIMessage("J'adore créer des applications.")

Message objects

from langchain.messages import HumanMessage, AIMessage, SystemMessage

conversation = [\
SystemMessage("You are a helpful assistant that translates English to French."),\
HumanMessage("Translate: I love programming."),\
AIMessage("J'adore la programmation."),\
HumanMessage("Translate: I love building applications.")\
]

If the return type of your invocation is a string, ensure that you are using a chat model as opposed to a LLM. Legacy, text-completion LLMs return strings directly. LangChain chat models are prefixed with “Chat”, e.g., `ChatOpenAI`(/oss/integrations/chat/openai).

### ​ Stream

Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.Calling `stream()` returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:

Basic text streaming

Stream tool calls, reasoning, and other content

for chunk in model.stream("Why do parrots have colorful feathers?"):
print(chunk.text, end="|", flush=True)

As opposed to `invoke()`, which returns a single `AIMessage` after the model has finished generating its full response, `stream()` returns multiple `AIMessageChunk` objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:

Construct an AIMessage

full = None # None | AIMessageChunk
for chunk in model.stream("What color is the sky?"):
full = chunk if full is None else full + chunk
print(full.text)

# The
# The sky
# The sky is
# The sky is typically
# The sky is typically blue
# ...

print(full.content_blocks)
# [{"type": "text", "text": "The sky is typically blue..."}]

The resulting message can be treated the same as a message that was generated with `invoke()` – for example, it can be aggregated into a message history and passed :

if event["event"] == "on_chat_model_start":
print(f"Input: {event['data']['input']}")

elif event["event"] == "on_chat_model_stream":
print(f"Token: {event['data']['chunk'].text}")

elif event["event"] == "on_chat_model_end":
print(f"Full message: {event['data']['output'].text}")

else:
pass

Input: Hello
Token: Hi
Token: there
Token: !
Token: How
Token: can
Token: I
...
Full message: Hi there! How can I help today?

See the `astream_events()` reference for event types and other details.

"Auto-streaming" chat models

LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you’re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.In LangGraph agents, for example, you can call `model.invoke()` within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.

#### ​ How it works

When you `invoke()` a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking `on_llm_new_token` events in LangChain’s callback system.Callback events allow LangGraph `stream()` and `astream_events()` to surface the chat model’s output in real-time.

### ​ Batch

Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:

Batch

responses = model.batch([\
"Why do parrots have colorful feathers?",\
"How do airplanes fly?",\
"What is quantum computing?"\
])
for response in responses:
print(response)

This section describes a chat model method `batch()`, which parallelizes model calls client-side.It is **distinct** from batch APIs supported by inference providers, such as OpenAI or Anthropic.

By default, `batch()` will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with `batch_as_completed()`:

Yield batch responses upon completion

for response in model.batch_as_completed([\
"Why do parrots have colorful feathers?",\
"How do airplanes fly?",\
"What is quantum computing?"\
]):
print(response)

When using `batch_as_completed()`, results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed.

When processing a large number of inputs using `batch()` or `batch_as_completed()`, you may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency` attribute in the `RunnableConfig` dictionary.

Batch with max concurrency

model.batch(
list_of_inputs,
config={
'max_concurrency': 5, # Limit to 5 parallel calls
}
)

See the `RunnableConfig` reference for a full list of supported attributes.

For more details on batching, see the reference.

## ​ Tool calling

Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:

1. A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)
2. A function or coroutine to execute.

You may hear the term “function calling”. We use this interchangeably with “tool calling”.

Here’s the basic tool calling flow between a user and a model:

ToolsModelUserToolsModelUserpar\[Parallel Tool Calls\]par\[Tool Execution\]"What's the weather in SF and NYC?"Analyze request & decide tools neededget\_weather("San Francisco")get\_weather("New York")SF weather dataNYC weather dataProcess results & generate response"SF: 72°F sunny, NYC: 68°F cloudy"

To make tools that you have defined available for use by a model, you must bind them using `bind_tools`. In subsequent invocations, the model can choose to call any of the bound tools as needed.Some model providers offer built-in tools that can be enabled via model or invocation parameters (e.g. `ChatOpenAI`, `ChatAnthropic`). Check the respective provider reference for details.

See the tools guide for details and other options for creating tools.

Binding user tools

from langchain.tools import tool

@tool

"""Get the weather at a location."""
return f"It's sunny in {location}."

model_with_tools = model.bind_tools([get_weather])

response = model_with_tools.invoke("What's the weather like in Boston?")
for tool_call in response.tool_calls:
# View tool calls made by the model
print(f"Tool: {tool_call['name']}")
print(f"Args: {tool_call['args']}")

When binding user-defined tools, the model’s response includes a **request** to execute a tool. When using a model separately from an agent, it is up to you to execute the requested tool and return the result , the agent loop will handle the tool execution loop for you.Below, we show some common ways you can use tool calling.

Tool execution loop

When a model returns tool calls, you need to execute the tools and pass the results abstractions that handle this orchestration for you.Here’s a simple example of how to do this:

# Bind (potentially multiple) tools to the model

# Step 1: Model generates tool calls
messages = [{"role": "user", "content": "What's the weather in Boston?"}]
ai_msg = model_with_tools.invoke(messages)
messages.append(ai_msg)

# Step 2: Execute tools and collect results
for tool_call in ai_msg.tool_calls:
# Execute the tool with the generated arguments
tool_result = get_weather.invoke(tool_call)
messages.append(tool_result)

# Step 3: Pass results returned by the tool includes a `tool_call_id` that matches the original tool call, helping the model correlate results with requests.

Forcing tool calls

By default, the model has the freedom to choose which bound tool to use based on the user’s input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or **any** tool from a given list:

Force use of any tool

Force use of specific tools

model_with_tools = model.bind_tools([tool_1], tool_choice="any")

Parallel tool calls

Many models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously.

response = model_with_tools.invoke(
"What's the weather in Boston and Tokyo?"
)

# The model may generate multiple tool calls
print(response.tool_calls)
# [\
# {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\
# {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},\
# ]

# Execute all tools (can be done in parallel with async)
results = []
for tool_call in response.tool_calls:
if tool_call['name'] == 'get_weather':
result = get_weather.invoke(tool_call)
...
results.append(result)

The model intelligently determines when parallel execution is appropriate based on the independence of the requested operations.

Most models supporting tool calling enable parallel tool calls by default. Some (including OpenAI and Anthropic) allow you to disable this feature. To do this, set `parallel_tool_calls=False`:

model.bind_tools([get_weather], parallel_tool_calls=False)

Streaming tool calls

When streaming responses, tool calls are progressively built through `ToolCallChunk`. This allows you to see tool calls as they’re being generated rather than waiting for the complete response.

for chunk in model_with_tools.stream(
"What's the weather in Boston and Tokyo?"
):
# Tool call chunks arrive progressively
for tool_chunk in chunk.tool_call_chunks:
if name := tool_chunk.get("name"):
print(f"Tool: {name}")
if id_ := tool_chunk.get("id"):
print(f"ID: {id_}")
if args := tool_chunk.get("args"):
print(f"Args: {args}")

# Output:
# Tool: get_weather
# ID: call_SvMlU1TVIZugrFLckFE2ceRE
# Args: {"lo
# Args: catio
# Args: n": "B
# Args: osto
# Args: n"}
# ID: call_QMZdy6qInx13oWKE7KhuhOLR
# Args: n": "T
# Args: okyo
# Args: "}

You can accumulate chunks to build complete tool calls:

Accumulate tool calls

gathered = None
for chunk in model_with_tools.stream("What's the weather in Boston?"):
gathered = chunk if gathered is None else gathered + chunk
print(gathered.tool_calls)

## ​ Structured output

Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.

- Pydantic

- TypedDict

- JSON Schema

Pydantic models provide the richest feature set with field validation, descriptions, and nested structures.

from pydantic import BaseModel, Field

class Movie(BaseModel):
"""A movie with details."""
title: str = Field(..., description="The title of the movie")
year: int = Field(..., description="The year the movie was released")
director: str = Field(..., description="The director of the movie")
rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie)
response = model_with_structure.invoke("Provide details about the movie Inception")
print(response) # Movie(title="Inception", year=2010, director="Christopher Nolan", rating=8.8)

`TypedDict` provides a simpler alternative using Python’s built-in typing, ideal when you don’t need runtime validation.

from typing_extensions import TypedDict, Annotated

class MovieDict(TypedDict):
"""A movie with details."""
title: Annotated[str, ..., "The title of the movie"]
year: Annotated[int, ..., "The year the movie was released"]
director: Annotated[str, ..., "The director of the movie"]
rating: Annotated[float, ..., "The movie's rating out of 10"]

model_with_structure = model.with_structured_output(MovieDict)
response = model_with_structure.invoke("Provide details about the movie Inception")
print(response) # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}

For maximum control or interoperability, you can provide a raw JSON Schema.

import json

json_schema = {
"title": "Movie",
"description": "A movie with details",
"type": "object",
"properties": {
"title": {
"type": "string",
"description": "The title of the movie"
},
"year": {
"type": "integer",
"description": "The year the movie was released"
},
"director": {
"type": "string",
"description": "The director of the movie"
},
"rating": {
"type": "number",
"description": "The movie's rating out of 10"
}
},
"required": ["title", "year", "director", "rating"]
}

model_with_structure = model.with_structured_output(
json_schema,
method="json_schema",
)
response = model_with_structure.invoke("Provide details about the movie Inception")
print(response) # {'title': 'Inception', 'year': 2010, ...}

**Key considerations for structured output:**

- **Method parameter**: Some providers support different methods (`'json_schema'`, `'function_calling'`, `'json_mode'`)

- `'json_schema'` typically refers to dedicated structured output features offered by a provider
- `'function_calling'` derives structured output by forcing a tool call following the given schema
- `'json_mode'` is a precursor to `'json_schema'` offered by some providers - it generates valid json, but the schema must be described in the prompt
- **Include raw**: Use `include_raw=True` to get both the parsed output and the raw AI message
- **Validation**: Pydantic models provide automatic validation, while `TypedDict` and JSON Schema require manual validation

Example: Message output alongside parsed structure

It can be useful to return the raw `AIMessage` object alongside the parsed representation to access response metadata such as token counts. To do this, set `include_raw=True` when calling `with_structured_output`:

model_with_structure = model.with_structured_output(Movie, include_raw=True)
response = model_with_structure.invoke("Provide details about the movie Inception")
response
# {
# "raw": AIMessage(...),
# "parsed": Movie(title=..., year=..., ...),
# "parsing_error": None,
# }

Example: Nested structures

Schemas can be nested:

Pydantic BaseModel

TypedDict

class Actor(BaseModel):
name: str
role: str

class MovieDetails(BaseModel):
title: str
year: int
cast: list[Actor]
genres: list[str]
budget: float | None = Field(None, description="Budget in millions USD")

model_with_structure = model.with_structured_output(MovieDetails)

## ​ Supported models

LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the integrations page.

## ​ Advanced topics

### ​ Model profiles

This is a beta feature. The format of model profiles is subject to change.

LangChain chat models can expose a dictionary of supported features and capabilities through a `.profile` attribute:

model.profile
# "max_input_tokens": 400000,
# "image_inputs": True,
# "reasoning_output": True,
# "tool_calling": True,

Refer to the full set of fields in the API reference.Much of the model profile data is powered by the models.dev project, an open source initiative that provides model capability data. These data are augmented with additional fields for purposes of use with LangChain. These augmentations are kept aligned with the upstream project as it evolves.Model profile data allow applications to work around model capabilities dynamically. For example:

1. Summarization middleware can trigger summarization based on a model’s context window size.
2. Structured output strategies in `create_agent` can be inferred automatically (e.g., by checking support for native structured output features).
3. Model inputs can be gated based on supported modalities and maximum input tokens.

Updating or overwriting profile data

Model profile data can be changed if it is missing, stale, or incorrect.**Option 1 (quick fix)**You can instantiate a chat model with any valid profile:

custom_profile = {
"max_input_tokens": 100_000,
"tool_calling": True,
"structured_output": True,
}
model = init_chat_model("...", profile=custom_profile)

The `profile` is also a regular `dict` and can be updated in place. If the model instance is shared, consider using `model_copy` to avoid mutating shared state.

new_profile = model.profile | {"key": "value"}
model.model_copy(update={"profile": new_profile})

**Option 2 (fix data upstream)**The primary source for the data is the models.dev project. This data is merged with additional fields and overrides in LangChain integration packages and are shipped with those packages.Model profile data can be updated through the following process:

1. (If needed) update the source data at models.dev through a pull request to its repository on GitHub.

3. Use the `langchain-model-profiles` CLI tool to pull the latest data from models.dev, merge in the augmentations and update the profile data:

pip install langchain-model-profiles

This command:

For example: from `libs/partners/anthropic` in the LangChain monorepo:

uv run --with langchain-model-profiles --provider anthropic --data-dir langchain_anthropic/data

### ​ Multimodal

Certain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing content blocks.

All LangChain chat models with underlying multimodal capabilities support:

1. Data in the cross-provider standard format (see our messages guide)
2. OpenAI chat completions format
3. Any format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format)

See the multimodal section of the messages guide for details.Some models can return multimodal data as part of their response. If invoked to do so, the resulting `AIMessage` will have content blocks with multimodal types.

Multimodal output

response = model.invoke("Create a picture of a cat")
print(response.content_blocks)
# {"type": "text", "text": "Here's a picture of a cat"},\
# {"type": "image", "base64": "...", "mime_type": "image/jpeg"},\

See the integrations page for details on specific providers.

### ​ Reasoning

Many models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.**If supported by the underlying model,** you can surface this reasoning process to better understand how the model arrived at its final answer.

Stream reasoning output

Complete reasoning output

for chunk in model.stream("Why do parrots have colorful feathers?"):
reasoning_steps = [r for r in chunk.content_blocks if r["type"] == "reasoning"]
print(reasoning_steps if reasoning_steps else chunk.text)

Depending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical “tiers” of reasoning (e.g., `'low'` or `'high'`) or integer token budgets.For details, see the integrations page or reference for your respective chat model.

### ​ Local models

LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.Ollama is one of the easiest ways to run models locally. See the full list of local integrations on the integrations page.

### ​ Prompt caching

Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be **implicit** or **explicit**:

- **Implicit prompt caching:** providers will automatically pass on cost savings if a request hits a cache. Examples: OpenAI and Gemini.
- **Explicit caching:**providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples:

- `ChatOpenAI` (via `prompt_cache_key`)
- Anthropic’s `AnthropicPromptCachingMiddleware`
- Gemini.
- AWS Bedrock

Prompt caching is often only engaged above a minimum input token threshold. See provider pages for details.

Cache usage will be reflected in the usage metadata of the model response.

### ​ Server-side tool use

Some providers support server-side tool-calling loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.If a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the content blocks of the response will return the server-side tool calls and results in a provider-agnostic format:

Invoke with server-side tool use

model = init_chat_model("gpt-4.1-mini")

tool = {"type": "web_search"}
model_with_tools = model.bind_tools([tool])

response = model_with_tools.invoke("What was a positive news story from today?")
response.content_blocks

Result

[\
{\
"type": "server_tool_call",\
"name": "web_search",\
"args": {\
"query": "positive news stories today",\
"type": "search"\
},\
"id": "ws_abc123"\
},\
{\
"type": "server_tool_result",\
"tool_call_id": "ws_abc123",\
"status": "success"\
},\
{\
"type": "text",\
"text": "Here are some positive news stories from today...",\
"annotations": [\
{\
"end_index": 410,\
"start_index": 337,\
"title": "article title",\
"type": "citation",\
"url": "..."\
}\
]\
}\
]

See all 29 lines

This represents a single conversational turn; there are no associated ToolMessage objects that need to be passed in as in client-side tool-calling.See the integration page for your given provider for available tools and usage details.

### ​ Rate limiting

Many chat model providers impose a limit on the number of invocations that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.To help manage rate limits, chat model integrations accept a `rate_limiter` parameter that can be provided during initialization to control the rate at which requests are made.

Initialize and use a rate limiter

LangChain in comes with (an optional) built-in `InMemoryRateLimiter`. This limiter is thread safe and can be shared by multiple threads in the same process.

Define a rate limiter

from langchain_core.rate_limiters import InMemoryRateLimiter

rate_limiter = InMemoryRateLimiter(
requests_per_second=0.1, # 1 request every 10s
check_every_n_seconds=0.1, # Check every 100ms whether allowed to make a request
max_bucket_size=10, # Controls the maximum burst size.
)

model = init_chat_model(
model="gpt-5",
model_provider="openai",
rate_limiter=rate_limiter
)

The provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size of the requests.

### ​ Base URL or proxy

For many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.

Base URL

Many model providers offer OpenAI-compatible APIs (e.g., Together AI, vLLM). You can use `init_chat_model` with these providers by specifying the appropriate `base_url` parameter:

model = init_chat_model(
model="MODEL_NAME",
model_provider="openai",
base_url="BASE_URL",
api_key="YOUR_API_KEY",
)

When using direct chat model class instantiation, the parameter name may vary by provider. Check the respective reference for details.

Proxy configuration

For deployments requiring HTTP proxies, some model integrations support proxy configuration:

from langchain_openai import ChatOpenAI

model = ChatOpenAI(
model="gpt-4o",
openai_proxy="http://proxy.example.com:8080"
)

Proxy support varies by integration. Check the specific model provider’s reference for proxy configuration options.

### ​ Log probabilities

Certain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the `logprobs` parameter when initializing the model:

model = init_chat_model(
model="gpt-4o",
model_provider="openai"
).bind(logprobs=True)

response = model.invoke("Why do parrots talk?")
print(response.response_metadata["logprobs"])

### ​ Token usage

A number of model providers return token usage information as part of the invocation response. When available, this information will be included on the `AIMessage` objects produced by the corresponding model. For more details, see the messages guide.

Some provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the streaming usage metadata section of the integration guide for details.

You can track aggregate token counts across models in an application using either a callback or context manager, as shown below:

- Callback handler

- Context manager

from langchain.chat_models import init_chat_model
from langchain_core.callbacks import UsageMetadataCallbackHandler

model_1 = init_chat_model(model="gpt-4o-mini")
model_2 = init_chat_model(model="claude-haiku-4-5-20251001")

callback = UsageMetadataCallbackHandler()
result_1 = model_1.invoke("Hello", config={"callbacks": [callback]})
result_2 = model_2.invoke("Hello", config={"callbacks": [callback]})
callback.usage_metadata

{
'gpt-4o-mini-2024-07-18': {
'input_tokens': 8,
'output_tokens': 10,
'total_tokens': 18,
'input_token_details': {'audio': 0, 'cache_read': 0},
'output_token_details': {'audio': 0, 'reasoning': 0}
},
'claude-haiku-4-5-20251001': {
'input_tokens': 8,
'output_tokens': 21,
'total_tokens': 29,
'input_token_details': {'cache_read': 0, 'cache_creation': 0}
}
}

from langchain.chat_models import init_chat_model
from langchain_core.callbacks import get_usage_metadata_callback

with get_usage_metadata_callback() as cb:
model_1.invoke("Hello")
model_2.invoke("Hello")
print(cb.usage_metadata)

### ​ Invocation config

When invoking a model, you can pass additional configuration through the `config` parameter using a `RunnableConfig` dictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking.Common configuration options include:

Invocation with config

response = model.invoke(
"Tell me a joke",
config={
"run_name": "joke_generation", # Custom name for this run
"tags": ["humor", "demo"], # Tags for categorization
"metadata": {"user_id": "123"}, # Custom metadata
"callbacks": [my_callback_handler], # Callback handlers
}
)

These configuration values are particularly useful when:

- Debugging with LangSmith tracing
- Implementing custom logging or monitoring
- Controlling resource usage in production
- Tracking invocations across complex pipelines

Key configuration attributes

run\_name

Identifies this specific invocation in logs and traces. Not inherited by sub-calls.

tags

string\[\]

Labels inherited by all sub-calls for filtering and organization in debugging tools.

metadata

object

Custom key-value pairs for tracking additional context, inherited by all sub-calls.

max\_concurrency

Controls the maximum number of parallel calls when using `batch()` or `batch_as_completed()`.

callbacks

array

Handlers for monitoring and responding to events during execution.

recursion\_limit

Maximum recursion depth for chains to prevent infinite loops in complex pipelines.

See full `RunnableConfig` reference for all supported attributes.

### ​ Configurable models

You can also create a runtime-configurable model by specifying `configurable_fields`. If you don’t specify a model value, then `'model'` and `'model_provider'` will be configurable by default.

configurable_model = init_chat_model(temperature=0)

configurable_model.invoke(
"what's your name",
config={"configurable": {"model": "gpt-5-nano"}}, # Run with GPT-5-Nano
)
configurable_model.invoke(
"what's your name",
config={"configurable": {"model": "claude-sonnet-4-5-20250929"}}, # Run with Claude
)

Configurable model with default values

We can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:

first_model = init_chat_model(
model="gpt-4.1-mini",
temperature=0,
configurable_fields=("model", "model_provider", "temperature", "max_tokens"),
config_prefix="first", # Useful when you have a chain with multiple models
)

first_model.invoke("what's your name")

first_model.invoke(
"what's your name",
config={
"configurable": {
"first_model": "claude-sonnet-4-5-20250929",
"first_temperature": 0.5,
"first_max_tokens": 100,
}
},
)

See the `init_chat_model` reference for more details on `configurable_fields` and `config_prefix`.

Using a configurable model declaratively

We can call declarative operations like `bind_tools`, `with_structured_output`, `with_configurable`, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.

class GetWeather(BaseModel):
"""Get the current weather in a given location"""

location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

class GetPopulation(BaseModel):
"""Get the current population in a given location"""

model = init_chat_model(temperature=0)
model_with_tools = model.bind_tools([GetWeather, GetPopulation])

model_with_tools.invoke(
"what's bigger in 2024 LA or NYC", config={"configurable": {"model": "gpt-4.1-mini"}}
).tool_calls

[\
{\
'name': 'GetPopulation',\
'args': {'location': 'Los Angeles, CA'},\
'id': 'call_Ga9m8FAArIyEjItHmztPYA22',\
'type': 'tool_call'\
},\
{\
'name': 'GetPopulation',\
'args': {'location': 'New York, NY'},\
'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',\
'type': 'tool_call'\
}\
]

model_with_tools.invoke(
"what's bigger in 2024 LA or NYC",
config={"configurable": {"model": "claude-sonnet-4-5-20250929"}},
).tool_calls

[\
{\
'name': 'GetPopulation',\
'args': {'location': 'Los Angeles, CA'},\
'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',\
'type': 'tool_call'\
},\
{\
'name': 'GetPopulation',\
'args': {'location': 'New York City, NY'},\
'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',\
'type': 'tool_call'\
}\
]

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Agents\\
\\
Previous Messages\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/messages

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Messages

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Basic usage
- Text prompts
- Message prompts
- Dictionary format
- Message types
- System Message
- Human Message
- Text content
- Message metadata
- AI Message
- Tool calls
- Token usage
- Streaming and chunks
- Tool Message
- Message content
- Standard content blocks
- Multimodal
- Content block reference
- Use with chat models

Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.Messages are objects that contain:

- **Role** \- Identifies the message type (e.g. `system`, `user`)
- **Content** \- Represents the actual content of the message (like text, images, audio, documents, etc.)
- **Metadata** \- Optional fields such as response information, message IDs, and token usage

LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.

## ​ Basic usage

The simplest way to use messages is to create message objects and pass them to a model when invoking.

Copy

from langchain.chat_models import init_chat_model
from langchain.messages import HumanMessage, AIMessage, SystemMessage

model = init_chat_model("gpt-5-nano")

system_msg = SystemMessage("You are a helpful assistant.")
human_msg = HumanMessage("Hello, how are you?")

# Use with chat models
messages = [system_msg, human_msg]
response = model.invoke(messages) # Returns AIMessage

### ​ Text prompts

Text prompts are strings - ideal for straightforward generation tasks where you don’t need to retain conversation history.

response = model.invoke("Write a haiku about spring")

**Use text prompts when:**

- You have a single, standalone request
- You don’t need conversation history
- You want minimal code complexity

### ​ Message prompts

Alternatively, you can pass in a list of messages to the model by providing a list of message objects.

from langchain.messages import SystemMessage, HumanMessage, AIMessage

messages = [\
SystemMessage("You are a poetry expert"),\
HumanMessage("Write a haiku about spring"),\
AIMessage("Cherry blossoms bloom...")\
]
response = model.invoke(messages)

**Use message prompts when:**

- Managing multi-turn conversations
- Working with multimodal content (images, audio, files)
- Including system instructions

### ​ Dictionary format

You can also specify messages directly in OpenAI chat completions format.

messages = [\
{"role": "system", "content": "You are a poetry expert"},\
{"role": "user", "content": "Write a haiku about spring"},\
{"role": "assistant", "content": "Cherry blossoms bloom..."}\
]
response = model.invoke(messages)

## ​ Message types

- System message \- Tells the model how to behave and provide context for interactions
- Human message \- Represents user input and interactions with the model
- AI message \- Responses generated by the model, including text content, tool calls, and metadata
- Tool message \- Represents the outputs of tool calls

### ​ System Message

A `SystemMessage` represent an initial set of instructions that primes the model’s behavior. You can use a system message to set the tone, define the model’s role, and establish guidelines for responses.

Basic instructions

system_msg = SystemMessage("You are a helpful coding assistant.")

messages = [\
system_msg,\
HumanMessage("How do I create a REST API?")\
]
response = model.invoke(messages)

Detailed persona

from langchain.messages import SystemMessage, HumanMessage

system_msg = SystemMessage("""
You are a senior Python developer with expertise in web frameworks.
Always provide code examples and explain your reasoning.
Be concise but thorough in your explanations.
""")

* * *

### ​ Human Message

A `HumanMessage` represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal content.

#### ​ Text content

Message object

String shortcut

response = model.invoke([\
HumanMessage("What is machine learning?")\
])

#### ​ Message metadata

Add metadata

human_msg = HumanMessage(
content="Hello!",
name="alice", # Optional: identify different users
id="msg_123", # Optional: unique identifier for tracing
)

The `name` field behavior varies by provider – some use it for user identification, others ignore it. To check, refer to the model provider’s reference.

### ​ AI Message

An `AIMessage` represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.

response = model.invoke("Explain AI")

`AIMessage` objects are returned by the model when calling it, which contains all of the associated metadata in the response.Providers weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new `AIMessage` object and insert it into the message history as if it came from the model.

from langchain.messages import AIMessage, SystemMessage, HumanMessage

# Create an AI message manually (e.g., for conversation history)
ai_msg = AIMessage("I'd be happy to help you with that question!")

# Add to conversation history
messages = [\
SystemMessage("You are a helpful assistant"),\
HumanMessage("Can you help me?"),\
ai_msg, # Insert as if it came from the model\
HumanMessage("Great! What's 2+2?")\
]

response = model.invoke(messages)

Attributes

​

text

string

The text content of the message.

content

string \| dict\[\]

The raw content of the message.

content\_blocks

ContentBlock\[\]

The standardized content blocks of the message.

tool\_calls

dict\[\] \| None

The tool calls made by the model.Empty if no tools are called.

id

A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)

usage\_metadata

dict \| None

The usage metadata of the message, which can contain token counts when available.

response\_metadata

ResponseMetadata \| None

The response metadata of the message.

#### ​ Tool calls

When models make tool calls, they’re included in the `AIMessage`:

from langchain.chat_models import init_chat_model

"""Get the weather at a location."""
...

model_with_tools = model.bind_tools([get_weather])
response = model_with_tools.invoke("What's the weather in Paris?")

for tool_call in response.tool_calls:
print(f"Tool: {tool_call['name']}")
print(f"Args: {tool_call['args']}")
print(f"ID: {tool_call['id']}")

Other structured data, such as reasoning or citations, can also appear in message content.

#### ​ Token usage

An `AIMessage` can hold token counts and other usage metadata in its `usage_metadata` field:

response = model.invoke("Hello!")
response.usage_metadata

{'input_tokens': 8,
'output_tokens': 304,
'total_tokens': 312,
'input_token_details': {'audio': 0, 'cache_read': 0},
'output_token_details': {'audio': 0, 'reasoning': 256}}

See `UsageMetadata` for details.

#### ​ Streaming and chunks

During streaming, you’ll receive `AIMessageChunk` objects that can be combined into a full message object:

chunks = []
full_message = None
for chunk in model.stream("Hi"):
chunks.append(chunk)
print(chunk.text)
full_message = chunk if full_message is None else full_message + chunk

Learn more:

- Streaming tokens from chat models
- Streaming tokens and/or steps from agents

### ​ Tool Message

For models that support tool calling, AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution can generate `ToolMessage` objects directly. Below, we show a simple example. Read more in the tools guide.

from langchain.messages import AIMessage
from langchain.messages import ToolMessage

# After a model makes a tool call
# (Here, we demonstrate manually creating the messages for brevity)
ai_message = AIMessage(
content=[],
tool_calls=[{\
"name": "get_weather",\
"args": {"location": "San Francisco"},\
"id": "call_123"\
}]
)

# Execute tool and create result message
weather_result = "Sunny, 72°F"
tool_message = ToolMessage(
content=weather_result,
tool_call_id="call_123" # Must match the call ID
)

# Continue conversation
messages = [\
HumanMessage("What's the weather in San Francisco?"),\
ai_message, # Model's tool call\
tool_message, # Tool execution result\
]
response = model.invoke(messages) # Model processes the result

required

The stringified output of the tool call.

tool\_call\_id

The ID of the tool call that this message is responding to. Must match the ID of the tool call in the `AIMessage`.

name

The name of the tool that was called.

artifact

dict

Additional data not sent to the model but can be accessed programmatically.

The `artifact` field stores supplementary data that won’t be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model’s context.

Example: Using artifact for retrieval metadata

For example, a retrieval tool could retrieve a passage from a document for reference by a model. Where message `content` contains text that the model will reference, an `artifact` can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:

from langchain.messages import ToolMessage

# Sent to model
message_content = "It was the best of times, it was the worst of times."

# Artifact available downstream
artifact = {"document_id": "doc_123", "page": 0}

tool_message = ToolMessage(
content=message_content,
tool_call_id="call_123",
name="search_books",
artifact=artifact,
)

See the RAG tutorial for an end-to-end example of building retrieval agents with LangChain.

## ​ Message content

You can think of a message’s content as the payload of data that gets sent to the model. Messages have a `content` attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as multimodal content and other data.Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See content blocks below.LangChain chat models accept message content in the `content` attribute.This may contain either:

1. A string
2. A list of content blocks in a provider-native format
3. A list of LangChain’s standard content blocks

See below for an example using multimodal inputs:

from langchain.messages import HumanMessage

# String content
human_message = HumanMessage("Hello, how are you?")

# Provider-native format (e.g., OpenAI)
human_message = HumanMessage(content=[\
{"type": "text", "text": "Hello, how are you?"},\
{"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}\
])

# List of standard content blocks
human_message = HumanMessage(content_blocks=[\
{"type": "text", "text": "Hello, how are you?"},\
{"type": "image", "url": "https://example.com/image.jpg"},\
])

Specifying `content_blocks` when initializing a message will still populate message
`content`, but provides a type-safe interface for doing so.

### ​ Standard content blocks

LangChain provides a standard representation for message content that works across providers.Message objects implement a `content_blocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from `ChatAnthropic` or `ChatOpenAI` will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent `ReasoningContentBlock` representation:

- Anthropic

- OpenAI

from langchain.messages import AIMessage

message = AIMessage(
content=[\
{"type": "thinking", "thinking": "...", "signature": "WaUjzkyp..."},\
{"type": "text", "text": "..."},\
],
response_metadata={"model_provider": "anthropic"}
)
message.content_blocks

[{'type': 'reasoning',\
'reasoning': '...',\
'extras': {'signature': 'WaUjzkyp...'}},\
{'type': 'text', 'text': '...'}]

message = AIMessage(
content=[\
{\
"type": "reasoning",\
"id": "rs_abc123",\
"summary": [\
{"type": "summary_text", "text": "summary 1"},\
{"type": "summary_text", "text": "summary 2"},\
],\
},\
{"type": "text", "text": "...", "id": "msg_abc123"},\
],
response_metadata={"model_provider": "openai"}
)
message.content_blocks

[{'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 1'},\
{'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 2'},\
{'type': 'text', 'text': '...', 'id': 'msg_abc123'}]

See the integrations guides to get started with the
inference provider of your choice.

**Serializing standard content**If an application outside of LangChain needs access to the standard content block
representation, you can opt-in to storing content blocks in message content.To do this, you can set the `LC_OUTPUT_VERSION` environment variable to `v1`. Or,
initialize any chat model with `output_version="v1"`:

model = init_chat_model("gpt-5-nano", output_version="v1")

### ​ Multimodal

**Multimodality** refers to the ability to work with data that comes in different
forms, such as text, audio, images, and video. LangChain includes standard types
for these data that can be used across providers.Chat models can accept multimodal data as input and generate
it as output. Below we show short examples of input messages featuring multimodal data.

Extra keys can be included top-level in the content block or nested in `"extras": {"key": value}`.OpenAI and AWS Bedrock Converse,
for example, require a filename for PDFs. See the provider page
for your chosen model for specifics.

Image input

PDF document input

Audio input

Video input

# From URL
message = {
"role": "user",
"content": [\
{"type": "text", "text": "Describe the content of this image."},\
{"type": "image", "url": "https://example.com/path/to/image.jpg"},\
]
}

# From base64 data
message = {
"role": "user",
"content": [\
{"type": "text", "text": "Describe the content of this image."},\
{\
"type": "image",\
"base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",\
"mime_type": "image/jpeg",\
},\
]
}

# From provider-managed File ID
message = {
"role": "user",
"content": [\
{"type": "text", "text": "Describe the content of this image."},\
{"type": "image", "file_id": "file-abc123"},\
]
}

Not all models support all file types. Check the model provider’s reference for supported formats and size limits.

### ​ Content block reference

Content blocks are represented (either when creating a message or accessing the `content_blocks` property) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:

Core

TextContentBlock

**Purpose:** Standard text output

type

Always `"text"`

The text content

annotations

object\[\]

List of annotations for the text

extras

object

Additional provider-specific data

**Example:**

{
"type": "text",
"text": "Hello world",
"annotations": []
}

ReasoningContentBlock

**Purpose:** Model reasoning steps

Always `"reasoning"`

reasoning

The reasoning content

{
"type": "reasoning",
"reasoning": "The user is asking about...",
"extras": {"signature": "abc123"},
}

Multimodal

ImageContentBlock

**Purpose:** Image data

Always `"image"`

url

URL pointing to the image location.

base64

Base64-encoded image data.

Unique identifier for this content block (either generated by the provider or by LangChain).

mime\_type

Image MIME type (e.g., `image/jpeg`, `image/png`). Required for base64 data.

AudioContentBlock

**Purpose:** Audio data

Always `"audio"`

URL pointing to the audio location.

Base64-encoded audio data.

Audio MIME type (e.g., `audio/mpeg`, `audio/wav`). Required for base64 data.

VideoContentBlock

**Purpose:** Video data

Always `"video"`

URL pointing to the video location.

Base64-encoded video data.

Video MIME type (e.g., `video/mp4`, `video/webm`). Required for base64 data.

FileContentBlock

**Purpose:** Generic files (PDF, etc)

Always `"file"`

URL pointing to the file location.

Base64-encoded file data.

File MIME type (e.g., `application/pdf`). Required for base64 data.

PlainTextContentBlock

**Purpose:** Document text (`.txt`, `.md`)

Always `"text-plain"`

MIME type of the text (e.g., `text/plain`, `text/markdown`)

Tool Calling

ToolCall

**Purpose:** Function calls

Always `"tool_call"`

Name of the tool to call

args

Arguments to pass to the tool

Unique identifier for this tool call

{
"type": "tool_call",
"name": "search",
"args": {"query": "weather"},
"id": "call_123"
}

ToolCallChunk

**Purpose:** Streaming tool call fragments

Always `"tool_call_chunk"`

Name of the tool being called

Partial tool arguments (may be incomplete JSON)

Tool call identifier

index

number \| string

Position of this chunk in the stream

InvalidToolCall

**Purpose:** Malformed calls, intended to catch JSON parsing errors.

Always `"invalid_tool_call"`

Name of the tool that failed to be called

error

Description of what went wrong

Server-Side Tool Execution

ServerToolCall

**Purpose:** Tool call that is executed server-side.

Always `"server_tool_call"`

An identifier associated with the tool call.

The name of the tool to be called.

ServerToolCallChunk

**Purpose:** Streaming server-side tool call fragments

Always `"server_tool_call_chunk"`

ServerToolResult

**Purpose:** Search results

Always `"server_tool_result"`

Identifier of the corresponding server tool call.

Identifier associated with the server tool result.

status

Execution status of the server-side tool. `"success"` or `"error"`.

output

Output of the executed tool.

Provider-Specific Blocks

NonStandardContentBlock

**Purpose:** Provider-specific escape hatch

Always `"non_standard"`

value

Provider-specific data structure

**Usage:** For experimental or provider-unique features

Additional provider-specific content types may be found within the reference documentation of each model provider.

View the canonical type definitions in the API reference.

Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code.Content blocks are not a replacement for the `content` property, but rather a new property that can be used to access the content of a message in a standardized format.

## ​ Use with chat models

Chat models accept a sequence of message objects as input and return an `AIMessage` as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.Refer to the below guides to learn more:

- Built-in features for persisting and managing conversation histories
- Strategies for managing context windows, including trimming and summarizing messages

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Models\\
\\
Previous Tools\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/tools

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Tools

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Create tools
- Basic tool definition
- Customize tool properties
- Custom tool name
- Custom tool description
- Advanced schema definition
- Reserved argument names
- Accessing Context
- ToolRuntime
- Context
- Memory (Store)
- Stream Writer

Tools extend what agents can do—letting them fetch real-time data, execute code, query external databases, and take actions in the world.Under the hood, tools are callable functions with well-defined inputs and outputs that get passed to a chat model. The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.

For details on how models handle tool calls, see Tool calling.

## ​ Create tools

### ​ Basic tool definition

The simplest way to create a tool is with the `@tool` decorator. By default, the function’s docstring becomes the tool’s description that helps the model understand when to use it:

Copy

from langchain.tools import tool

@tool

"""Search the customer database for records matching the query.

Args:
query: Search terms to look for
limit: Maximum number of results to return
"""
return f"Found {limit} results for '{query}'"

Type hints are **required** as they define the tool’s input schema. The docstring should be informative and concise to help the model understand the tool’s purpose.

**Server-side tool use**Some chat models (e.g., OpenAI, Anthropic, and Gemini) feature built-in tools that are executed server-side, such as web search and code interpreters. Refer to the provider overview to learn how to access these tools with your specific chat model.

#### ​ Custom tool name

By default, the tool name comes from the function name. Override it when you need something more descriptive:

@tool("web_search") # Custom name

"""Search the web for information."""
return f"Results for: {query}"

print(search.name) # web_search

#### ​ Custom tool description

Override the auto-generated tool description for clearer model guidance:

@tool("calculator", description="Performs arithmetic calculations. Use this for any math problems.")

"""Evaluate mathematical expressions."""
return str(eval(expression))

### ​ Advanced schema definition

Define complex inputs with Pydantic models or JSON schemas:

Pydantic model

JSON Schema

from pydantic import BaseModel, Field
from typing import Literal

class WeatherInput(BaseModel):
"""Input for weather queries."""
location: str = Field(description="City name or coordinates")
units: Literal["celsius", "fahrenheit"] = Field(
default="celsius",
description="Temperature unit preference"
)
include_forecast: bool = Field(
default=False,
description="Include 5-day forecast"
)

@tool(args_schema=WeatherInput)

"""Get current weather and optional forecast."""
temp = 22 if units == "celsius" else 72
result = f"Current weather in {location}: {temp} degrees {units[0].upper()}"
if include_forecast:
result += "\nNext 5 days: Sunny"
return result

### ​ Reserved argument names

The following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors.

| Parameter name | Purpose |
| --- | --- |
| `config` | Reserved for passing `RunnableConfig` to tools internally |
| `runtime` | Reserved for `ToolRuntime` parameter (accessing state, context, store) |

To access runtime information, use the `ToolRuntime` parameter instead of naming your own arguments `config` or `runtime`.

## ​ Accessing Context

**Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.Runtime context provides a way to inject dependencies (like database connections, user IDs, or configuration) into your tools at runtime, making them more testable and reusable.

Tools can access runtime information through the `ToolRuntime` parameter, which provides:

- **State** \- Mutable data that flows through execution (e.g., messages, counters, custom fields)
- **Context** \- Immutable configuration like user IDs, session details, or application-specific configuration
- **Store** \- Persistent long-term memory across conversations
- **Stream Writer** \- Stream custom updates as tools execute
- **Config** \- `RunnableConfig` for the execution
- **Tool Call ID** \- ID of the current tool call

⚡ Enhanced Tool Capabilities

📊 Available Resources

🔧 Tool Runtime Context

Tool Call

ToolRuntime

State Access

Context Access

Store Access

Stream Writer

Messages

Custom State

User ID

Session Info

Long-term Memory

User Preferences

Context-Aware Tools

Stateful Tools

Memory-Enabled Tools

Streaming Tools

### ​ `ToolRuntime`

Use `ToolRuntime` to access all runtime information in a single parameter. Simply add `runtime: ToolRuntime` to your tool signature, and it will be automatically injected without being exposed to the LLM.

**`ToolRuntime`**: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate `InjectedState`, `InjectedStore`, `get_runtime`, and `InjectedToolCallId` annotations.The runtime automatically provides these capabilities to your tool functions without you having to pass them explicitly or use global state.

**Accessing state:**Tools can access the current graph state using `ToolRuntime`:

from langchain.tools import tool, ToolRuntime

# Access the current conversation state
@tool
def summarize_conversation(
runtime: ToolRuntime

"""Summarize the conversation so far."""
messages = runtime.state["messages"]

human_msgs = sum(1 for m in messages if m.__class__.__name__ == "HumanMessage")
ai_msgs = sum(1 for m in messages if m.__class__.__name__ == "AIMessage")
tool_msgs = sum(1 for m in messages if m.__class__.__name__ == "ToolMessage")

return f"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results"

# Access custom state fields
@tool
def get_user_preference(
pref_name: str,
runtime: ToolRuntime # ToolRuntime parameter is not visible to the model

"""Get a user preference value."""
preferences = runtime.state.get("user_preferences", {})
return preferences.get(pref_name, "Not set")

The `runtime` parameter is hidden from the model. For the example above, the model only sees `pref_name` in the tool schema - `runtime` is _not_ included in the request.

**Updating state:**Use `Command` to update the agent’s state or control the graph’s execution flow:

from langgraph.types import Command
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langchain.tools import tool, ToolRuntime

# Update the conversation history by removing all messages

"""Clear the conversation history."""

return Command(
update={
"messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],
}
)

# Update the user_name in the agent state
@tool
def update_user_name(
new_name: str,
runtime: ToolRuntime

"""Update the user's name."""
return Command(update={"user_name": new_name})

#### ​ Context

Access immutable configuration and contextual data like user IDs, session details, or application-specific configuration through `runtime.context`.Tools can access runtime context through `ToolRuntime`:

from dataclasses import dataclass
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime

USER_DATABASE = {
"user123": {
"name": "Alice Johnson",
"account_type": "Premium",
"balance": 5000,
"email": "alice@example.com"
},
"user456": {
"name": "Bob Smith",
"account_type": "Standard",
"balance": 1200,
"email": "bob@example.com"
}
}

@dataclass
class UserContext:
user_id: str

"""Get the current user's account information."""
user_id = runtime.context.user_id

if user_id in USER_DATABASE:
user = USER_DATABASE[user_id]
return f"Account holder: {user['name']}\nType: {user['account_type']}\nBalance: ${user['balance']}"
return "User not found"

model = ChatOpenAI(model="gpt-4o")
agent = create_agent(
model,
tools=[get_account_info],
context_schema=UserContext,
system_prompt="You are a financial assistant."
)

result = agent.invoke(
{"messages": [{"role": "user", "content": "What's my current balance?"}]},
context=UserContext(user_id="user123")
)

#### ​ Memory (Store)

Access persistent data across conversations using the store. The store is accessed via `runtime.store` and allows you to save and retrieve user-specific or application-specific data.Tools can access and update the store through `ToolRuntime`:

from typing import Any
from langgraph.store.memory import InMemoryStore
from langchain.agents import create_agent
from langchain.tools import tool, ToolRuntime

# Access memory

"""Look up user info."""
store = runtime.store
user_info = store.get(("users",), user_id)
return str(user_info.value) if user_info else "Unknown user"

# Update memory

"""Save user info."""
store = runtime.store
store.put(("users",), user_id, user_info)
return "Successfully saved user info."

store = InMemoryStore()
agent = create_agent(
model,
tools=[get_user_info, save_user_info],
store=store
)

# First session: save user info
agent.invoke({
"messages": [{"role": "user", "content": "Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev"}]
})

# Second session: get user info
agent.invoke({
"messages": [{"role": "user", "content": "Get user info for user with id 'abc123'"}]
})
# Here is the user info for user with ID "abc123":
# - Name: Foo
# - Age: 25
# - Email: foo@langchain.dev

See all 42 lines

#### ​ Stream Writer

Stream custom updates from tools as they execute using `runtime.stream_writer`. This is useful for providing real-time feed for more details.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Messages\\
\\
Previous Short-term memory\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/short-term-memory

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Short-term memory

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Overview
- Usage
- In production
- Customizing agent memory
- Common patterns
- Trim messages
- Delete messages
- Summarize messages
- Access memory
- Tools
- Read short-term memory in a tool
- Write short-term memory from tools
- Prompt
- Before model
- After model

## ​ Overview

Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.Short term memory lets your application remember previous interactions within a single thread or conversation.

A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.

Conversation history is the most common form of short-term memory. Long conversations pose a challenge to today’s LLMs; a full history may not fit inside an LLM’s context window, resulting in an context loss or errors.Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get “distracted” by stale or off-topic content, all while suffering from slower response times and higher costs.Chat models accept context using messages, which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or “forget” stale information.

## ​ Usage

To add short-term memory (thread-level persistence) to an agent, you need to specify a `checkpointer` when creating an agent.

LangChain’s agent manages short-term memory as a part of your agent’s state.By storing these in the graph’s state, the agent can access the full context for a given conversation while maintaining separation between different threads.State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.

Copy

from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
"gpt-5",
tools=[get_user_info],
checkpointer=InMemorySaver(),
)

agent.invoke(
{"messages": [{"role": "user", "content": "Hi! My name is Bob."}]},
{"configurable": {"thread_id": "1"}},
)

### ​ In production

In production, use a checkpointer backed by a database:

pip install langgraph-checkpoint-postgres

from langchain.agents import create_agent

from langgraph.checkpoint.postgres import PostgresSaver

DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"
with PostgresSaver.from_conn_string(DB_URI) as checkpointer:
checkpointer.setup() # auto create tables in PostgresSql
agent = create_agent(
"gpt-5",
tools=[get_user_info],
checkpointer=checkpointer,
)

## ​ Customizing agent memory

By default, agents use `AgentState` to manage short term memory, specifically the conversation history via a `messages` key.You can extend `AgentState` to add additional fields. Custom state schemas are passed to `create_agent` using the `state_schema` parameter.

from langchain.agents import create_agent, AgentState
from langgraph.checkpoint.memory import InMemorySaver

class CustomAgentState(AgentState):
user_id: str
preferences: dict

agent = create_agent(
"gpt-5",
tools=[get_user_info],
state_schema=CustomAgentState,
checkpointer=InMemorySaver(),
)

# Custom state can be passed in invoke
result = agent.invoke(
{
"messages": [{"role": "user", "content": "Hello"}],
"user_id": "user_123",
"preferences": {"theme": "dark"}
},
{"configurable": {"thread_id": "1"}})

## ​ Common patterns

With short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:

**Trim messages** \\
\\
Remove first or last N messages (before calling LLM) **Delete messages** \\
\\
Delete messages from LangGraph state permanently **Summarize messages** \\
\\
Summarize earlier messages in the history and replace them with a summary

## Custom strategies

Custom strategies (e.g., message filtering, etc.)

This allows the agent to keep track of the conversation without exceeding the LLM’s context window.

### ​ Trim messages

Most LLMs have a maximum supported context window (denominated in tokens).One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.To trim message history in an agent, use the `@before_model` middleware decorator:

from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import before_model
from langgraph.runtime import Runtime
from langchain_core.runnables import RunnableConfig
from typing import Any

@before_model

"""Keep only the last few messages to fit context window."""
messages = state["messages"]

if len(messages) <= 3:
return None # No changes needed

first_msg = messages[0]
recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]
new_messages = [first_msg] + recent_messages

return {
"messages": [\
RemoveMessage(id=REMOVE_ALL_MESSAGES),\
*new_messages\
]
}

agent = create_agent(
your_model_here,
tools=your_tools_here,
middleware=[trim_messages],
checkpointer=InMemorySaver(),
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}

agent.invoke({"messages": "hi, my name is bob"}, config)
agent.invoke({"messages": "write a short poem about cats"}, config)
agent.invoke({"messages": "now do the same but for dogs"}, config)
final_response = agent.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
"""
================================== Ai Message ==================================

Your name is Bob. You told me that earlier.
If you'd like me to call you a nickname or use a different name, just say the word.
"""

### ​ Delete messages

You can delete messages from the graph state to manage the message history.This is useful when you want to remove specific messages or clear the entire message history.To delete messages from the graph state, you can use the `RemoveMessage`.For `RemoveMessage` to work, you need to use a state key with `add_messages` reducer.The default `AgentState` provides this.To remove specific messages:

from langchain.messages import RemoveMessage

def delete_messages(state):
messages = state["messages"]

# remove the earliest two messages
return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}

To remove **all** messages:

from langgraph.graph.message import REMOVE_ALL_MESSAGES

def delete_messages(state):
return {"messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}

When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:

- Some providers expect message history to start with a `user` message
- Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.

from langchain.messages import RemoveMessage
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import after_model
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.runtime import Runtime
from langchain_core.runnables import RunnableConfig

@after_model

"""Remove old messages to keep conversation manageable."""
messages = state["messages"]

return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}
return None

agent = create_agent(
"gpt-5-nano",
tools=[],
system_prompt="Please be concise and to the point.",
middleware=[delete_old_messages],
checkpointer=InMemorySaver(),
)

for event in agent.stream(
{"messages": [{"role": "user", "content": "hi! I'm bob"}]},
config,
stream_mode="values",
):
print([(message.type, message.content) for message in event["messages"]])

for event in agent.stream(
{"messages": [{"role": "user", "content": "what's my name?"}]},
config,
stream_mode="values",
):
print([(message.type, message.content) for message in event["messages"]])

[('human', "hi! I'm bob")]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.')]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', "what's my name?")]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', "what's my name?"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]
[('human', "what's my name?"), ('ai', 'Your name is Bob. How can I help you today, Bob?')]

### ​ Summarize messages

The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue.
Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.!SummaryTo summarize message history in an agent, use the built-in `SummarizationMiddleware`:

from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.runnables import RunnableConfig

checkpointer = InMemorySaver()

agent = create_agent(
model="gpt-4o",
tools=[],
middleware=[\
SummarizationMiddleware(\
model="gpt-4o-mini",\
trigger=("tokens", 4000),\
keep=("messages", 20)\
)\
],
checkpointer=checkpointer,
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}
agent.invoke({"messages": "hi, my name is bob"}, config)
agent.invoke({"messages": "write a short poem about cats"}, config)
agent.invoke({"messages": "now do the same but for dogs"}, config)
final_response = agent.invoke({"messages": "what's my name?"}, config)

Your name is Bob!
"""

See `SummarizationMiddleware` for more configuration options.

## ​ Access memory

You can access and modify the short-term memory (state) of an agent in several ways:

#### ​ Read short-term memory in a tool

Access short term memory (state) in a tool using the `ToolRuntime` parameter.The `tool_runtime` parameter is hidden from the tool signature (so the model doesn’t see it), but the tool can access the state through it.

from langchain.agents import create_agent, AgentState
from langchain.tools import tool, ToolRuntime

class CustomState(AgentState):
user_id: str

@tool
def get_user_info(
runtime: ToolRuntime

"""Look up user info."""
user_id = runtime.state["user_id"]
return "User is John Smith" if user_id == "user_123" else "Unknown user"

agent = create_agent(
model="gpt-5-nano",
tools=[get_user_info],
state_schema=CustomState,
)

result = agent.invoke({
"messages": "look up user information",
"user_id": "user_123"
})
print(result["messages"][-1].content)

#### ​ Write short-term memory from tools

To modify the agent’s short-term memory (state) during execution, you can return state updates directly from the tools.This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.

from langchain.tools import tool, ToolRuntime
from langchain_core.runnables import RunnableConfig
from langchain.messages import ToolMessage
from langchain.agents import create_agent, AgentState
from langgraph.types import Command
from pydantic import BaseModel

class CustomState(AgentState):
user_name: str

class CustomContext(BaseModel):
user_id: str

@tool
def update_user_info(
runtime: ToolRuntime[CustomContext, CustomState],

"""Look up and update user info."""
user_id = runtime.context.user_id
name = "John Smith" if user_id == "user_123" else "Unknown user"
return Command(update={
"user_name": name,
# update the message history
"messages": [\
ToolMessage(\
"Successfully looked up user information",\
tool_call_id=runtime.tool_call_id\
)\
]
})

@tool
def greet(
runtime: ToolRuntime[CustomContext, CustomState]

"""Use this to greet the user once you found their info."""
user_name = runtime.state.get("user_name", None)
if user_name is None:
return Command(update={
"messages": [\
ToolMessage(\
"Please call the 'update_user_info' tool it will get and update the user's name.",\
tool_call_id=runtime.tool_call_id\
)\
]
})
return f"Hello {user_name}!"

agent = create_agent(
model="gpt-5-nano",
tools=[update_user_info, greet],
state_schema=CustomState,
context_schema=CustomContext,
)

agent.invoke(
{"messages": [{"role": "user", "content": "greet the user"}]},
context=CustomContext(user_id="user_123"),
)

### ​ Prompt

Access short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.

from langchain.agents import create_agent
from typing import TypedDict
from langchain.agents.middleware import dynamic_prompt, ModelRequest

class CustomContext(TypedDict):
user_name: str

"""Get the weather in a city."""
return f"The weather in {city} is always sunny!"

@dynamic_prompt

user_name = request.runtime.context["user_name"]
system_prompt = f"You are a helpful assistant. Address the user as {user_name}."
return system_prompt

agent = create_agent(
model="gpt-5-nano",
tools=[get_weather],
middleware=[dynamic_system_prompt],
context_schema=CustomContext,
)

result = agent.invoke(
{"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
context=CustomContext(user_name="John Smith"),
)
for msg in result["messages"]:
msg.pretty_print()

Output

================================ Human Message =================================

What is the weather in SF?
================================== Ai Message ==================================
Tool Calls:
get_weather (call_WFQlOGn4b2yoJrv7cih342FG)
Call ID: call_WFQlOGn4b2yoJrv7cih342FG
Args:
city: San Francisco
================================= Tool Message =================================
Name: get_weather

The weather in San Francisco is always sunny!
================================== Ai Message ==================================

Hi John Smith, the weather in San Francisco is always sunny!

### ​ Before model

Access short term memory (state) in `@before_model` middleware to process messages before model calls.

\_\_start\_\_

before\_model

model

tools

\_\_end\_\_

from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import before_model
from langchain_core.runnables import RunnableConfig
from langgraph.runtime import Runtime
from typing import Any

agent = create_agent(
"gpt-5-nano",
tools=[],
middleware=[trim_messages],
checkpointer=InMemorySaver()
)

### ​ After model

Access short term memory (state) in `@after_model` middleware to process messages after model calls.

after\_model

from langchain.messages import RemoveMessage
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import after_model
from langgraph.runtime import Runtime

"""Remove messages containing sensitive words."""
STOP_WORDS = ["password", "secret"]
last_message = state["messages"][-1]
if any(word in last_message.content for word in STOP_WORDS):
return {"messages": [RemoveMessage(id=last_message.id)]}
return None

agent = create_agent(
model="gpt-5-nano",
tools=[],
middleware=[validate_response],
checkpointer=InMemorySaver(),
)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Tools\\
\\
Previous Streaming\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/streaming

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Streaming

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Overview
- Supported stream modes
- Agent progress
- LLM tokens
- Custom updates
- Stream multiple modes
- Common patterns
- Streaming tool calls
- Accessing completed messages
- Streaming with human-in-the-loop
- Streaming from sub-agents
- Disable streaming
- Related

LangChain implements a streaming system to surface real-time updates.Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.

## ​ Overview

LangChain’s streaming system lets you surface live feedback from agent runs to your application.What’s possible with LangChain streaming:

- **Stream agent progress** — get state updates after each agent step.
- **Stream LLM tokens** — stream language model tokens as they’re generated.
- **Stream custom updates** — emit user-defined signals (e.g., `"Fetched 10/100 records"`).
- **Stream multiple modes** — choose from `updates` (agent progress), `messages` (LLM tokens + metadata), or `custom` (arbitrary user data).

See the common patterns section below for additional end-to-end examples.

## ​ Supported stream modes

Pass one or more of the following stream modes as a list to the `stream` or `astream` methods:

| Mode | Description |
| --- | --- |
| `updates` | Streams state updates after each agent step. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |
| `messages` | Streams tuples of `(token, metadata)` from any graph nodes where an LLM is invoked. |
| `custom` | Streams custom data from inside your graph nodes using the stream writer. |

## ​ Agent progress

To stream agent progress, use the `stream` or `astream` methods with `stream_mode="updates"`. This emits an event after every agent step.For example, if you have an agent that calls a tool once, you should see the following updates:

- **LLM node**: `AIMessage` with tool call requests
- **Tool node**: `ToolMessage` with execution result
- **LLM node**: Final AI response

Streaming agent progress

Copy

from langchain.agents import create_agent

"""Get weather for a given city."""

return f"It's always sunny in {city}!"

agent = create_agent(
model="gpt-5-nano",
tools=[get_weather],
)
for chunk in agent.stream(
{"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
stream_mode="updates",
):
for step, data in chunk.items():
print(f"step: {step}")
print(f"content: {data['messages'][-1].content_blocks}")

Output

step: model
content: [{'type': 'tool_call', 'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_OW2NYNsNSKhRZpjW0wm2Aszd'}]

step: tools
content: [{'type': 'text', 'text': "It's always sunny in San Francisco!"}]

step: model
content: [{'type': 'text', 'text': 'It's always sunny in San Francisco!'}]

## ​ LLM tokens

To stream tokens as they are produced by the LLM, use `stream_mode="messages"`. Below you can see the output of the agent streaming tool calls and the final response.

Streaming LLM tokens

agent = create_agent(
model="gpt-5-nano",
tools=[get_weather],
)
for token, metadata in agent.stream(
{"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
stream_mode="messages",
):
print(f"node: {metadata['langgraph_node']}")
print(f"content: {token.content_blocks}")
print("\n")

node: model
content: [{'type': 'tool_call_chunk', 'id': 'call_vbCyBcP8VuneUzyYlSBZZsVa', 'name': 'get_weather', 'args': '', 'index': 0}]

node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '{"', 'index': 0}]

node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'city', 'index': 0}]

node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '":"', 'index': 0}]

node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': 'San', 'index': 0}]

node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': ' Francisco', 'index': 0}]

node: model
content: [{'type': 'tool_call_chunk', 'id': None, 'name': None, 'args': '"}', 'index': 0}]

node: model
content: []

node: tools
content: [{'type': 'text', 'text': "It's always sunny in San Francisco!"}]

node: model
content: [{'type': 'text', 'text': 'Here'}]

node: model
content: [{'type': 'text', 'text': ''s'}]

node: model
content: [{'type': 'text', 'text': ' what'}]

node: model
content: [{'type': 'text', 'text': ' I'}]

node: model
content: [{'type': 'text', 'text': ' got'}]

node: model
content: [{'type': 'text', 'text': ':'}]

node: model
content: [{'type': 'text', 'text': ' "'}]

node: model
content: [{'type': 'text', 'text': "It's"}]

node: model
content: [{'type': 'text', 'text': ' always'}]

node: model
content: [{'type': 'text', 'text': ' sunny'}]

node: model
content: [{'type': 'text', 'text': ' in'}]

node: model
content: [{'type': 'text', 'text': ' San'}]

node: model
content: [{'type': 'text', 'text': ' Francisco'}]

node: model
content: [{'type': 'text', 'text': '!"\n\n'}]

See all 94 lines

## ​ Custom updates

To stream updates from tools as they are executed, you can use `get_stream_writer`.

Streaming custom updates

from langchain.agents import create_agent
from langgraph.config import get_stream_writer

"""Get weather for a given city."""
writer = get_stream_writer()
# stream any arbitrary data
writer(f"Looking up data for city: {city}")
writer(f"Acquired data for city: {city}")
return f"It's always sunny in {city}!"

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[get_weather],
)

for chunk in agent.stream(
{"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
stream_mode="custom"
):
print(chunk)

Looking up data for city: San Francisco
Acquired data for city: San Francisco

If you add `get_stream_writer` inside your tool, you won’t be able to invoke the tool outside of a LangGraph execution context.

## ​ Stream multiple modes

You can specify multiple streaming modes by passing stream mode as a list: `stream_mode=["updates", "custom"]`.The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

Streaming multiple modes

"""Get weather for a given city."""
writer = get_stream_writer()
writer(f"Looking up data for city: {city}")
writer(f"Acquired data for city: {city}")
return f"It's always sunny in {city}!"

agent = create_agent(
model="gpt-5-nano",
tools=[get_weather],
)

for stream_mode, chunk in agent.stream(
{"messages": [{"role": "user", "content": "What is the weather in SF?"}]},
stream_mode=["updates", "custom"]
):
print(f"stream_mode: {stream_mode}")
print(f"content: {chunk}")
print("\n")

stream_mode: updates
content: {'model': {'messages': [AIMessage(content='', response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 132, 'total_tokens': 412, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tlgBzGEbedGYxZ0rTCz5F7OXpL7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--480c07cb-e405-4411-aa7f-0520fddeed66-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_KTNQIftMrl9vgNwEfAJMVu7r', 'type': 'tool_call'}], usage_metadata={'input_tokens': 132, 'output_tokens': 280, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}}

stream_mode: custom
content: Looking up data for city: San Francisco

stream_mode: custom
content: Acquired data for city: San Francisco

stream_mode: updates
content: {'tools': {'messages': [ToolMessage(content="It's always sunny in San Francisco!", name='get_weather', tool_call_id='call_KTNQIftMrl9vgNwEfAJMVu7r')]}}

stream_mode: updates
content: {'model': {'messages': [AIMessage(content='San Francisco weather: It's always sunny in San Francisco!\n\n', response_metadata={'token_usage': {'completion_tokens': 764, 'prompt_tokens': 168, 'total_tokens': 932, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9tljDFVki1e1haCyikBptAuXuHYG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--acbc740a-18fe-4a14-8619-da92a0d0ee90-0', usage_metadata={'input_tokens': 168, 'output_tokens': 764, 'total_tokens': 932, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}})]}}

## ​ Common patterns

Below are examples showing common use cases for streaming.

### ​ Streaming tool calls

You may want to stream both:

1. Partial JSON as tool calls are generated
2. The completed, parsed tool calls that are executed

Specifying `stream_mode="messages"` will stream incremental message chunks generated by all LLM calls in the agent. To access the completed messages with parsed tool calls:

1. If those messages are tracked in the state (as in the model node of `create_agent`), use `stream_mode=["messages", "updates"]` to access completed messages through state updates (demonstrated below).
2. If those messages are not tracked in the state, use custom updates or aggregate the chunks during the streaming loop ( next section).

Refer to the section below on streaming from sub-agents if your agent includes multiple LLMs.

from typing import Any

from langchain.agents import create_agent
from langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage

agent = create_agent("openai:gpt-5.2", tools=[get_weather])

if token.text:
print(token.text, end="|")
if token.tool_call_chunks:
print(token.tool_call_chunks)
# N.B. all content is available through token.content_blocks

if isinstance(message, AIMessage) and message.tool_calls:
print(f"Tool calls: {message.tool_calls}")
if isinstance(message, ToolMessage):
print(f"Tool response: {message.content_blocks}")

input_message = {"role": "user", "content": "What is the weather in Boston?"}
for stream_mode, data in agent.stream(
{"messages": [input_message]},
stream_mode=["messages", "updates"],
):
if stream_mode == "messages":
token, metadata = data
if isinstance(token, AIMessageChunk):
_render_message_chunk(token)
if stream_mode == "updates":
for source, update in data.items():
if source in ("model", "tools"): # `source` captures node name
_render_completed_message(update["messages"][-1])

[{'name': 'get_weather', 'args': '', 'id': 'call_D3Orjr89KgsLTZ9hTzYv7Hpf', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_D3Orjr89KgsLTZ9hTzYv7Hpf', 'type': 'tool_call'}]
Tool response: [{'type': 'text', 'text': "It's always sunny in Boston!"}]
The| weather| in| Boston| is| **|sun|ny|**|.|

See all 9 lines

#### ​ Accessing completed messages

If completed messages are tracked in an agent’s state, you can use `stream_mode=["messages", "updates"]` as demonstrated above to access completed messages during streaming.

In some cases, completed messages are not reflected in state updates. If you have access to the agent internals, you can use custom updates to access these messages during streaming. Otherwise, you can aggregate message chunks in the streaming loop (see below).Consider the below example, where we incorporate a stream writer into a simplified guardrail middleware. This middleware demonstrates tool calling to generate a structured “safe / unsafe” evaluation (one could also use structured outputs for this):

from typing import Any, Literal

from langchain.agents.middleware import after_agent, AgentState
from langgraph.runtime import Runtime
from langchain.messages import AIMessage
from langchain.chat_models import init_chat_model
from langgraph.config import get_stream_writer
from pydantic import BaseModel

class ResponseSafety(BaseModel):
"""Evaluate a response as safe or unsafe."""
evaluation: Literal["safe", "unsafe"]

safety_model = init_chat_model("openai:gpt-5.2")

@after_agent(can_jump_to=["end"])

"""Model-based guardrail: Use an LLM to evaluate response safety."""
stream_writer = get_stream_writer()
# Get the model response
if not state["messages"]:
return None

last_message = state["messages"][-1]
if not isinstance(last_message, AIMessage):
return None

# Use another model to evaluate safety
model_with_tools = safety_model.bind_tools([ResponseSafety], tool_choice="any")
result = model_with_tools.invoke(
[\
{\
"role": "system",\
"content": "Evaluate this AI response as generally safe or unsafe.",\
}\
],
{"role": "user", "content": f"AI response: {last_message.text}"},
)
stream_writer(result)

tool_call = result.tool_calls[0]
if tool_call["args"]["evaluation"] == "unsafe":
last_message.content = "I cannot provide that response. Please rephrase your request."

return None

We can then incorporate this middleware into our agent and include its custom stream events:

from langchain.agents import create_agent
from langchain.messages import AIMessageChunk, AIMessage, AnyMessage

agent = create_agent(
model="openai:gpt-5.2",
tools=[get_weather],
middleware=[safety_guardrail],
)

input_message = {"role": "user", "content": "What is the weather in Boston?"}
for stream_mode, data in agent.stream(
{"messages": [input_message]},
stream_mode=["messages", "updates", "custom"],
):
if stream_mode == "messages":
token, metadata = data
if isinstance(token, AIMessageChunk):
_render_message_chunk(token)
if stream_mode == "updates":
for source, update in data.items():
if source in ("model", "tools"):
_render_completed_message(update["messages"][-1])
if stream_mode == "custom":
# access completed message in stream
print(f"Tool calls: {data.tool_calls}")

[{'name': 'get_weather', 'args': '', 'id': 'call_je6LWgxYzuZ84mmoDalTYMJC', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_je6LWgxYzuZ84mmoDalTYMJC', 'type': 'tool_call'}]
Tool response: [{'type': 'text', 'text': "It's always sunny in Boston!"}]
The| weather| in| **|Boston|**| is| **|sun|ny|**|.|[{'name': 'ResponseSafety', 'args': '', 'id': 'call_O8VJIbOG4Q9nQF0T8ltVi58O', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'evaluation', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'unsafe', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'ResponseSafety', 'args': {'evaluation': 'unsafe'}, 'id': 'call_O8VJIbOG4Q9nQF0T8ltVi58O', 'type': 'tool_call'}]

See all 15 lines

Alternatively, if you aren’t able to add custom events to the stream, you can aggregate message chunks within the streaming loop:

input_message = {"role": "user", "content": "What is the weather in Boston?"}
full_message = None
for stream_mode, data in agent.stream(
{"messages": [input_message]},
stream_mode=["messages", "updates"],
):
if stream_mode == "messages":
token, metadata = data
if isinstance(token, AIMessageChunk):
_render_message_chunk(token)
full_message = token if full_message is None else full_message + token
if token.chunk_position == "last":
if full_message.tool_calls:
print(f"Tool calls: {full_message.tool_calls}")
full_message = None
if stream_mode == "updates":
for source, update in data.items():
if source == "tools":
_render_completed_message(update["messages"][-1])

### ​ Streaming with human-in-the-loop

To handle human-in-the-loop interrupts, we build on the above example:

1. We configure the agent with human-in-the-loop middleware and a checkpointer
2. We collect interrupts generated during the `"updates"` stream mode
3. We respond to those interrupts with a command

from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langchain.messages import AIMessage, AIMessageChunk, AnyMessage, ToolMessage
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import Command, Interrupt

checkpointer = InMemorySaver()

agent = create_agent(
"openai:gpt-5.2",
tools=[get_weather],
middleware=[\
HumanInTheLoopMiddleware(interrupt_on={"get_weather": True}),\
],
checkpointer=checkpointer,
)

interrupts = interrupt.value
for request in interrupts["action_requests"]:
print(request["description"])

input_message = {
"role": "user",
"content": (
"Can you look up the weather in Boston and San Francisco?"
),
}
config = {"configurable": {"thread_id": "some_id"}}
interrupts = []
for stream_mode, data in agent.stream(
{"messages": [input_message]},
config=config,
stream_mode=["messages", "updates"],
):
if stream_mode == "messages":
token, metadata = data
if isinstance(token, AIMessageChunk):
_render_message_chunk(token)
if stream_mode == "updates":
for source, update in data.items():
if source in ("model", "tools"):
_render_completed_message(update["messages"][-1])
if source == "__interrupt__":
interrupts.extend(update)
_render_interrupt(update[0])

[{'name': 'get_weather', 'args': '', 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"ci', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'ty": ', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"Bosto', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'n"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': 'get_weather', 'args': '', 'id': 'call_Ndb4jvWm2uMA0JDQXu37wDH6', 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"ci', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'ty": ', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"San F', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'ranc', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'isco"', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '}', 'id': None, 'index': 1, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_GOwNaQHeqMixay2qy80padfE', 'type': 'tool_call'}, {'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_Ndb4jvWm2uMA0JDQXu37wDH6', 'type': 'tool_call'}]
Tool execution requires approval

Tool: get_weather
Args: {'city': 'Boston'}
Tool execution requires approval

Tool: get_weather
Args: {'city': 'San Francisco'}

See all 21 lines

We next collect a decision for each interrupt. Importantly, the order of decisions must match the order of actions we collected.To illustrate, we will edit one tool call and accept the other:

return [\
{\
"type": "edit",\
"edited_action": {\
"name": "get_weather",\
"args": {"city": "Boston, U.K."},\
},\
}\
if "boston" in request["description"].lower()\
else {"type": "approve"}\
for request in interrupt.value["action_requests"]\
]

decisions = {}
for interrupt in interrupts:
decisions[interrupt.id] = {
"decisions": _get_interrupt_decisions(interrupt)
}

decisions

{
'a96c40474e429d661b5b32a8d86f0f3e': {
'decisions': [\
{\
'type': 'edit',\
'edited_action': {\
'name': 'get_weather',\
'args': {'city': 'Boston, U.K.'}\
}\
},\
{'type': 'approve'},\
]
}
}

We can then resume by passing a command into the same streaming loop:

interrupts = []
for stream_mode, data in agent.stream(
Command(resume=decisions),
config=config,
stream_mode=["messages", "updates"],
):
# Streaming loop is unchanged
if stream_mode == "messages":
token, metadata = data
if isinstance(token, AIMessageChunk):
_render_message_chunk(token)
if stream_mode == "updates":
for source, update in data.items():
if source in ("model", "tools"):
_render_completed_message(update["messages"][-1])
if source == "__interrupt__":
interrupts.extend(update)
_render_interrupt(update[0])

Tool response: [{'type': 'text', 'text': "It's always sunny in Boston, U.K.!"}]
Tool response: [{'type': 'text', 'text': "It's always sunny in San Francisco!"}]
-| **|Boston|**|:| It|’s| always| sunny| in| Boston|,| U|.K|.|
|-| **|San| Francisco|**|:| It|’s| always| sunny| in| San| Francisco|!|

### ​ Streaming from sub-agents

When there are multiple LLMs at any point in an agent, it’s often necessary to disambiguate the source of messages as they are generated.To do this, you can initialize any model with `tags`. These tags are then available in metadata when streaming in `"messages"` mode.Below, we update the streaming tool calls example:

1. We replace our tool with a `call_weather_agent` tool that invokes an agent internally
2. We add a string tag to this LLM and the outer “supervisor” LLM
3. We specify `subgraphs=True` when creating the stream
4. Our stream processing is identical to before, but we add logic to keep track of what LLM is active

If streaming tokens from sub-agents is not needed, you can initialize the sub-agent with a name. This name is accessible on messages generated by the sub-agent when streaming `updates`.

First we construct the agent:

from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.messages import AIMessage, AnyMessage

weather_model = init_chat_model(
"openai:gpt-5.2",
tags=["weather_sub_agent"],
)

weather_agent = create_agent(model=weather_model, tools=[get_weather])

"""Query the weather agent."""
result = weather_agent.invoke({
"messages": [{"role": "user", "content": query}]
})
return result["messages"][-1].text

supervisor_model = init_chat_model(
"openai:gpt-5.2",
tags=["supervisor"],
)

agent = create_agent(model=supervisor_model, tools=[call_weather_agent])

Next, we add logic to the streaming loop to report which agent is emitting tokens:

input_message = {"role": "user", "content": "What is the weather in Boston?"}
current_agent = None
for _, stream_mode, data in agent.stream(
{"messages": [input_message]},
stream_mode=["messages", "updates"],
subgraphs=True,
):
if stream_mode == "messages":
token, metadata = data
if tags := metadata.get("tags", []):
this_agent = tags[0]
if this_agent != current_agent:
print(f"🤖 {this_agent}: ")
current_agent = this_agent
if isinstance(token, AIMessage):
_render_message_chunk(token)
if stream_mode == "updates":
for source, update in data.items():
if source in ("model", "tools"):
_render_completed_message(update["messages"][-1])

🤖 supervisor:
[{'name': 'call_weather_agent', 'args': '', 'id': 'call_asorzUf0mB6sb7MiKfgojp7I', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'query', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' weather', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' right', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' now', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' and', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': " today's", 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': ' forecast', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'call_weather_agent', 'args': {'query': "Boston weather right now and today's forecast"}, 'id': 'call_asorzUf0mB6sb7MiKfgojp7I', 'type': 'tool_call'}]
🤖 weather_sub_agent:
[{'name': 'get_weather', 'args': '', 'id': 'call_LZ89lT8fW6w8vqck5pZeaDIx', 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '{"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'city', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '":"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': 'Boston', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
[{'name': None, 'args': '"}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]
Tool calls: [{'name': 'get_weather', 'args': {'city': 'Boston'}, 'id': 'call_LZ89lT8fW6w8vqck5pZeaDIx', 'type': 'tool_call'}]
Tool response: [{'type': 'text', 'text': "It's always sunny in Boston!"}]
Boston| weather| right| now|:| **|Sunny|**|.

|Today|’s| forecast| for| Boston|:| **|Sunny| all| day|**|.|Tool response: [{'type': 'text', 'text': 'Boston weather right now: **Sunny**.\n\nToday’s forecast for Boston: **Sunny all day**.'}]
🤖 supervisor:
Boston| weather| right| now|:| **|Sunny|**|.

|Today|’s| forecast| for| Boston|:| **|Sunny| all| day|**|.|

See all 30 lines

## ​ Disable streaming

In some applications you might need to disable streaming of individual tokens for a given model. This is useful when:

- Working with multi-agent systems to control which agents stream their output
- Mixing models that support streaming with those that do not
- Deploying to LangSmith and wanting to prevent certain model outputs from being streamed to the client

Set `streaming=False` when initializing the model.

from langchain_openai import ChatOpenAI

model = ChatOpenAI(
model="gpt-4o",
streaming=False
)

When deploying to LangSmith, set `streaming=False` on any models whose output you don’t want streamed to the client. This is configured in your graph code before deployment.

Not all chat model integrations support the `streaming` parameter. If your model doesn’t support it, use `disable_streaming=True` instead. This parameter is available on all chat models via the base class.

See the LangGraph streaming guide for more details.

## ​ Related

- Streaming with chat models — Stream tokens directly from a chat model without using an agent or graph
- Streaming with human-in-the-loop — Stream agent progress while handling interrupts for human review
- LangGraph streaming — Advanced streaming options including `values`, `debug` modes, and subgraph streaming

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Short-term memory\\
\\
Previous Structured output\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/structured-output

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Structured output

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Response Format
- Provider strategy
- Tool calling strategy
- Custom tool message content
- Error handling
- Multiple structured outputs error
- Schema validation error
- Error handling strategies

Structured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured data in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use.LangChain’s `create_agent` handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it’s captured, validated, and returned in the `'structured_response'` key of the agent’s state.

Copy

def create_agent(
...
response_format: Union[\
ToolStrategy[StructuredResponseT],\
ProviderStrategy[StructuredResponseT],\
type[StructuredResponseT],\
]

## ​ Response Format

Controls how the agent returns structured data:

- **`ToolStrategy[StructuredResponseT]`**: Uses tool calling for structured output
- **`ProviderStrategy[StructuredResponseT]`**: Uses provider-native structured output
- **`type[StructuredResponseT]`**: Schema type - automatically selects best strategy based on model capabilities
- **`None`**: No structured output

When a schema type is provided directly, LangChain automatically chooses:

- `ProviderStrategy` for models supporting native structured output (e.g. OpenAI, Anthropic, or Grok).
- `ToolStrategy` for all other models.

custom_profile = {
"structured_output": True,
# ...
}
model = init_chat_model("...", profile=custom_profile)

If tools are specified, the model must support simultaneous use of tools and structured output.

The structured response is returned in the `structured_response` key of the agent’s final state.

## ​ Provider strategy

Some model providers support structured output natively through their APIs (e.g. OpenAI, Grok, Gemini). This is the most reliable method when available.To use this strategy, configure a `ProviderStrategy`:

class ProviderStrategy(Generic[SchemaT]):
schema: type[SchemaT]
strict: bool | None = None

​

schema

required

The schema defining the structured output format. Supports:

- **Pydantic models**: `BaseModel` subclasses with field validation
- **Dataclasses**: Python dataclasses with type annotations
- **TypedDict**: Typed dictionary classes
- **JSON Schema**: Dictionary with JSON schema specification

strict

Optional boolean parameter to enable strict schema adherence. Supported by some providers (e.g., OpenAI and xAI). Defaults to `None` (disabled).

LangChain automatically uses `ProviderStrategy` when you pass a schema type directly to `create_agent.response_format` and the model supports native structured output:

Pydantic Model

Dataclass

TypedDict

JSON Schema

from pydantic import BaseModel, Field
from langchain.agents import create_agent

class ContactInfo(BaseModel):
"""Contact information for a person."""
name: str = Field(description="The name of the person")
email: str = Field(description="The email address of the person")
phone: str = Field(description="The phone number of the person")

agent = create_agent(
model="gpt-5",
response_format=ContactInfo # Auto-selects ProviderStrategy
)

result = agent.invoke({
"messages": [{"role": "user", "content": "Extract contact info from: John Doe, john@example.com, (555) 123-4567"}]
})

print(result["structured_response"])
# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')

Provider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.

If the provider natively supports structured output for your model choice, it is functionally equivalent to write `response_format=ProductReview` instead of `response_format=ProviderStrategy(ProductReview)`. In either case, if structured output is not supported, the agent will fall Tool calling strategy

For models that don’t support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models.To use this strategy, configure a `ToolStrategy`:

class ToolStrategy(Generic[SchemaT]):
schema: type[SchemaT]
tool_message_content: str | None
handle_errors: Union[\
bool,\
str,\
type[Exception],\
tuple[type[Exception], ...],\
Callable[[Exception], str],\
]

- **Pydantic models**: `BaseModel` subclasses with field validation
- **Dataclasses**: Python dataclasses with type annotations
- **TypedDict**: Typed dictionary classes
- **JSON Schema**: Dictionary with JSON schema specification
- **Union types**: Multiple schema options. The model will choose the most appropriate schema based on the context.

tool\_message\_content

Custom content for the tool message returned when structured output is generated.
If not provided, defaults to a message showing the structured response data.

handle\_errors

Error handling strategy for structured output validation failures. Defaults to `True`.

- **`True`**: Catch all errors with default error template
- **`str`**: Catch all errors with this custom message
- **`type[Exception]`**: Only catch this exception type with default message
- **`tuple[type[Exception], ...]`**: Only catch these exception types with default message
- **`Callable[[Exception], str]`**: Custom function that returns error message
- **`False`**: No retry, let exceptions propagate

Union Types

from pydantic import BaseModel, Field
from typing import Literal
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy

class ProductReview(BaseModel):
"""Analysis of a product review."""
rating: int | None = Field(description="The rating of the product", ge=1, le=5)
sentiment: Literal["positive", "negative"] = Field(description="The sentiment of the review")
key_points: list[str] = Field(description="The key points of the review. Lowercase, 1-3 words each.")

agent = create_agent(
model="gpt-5",
tools=tools,
response_format=ToolStrategy(ProductReview)
)

result = agent.invoke({
"messages": [{"role": "user", "content": "Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'"}]
})
result["structured_response"]
# ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])

### ​ Custom tool message content

The `tool_message_content` parameter allows you to customize the message that appears in the conversation history when structured output is generated:

class MeetingAction(BaseModel):
"""Action items extracted from a meeting transcript."""
task: str = Field(description="The specific task to be completed")
assignee: str = Field(description="Person responsible for the task")
priority: Literal["low", "medium", "high"] = Field(description="Priority level")

agent = create_agent(
model="gpt-5",
tools=[],
response_format=ToolStrategy(
schema=MeetingAction,
tool_message_content="Action item captured and added to meeting notes!"
)
)

agent.invoke({
"messages": [{"role": "user", "content": "From our meeting: Sarah needs to update the project timeline as soon as possible"}]
})

================================ Human Message =================================

From our meeting: Sarah needs to update the project timeline as soon as possible
================================== Ai Message ==================================
Tool Calls:
MeetingAction (call_1)
Call ID: call_1
Args:
task: Update the project timeline
assignee: Sarah
priority: high
================================= Tool Message =================================
Name: MeetingAction

Action item captured and added to meeting notes!

Without `tool_message_content`, our final `ToolMessage` would be:

================================= Tool Message =================================
Name: MeetingAction

Returning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}

### ​ Error handling

Models can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.

#### ​ Multiple structured outputs error

When a model incorrectly calls multiple structured output tools, the agent provides error feedback in a `ToolMessage` and prompts the model to retry:

from pydantic import BaseModel, Field
from typing import Union
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy

class ContactInfo(BaseModel):
name: str = Field(description="Person's name")
email: str = Field(description="Email address")

class EventDetails(BaseModel):
event_name: str = Field(description="Name of the event")
date: str = Field(description="Event date")

agent = create_agent(
model="gpt-5",
tools=[],
response_format=ToolStrategy(Union[ContactInfo, EventDetails]) # Default: handle_errors=True
)

agent.invoke({
"messages": [{"role": "user", "content": "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th"}]
})

Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th
None
================================== Ai Message ==================================
Tool Calls:
ContactInfo (call_1)
Call ID: call_1
Args:
name: John Doe
email: john@email.com
EventDetails (call_2)
Call ID: call_2
Args:
event_name: Tech Conference
date: March 15th
================================= Tool Message =================================
Name: ContactInfo

Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.
Please fix your mistakes.
================================= Tool Message =================================
Name: EventDetails

Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.
Please fix your mistakes.
================================== Ai Message ==================================
Tool Calls:
ContactInfo (call_3)
Call ID: call_3
Args:
name: John Doe
email: john@email.com
================================= Tool Message =================================
Name: ContactInfo

Returning structured response: {'name': 'John Doe', 'email': 'john@email.com'}

#### ​ Schema validation error

When structured output doesn’t match the expected schema, the agent provides specific error feedback:

from pydantic import BaseModel, Field
from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy

class ProductRating(BaseModel):
rating: int | None = Field(description="Rating from 1-5", ge=1, le=5)
comment: str = Field(description="Review comment")

agent = create_agent(
model="gpt-5",
tools=[],
response_format=ToolStrategy(ProductRating), # Default: handle_errors=True
system_prompt="You are a helpful assistant that parses product reviews. Do not make any field or value up."
)

agent.invoke({
"messages": [{"role": "user", "content": "Parse this: Amazing product, 10/10!"}]
})

Parse this: Amazing product, 10/10!
================================== Ai Message ==================================
Tool Calls:
ProductRating (call_1)
Call ID: call_1
Args:
rating: 10
comment: Amazing product
================================= Tool Message =================================
Name: ProductRating

Error: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating.rating
Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].
Please fix your mistakes.
================================== Ai Message ==================================
Tool Calls:
ProductRating (call_2)
Call ID: call_2
Args:
rating: 5
comment: Amazing product
================================= Tool Message =================================
Name: ProductRating

Returning structured response: {'rating': 5, 'comment': 'Amazing product'}

#### ​ Error handling strategies

You can customize how errors are handled using the `handle_errors` parameter:**Custom error message:**

ToolStrategy(
schema=ProductRating,
handle_errors="Please provide a valid rating between 1-5 and include a comment."
)

If `handle_errors` is a string, the agent will _always_ prompt the model to re-try with a fixed tool message:

================================= Tool Message =================================
Name: ProductRating

Please provide a valid rating between 1-5 and include a comment.

**Handle specific exceptions only:**

ToolStrategy(
schema=ProductRating,
handle_errors=ValueError # Only retry on ValueError, raise others
)

If `handle_errors` is an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised.**Handle multiple exception types:**

ToolStrategy(
schema=ProductRating,
handle_errors=(ValueError, TypeError) # Retry on ValueError and TypeError
)

If `handle_errors` is a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one of the specified types. In all other cases, the exception will be raised.**Custom error handler function:**

from langchain.agents.structured_output import StructuredOutputValidationError
from langchain.agents.structured_output import MultipleStructuredOutputsError

if isinstance(error, StructuredOutputValidationError):
return "There was an issue with the format. Try again."
elif isinstance(error, MultipleStructuredOutputsError):
return "Multiple structured outputs were returned. Pick the most relevant one."
else:
return f"Error: {str(error)}"

agent = create_agent(
model="gpt-5",
tools=[],
response_format=ToolStrategy(
schema=Union[ContactInfo, EventDetails],
handle_errors=custom_error_handler
) # Default: handle_errors=True
)

result = agent.invoke({
"messages": [{"role": "user", "content": "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th"}]
})

for msg in result['messages']:
# If message is actually a ToolMessage object (not a dict), check its class name
if type(msg).__name__ == "ToolMessage":
print(msg.content)
# If message is a dictionary or you want a fallback
elif isinstance(msg, dict) and msg.get('tool_call_id'):
print(msg['content'])

On `StructuredOutputValidationError`:

================================= Tool Message =================================
Name: ToolStrategy

There was an issue with the format. Try again.

On `MultipleStructuredOutputsError`:

Multiple structured outputs were returned. Pick the most relevant one.

On other errors:

**No error handling:**

response_format = ToolStrategy(
schema=ProductRating,
handle_errors=False # All errors raised
)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Streaming\\
\\
Previous Overview\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/middleware/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Middleware

Overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- The agent loop
- Additional resources

Middleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following:

- Tracking agent behavior with logging, analytics, and debugging.
- Transforming prompts, tool selection, and output formatting.
- Adding retries, fallbacks, and early termination logic.
- Applying rate limits, guardrails, and PII detection.

Add middleware by passing them to `create_agent`:

Copy

from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[\
SummarizationMiddleware(...),\
HumanInTheLoopMiddleware(...)\
],
)

## ​ The agent loop

The core agent loop involves calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:!Core agent loop diagramMiddleware exposes hooks before and after each of those steps:!Middleware flow diagram

## ​ Additional resources

**Built-in middleware** \\
\\
Explore built-in middleware for common use cases. **Custom middleware** \\
\\
Build your own middleware with hooks and decorators. **Middleware API reference** \\
\\
Complete API reference for middleware. **Testing agents** \\
\\
Test your agents with LangSmith.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Structured output\\
\\
Previous Built-in middleware\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/middleware/built-in

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Middleware

Built-in middleware

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Provider-agnostic middleware
- Summarization
- Human-in-the-loop
- Model call limit
- Tool call limit
- Model fallback
- PII detection
- Custom PII types
- To-do list
- LLM tool selector
- Tool retry
- Model retry
- LLM tool emulator
- Context editing
- Shell tool
- File search
- Provider-specific middleware

LangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.

## ​ Provider-agnostic middleware

The following middleware work with any LLM provider:

| Middleware | Description |
| --- | --- |
| Summarization | Automatically summarize conversation history when approaching token limits. |
| Human-in-the-loop | Pause execution for human approval of tool calls. |
| Model call limit | Limit the number of model calls to prevent excessive costs. |
| Tool call limit | Control tool execution by limiting call counts. |
| Model fallback | Automatically fall | Detect and handle Personally Identifiable Information (PII). |
| To-do list | Equip agents with task planning and tracking capabilities. |
| LLM tool selector | Use an LLM to select relevant tools before calling main model. |
| Tool retry | Automatically retry failed tool calls with exponential backoff. |
| Model retry | Automatically retry failed model calls with exponential backoff. |
| LLM tool emulator | Emulate tool execution using an LLM for testing purposes. |
| Context editing | Manage conversation context by trimming or clearing tool uses. |
| Shell tool | Expose a persistent shell session to agents for command execution. |
| File search | Provide Glob and Grep search tools over filesystem files. |

### ​ Summarization

Automatically summarize conversation history when approaching token limits, preserving recent messages while compressing older context. Summarization is useful for the following:

- Long-running conversations that exceed context windows.
- Multi-turn dialogues with extensive history.
- Applications where preserving full conversation context matters.

**API reference:** `SummarizationMiddleware`

Copy

from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
model="gpt-4o",
tools=[your_weather_tool, your_calculator_tool],
middleware=[\
SummarizationMiddleware(\
model="gpt-4o-mini",\
trigger=("tokens", 4000),\
keep=("messages", 20),\
),\
],
)

Configuration options

from langchain.chat_models import init_chat_model

custom_profile = {
"max_input_tokens": 100_000,
# ...
}
model = init_chat_model("gpt-4o", profile=custom_profile)

​

model

string \| BaseChatModel

required

Model for generating summaries. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance. See `init_chat_model` for more information.

trigger

ContextSize \| list\[ContextSize\] \| None

Condition(s) for triggering summarization. Can be:

- A single `ContextSize` tuple (specified condition must be met)
- A list of `ContextSize` tuples (any condition must be met - OR logic)

Condition should be one of the following:

- `fraction` (float): Fraction of model’s context size (0-1)
- `tokens` (int): Absolute token count
- `messages` (int): Message count

At least one condition must be specified. If not provided, summarization will not trigger automatically.See the API reference for `ContextSize` for more information.

keep

ContextSize

default:"('messages', 20)"

How much context to preserve after summarization. Specify exactly one of:

- `fraction` (float): Fraction of model’s context size to keep (0-1)
- `tokens` (int): Absolute token count to keep
- `messages` (int): Number of recent messages to keep

See the API reference for `ContextSize` for more information.

token\_counter

function

Custom token counting function. Defaults to character-based counting.

summary\_prompt

string

Custom prompt template for summarization. Uses built-in template if not specified. The template should include `{messages}` placeholder where conversation history will be inserted.

trim\_tokens\_to\_summarize

number

default:"4000"

Maximum number of tokens to include when generating the summary. Messages will be trimmed to fit this limit before summarization.

summary\_prefix

Prefix to add to the summary message. If not provided, a default prefix is used.

max\_tokens\_before\_summary

deprecated

**Deprecated:** Use `trigger: {"tokens": value}` instead. Token threshold for triggering summarization.

messages\_to\_keep

**Deprecated:** Use `keep: {"messages": value}` instead. Recent messages to preserve.

Full example

The summarization middleware monitors message token counts and automatically summarizes older messages when thresholds are reached.**Trigger conditions** control when summarization runs:

- Single condition object (specified must be met)
- Array of conditions (any condition must be met - OR logic)
- Each condition can use `fraction` (of model’s context size), `tokens` (absolute count), or `messages` (message count)

**Keep condition** control how much context to preserve (specify exactly one):

- `fraction` \- Fraction of model’s context size to keep
- `tokens` \- Absolute token count to keep
- `messages` \- Number of recent messages to keep

agent2 = create_agent(
model="gpt-4o",
tools=[your_weather_tool, your_calculator_tool],
middleware=[\
SummarizationMiddleware(\
model="gpt-4o-mini",\
trigger=[\
("tokens", 3000),\
("messages", 6),\
],\
keep=("messages", 20),\
),\
],
)

# Using fractional limits
agent3 = create_agent(
model="gpt-4o",
tools=[your_weather_tool, your_calculator_tool],
middleware=[\
SummarizationMiddleware(\
model="gpt-4o-mini",\
trigger=("fraction", 0.8),\
keep=("fraction", 0.3),\
),\
],
)

### ​ Human-in-the-loop

Pause agent execution for human approval, editing, or rejection of tool calls before they execute. Human-in-the-loop is useful for the following:

- High-stakes operations requiring human approval (e.g. database writes, financial transactions).
- Compliance workflows where human oversight is mandatory.
- Long-running conversations where human feedback guides the agent.

**API reference:** `HumanInTheLoopMiddleware`

Human-in-the-loop middleware requires a checkpointer to maintain state across interruptions.

from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver

"""Mock function to read an email by its ID."""
return f"Email content for ID: {email_id}"

"""Mock function to send an email."""
return f"Email sent to {recipient} with subject '{subject}'"

agent = create_agent(
model="gpt-4o",
tools=[your_read_email_tool, your_send_email_tool],
checkpointer=InMemorySaver(),
middleware=[\
HumanInTheLoopMiddleware(\
interrupt_on={\
"your_send_email_tool": {\
"allowed_decisions": ["approve", "edit", "reject"],\
},\
"your_read_email_tool": False,\
}\
),\
],
)

For complete examples, configuration options, and integration patterns, see the Human-in-the-loop documentation.

Watch this video guide demonstrating Human-in-the-loop middleware behavior.

### ​ Model call limit

Limit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following:

- Preventing runaway agents from making too many API calls.
- Enforcing cost controls on production deployments.
- Testing agent behavior within specific call budgets.

**API reference:** `ModelCallLimitMiddleware`

from langchain.agents import create_agent
from langchain.agents.middleware import ModelCallLimitMiddleware
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
model="gpt-4o",
checkpointer=InMemorySaver(), # Required for thread limiting
tools=[],
middleware=[\
ModelCallLimitMiddleware(\
thread_limit=10,\
run_limit=5,\
exit_behavior="end",\
),\
],
)

Watch this video guide demonstrating Model Call Limit middleware behavior.

thread\_limit

Maximum model calls across all runs in a thread. Defaults to no limit.

run\_limit

Maximum model calls per single invocation. Defaults to no limit.

exit\_behavior

default:"end"

Behavior when limit is reached. Options: `'end'` (graceful termination) or `'error'` (raise exception)

### ​ Tool call limit

Control agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following:

- Preventing excessive calls to expensive external APIs.
- Limiting web searches or database queries.
- Enforcing rate limits on specific tool usage.
- Protecting against runaway agent loops.

**API reference:** `ToolCallLimitMiddleware`

from langchain.agents import create_agent
from langchain.agents.middleware import ToolCallLimitMiddleware

agent = create_agent(
model="gpt-4o",
tools=[search_tool, database_tool],
middleware=[\
# Global limit\
ToolCallLimitMiddleware(thread_limit=20, run_limit=10),\
# Tool-specific limit\
ToolCallLimitMiddleware(\
tool_name="search",\
thread_limit=5,\
run_limit=3,\
),\
],
)

Watch this video guide demonstrating Tool Call Limit middleware behavior.

tool\_name

Name of specific tool to limit. If not provided, limits apply to **all tools globally**.

Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. `None` means no thread limit.

Maximum tool calls per single invocation (one user message → response cycle). Resets with each new user message. `None` means no run limit.**Note:** At least one of `thread_limit` or `run_limit` must be specified.

default:"continue"

Behavior when limit is reached:

- `'continue'` (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages.
- `'error'` \- Raise a `ToolCallLimitExceededError` exception, stopping execution immediately
- `'end'` \- Stop execution immediately with a `ToolMessage` and AI message for the exceeded tool call. Only works when limiting a single tool; raises `NotImplementedError` if other tools have pending calls.

Specify limits with:

- **Thread limit** \- Max calls across all runs in a conversation (requires checkpointer)
- **Run limit** \- Max calls per single invocation (resets each turn)

Exit behaviors:

- `'continue'` (default) - Block exceeded calls with error messages, agent continues
- `'error'` \- Raise exception immediately
- `'end'` \- Stop with ToolMessage + AI message (single-tool scenarios only)

global_limiter = ToolCallLimitMiddleware(thread_limit=20, run_limit=10)
search_limiter = ToolCallLimitMiddleware(tool_name="search", thread_limit=5, run_limit=3)
database_limiter = ToolCallLimitMiddleware(tool_name="query_database", thread_limit=10)
strict_limiter = ToolCallLimitMiddleware(tool_name="scrape_webpage", run_limit=2, exit_behavior="error")

agent = create_agent(
model="gpt-4o",
tools=[search_tool, database_tool, scraper_tool],
middleware=[global_limiter, search_limiter, database_limiter, strict_limiter],
)

### ​ Model fallback

Automatically fall

from langchain.agents import create_agent
from langchain.agents.middleware import ModelFallbackMiddleware

agent = create_agent(
model="gpt-4o",
tools=[],
middleware=[\
ModelFallbackMiddleware(\
"gpt-4o-mini",\
"claude-3-5-sonnet-20241022",\
),\
],
)

Watch this video guide demonstrating Model Fallback middleware behavior.

first\_model

First fallback model to try when the primary model fails. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance.

\*additional\_models

Additional fallback models to try in order if previous models fail

### ​ PII detection

Detect and handle Personally Identifiable Information (PII) in conversations using configurable strategies. PII detection is useful for the following:

- Healthcare and financial applications with compliance requirements.
- Customer service agents that need to sanitize logs.
- Any application handling sensitive user data.

**API reference:** `PIIMiddleware`

from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware

agent = create_agent(
model="gpt-4o",
tools=[],
middleware=[\
PIIMiddleware("email", strategy="redact", apply_to_input=True),\
PIIMiddleware("credit_card", strategy="mask", apply_to_input=True),\
],
)

#### ​ Custom PII types

You can create custom PII types by providing a `detector` parameter. This allows you to detect patterns specific to your use case beyond the built-in types.**Three ways to create custom detectors:**

1. **Regex pattern string** \- Simple pattern matching
2. **Custom function** \- Complex detection logic with validation

from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware
import re

# Method 1: Regex pattern string
agent1 = create_agent(
model="gpt-4o",
tools=[],
middleware=[\
PIIMiddleware(\
"api_key",\
detector=r"sk-[a-zA-Z0-9]{32}",\
strategy="block",\
),\
],
)

# Method 2: Compiled regex pattern
agent2 = create_agent(
model="gpt-4o",
tools=[],
middleware=[\
PIIMiddleware(\
"phone_number",\
detector=re.compile(r"\+?\d{1,3}[\s.-]?\d{3,4}[\s.-]?\d{4}"),\
strategy="mask",\
),\
],
)

# Method 3: Custom detector function

"""Detect SSN with validation.

Returns a list of dictionaries with 'text', 'start', and 'end' keys.
"""
import re
matches = []
pattern = r"\d{3}-\d{2}-\d{4}"
for match in re.finditer(pattern, content):
ssn = match.group(0)
# Validate: first 3 digits shouldn't be 000, 666, or 900-999
first_three = int(ssn[:3])
if first_three not in [0, 666] and not (900 <= first_three <= 999):
matches.append({
"text": ssn,
"start": match.start(),
"end": match.end(),
})
return matches

agent3 = create_agent(
model="gpt-4o",
tools=[],
middleware=[\
PIIMiddleware(\
"ssn",\
detector=detect_ssn,\
strategy="hash",\
),\
],
)

**Custom detector function signature:**The detector function must accept a string (content) and return matches:Returns a list of dictionaries with `text`, `start`, and `end` keys:

return [\
{"text": "matched_text", "start": 0, "end": 12},\
# ... more matches\
]

For custom detectors:

- Use regex strings for simple patterns
- Use RegExp objects when you need flags (e.g., case-insensitive matching)
- Use custom functions when you need validation logic beyond pattern matching
- Custom functions give you full control over detection logic and can implement complex validation rules

pii\_type

Type of PII to detect. Can be a built-in type (`email`, `credit_card`, `ip`, `mac_address`, `url`) or a custom type name.

strategy

default:"redact"

How to handle detected PII. Options:

- `'block'` \- Raise exception when detected
- `'redact'` \- Replace with `[REDACTED_{PII_TYPE}]`
- `'mask'` \- Partially mask (e.g., `****-****-****-1234`)
- `'hash'` \- Replace with deterministic hash

detector

function \| regex

Custom detector function or regex pattern. If not provided, uses built-in detector for the PII type.

apply\_to\_input

boolean

default:"True"

Check user messages before model call

apply\_to\_output

default:"False"

Check AI messages after model call

apply\_to\_tool\_results

Check tool result messages after execution

### ​ To-do list

Equip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:

- Complex multi-step tasks requiring coordination across multiple tools.
- Long-running operations where progress visibility is important.

This middleware automatically provides agents with a `write_todos` tool and system prompts to guide effective task planning.

**API reference:** `TodoListMiddleware`

from langchain.agents import create_agent
from langchain.agents.middleware import TodoListMiddleware

agent = create_agent(
model="gpt-4o",
tools=[read_file, write_file, run_tests],
middleware=[TodoListMiddleware()],
)

Watch this video guide demonstrating To-do List middleware behavior.

system\_prompt

Custom system prompt for guiding todo usage. Uses built-in prompt if not specified.

tool\_description

Custom description for the `write_todos` tool. Uses built-in description if not specified.

### ​ LLM tool selector

Use an LLM to intelligently select relevant tools before calling the main model. LLM tool selectors are useful for the following:

- Agents with many tools (10+) where most aren’t relevant per query.
- Reducing token usage by filtering irrelevant tools.
- Improving model focus and accuracy.

This middleware uses structured output to ask an LLM which tools are most relevant for the current query. The structured output schema defines the available tool names and descriptions. Model providers often add this structured output information to the system prompt behind the scenes.**API reference:** `LLMToolSelectorMiddleware`

from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolSelectorMiddleware

agent = create_agent(
model="gpt-4o",
tools=[tool1, tool2, tool3, tool4, tool5, ...],
middleware=[\
LLMToolSelectorMiddleware(\
model="gpt-4o-mini",\
max_tools=3,\
always_include=["search"],\
),\
],
)

Model for tool selection. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance. See `init_chat_model` for more information.Defaults to the agent’s main model.

Instructions for the selection model. Uses built-in prompt if not specified.

max\_tools

Maximum number of tools to select. If the model selects more, only the first max\_tools will be used. No limit if not specified.

always\_include

list\[string\]

Tool names to always include regardless of selection. These do not count against the max\_tools limit.

### ​ Tool retry

Automatically retry failed tool calls with configurable exponential backoff. Tool retry is useful for the following:

- Handling transient failures in external API calls.
- Improving reliability of network-dependent tools.
- Building resilient agents that gracefully handle temporary errors.

**API reference:** `ToolRetryMiddleware`

from langchain.agents import create_agent
from langchain.agents.middleware import ToolRetryMiddleware

agent = create_agent(
model="gpt-4o",
tools=[search_tool, database_tool],
middleware=[\
ToolRetryMiddleware(\
max_retries=3,\
backoff_factor=2.0,\
initial_delay=1.0,\
),\
],
)

max\_retries

default:"2"

Maximum number of retry attempts after the initial call (3 total attempts with default)

tools

list\[BaseTool \| str\]

Optional list of tools or tool names to apply retry logic to. If `None`, applies to all tools.

retry\_on

tuple\[type\[Exception\], ...\] \| callable

default:"(Exception,)"

Either a tuple of exception types to retry on, or a callable that takes an exception and returns `True` if it should be retried.

on\_failure

string \| callable

default:"return\_message"

Behavior when all retries are exhausted. Options:

- `'return_message'` \- Return a `ToolMessage` with error details (allows LLM to handle failure)
- `'raise'` \- Re-raise the exception (stops agent execution)
- Custom callable - Function that takes the exception and returns a string for the `ToolMessage` content

backoff\_factor

default:"2.0"

Multiplier for exponential backoff. Each retry waits `initial_delay * (backoff_factor ** retry_number)` seconds. Set to `0.0` for constant delay.

initial\_delay

default:"1.0"

Initial delay in seconds before first retry

max\_delay

default:"60.0"

Maximum delay in seconds between retries (caps exponential backoff growth)

jitter

default:"true"

Whether to add random jitter (`±25%`) to delay to avoid thundering herd

The middleware automatically retries failed tool calls with exponential backoff.**Key configuration:**

- `max_retries` \- Number of retry attempts (default: 2)
- `backoff_factor` \- Multiplier for exponential backoff (default: 2.0)
- `initial_delay` \- Starting delay in seconds (default: 1.0)
- `max_delay` \- Cap on delay growth (default: 60.0)
- `jitter` \- Add random variation (default: True)

**Failure handling:**

- `on_failure='return_message'` \- Return error message
- `on_failure='raise'` \- Re-raise exception
- Custom function - Function returning error message

agent = create_agent(
model="gpt-4o",
tools=[search_tool, database_tool, api_tool],
middleware=[\
ToolRetryMiddleware(\
max_retries=3,\
backoff_factor=2.0,\
initial_delay=1.0,\
max_delay=60.0,\
jitter=True,\
tools=["api_tool"],\
retry_on=(ConnectionError, TimeoutError),\
on_failure="continue",\
),\
],
)

### ​ Model retry

Automatically retry failed model calls with configurable exponential backoff. Model retry is useful for the following:

- Handling transient failures in model API calls.
- Improving reliability of network-dependent model requests.
- Building resilient agents that gracefully handle temporary model errors.

**API reference:** `ModelRetryMiddleware`

from langchain.agents import create_agent
from langchain.agents.middleware import ModelRetryMiddleware

agent = create_agent(
model="gpt-4o",
tools=[search_tool, database_tool],
middleware=[\
ModelRetryMiddleware(\
max_retries=3,\
backoff_factor=2.0,\
initial_delay=1.0,\
),\
],
)

- `'continue'` (default) - Return an `AIMessage` with error details, allowing the agent to potentially handle the failure gracefully
- `'error'` \- Re-raise the exception (stops agent execution)
- Custom callable - Function that takes the exception and returns a string for the `AIMessage` content

The middleware automatically retries failed model calls with exponential backoff.

# Basic usage with default settings (2 retries, exponential backoff)
agent = create_agent(
model="gpt-4o",
tools=[search_tool],
middleware=[ModelRetryMiddleware()],
)

# Custom exception filtering
class TimeoutError(Exception):
"""Custom exception for timeout errors."""
pass

class ConnectionError(Exception):
"""Custom exception for connection errors."""
pass

# Retry specific exceptions only
retry = ModelRetryMiddleware(
max_retries=4,
retry_on=(TimeoutError, ConnectionError),
backoff_factor=1.5,
)

# Only retry on rate limit errors
if isinstance(error, TimeoutError):
return True
# Or check for specific HTTP status codes
if hasattr(error, "status_code"):
return error.status_code in (429, 503)
return False

retry_with_filter = ModelRetryMiddleware(
max_retries=3,
retry_on=should_retry,
)

# Return error message instead of raising
retry_continue = ModelRetryMiddleware(
max_retries=4,
on_failure="continue", # Return AIMessage with error instead of raising
)

# Custom error message formatting

return f"Model call failed: {error}. Please try again later."

retry_with_formatter = ModelRetryMiddleware(
max_retries=4,
on_failure=format_error,
)

# Constant backoff (no exponential growth)
constant_backoff = ModelRetryMiddleware(
max_retries=5,
backoff_factor=0.0, # No exponential growth
initial_delay=2.0, # Always wait 2 seconds
)

# Raise exception on failure
strict_retry = ModelRetryMiddleware(
max_retries=2,
on_failure="error", # Re-raise exception instead of returning message
)

### ​ LLM tool emulator

Emulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses. LLM tool emulators are useful for the following:

- Testing agent behavior without executing real tools.
- Developing agents when external tools are unavailable or expensive.
- Prototyping agent workflows before implementing actual tools.

**API reference:** `LLMToolEmulator`

from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolEmulator

agent = create_agent(
model="gpt-4o",
tools=[get_weather, search_database, send_email],
middleware=[\
LLMToolEmulator(), # Emulate all tools\
],
)

list\[str \| BaseTool\]

List of tool names (str) or BaseTool instances to emulate. If `None` (default), ALL tools will be emulated. If empty list `[]`, no tools will be emulated. If array with tool names/instances, only those tools will be emulated.

Model to use for generating emulated tool responses. Can be a model identifier string (e.g., `'anthropic:claude-sonnet-4-5-20250929'`) or a `BaseChatModel` instance. Defaults to the agent’s model if not specified. See `init_chat_model` for more information.

The middleware uses an LLM to generate plausible responses for tool calls instead of executing the actual tools.

from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolEmulator
from langchain.tools import tool

@tool

"""Get the current weather for a location."""
return f"Weather in {location}"

"""Send an email."""
return "Email sent"

# Emulate all tools (default behavior)
agent = create_agent(
model="gpt-4o",
tools=[get_weather, send_email],
middleware=[LLMToolEmulator()],
)

# Emulate specific tools only
agent2 = create_agent(
model="gpt-4o",
tools=[get_weather, send_email],
middleware=[LLMToolEmulator(tools=["get_weather"])],
)

# Use custom model for emulation
agent4 = create_agent(
model="gpt-4o",
tools=[get_weather, send_email],
middleware=[LLMToolEmulator(model="claude-sonnet-4-5-20250929")],
)

### ​ Context editing

Manage conversation context by clearing older tool call outputs when token limits are reached, while preserving recent results. This helps keep context windows manageable in long conversations with many tool calls. Context editing is useful for the following:

- Long conversations with many tool calls that exceed token limits
- Reducing token costs by removing older tool outputs that are no longer relevant
- Maintaining only the most recent N tool results in context

**API reference:** `ContextEditingMiddleware`, `ClearToolUsesEdit`

from langchain.agents import create_agent
from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit

agent = create_agent(
model="gpt-4o",
tools=[],
middleware=[\
ContextEditingMiddleware(\
edits=[\
ClearToolUsesEdit(\
trigger=100000,\
keep=3,\
),\
],\
),\
],
)

edits

list\[ContextEdit\]

default:"\[ClearToolUsesEdit()\]"

List of `ContextEdit` strategies to apply

token\_count\_method

default:"approximate"

Token counting method. Options: `'approximate'` or `'model'`

**`ClearToolUsesEdit` options:**

default:"100000"

Token count that triggers the edit. When the conversation exceeds this token count, older tool outputs will be cleared.

clear\_at\_least

default:"0"

Minimum number of tokens to reclaim when the edit runs. If set to 0, clears as much as needed.

default:"3"

Number of most recent tool results that must be preserved. These will never be cleared.

clear\_tool\_inputs

Whether to clear the originating tool call parameters on the AI message. When `True`, tool call arguments are replaced with empty objects.

exclude\_tools

default:"()"

List of tool names to exclude from clearing. These tools will never have their outputs cleared.

placeholder

default:"\[cleared\]"

Placeholder text inserted for cleared tool outputs. This replaces the original tool message content.

The middleware applies context editing strategies when token limits are reached. The most common strategy is `ClearToolUsesEdit`, which clears older tool results while preserving recent ones.**How it works:**

1. Monitor token count in conversation
2. When threshold is reached, clear older tool outputs
3. Keep most recent N tool results
4. Optionally preserve tool call arguments for context

agent = create_agent(
model="gpt-4o",
tools=[search_tool, your_calculator_tool, database_tool],
middleware=[\
ContextEditingMiddleware(\
edits=[\
ClearToolUsesEdit(\
trigger=2000,\
keep=3,\
clear_tool_inputs=False,\
exclude_tools=[],\
placeholder="[cleared]",\
),\
],\
),\
],
)

### ​ Shell tool

Expose a persistent shell session to agents for command execution. Shell tool middleware is useful for the following:

- Agents that need to execute system commands
- Development and deployment automation tasks
- Testing and validation workflows
- File system operations and script execution

**Security consideration**: Use appropriate execution policies (`HostExecutionPolicy`, `DockerExecutionPolicy`, or `CodexSandboxExecutionPolicy`) to match your deployment’s security requirements.

**Limitation**: Persistent shell sessions do not currently work with interrupts (human-in-the-loop). We anticipate adding support for this in the future.

**API reference:** `ShellToolMiddleware`

from langchain.agents import create_agent
from langchain.agents.middleware import (
ShellToolMiddleware,
HostExecutionPolicy,
)

agent = create_agent(
model="gpt-4o",
tools=[search_tool],
middleware=[\
ShellToolMiddleware(\
workspace_root="/workspace",\
execution_policy=HostExecutionPolicy(),\
),\
],
)

workspace\_root

str \| Path \| None

Base directory for the shell session. If omitted, a temporary directory is created when the agent starts and removed when it ends.

startup\_commands

tuple\[str, ...\] \| list\[str\] \| str \| None

Optional commands executed sequentially after the session starts

shutdown\_commands

Optional commands executed before the session shuts down

execution\_policy

BaseExecutionPolicy \| None

Execution policy controlling timeouts, output limits, and resource configuration. Options:

- `HostExecutionPolicy` \- Full host access (default); best for trusted environments where the agent already runs inside a container or VM
- `DockerExecutionPolicy` \- Launches a separate Docker container for each agent run, providing harder isolation
- `CodexSandboxExecutionPolicy` \- Reuses the Codex CLI sandbox for additional syscall/filesystem restrictions

redaction\_rules

tuple\[RedactionRule, ...\] \| list\[RedactionRule\] \| None

Optional redaction rules to sanitize command output before returning it to the model

str \| None

Optional override for the registered shell tool description

shell\_command

Sequence\[str\] \| str \| None

Optional shell executable (string) or argument sequence used to launch the persistent session. Defaults to `/bin/bash`.

env

Mapping\[str, Any\] \| None

Optional environment variables to supply to the shell session. Values are coerced to strings before command execution.

The middleware provides a single persistent shell session that agents can use to execute commands sequentially.**Execution policies:**

- `HostExecutionPolicy` (default) - Native execution with full host access
- `DockerExecutionPolicy` \- Isolated Docker container execution
- `CodexSandboxExecutionPolicy` \- Sandboxed execution via Codex CLI

from langchain.agents import create_agent
from langchain.agents.middleware import (
ShellToolMiddleware,
HostExecutionPolicy,
DockerExecutionPolicy,
RedactionRule,
)

# Basic shell tool with host execution

# Docker isolation with startup commands
agent_docker = create_agent(
model="gpt-4o",
tools=[],
middleware=[\
ShellToolMiddleware(\
workspace_root="/workspace",\
startup_commands=["pip install requests", "export PYTHONPATH=/workspace"],\
execution_policy=DockerExecutionPolicy(\
image="python:3.11-slim",\
command_timeout=60.0,\
),\
),\
],
)

# With output redaction
agent_redacted = create_agent(
model="gpt-4o",
tools=[],
middleware=[\
ShellToolMiddleware(\
workspace_root="/workspace",\
redaction_rules=[\
RedactionRule(pii_type="api_key", detector=r"sk-[a-zA-Z0-9]{32}"),\
],\
),\
],
)

### ​ File search

Provide Glob and Grep search tools over a filesystem. File search middleware is useful for the following:

- Code exploration and analysis
- Finding files by name patterns
- Searching code content with regex
- Large codebases where file discovery is needed

**API reference:** `FilesystemFileSearchMiddleware`

from langchain.agents import create_agent
from langchain.agents.middleware import FilesystemFileSearchMiddleware

agent = create_agent(
model="gpt-4o",
tools=[],
middleware=[\
FilesystemFileSearchMiddleware(\
root_path="/workspace",\
use_ripgrep=True,\
),\
],
)

root\_path

str

Root directory to search. All file operations are relative to this path.

use\_ripgrep

bool

Whether to use ripgrep for search. Falls

max\_file\_size\_mb

int

default:"10"

Maximum file size to search in MB. Files larger than this are skipped.

The middleware adds two search tools to agents:**Glob tool** \- Fast file pattern matching:

- Supports patterns like `**/*.py`, `src/**/*.ts`
- Returns matching file paths sorted by modification time

**Grep tool** \- Content search with regex:

- Full regex syntax support
- Filter by file patterns with `include` parameter
- Three output modes: `files_with_matches`, `content`, `count`

from langchain.agents import create_agent
from langchain.agents.middleware import FilesystemFileSearchMiddleware
from langchain.messages import HumanMessage

agent = create_agent(
model="gpt-4o",
tools=[],
middleware=[\
FilesystemFileSearchMiddleware(\
root_path="/workspace",\
use_ripgrep=True,\
max_file_size_mb=10,\
),\
],
)

# Agent can now use glob_search and grep_search tools
result = agent.invoke({
"messages": [HumanMessage("Find all Python files containing 'async def'")]
})

# The agent will use:
# 1. glob_search(pattern="**/*.py") to find Python files
# 2. grep_search(pattern="async def", include="*.py") to find async functions

## ​ Provider-specific middleware

These middleware are optimized for specific LLM providers. See each provider’s documentation for full details and examples.

**Anthropic** \\
\\
Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models. **OpenAI** \\
\\
Content moderation middleware for OpenAI models.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Overview\\
\\
Previous Custom middleware\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/middleware/custom

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Middleware

Custom middleware

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Hooks
- Node-style hooks
- Wrap-style hooks
- Create middleware
- Decorator-based middleware
- Class-based middleware
- Custom state schema
- Execution order
- Agent jumps
- Best practices
- Examples
- Dynamic model selection
- Tool call monitoring
- Dynamically selecting tools
- Working with system messages
- Additional resources

Build custom middleware by implementing hooks that run at specific points in the agent execution flow.

## ​ Hooks

Middleware provides two styles of hooks to intercept agent execution:

**Node-style hooks** \\
\\
Run sequentially at specific execution points. **Wrap-style hooks** \\
\\
Run around each model or tool call.

### ​ Node-style hooks

Run sequentially at specific execution points. Use for logging, validation, and state updates.**Available hooks:**

- `before_agent` \- Before agent starts (once per invocation)
- `before_model` \- Before each model call
- `after_model` \- After each model response
- `after_agent` \- After agent completes (once per invocation)

**Example:**

- Decorator

- Class

Copy

from langchain.agents.middleware import before_model, after_model, AgentState
from langchain.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any

@before_model(can_jump_to=["end"])

return {
"messages": [AIMessage("Conversation limit reached.")],
"jump_to": "end"
}
return None

@after_model

print(f"Model returned: {state['messages'][-1].content}")
return None

from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
from langchain.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any

class MessageLimitMiddleware(AgentMiddleware):
def __init__(self, max_messages: int = 50):
super().__init__()
self.max_messages = max_messages

@hook_config(can_jump_to=["end"])

if len(state["messages"]) == self.max_messages:
return {
"messages": [AIMessage("Conversation limit reached.")],
"jump_to": "end"
}
return None

### ​ Wrap-style hooks

Intercept execution and control when the handler is called. Use for retries, caching, and transformation.You decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).**Available hooks:**

- `wrap_model_call` \- Around each model call
- `wrap_tool_call` \- Around each tool call

from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable

@wrap_model_call
def retry_model(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

for attempt in range(3):
try:
return handler(request)
except Exception as e:
if attempt == 2:
raise
print(f"Retry {attempt + 1}/3 after error: {e}")

from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from typing import Callable

class RetryMiddleware(AgentMiddleware):
def __init__(self, max_retries: int = 3):
super().__init__()
self.max_retries = max_retries

def wrap_model_call(
self,
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

for attempt in range(self.max_retries):
try:
return handler(request)
except Exception as e:
if attempt == self.max_retries - 1:
raise
print(f"Retry {attempt + 1}/{self.max_retries} after error: {e}")

## ​ Create middleware

You can create middleware in two ways:

**Decorator-based middleware** \\
\\
Quick and simple for single-hook middleware. Use decorators to wrap individual functions. **Class-based middleware** \\
\\
More powerful for complex middleware with multiple hooks or configuration.

### ​ Decorator-based middleware

Quick and simple for single-hook middleware. Use decorators to wrap individual functions.**Available decorators:****Node-style:**

- `@before_agent` \- Runs before agent starts (once per invocation)
- `@before_model` \- Runs before each model call
- `@after_model` \- Runs after each model response
- `@after_agent` \- Runs after agent completes (once per invocation)

**Wrap-style:**

- `@wrap_model_call` \- Wraps each model call with custom logic
- `@wrap_tool_call` \- Wraps each tool call with custom logic

**Convenience:**

- `@dynamic_prompt` \- Generates dynamic system prompts

from langchain.agents.middleware import (
before_model,
wrap_model_call,
AgentState,
ModelRequest,
ModelResponse,
)
from langchain.agents import create_agent
from langgraph.runtime import Runtime
from typing import Any, Callable

@before_model

print(f"About to call model with {len(state['messages'])} messages")
return None

agent = create_agent(
model="gpt-4o",
middleware=[log_before_model, retry_model],
tools=[...],
)

**When to use decorators:**

- Single hook needed
- No complex configuration
- Quick prototyping

### ​ Class-based middleware

More powerful for complex middleware with multiple hooks or configuration. Use classes when you need to define both sync and async implementations for the same hook, or when you want to combine multiple hooks in a single middleware.**Example:**

from langchain.agents.middleware import (
AgentMiddleware,
AgentState,
ModelRequest,
ModelResponse,
)
from langgraph.runtime import Runtime
from typing import Any, Callable

class LoggingMiddleware(AgentMiddleware):

agent = create_agent(
model="gpt-4o",
middleware=[LoggingMiddleware()],
tools=[...],
)

**When to use classes:**

- Defining both sync and async implementations for the same hook
- Multiple hooks needed in a single middleware
- Complex configuration required (e.g., configurable thresholds, custom models)
- Reuse across projects with init-time configuration

## ​ Custom state schema

Middleware can extend the agent’s state with custom properties. This enables middleware to:

- **Track state across execution**: Maintain counters, flags, or other values that persist throughout the agent’s execution lifecycle
- **Share data between hooks**: Pass information from `before_model` to `after_model` or between different middleware instances
- **Implement cross-cutting concerns**: Add functionality like rate limiting, usage tracking, user context, or audit logging without modifying the core agent logic
- **Make conditional decisions**: Use accumulated state to determine whether to continue execution, jump to different nodes, or modify behavior dynamically

from langchain.agents import create_agent
from langchain.messages import HumanMessage
from langchain.agents.middleware import AgentState, before_model, after_model
from typing_extensions import NotRequired
from typing import Any
from langgraph.runtime import Runtime

class CustomState(AgentState):
model_call_count: NotRequired[int]
user_id: NotRequired[str]

@before_model(state_schema=CustomState, can_jump_to=["end"])

count = state.get("model_call_count", 0)

return {"jump_to": "end"}
return None

@after_model(state_schema=CustomState)

return {"model_call_count": state.get("model_call_count", 0) + 1}

agent = create_agent(
model="gpt-4o",
middleware=[check_call_limit, increment_counter],
tools=[],
)

# Invoke with custom state
result = agent.invoke({
"messages": [HumanMessage("Hello")],
"model_call_count": 0,
"user_id": "user-123",
})

from langchain.agents import create_agent
from langchain.messages import HumanMessage
from langchain.agents.middleware import AgentState, AgentMiddleware
from typing_extensions import NotRequired
from typing import Any

class CallCounterMiddleware(AgentMiddleware[CustomState]):
state_schema = CustomState

agent = create_agent(
model="gpt-4o",
middleware=[CallCounterMiddleware()],
tools=[],
)

## ​ Execution order

When using multiple middleware, understand how they execute:

agent = create_agent(
model="gpt-4o",
middleware=[middleware1, middleware2, middleware3],
tools=[...],
)

Execution flow

**Before hooks run in order:**

1. `middleware1.before_agent()`
2. `middleware2.before_agent()`
3. `middleware3.before_agent()`

**Agent loop starts**

4. `middleware1.before_model()`
5. `middleware2.before_model()`
6. `middleware3.before_model()`

**Wrap hooks nest like function calls:**

7. `middleware1.wrap_model_call()` → `middleware2.wrap_model_call()` → `middleware3.wrap_model_call()` → model

**After hooks run in reverse order:**

8. `middleware3.after_model()`
9. `middleware2.after_model()`
10. `middleware1.after_model()`

**Agent loop ends**

11. `middleware3.after_agent()`
12. `middleware2.after_agent()`
13. `middleware1.after_agent()`

**Key rules:**

- `before_*` hooks: First to last
- `after_*` hooks: Last to first (reverse)
- `wrap_*` hooks: Nested (first middleware wraps all others)

## ​ Agent jumps

To exit early from middleware, return a dictionary with `jump_to`:**Available jump targets:**

- `'end'`: Jump to the end of the agent execution (or the first `after_agent` hook)
- `'tools'`: Jump to the tools node
- `'model'`: Jump to the model node (or the first `before_model` hook)

from langchain.agents.middleware import after_model, hook_config, AgentState
from langchain.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any

@after_model
@hook_config(can_jump_to=["end"])

last_message = state["messages"][-1]
if "BLOCKED" in last_message.content:
return {
"messages": [AIMessage("I cannot respond to that request.")],
"jump_to": "end"
}
return None

from langchain.agents.middleware import AgentMiddleware, hook_config, AgentState
from langchain.messages import AIMessage
from langgraph.runtime import Runtime
from typing import Any

class BlockedContentMiddleware(AgentMiddleware):
@hook_config(can_jump_to=["end"])

## ​ Best practices

1. Keep middleware focused - each should do one thing well
2. Handle errors gracefully - don’t let middleware errors crash the agent
3. **Use appropriate hook types**:

- Node-style for sequential logic (logging, validation)
- Wrap-style for control flow (retry, fallback, caching)
4. Clearly document any custom state properties
5. Unit test middleware independently before integrating
6. Consider execution order - place critical middleware first in the list
7. Use built-in middleware when possible

## ​ Examples

### ​ Dynamic model selection

from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable

complex_model = init_chat_model("gpt-4o")
simple_model = init_chat_model("gpt-4o-mini")

@wrap_model_call
def dynamic_model(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

# Use different model based on conversation length

model = complex_model
else:
model = simple_model
return handler(request.override(model=model))

from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable

class DynamicModelMiddleware(AgentMiddleware):
def wrap_model_call(
self,
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

### ​ Tool call monitoring

from langchain.agents.middleware import wrap_tool_call
from langchain.tools.tool_node import ToolCallRequest
from langchain.messages import ToolMessage
from langgraph.types import Command
from typing import Callable

@wrap_tool_call
def monitor_tool(
request: ToolCallRequest,
handler: Callable[[ToolCallRequest], ToolMessage | Command],

print(f"Executing tool: {request.tool_call['name']}")
print(f"Arguments: {request.tool_call['args']}")
try:
result = handler(request)
print(f"Tool completed successfully")
return result
except Exception as e:
print(f"Tool failed: {e}")
raise

from langchain.tools.tool_node import ToolCallRequest
from langchain.agents.middleware import AgentMiddleware
from langchain.messages import ToolMessage
from langgraph.types import Command
from typing import Callable

class ToolMonitoringMiddleware(AgentMiddleware):
def wrap_tool_call(
self,
request: ToolCallRequest,
handler: Callable[[ToolCallRequest], ToolMessage | Command],

### ​ Dynamically selecting tools

Select relevant tools at runtime to improve performance and accuracy.**Benefits:**

- **Shorter prompts** \- Reduce complexity by exposing only relevant tools
- **Better accuracy** \- Models choose correctly from fewer options
- **Permission control** \- Dynamically filter tools based on user access

from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable

@wrap_model_call
def select_tools(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

"""Middleware to select relevant tools based on state/context."""
# Select a small, relevant subset of tools based on state/context
relevant_tools = select_relevant_tools(request.state, request.runtime)
return handler(request.override(tools=relevant_tools))

agent = create_agent(
model="gpt-4o",
tools=all_tools, # All available tools need to be registered upfront
middleware=[select_tools],
)

from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from typing import Callable

class ToolSelectorMiddleware(AgentMiddleware):
def wrap_model_call(
self,
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

"""Middleware to select relevant tools based on state/context."""
relevant_tools = select_relevant_tools(request.state, request.runtime)
return handler(request.override(tools=relevant_tools))

agent = create_agent(
model="gpt-4o",
tools=all_tools, # All available tools need to be registered upfront
middleware=[ToolSelectorMiddleware()],
)

### ​ Working with system messages

Modify system messages in middleware using the `system_message` field on `ModelRequest`. The `system_message` field contains a `SystemMessage` object (even if the agent was created with a string `system_prompt`).**Example: Adding context to system message**

from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from langchain.messages import SystemMessage
from typing import Callable

@wrap_model_call
def add_context(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

# Always work with content blocks
new_content = list(request.system_message.content_blocks) + [\
{"type": "text", "text": "Additional context."}\
]
new_system_message = SystemMessage(content=new_content)
return handler(request.override(system_message=new_system_message))

from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
from langchain.messages import SystemMessage
from typing import Callable

class ContextMiddleware(AgentMiddleware):
def wrap_model_call(
self,
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

**Example: Working with cache control (Anthropic)**When working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts:

@wrap_model_call
def add_cached_context(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

new_content = list(request.system_message.content_blocks) + [\
{\
"type": "text",\

# content up until this point is cached\
"cache_control": {"type": "ephemeral"}\
}\
]

new_system_message = SystemMessage(content=new_content)
return handler(request.override(system_message=new_system_message))

class CachedContextMiddleware(AgentMiddleware):
def wrap_model_call(
self,
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

"cache_control": {"type": "ephemeral"} # This content will be cached\
}\
]

**Notes:**

- `ModelRequest.system_message` is always a `SystemMessage` object, even if the agent was created with `system_prompt="string"`
- Use `SystemMessage.content_blocks` to access content as a list of blocks, regardless of whether the original content was a string or list
- When modifying system messages, use `content_blocks` and append new blocks to preserve existing structure
- You can pass `SystemMessage` objects directly to `create_agent`’s `system_prompt` parameter for advanced use cases like cache control

## ​ Additional resources

- Middleware API reference
- Built-in middleware
- Testing agents

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Built-in middleware\\
\\
Previous Guardrails\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/guardrails

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Guardrails

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Built-in guardrails
- PII detection
- Human-in-the-loop
- Custom guardrails
- Before agent guardrails
- After agent guardrails
- Combine multiple guardrails
- Additional resources

Guardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent’s execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.Common use cases include:

- Preventing PII leakage
- Detecting and blocking prompt injection attacks
- Blocking inappropriate or harmful content
- Enforcing business rules and compliance requirements
- Validating output quality and accuracy

You can implement guardrails using middleware to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.

Guardrails can be implemented using two complementary approaches:

## Deterministic guardrails

Use rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.

## Model-based guardrails

Use LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.

LangChain provides both built-in guardrails (e.g., PII detection, human-in-the-loop) and a flexible middleware system for building custom guardrails using either approach.

## ​ Built-in guardrails

### ​ PII detection

LangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more.PII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data.The PII middleware supports multiple strategies for handling detected PII:

| Strategy | Description | Example |
| --- | --- | --- |
| `redact` | Replace with `[REDACTED_{PII_TYPE}]` | `[REDACTED_EMAIL]` |
| `mask` | Partially obscure (e.g., last 4 digits) | `****-****-****-1234` |
| `hash` | Replace with deterministic hash | `a8f5f167...` |
| `block` | Raise exception when detected | Error thrown |

Copy

from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware

agent = create_agent(
model="gpt-4o",
tools=[customer_service_tool, email_tool],
middleware=[\
# Redact emails in user input before sending to model\
PIIMiddleware(\
"email",\
strategy="redact",\
apply_to_input=True,\
),\
# Mask credit cards in user input\
PIIMiddleware(\
"credit_card",\
strategy="mask",\
apply_to_input=True,\
),\
# Block API keys - raise error if detected\
PIIMiddleware(\
"api_key",\
detector=r"sk-[a-zA-Z0-9]{32}",\
strategy="block",\
apply_to_input=True,\
),\
],
)

# When user provides PII, it will be handled according to the strategy
result = agent.invoke({
"messages": [{"role": "user", "content": "My email is john.doe@example.com and card is 5105-1051-0510-5100"}]
})

Built-in PII types and configuration

**Built-in PII types:**

- `email` \- Email addresses
- `credit_card` \- Credit card numbers (Luhn validated)
- `ip` \- IP addresses
- `mac_address` \- MAC addresses
- `url` \- URLs

**Configuration options:**

| Parameter | Description | Default |
| --- | --- | --- |
| `pii_type` | Type of PII to detect (built-in or custom) | Required |
| `strategy` | How to handle detected PII (`"block"`, `"redact"`, `"mask"`, `"hash"`) | `"redact"` |
| `detector` | Custom detector function or regex pattern | `None` (uses built-in) |
| `apply_to_input` | Check user messages before model call | `True` |
| `apply_to_output` | Check AI messages after model call | `False` |
| `apply_to_tool_results` | Check tool result messages after execution | `False` |

See the middleware documentation for complete details on PII detection capabilities.

### ​ Human-in-the-loop

LangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.Human-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.

from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import Command

agent = create_agent(
model="gpt-4o",
tools=[search_tool, send_email_tool, delete_database_tool],
middleware=[\
HumanInTheLoopMiddleware(\
interrupt_on={\
# Require approval for sensitive operations\
"send_email": True,\
"delete_database": True,\
# Auto-approve safe operations\
"search": False,\
}\
),\
],
# Persist the state across interrupts
checkpointer=InMemorySaver(),
)

# Human-in-the-loop requires a thread ID for persistence
config = {"configurable": {"thread_id": "some_id"}}

# Agent will pause and wait for approval before executing sensitive tools
result = agent.invoke(
{"messages": [{"role": "user", "content": "Send an email to the team"}]},
config=config
)

result = agent.invoke(
Command(resume={"decisions": [{"type": "approve"}]}),
config=config # Same thread ID to resume the paused conversation
)

See the human-in-the-loop documentation for complete details on implementing approval workflows.

## ​ Custom guardrails

For more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.

### ​ Before agent guardrails

Use “before agent” hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.

Class syntax

Decorator syntax

from typing import Any

from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
from langgraph.runtime import Runtime

class ContentFilterMiddleware(AgentMiddleware):
"""Deterministic guardrail: Block requests containing banned keywords."""

def __init__(self, banned_keywords: list[str]):
super().__init__()
self.banned_keywords = [kw.lower() for kw in banned_keywords]

@hook_config(can_jump_to=["end"])

# Get the first user message
if not state["messages"]:
return None

first_message = state["messages"][0]
if first_message.type != "human":
return None

content = first_message.content.lower()

# Check for banned keywords
for keyword in self.banned_keywords:
if keyword in content:
# Block execution before any processing
return {
"messages": [{\
"role": "assistant",\
"content": "I cannot process requests containing inappropriate content. Please rephrase your request."\
}],
"jump_to": "end"
}

return None

# Use the custom guardrail
from langchain.agents import create_agent

agent = create_agent(
model="gpt-4o",
tools=[search_tool, calculator_tool],
middleware=[\
ContentFilterMiddleware(\
banned_keywords=["hack", "exploit", "malware"]\
),\
],
)

# This request will be blocked before any processing
result = agent.invoke({
"messages": [{"role": "user", "content": "How do I hack into a database?"}]
})

### ​ After agent guardrails

Use “after agent” hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.

from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config
from langgraph.runtime import Runtime
from langchain.messages import AIMessage
from langchain.chat_models import init_chat_model
from typing import Any

class SafetyGuardrailMiddleware(AgentMiddleware):
"""Model-based guardrail: Use an LLM to evaluate response safety."""

def __init__(self):
super().__init__()
self.safety_model = init_chat_model("gpt-4o-mini")

# Get the final AI response

last_message = state["messages"][-1]
if not isinstance(last_message, AIMessage):
return None

# Use a model to evaluate safety
safety_prompt = f"""Evaluate if this response is safe and appropriate.
Respond with only 'SAFE' or 'UNSAFE'.

Response: {last_message.content}"""

result = self.safety_model.invoke([{"role": "user", "content": safety_prompt}])

if "UNSAFE" in result.content:
last_message.content = "I cannot provide that response. Please rephrase your request."

# Use the safety guardrail

agent = create_agent(
model="gpt-4o",
tools=[search_tool, calculator_tool],
middleware=[SafetyGuardrailMiddleware()],
)

result = agent.invoke({
"messages": [{"role": "user", "content": "How do I make explosives?"}]
})

### ​ Combine multiple guardrails

You can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:

from langchain.agents import create_agent
from langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware

agent = create_agent(
model="gpt-4o",
tools=[search_tool, send_email_tool],
middleware=[\
# Layer 1: Deterministic input filter (before agent)\
ContentFilterMiddleware(banned_keywords=["hack", "exploit"]),\
\
# Layer 2: PII protection (before and after model)\
PIIMiddleware("email", strategy="redact", apply_to_input=True),\
PIIMiddleware("email", strategy="redact", apply_to_output=True),\
\
# Layer 3: Human approval for sensitive tools\
HumanInTheLoopMiddleware(interrupt_on={"send_email": True}),\
\
# Layer 4: Model-based safety check (after agent)\
SafetyGuardrailMiddleware(),\
],
)

## ​ Additional resources

- Middleware documentation \- Complete guide to custom middleware
- Middleware API reference \- Complete guide to custom middleware
- Human-in-the-loop \- Add human review for sensitive operations
- Testing agents \- Strategies for testing safety mechanisms

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Custom middleware\\
\\
Previous Runtime\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/runtime

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Runtime

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Overview
- Access
- Inside tools
- Inside middleware

## ​ Overview

LangChain’s `create_agent` runs on LangGraph’s runtime under the hood.LangGraph exposes a `Runtime` object with the following information:

1. **Context**: static information like user id, db connections, or other dependencies for an agent invocation
2. **Store**: a BaseStore instance used for long-term memory
3. **Stream writer**: an object used for streaming information via the `"custom"` stream mode

Runtime context provides **dependency injection** for your tools and middleware. Instead of hardcoding values or using global state, you can inject runtime dependencies (like database connections, user IDs, or configuration) when invoking your agent. This makes your tools more testable, reusable, and flexible.

You can access the runtime information within tools and middleware.

## ​ Access

When creating an agent with `create_agent`, you can specify a `context_schema` to define the structure of the `context` stored in the agent `Runtime`.When invoking the agent, pass the `context` argument with the relevant configuration for the run:

Copy

from dataclasses import dataclass

from langchain.agents import create_agent

@dataclass
class Context:
user_name: str

agent = create_agent(
model="gpt-5-nano",
tools=[...],
context_schema=Context
)

agent.invoke(
{"messages": [{"role": "user", "content": "What's my name?"}]},
context=Context(user_name="John Smith")
)

### ​ Inside tools

You can access the runtime information inside tools to:

- Access the context
- Read or write long-term memory
- Write to the custom stream (ex, tool progress / updates)

Use the `ToolRuntime` parameter to access the `Runtime` object inside a tool.

from dataclasses import dataclass
from langchain.tools import tool, ToolRuntime

@dataclass
class Context:
user_id: str

@tool

"""Fetch the user's email preferences from the store."""
user_id = runtime.context.user_id

preferences: str = "The user prefers you to write a brief and polite email."
if runtime.store:
if memory := runtime.store.get(("users",), user_id):
preferences = memory.value["preferences"]

return preferences

### ​ Inside middleware

You can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.Use `request.runtime` to access the `Runtime` object inside middleware decorators. The runtime object is available in the `ModelRequest` parameter passed to middleware functions.

from langchain.messages import AnyMessage
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import dynamic_prompt, ModelRequest, before_model, after_model
from langgraph.runtime import Runtime

# Dynamic prompts
@dynamic_prompt

user_name = request.runtime.context.user_name
system_prompt = f"You are a helpful assistant. Address the user as {user_name}."
return system_prompt

# Before model hook
@before_model

print(f"Processing request for user: {runtime.context.user_name}")
return None

# After model hook
@after_model

print(f"Completed request for user: {runtime.context.user_name}")
return None

agent = create_agent(
model="gpt-5-nano",
tools=[...],
middleware=[dynamic_system_prompt, log_before_model, log_after_model],
context_schema=Context
)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Guardrails\\
\\
Previous Context engineering in agents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/context-engineering

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Context engineering in agents

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Overview
- Why do agents fail?
- The agent loop
- What you can control
- Data sources
- How it works
- Model Context
- System Prompt
- Messages
- Tools
- Defining tools
- Selecting tools
- Model
- Response Format
- Defining formats
- Selecting formats
- Tool Context
- Reads
- Writes
- Life-cycle Context
- Example: Summarization
- Best practices
- Related resources

## ​ Overview

The hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.

### ​ Why do agents fail?

When agents fail, it’s usually because the LLM call inside the agent took the wrong action / didn’t do what we expected. LLMs fail for one of two reasons:

1. The underlying LLM is not capable enough
2. The “right” context was not passed to the LLM

More often than not - it’s actually the second reason that causes agents to not be reliable.**Context engineering** is providing the right information and tools in the right format so the LLM can accomplish a task. This is the number one job of AI Engineers. This lack of “right” context is the number one blocker for more reliable agents, and LangChain’s agent abstractions are uniquely designed to facilitate context engineering.

New to context engineering? Start with the conceptual overview to understand the different types of context and when to use them.

### ​ The agent loop

A typical agent loop consists of two main steps:

1. **Model call** \- calls the LLM with a prompt and available tools, returns either a response or a request to execute tools
2. **Tool execution** \- executes the tools that the LLM requested, returns tool results

This loop continues until the LLM decides to finish.

### ​ What you can control

To build reliable agents, you need to control what happens at each step of the agent loop, as well as what happens between steps.

| Context Type | What You Control | Transient or Persistent |
| --- | --- | --- |
| **Model Context** | What goes into model calls (instructions, message history, tools, response format) | Transient |
| **Tool Context** | What tools can access and produce (reads/writes to state, store, runtime context) | Persistent |
| **Life-cycle Context** | What happens between model and tool calls (summarization, guardrails, logging, etc.) | Persistent |

## Transient context

What the LLM sees for a single call. You can modify messages, tools, or prompts without changing what’s saved in state.

## Persistent context

What gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently.

### ​ Data sources

Throughout this process, your agent accesses (reads / writes) different sources of data:

| Data Source | Also Known As | Scope | Examples |
| --- | --- | --- | --- |
| **Runtime Context** | Static configuration | Conversation-scoped | User ID, API keys, database connections, permissions, environment settings |
| **State** | Short-term memory | Conversation-scoped | Current messages, uploaded files, authentication status, tool results |
| **Store** | Long-term memory | Cross-conversation | User preferences, extracted insights, memories, historical data |

### ​ How it works

LangChain middleware is the mechanism under the hood that makes context engineering practical for developers using LangChain.Middleware allows you to hook into any step in the agent lifecycle and:

- Update context
- Jump to a different step in the agent lifecycle

Throughout this guide, you’ll see frequent use of the middleware API as a means to the context engineering end.

## ​ Model Context

Control what goes into each model call - instructions, available tools, which model to use, and output format. These decisions directly impact reliability and cost.

**System Prompt** \\
\\
Base instructions from the developer to the LLM. **Messages** \\
\\
The full list of messages (conversation history) sent to the LLM. **Tools** \\
\\
Utilities the agent has access to to take actions. **Model** \\
\\
The actual model (including configuration) to be called. **Response Format** \\
\\
Schema specification for the model’s final response.

All of these types of model context can draw from **state** (short-term memory), **store** (long-term memory), or **runtime context** (static configuration).

### ​ System Prompt

The system prompt sets the LLM’s behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.

- State

- Store

- Runtime Context

Access message count or conversation context from state:

Copy

from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dynamic_prompt

# request.messages is a shortcut for request.state["messages"]
message_count = len(request.messages)

base = "You are a helpful assistant."

base += "\nThis is a long conversation - be extra concise."

return base

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[state_aware_prompt]
)

Access user preferences from long-term memory:

from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest
from langgraph.store.memory import InMemoryStore

@dataclass
class Context:
user_id: str

user_id = request.runtime.context.user_id

# Read from Store: get user preferences
store = request.runtime.store
user_prefs = store.get(("preferences",), user_id)

if user_prefs:
style = user_prefs.value.get("communication_style", "balanced")
base += f"\nUser prefers {style} responses."

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[store_aware_prompt],
context_schema=Context,
store=InMemoryStore()
)

Access user ID or configuration from Runtime Context:

from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dataclass
class Context:
user_role: str
deployment_env: str

# Read from Runtime Context: user role and environment
user_role = request.runtime.context.user_role
env = request.runtime.context.deployment_env

if user_role == "admin":
base += "\nYou have admin access. You can perform all operations."
elif user_role == "viewer":
base += "\nYou have read-only access. Guide users to read operations only."

if env == "production":
base += "\nBe extra careful with any data modifications."

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[context_aware_prompt],
context_schema=Context
)

### ​ Messages

Messages make up the prompt that is sent to the LLM.
It’s critical to manage the content of messages to ensure that the LLM has the right information to respond well.

Inject uploaded file context from State when relevant to current query:

from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable

@wrap_model_call
def inject_file_context(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Inject context about files user has uploaded this session."""
# Read from State: get uploaded files metadata
uploaded_files = request.state.get("uploaded_files", [])

if uploaded_files:
# Build context about available files
file_descriptions = []
for file in uploaded_files:
file_descriptions.append(
f"- {file['name']} ({file['type']}): {file['summary']}"
)

file_context = f"""Files you have access to in this conversation:
{chr(10).join(file_descriptions)}

Reference these files when answering questions."""

# Inject file context before recent messages
messages = [\
*request.messages,\
{"role": "user", "content": file_context},\
]
request = request.override(messages=messages)

return handler(request)

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[inject_file_context]
)

Inject user’s email writing style from Store to guide drafting:

from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable
from langgraph.store.memory import InMemoryStore

@wrap_model_call
def inject_writing_style(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Inject user's email writing style from Store."""
user_id = request.runtime.context.user_id

# Read from Store: get user's writing style examples
store = request.runtime.store
writing_style = store.get(("writing_style",), user_id)

if writing_style:
style = writing_style.value
# Build style guide from stored examples
style_context = f"""Your writing style:
- Tone: {style.get('tone', 'professional')}
- Typical greeting: "{style.get('greeting', 'Hi')}"
- Typical sign-off: "{style.get('sign_off', 'Best')}"
- Example email you've written:
{style.get('example_email', '')}"""

# Append at end - models pay more attention to final messages
messages = [\
*request.messages,\
{"role": "user", "content": style_context}\
]
request = request.override(messages=messages)

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[inject_writing_style],
context_schema=Context,
store=InMemoryStore()
)

Inject compliance rules from Runtime Context based on user’s jurisdiction:

from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable

@dataclass
class Context:
user_jurisdiction: str
industry: str
compliance_frameworks: list[str]

@wrap_model_call
def inject_compliance_rules(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Inject compliance constraints from Runtime Context."""
# Read from Runtime Context: get compliance requirements
jurisdiction = request.runtime.context.user_jurisdiction
industry = request.runtime.context.industry
frameworks = request.runtime.context.compliance_frameworks

# Build compliance constraints
rules = []
if "GDPR" in frameworks:
rules.append("- Must obtain explicit consent before processing personal data")
rules.append("- Users have right to data deletion")
if "HIPAA" in frameworks:
rules.append("- Cannot share patient health information without authorization")
rules.append("- Must use secure, encrypted communication")
if industry == "finance":
rules.append("- Cannot provide financial advice without proper disclaimers")

if rules:
compliance_context = f"""Compliance requirements for {jurisdiction}:
{chr(10).join(rules)}"""

messages = [\
*request.messages,\
{"role": "user", "content": compliance_context}\
]
request = request.override(messages=messages)

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[inject_compliance_rules],
context_schema=Context
)

**Transient vs Persistent Message Updates:**The examples above use `wrap_model_call` to make **transient** updates - modifying what messages are sent to the model for a single call without changing what’s saved in state.For **persistent** updates that modify state (like the summarization example in Life-cycle Context), use life-cycle hooks like `before_model` or `after_model` to permanently update the conversation history. See the middleware documentation for more details.

### ​ Tools

Tools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.

#### ​ Defining tools

Each tool needs a clear name, description, argument names, and argument descriptions. These aren’t just metadata—they guide the model’s reasoning about when and how to use the tool.

from langchain.tools import tool

@tool(parse_docstring=True)
def search_orders(
user_id: str,
status: str,
limit: int = 10

"""Search for user orders by status.

Use this when the user asks about order history or wants to check
order status. Always filter by the provided status.

Args:
user_id: Unique identifier for the user
status: Order status: 'pending', 'shipped', or 'delivered'
limit: Maximum number of results to return
"""
# Implementation here
pass

#### ​ Selecting tools

Not every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection adapts the available toolset based on authentication state, user permissions, feature flags, or conversation stage.

Enable advanced tools only after certain conversation milestones:

@wrap_model_call
def state_based_tools(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Filter tools based on conversation State."""
# Read from State: check if user has authenticated
state = request.state
is_authenticated = state.get("authenticated", False)
message_count = len(state["messages"])

# Only enable sensitive tools after authentication
if not is_authenticated:
tools = [t for t in request.tools if t.name.startswith("public_")]
request = request.override(tools=tools)
elif message_count < 5:
# Limit tools early in conversation
tools = [t for t in request.tools if t.name != "advanced_search"]
request = request.override(tools=tools)

agent = create_agent(
model="gpt-4o",
tools=[public_search, private_search, advanced_search],
middleware=[state_based_tools]
)

Filter tools based on user preferences or feature flags in Store:

@wrap_model_call
def store_based_tools(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Filter tools based on Store preferences."""
user_id = request.runtime.context.user_id

# Read from Store: get user's enabled features
store = request.runtime.store
feature_flags = store.get(("features",), user_id)

if feature_flags:
enabled_features = feature_flags.value.get("enabled_tools", [])
# Only include tools that are enabled for this user
tools = [t for t in request.tools if t.name in enabled_features]
request = request.override(tools=tools)

agent = create_agent(
model="gpt-4o",
tools=[search_tool, analysis_tool, export_tool],
middleware=[store_based_tools],
context_schema=Context,
store=InMemoryStore()
)

Filter tools based on user permissions from Runtime Context:

@dataclass
class Context:
user_role: str

@wrap_model_call
def context_based_tools(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Filter tools based on Runtime Context permissions."""
# Read from Runtime Context: get user role
user_role = request.runtime.context.user_role

if user_role == "admin":
# Admins get all tools
pass
elif user_role == "editor":
# Editors can't delete
tools = [t for t in request.tools if t.name != "delete_data"]
request = request.override(tools=tools)
else:
# Viewers get read-only tools
tools = [t for t in request.tools if t.name.startswith("read_")]
request = request.override(tools=tools)

agent = create_agent(
model="gpt-4o",
tools=[read_data, write_data, delete_data],
middleware=[context_based_tools],
context_schema=Context
)

See Dynamically selecting tools for more examples.

### ​ Model

Different models have different strengths, costs, and context windows. Select the right model for the task at hand, which
might change during an agent run.

Use different models based on conversation length from State:

from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable

# Initialize models once outside the middleware
large_model = init_chat_model("claude-sonnet-4-5-20250929")
standard_model = init_chat_model("gpt-4o")
efficient_model = init_chat_model("gpt-4o-mini")

@wrap_model_call
def state_based_model(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Select model based on State conversation length."""

# Long conversation - use model with larger context window
model = large_model

# Medium conversation
model = standard_model
else:
# Short conversation - use efficient model
model = efficient_model

request = request.override(model=model)

agent = create_agent(
model="gpt-4o-mini",
tools=[...],
middleware=[state_based_model]
)

Use user’s preferred model from Store:

from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable
from langgraph.store.memory import InMemoryStore

# Initialize available models once
MODEL_MAP = {
"gpt-4o": init_chat_model("gpt-4o"),
"gpt-4o-mini": init_chat_model("gpt-4o-mini"),
"claude-sonnet": init_chat_model("claude-sonnet-4-5-20250929"),
}

@wrap_model_call
def store_based_model(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Select model based on Store preferences."""
user_id = request.runtime.context.user_id

# Read from Store: get user's preferred model

if user_prefs:
preferred_model = user_prefs.value.get("preferred_model")
if preferred_model and preferred_model in MODEL_MAP:
request = request.override(model=MODEL_MAP[preferred_model])

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[store_based_model],
context_schema=Context,
store=InMemoryStore()
)

Select model based on cost limits or environment from Runtime Context:

from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from langchain.chat_models import init_chat_model
from typing import Callable

@dataclass
class Context:
cost_tier: str
environment: str

premium_model = init_chat_model("claude-sonnet-4-5-20250929")
standard_model = init_chat_model("gpt-4o")
budget_model = init_chat_model("gpt-4o-mini")

@wrap_model_call
def context_based_model(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Select model based on Runtime Context."""
# Read from Runtime Context: cost tier and environment
cost_tier = request.runtime.context.cost_tier
environment = request.runtime.context.environment

if environment == "production" and cost_tier == "premium":
# Production premium users get best model
model = premium_model
elif cost_tier == "budget":
# Budget tier gets efficient model
model = budget_model
else:
# Standard tier
model = standard_model

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[context_based_model],
context_schema=Context
)

See Dynamic model for more examples.

### ​ Response Format

Structured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isn’t sufficient.**How it works:** When you provide a schema as the response format, the model’s final response is guaranteed to conform to that schema. The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format.

#### ​ Defining formats

Schema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to.

from pydantic import BaseModel, Field

class CustomerSupportTicket(BaseModel):
"""Structured ticket information extracted from customer message."""

category: str = Field(
description="Issue category: 'billing', 'technical', 'account', or 'product'"
)
priority: str = Field(
description="Urgency level: 'low', 'medium', 'high', or 'critical'"
)
summary: str = Field(
description="One-sentence summary of the customer's issue"
)
customer_sentiment: str = Field(
description="Customer's emotional tone: 'frustrated', 'neutral', or 'satisfied'"
)

#### ​ Selecting formats

Dynamic response format selection adapts schemas based on user preferences, conversation stage, or role—returning simple formats early and detailed formats as complexity increases.

Configure structured output based on conversation state:

from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from pydantic import BaseModel, Field
from typing import Callable

class SimpleResponse(BaseModel):
"""Simple response for early conversation."""
answer: str = Field(description="A brief answer")

class DetailedResponse(BaseModel):
"""Detailed response for established conversation."""
answer: str = Field(description="A detailed answer")
reasoning: str = Field(description="Explanation of reasoning")
confidence: float = Field(description="Confidence score 0-1")

@wrap_model_call
def state_based_output(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Select output format based on State."""

if message_count < 3:
# Early conversation - use simple format
request = request.override(response_format=SimpleResponse)
else:
# Established conversation - use detailed format
request = request.override(response_format=DetailedResponse)

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[state_based_output]
)

Configure output format based on user preferences in Store:

from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from pydantic import BaseModel, Field
from typing import Callable
from langgraph.store.memory import InMemoryStore

class VerboseResponse(BaseModel):
"""Verbose response with details."""
answer: str = Field(description="Detailed answer")
sources: list[str] = Field(description="Sources used")

class ConciseResponse(BaseModel):
"""Concise response."""
answer: str = Field(description="Brief answer")

@wrap_model_call
def store_based_output(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Select output format based on Store preferences."""
user_id = request.runtime.context.user_id

# Read from Store: get user's preferred response style

if user_prefs:
style = user_prefs.value.get("response_style", "concise")
if style == "verbose":
request = request.override(response_format=VerboseResponse)
else:
request = request.override(response_format=ConciseResponse)

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[store_based_output],
context_schema=Context,
store=InMemoryStore()
)

Configure output format based on Runtime Context like user role or environment:

from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from pydantic import BaseModel, Field
from typing import Callable

@dataclass
class Context:
user_role: str
environment: str

class AdminResponse(BaseModel):
"""Response with technical details for admins."""
answer: str = Field(description="Answer")
debug_info: dict = Field(description="Debug information")
system_status: str = Field(description="System status")

class UserResponse(BaseModel):
"""Simple response for regular users."""
answer: str = Field(description="Answer")

@wrap_model_call
def context_based_output(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

"""Select output format based on Runtime Context."""
user_role = request.runtime.context.user_role
environment = request.runtime.context.environment

if user_role == "admin" and environment == "production":
# Admins in production get detailed output
request = request.override(response_format=AdminResponse)
else:
# Regular users get simple output
request = request.override(response_format=UserResponse)

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[context_based_output],
context_schema=Context
)

## ​ Tool Context

Tools are special in that they both read and write context.In the most basic case, when a tool executes, it receives the LLM’s request parameters and returns a tool message back. The tool does its work and produces a result.Tools can also fetch important information for the model that allows it to perform and complete tasks.

### ​ Reads

Most real-world tools need more than just the LLM’s parameters. They need user IDs for database queries, API keys for external services, or current session state to make decisions. Tools read from state, store, and runtime context to access this information.

Read from State to check current session information:

from langchain.tools import tool, ToolRuntime
from langchain.agents import create_agent

@tool
def check_authentication(
runtime: ToolRuntime

"""Check if user is authenticated."""
# Read from State: check current auth status
current_state = runtime.state
is_authenticated = current_state.get("authenticated", False)

if is_authenticated:
return "User is authenticated"
else:
return "User is not authenticated"

agent = create_agent(
model="gpt-4o",
tools=[check_authentication]
)

Read from Store to access persisted user preferences:

from dataclasses import dataclass
from langchain.tools import tool, ToolRuntime
from langchain.agents import create_agent
from langgraph.store.memory import InMemoryStore

@tool
def get_preference(
preference_key: str,
runtime: ToolRuntime[Context]

"""Get user preference from Store."""
user_id = runtime.context.user_id

# Read from Store: get existing preferences
store = runtime.store
existing_prefs = store.get(("preferences",), user_id)

if existing_prefs:
value = existing_prefs.value.get(preference_key)
return f"{preference_key}: {value}" if value else f"No preference set for {preference_key}"
else:
return "No preferences found"

agent = create_agent(
model="gpt-4o",
tools=[get_preference],
context_schema=Context,
store=InMemoryStore()
)

Read from Runtime Context for configuration like API keys and user IDs:

from dataclasses import dataclass
from langchain.tools import tool, ToolRuntime
from langchain.agents import create_agent

@dataclass
class Context:
user_id: str
api_key: str
db_connection: str

@tool
def fetch_user_data(
query: str,
runtime: ToolRuntime[Context]

"""Fetch data using Runtime Context configuration."""
# Read from Runtime Context: get API key and DB connection
user_id = runtime.context.user_id
api_key = runtime.context.api_key
db_connection = runtime.context.db_connection

# Use configuration to fetch data
results = perform_database_query(db_connection, query, api_key)

return f"Found {len(results)} results for user {user_id}"

agent = create_agent(
model="gpt-4o",
tools=[fetch_user_data],
context_schema=Context
)

# Invoke with runtime context
result = agent.invoke(
{"messages": [{"role": "user", "content": "Get my data"}]},
context=Context(
user_id="user_123",
api_key="sk-...",
db_connection="postgresql://..."
)
)

### ​ Writes

Tool results can be used to help an agent complete a given task. Tools can both return results directly to the model
and update the memory of the agent to make important context available to future steps.

Write to State to track session-specific information using Command:

from langchain.tools import tool, ToolRuntime
from langchain.agents import create_agent
from langgraph.types import Command

@tool
def authenticate_user(
password: str,
runtime: ToolRuntime

"""Authenticate user and update State."""
# Perform authentication (simplified)
if password == "correct":
# Write to State: mark as authenticated using Command
return Command(
update={"authenticated": True},
)
else:
return Command(update={"authenticated": False})

agent = create_agent(
model="gpt-4o",
tools=[authenticate_user]
)

Write to Store to persist data across sessions:

@tool
def save_preference(
preference_key: str,
preference_value: str,
runtime: ToolRuntime[Context]

"""Save user preference to Store."""
user_id = runtime.context.user_id

# Read existing preferences

# Merge with new preference
prefs = existing_prefs.value if existing_prefs else {}
prefs[preference_key] = preference_value

# Write to Store: save updated preferences
store.put(("preferences",), user_id, prefs)

return f"Saved preference: {preference_key} = {preference_value}"

agent = create_agent(
model="gpt-4o",
tools=[save_preference],
context_schema=Context,
store=InMemoryStore()
)

See Tools for comprehensive examples of accessing state, store, and runtime context in tools.

## ​ Life-cycle Context

Control what happens **between** the core agent steps - intercepting data flow to implement cross-cutting concerns like summarization, guardrails, and logging.As you’ve seen in Model Context and Tool Context, middleware is the mechanism that makes context engineering practical. Middleware allows you to hook into any step in the agent lifecycle and either:

1. **Update context** \- Modify state and store to persist changes, update conversation history, or save insights
2. **Jump in the lifecycle** \- Move to different steps in the agent cycle based on context (e.g., skip tool execution if a condition is met, repeat model call with modified context)

### ​ Example: Summarization

One of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in Model Context, summarization **persistently updates state** \- permanently replacing old messages with a summary that’s saved for all future turns.LangChain offers built-in middleware for this:

from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[\
SummarizationMiddleware(\
model="gpt-4o-mini",\
trigger={"tokens": 4000},\
keep={"messages": 20},\
),\
],
)

When the conversation exceeds the token limit, `SummarizationMiddleware` automatically:

1. Summarizes older messages using a separate LLM call
2. Replaces them with a summary message in State (permanently)
3. Keeps recent messages intact for context

The summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.

For a complete list of built-in middleware, available hooks, and how to create custom middleware, see the Middleware documentation.

## ​ Best practices

1. **Start simple** \- Begin with static prompts and tools, add dynamics only when needed
2. **Test incrementally** \- Add one context engineering feature at a time
3. **Monitor performance** \- Track model calls, token usage, and latency
4. **Use built-in middleware** \- Leverage `SummarizationMiddleware`, `LLMToolSelectorMiddleware`, etc.
5. **Document your context strategy** \- Make it clear what context is being passed and why
6. **Understand transient vs persistent**: Model context changes are transient (per-call), while life-cycle context changes persist to state

## ​ Related resources

- Context conceptual overview \- Understand context types and when to use them
- Middleware \- Complete middleware guide
- Tools \- Tool creation and context access
- Memory \- Short-term and long-term memory patterns
- Agents \- Core agent concepts

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Runtime\\
\\
Previous Model Context Protocol (MCP)\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/mcp

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Model Context Protocol (MCP)

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Quickstart
- Custom servers
- Transports
- HTTP
- Passing headers
- Authentication
- stdio
- Stateful sessions
- Core features
- Tools
- Loading tools
- Structured content
- Multimodal tool content
- Resources
- Loading resources
- Prompts
- Loading prompts
- Advanced features
- Tool Interceptors
- Accessing runtime context
- State updates and commands
- Custom interceptors
- Progress notifications
- Logging
- Elicitation
- Server setup
- Client setup
- Response actions
- Additional resources

Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the `langchain-mcp-adapters` library.

## ​ Quickstart

Install the `langchain-mcp-adapters` library:

pip

uv

Copy

pip install langchain-mcp-adapters

`langchain-mcp-adapters` enables agents to use tools defined across one or more MCP servers.

`MultiServerMCPClient` is **stateless by default**. Each tool invocation creates a fresh MCP `ClientSession`, executes the tool, and then cleans up. See the stateful sessions section for more details.

Accessing multiple MCP servers

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.agents import create_agent

client = MultiServerMCPClient(
{
"math": {
"transport": "stdio", # Local subprocess communication
"command": "python",
# Absolute path to your math_server.py file
"args": ["/path/to/math_server.py"],
},
"weather": {
"transport": "http", # HTTP-based remote server
# Ensure you start your weather server on port 8000
"url": "http://localhost:8000/mcp",
}
}
)

tools = await client.get_tools()
agent = create_agent(
"claude-sonnet-4-5-20250929",
tools
)
math_response = await agent.ainvoke(
{"messages": [{"role": "user", "content": "what's (3 + 5) x 12?"}]}
)
weather_response = await agent.ainvoke(
{"messages": [{"role": "user", "content": "what is the weather in nyc?"}]}
)

## ​ Custom servers

To create a custom MCP server, use the FastMCP library:

pip install fastmcp

To test your agent with MCP tool servers, use the following examples:

Math server (stdio transport)

Weather server (streamable HTTP transport)

from fastmcp import FastMCP

mcp = FastMCP("Math")

@mcp.tool()

"""Add two numbers"""
return a + b

"""Multiply two numbers"""
return a * b

if __name__ == "__main__":
mcp.run(transport="stdio")

## ​ Transports

MCP supports different transport mechanisms for client-server communication.

### ​ HTTP

The `http` transport (also referred to as `streamable-http`) uses HTTP requests for client-server communication. See the MCP HTTP transport specification for more details.

client = MultiServerMCPClient(
{
"weather": {
"transport": "http",
"url": "http://localhost:8000/mcp",
}
}
)

#### ​ Passing headers

When connecting to MCP servers over HTTP, you can include custom headers (e.g., for authentication or tracing) using the `headers` field in the connection configuration. This is supported for `sse` (deprecated by MCP spec) and `streamable_http` transports.

Passing headers with MultiServerMCPClient

client = MultiServerMCPClient(
{
"weather": {
"transport": "http",
"url": "http://localhost:8000/mcp",
"headers": {
"Authorization": "Bearer YOUR_TOKEN",
"X-Custom-Header": "custom-value"
},
}
}
)
tools = await client.get_tools()
agent = create_agent("openai:gpt-4.1", tools)
response = await agent.ainvoke({"messages": "what is the weather in nyc?"})

#### ​ Authentication

The `langchain-mcp-adapters` library uses the official MCP SDK under the hood, which allows you to provide a custom authentication mechanism by implementing the `httpx.Auth` interface.

from langchain_mcp_adapters.client import MultiServerMCPClient

client = MultiServerMCPClient(
{
"weather": {
"transport": "http",
"url": "http://localhost:8000/mcp",
"auth": auth,
}
}
)

- Example custom auth implementation
- Built-in OAuth flow

### ​ stdio

Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.

Unlike HTTP transports, `stdio` connections are inherently **stateful**—the subprocess persists for the lifetime of the client connection. However, when using `MultiServerMCPClient` without explicit session management, each tool call still creates a new session. See stateful sessions for managing persistent connections.

client = MultiServerMCPClient(
{
"math": {
"transport": "stdio",
"command": "python",
"args": ["/path/to/math_server.py"],
}
}
)

## ​ Stateful sessions

By default, `MultiServerMCPClient` is **stateless**—each tool invocation creates a fresh MCP session, executes the tool, and then cleans up.If you need to control the lifecycle of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent `ClientSession` using `client.session()`.

Using MCP ClientSession for stateful tool usage

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain.agents import create_agent

client = MultiServerMCPClient({...})

# Create a session explicitly
async with client.session("server_name") as session:
# Pass the session to load tools, resources, or prompts
tools = await load_mcp_tools(session)
agent = create_agent(
"anthropic:claude-3-7-sonnet-latest",
tools
)

## ​ Core features

### ​ Tools

Tools allow MCP servers to expose executable functions that LLMs can invoke to perform actions—such as querying databases, calling APIs, or interacting with external systems. LangChain converts MCP tools into LangChain tools, making them directly usable in any LangChain agent or workflow.

#### ​ Loading tools

Use `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:

client = MultiServerMCPClient({...})
tools = await client.get_tools()
agent = create_agent("claude-sonnet-4-5-20250929", tools)

#### ​ Structured content

MCP tools can return structured content alongside the human-readable text response. This is useful when a tool needs to return machine-parseable data (like JSON) in addition to text that gets shown to the model.When an MCP tool returns `structuredContent`, the adapter wraps it in an `MCPToolArtifact` and returns it as the tool’s artifact. You can access this using the `artifact` field on the `ToolMessage`. You can also use interceptors to process or transform structured content automatically.**Extracting structured content from artifact**After invoking your agent, you can access the structured content from tool messages in the response:

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.agents import create_agent
from langchain.messages import ToolMessage

result = await agent.ainvoke(
{"messages": [{"role": "user", "content": "Get data from the server"}]}
)

# Extract structured content from tool messages
for message in result["messages"]:
if isinstance(message, ToolMessage) and message.artifact:
structured_content = message.artifact["structured_content"]

**Appending structured content via interceptor**If you want structured content to be visible in the conversation history (visible to the model), you can use an interceptor to automatically append structured content to the tool result:

import json

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.interceptors import MCPToolCallRequest
from mcp.types import TextContent

async def append_structured_content(request: MCPToolCallRequest, handler):
"""Append structured content from artifact to tool message."""
result = await handler(request)
if result.structuredContent:
result.content += [\
TextContent(type="text", text=json.dumps(result.structuredContent)),\
]
return result

client = MultiServerMCPClient({...}, tool_interceptors=[append_structured_content])

#### ​ Multimodal tool content

MCP tools can return multimodal content (images, text, etc.) in their responses. When an MCP server returns content with multiple parts (e.g., text and images), the adapter converts them to LangChain’s standard content blocks. You can access the standardized representation via the `content_blocks` property on the `ToolMessage`:

result = await agent.ainvoke(
{"messages": [{"role": "user", "content": "Take a screenshot of the current page"}]}
)

# Access multimodal content from tool messages
for message in result["messages"]:
if message.type == "tool":
# Raw content in provider-native format
print(f"Raw content: {message.content}")

# Standardized content blocks #
for block in message.content_blocks:
if block["type"] == "text":
print(f"Text: {block['text']}")
elif block["type"] == "image":
print(f"Image URL: {block.get('url')}")
print(f"Image base64: {block.get('base64', '')[:50]}...")

This allows you to handle multimodal tool responses in a provider-agnostic way, regardless of how the underlying MCP server formats its content.

### ​ Resources

Resources allow MCP servers to expose data—such as files, database records, or API responses—that can be read by clients. LangChain converts MCP resources into Blob objects, which provide a unified interface for handling both text and binary content.

#### ​ Loading resources

Use `client.get_resources()` to load resources from an MCP server:

# Load all resources from a server
blobs = await client.get_resources("server_name")

# Or load specific resources by URI
blobs = await client.get_resources("server_name", uris=["file:///path/to/file.txt"])

for blob in blobs:
print(f"URI: {blob.metadata['uri']}, MIME type: {blob.mimetype}")
print(blob.as_string()) # For text content

You can also use `load_mcp_resources` directly with a session for more control:

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.resources import load_mcp_resources

# Load all resources
async with client.session("server_name") as session:
blobs = await load_mcp_resources(session)

blobs = await load_mcp_resources(session, uris=["file:///path/to/file.txt"])

### ​ Prompts

Prompts allow MCP servers to expose reusable prompt templates that can be retrieved and used by clients. LangChain converts MCP prompts into messages, making them easy to integrate into chat-based workflows.

#### ​ Loading prompts

Use `client.get_prompt()` to load a prompt from an MCP server:

# Load a prompt by name
messages = await client.get_prompt("server_name", "summarize")

# Load a prompt with arguments
messages = await client.get_prompt(
"server_name",
"code_review",
arguments={"language": "python", "focus": "security"}
)

# Use the messages in your workflow
for message in messages:
print(f"{message.type}: {message.content}")

You can also use `load_mcp_prompt` directly with a session for more control:

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.prompts import load_mcp_prompt

async with client.session("server_name") as session:
messages = await load_mcp_prompt(session, "summarize")

messages = await load_mcp_prompt(
session,
"code_review",
arguments={"language": "python", "focus": "security"}
)

## ​ Advanced features

### ​ Tool Interceptors

MCP servers run as separate processes—they can’t access LangGraph runtime information like the store, context, or agent state. **Interceptors bridge this gap** by giving you access to this runtime context during MCP tool execution.Interceptors also provide middleware-like control over tool calls: you can modify requests, implement retries, add headers dynamically, or short-circuit execution entirely.

| Section | Description |
| --- | --- |
| Accessing runtime context | Read user IDs, API keys, store data, and agent state |
| State updates and commands | Update agent state or control graph flow with `Command` |
| Writing interceptors | Patterns for modifying requests, composing interceptors, and error handling |

#### ​ Accessing runtime context

When MCP tools are used within a LangChain agent (via `create_agent`), interceptors receive access to the `ToolRuntime` context. This provides access to the tool call ID, state, config, and store—enabling powerful patterns for accessing user data, persisting information, and controlling agent behavior.

- Runtime context

- Store

- State

- Tool call ID

Access user-specific configuration like user IDs, API keys, or permissions that are passed at invocation time:

Inject user context into MCP tool calls

from dataclasses import dataclass
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.interceptors import MCPToolCallRequest
from langchain.agents import create_agent

@dataclass
class Context:
user_id: str
api_key: str

async def inject_user_context(
request: MCPToolCallRequest,
handler,
):
"""Inject user credentials into MCP tool calls."""
runtime = request.runtime
user_id = runtime.context.user_id
api_key = runtime.context.api_key

# Add user context to tool arguments
modified_request = request.override(
args={**request.args, "user_id": user_id}
)
return await handler(modified_request)

client = MultiServerMCPClient(
{...},
tool_interceptors=[inject_user_context],
)
tools = await client.get_tools()
agent = create_agent("gpt-4o", tools, context_schema=Context)

# Invoke with user context
result = await agent.ainvoke(
{"messages": [{"role": "user", "content": "Search my orders"}]},
context={"user_id": "user_123", "api_key": "sk-..."}
)

Access long-term memory to retrieve user preferences or persist data across conversations:

Access user preferences from store

from dataclasses import dataclass
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.interceptors import MCPToolCallRequest
from langchain.agents import create_agent
from langgraph.store.memory import InMemoryStore

@dataclass
class Context:
user_id: str

async def personalize_search(
request: MCPToolCallRequest,
handler,
):
"""Personalize MCP tool calls using stored preferences."""
runtime = request.runtime
user_id = runtime.context.user_id
store = runtime.store

# Read user preferences from store
prefs = store.get(("preferences",), user_id)

if prefs and request.name == "search":
# Apply user's preferred language and result limit
modified_args = {
**request.args,
"language": prefs.value.get("language", "en"),
"limit": prefs.value.get("result_limit", 10),
}
request = request.override(args=modified_args)

return await handler(request)

client = MultiServerMCPClient(
{...},
tool_interceptors=[personalize_search],
)
tools = await client.get_tools()
agent = create_agent(
"gpt-4o",
tools,
context_schema=Context,
store=InMemoryStore()
)

Access conversation state to make decisions based on the current session:

Filter tools based on authentication state

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.interceptors import MCPToolCallRequest
from langchain.messages import ToolMessage

async def require_authentication(
request: MCPToolCallRequest,
handler,
):
"""Block sensitive MCP tools if user is not authenticated."""
runtime = request.runtime
state = runtime.state
is_authenticated = state.get("authenticated", False)

sensitive_tools = ["delete_file", "update_settings", "export_data"]

if request.name in sensitive_tools and not is_authenticated:
# Return error instead of calling tool
return ToolMessage(
content="Authentication required. Please log in first.",
tool_call_id=runtime.tool_call_id,
)

client = MultiServerMCPClient(
{...},
tool_interceptors=[require_authentication],
)

Access the tool call ID to return properly formatted responses or track tool executions:

Return custom responses with tool call ID

async def rate_limit_interceptor(
request: MCPToolCallRequest,
handler,
):
"""Rate limit expensive MCP tool calls."""
runtime = request.runtime
tool_call_id = runtime.tool_call_id

# Check rate limit (simplified example)
if is_rate_limited(request.name):
return ToolMessage(
content="Rate limit exceeded. Please try again later.",
tool_call_id=tool_call_id,
)

result = await handler(request)

# Log successful tool call
log_tool_execution(tool_call_id, request.name, success=True)

return result

client = MultiServerMCPClient(
{...},
tool_interceptors=[rate_limit_interceptor],
)

For more context engineering patterns, see Context engineering and Tools.

#### ​ State updates and commands

Interceptors can return `Command` objects to update agent state or control graph execution flow. This is useful for tracking task progress, switching between agents, or ending execution early.

Mark task complete and switch agents

from langchain.agents import AgentState, create_agent
from langchain_mcp_adapters.interceptors import MCPToolCallRequest
from langchain.messages import ToolMessage
from langgraph.types import Command

async def handle_task_completion(
request: MCPToolCallRequest,
handler,
):
"""Mark task complete and hand off to summary agent."""
result = await handler(request)

if request.name == "submit_order":
return Command(
update={
"messages": [result] if isinstance(result, ToolMessage) else [],
"task_status": "completed",
},
goto="summary_agent",
)

Use `Command` with `goto="__end__"` to end execution early:

End agent run on completion

async def end_on_success(
request: MCPToolCallRequest,
handler,
):
"""End agent run when task is marked complete."""
result = await handler(request)

if request.name == "mark_complete":
return Command(
update={"messages": [result], "status": "done"},
goto="__end__",
)

#### ​ Custom interceptors

Interceptors are async functions that wrap tool execution, enabling request/response modification, retry logic, and other cross-cutting concerns. They follow an “onion” pattern where the first interceptor in the list is the outermost layer.**Basic pattern**An interceptor is an async function that receives a request and a handler. You can modify the request before calling the handler, modify the response after, or skip the handler entirely.

Basic interceptor pattern

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.interceptors import MCPToolCallRequest

async def logging_interceptor(
request: MCPToolCallRequest,
handler,
):
"""Log tool calls before and after execution."""
print(f"Calling tool: {request.name} with args: {request.args}")
result = await handler(request)
print(f"Tool {request.name} returned: {result}")
return result

client = MultiServerMCPClient(
{"math": {"transport": "stdio", "command": "python", "args": ["/path/to/server.py"]}},
tool_interceptors=[logging_interceptor],
)

**Modifying requests**Use `request.override()` to create a modified request. This follows an immutable pattern, leaving the original request unchanged.

Modifying tool arguments

async def double_args_interceptor(
request: MCPToolCallRequest,
handler,
):
"""Double all numeric arguments before execution."""
modified_args = {k: v * 2 for k, v in request.args.items()}
modified_request = request.override(args=modified_args)
return await handler(modified_request)

# Original call: add(a=2, b=3) becomes add(a=4, b=6)

**Modifying headers at runtime**Interceptors can modify HTTP headers dynamically based on the request context:

Dynamic header modification

async def auth_header_interceptor(
request: MCPToolCallRequest,
handler,
):
"""Add authentication headers based on the tool being called."""
token = get_token_for_tool(request.name)
modified_request = request.override(
headers={"Authorization": f"Bearer {token}"}
)
return await handler(modified_request)

**Composing interceptors**Multiple interceptors compose in “onion” order — the first interceptor in the list is the outermost layer:

Composing multiple interceptors

async def outer_interceptor(request, handler):
print("outer: before")
result = await handler(request)
print("outer: after")
return result

async def inner_interceptor(request, handler):
print("inner: before")
result = await handler(request)
print("inner: after")
return result

client = MultiServerMCPClient(
{...},
tool_interceptors=[outer_interceptor, inner_interceptor],
)

# Execution order:

**Error handling**Use interceptors to catch tool execution errors and implement retry logic:

Retry on error

import asyncio

async def retry_interceptor(
request: MCPToolCallRequest,
handler,
max_retries: int = 3,
delay: float = 1.0,
):
"""Retry failed tool calls with exponential backoff."""
last_error = None
for attempt in range(max_retries):
try:
return await handler(request)
except Exception as e:
last_error = e
if attempt < max_retries - 1:
wait_time = delay * (2 ** attempt) # Exponential backoff
print(f"Tool {request.name} failed (attempt {attempt + 1}), retrying in {wait_time}s...")
await asyncio.sleep(wait_time)
raise last_error

client = MultiServerMCPClient(
{...},
tool_interceptors=[retry_interceptor],
)

You can also catch specific error types and return fallback values:

Error handling with fallback

async def fallback_interceptor(
request: MCPToolCallRequest,
handler,
):
"""Return a fallback value if tool execution fails."""
try:
return await handler(request)
except TimeoutError:
return f"Tool {request.name} timed out. Please try again later."
except ConnectionError:
return f"Could not connect to {request.name} service. Using cached data."

### ​ Progress notifications

Subscribe to progress updates for long-running tool executions:

Progress callback

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.callbacks import Callbacks, CallbackContext

async def on_progress(
progress: float,
total: float | None,
message: str | None,
context: CallbackContext,
):
"""Handle progress updates from MCP servers."""
percent = (progress / total * 100) if total else progress
tool_info = f" ({context.tool_name})" if context.tool_name else ""
print(f"[{context.server_name}{tool_info}] Progress: {percent:.1f}% - {message}")

client = MultiServerMCPClient(
{...},
callbacks=Callbacks(on_progress=on_progress),
)

The `CallbackContext` provides:

- `server_name`: Name of the MCP server
- `tool_name`: Name of the tool being executed (available during tool calls)

### ​ Logging

The MCP protocol supports logging notifications from servers. Use the `Callbacks` class to subscribe to these events.

Logging callback

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.callbacks import Callbacks, CallbackContext
from mcp.types import LoggingMessageNotificationParams

async def on_logging_message(
params: LoggingMessageNotificationParams,
context: CallbackContext,
):
"""Handle log messages from MCP servers."""
print(f"[{context.server_name}] {params.level}: {params.data}")

client = MultiServerMCPClient(
{...},
callbacks=Callbacks(on_logging_message=on_logging_message),
)

### ​ Elicitation

Elicitation allows MCP servers to request additional input from users during tool execution. Instead of requiring all inputs upfront, servers can interactively ask for information as needed.

#### ​ Server setup

Define a tool that uses `ctx.elicit()` to request user input with a schema:

MCP server with elicitation

from pydantic import BaseModel
from mcp.server.fastmcp import Context, FastMCP

server = FastMCP("Profile")

class UserDetails(BaseModel):
email: str
age: int

@server.tool()

"""Create a user profile, requesting details via elicitation."""
result = await ctx.elicit(
message=f"Please provide details for {name}'s profile:",
schema=UserDetails,
)
if result.action == "accept" and result.data:
return f"Created profile for {name}: email={result.data.email}, age={result.data.age}"
if result.action == "decline":
return f"User declined. Created minimal profile for {name}."
return "Profile creation cancelled."

if __name__ == "__main__":
server.run(transport="http")

#### ​ Client setup

Handle elicitation requests by providing a call Response actions

The elicitation callback can return one of three actions:

| Action | Description |
| --- | --- |
| `accept` | User provided valid input. Include the data in the `content` field. |
| `decline` | User chose not to provide the requested information. |
| `cancel` | User cancelled the operation entirely. |

Response action examples

# Accept with data
ElicitResult(action="accept", content={"email": "user@example.com", "age": 25})

# Decline (user doesn't want to provide info)
ElicitResult(action="decline")

# Cancel (abort the operation)
ElicitResult(action="cancel")

## ​ Additional resources

- MCP documentation
- MCP Transport documentation
- `langchain-mcp-adapters`

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Context engineering in agents\\
\\
Previous Human-in-the-loop\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/human-in-the-loop

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Human-in-the-loop

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Interrupt decision types
- Configuring interrupts
- Responding to interrupts
- Decision types
- Streaming with human-in-the-loop
- Execution lifecycle
- Custom HITL logic

The Human-in-the-Loop (HITL) middleware lets you add human oversight to agent tool calls.
When a model proposes an action that might require review — for example, writing to a file or executing SQL — the middleware can pause execution and wait for a decision.It does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an interrupt that halts execution. The graph state is saved using LangGraph’s persistence layer, so execution can pause safely and resume later.A human decision then determines what happens next: the action can be approved as-is (`approve`), modified before running (`edit`), or rejected with feedback (`reject`).

## ​ Interrupt decision types

The middleware defines three built-in ways a human can respond to an interrupt:

| Decision Type | Description | Example Use Case |
| --- | --- | --- |
| ✅ `approve` | The action is approved as-is and executed without changes. | Send an email draft exactly as written |
| ✏️ `edit` | The tool call is executed with modifications. | Change the recipient before sending an email |
| ❌ `reject` | The tool call is rejected, with an explanation added to the conversation. | Reject an email draft and explain how to rewrite it |

The available decision types for each tool depend on the policy you configure in `interrupt_on`.
When multiple tool calls are paused at the same time, each action requires a separate decision.
Decisions must be provided in the same order as the actions appear in the interrupt request.

When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.

## ​ Configuring interrupts

To use HITL, add the middleware to the agent’s `middleware` list when creating the agent.You configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.

Copy

from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
model="gpt-4o",
tools=[write_file_tool, execute_sql_tool, read_data_tool],
middleware=[\
HumanInTheLoopMiddleware(\
interrupt_on={\
"write_file": True, # All decisions (approve, edit, reject) allowed\
"execute_sql": {"allowed_decisions": ["approve", "reject"]}, # No editing allowed\
# Safe operation, no approval needed\
"read_data": False,\
},\
# Prefix for interrupt messages - combined with tool name and args to form the full message\
# e.g., "Tool execution pending approval: execute_sql with query='DELETE FROM...'"\
# Individual tools can override this by specifying a "description" in their interrupt config\
description_prefix="Tool execution pending approval",\
),\
],
# Human-in-the-loop requires checkpointing to handle interrupts.
# In production, use a persistent checkpointer like AsyncPostgresSaver.
checkpointer=InMemorySaver(),
)

You must configure a checkpointer to persist the graph state across interrupts.
In production, use a persistent checkpointer like `AsyncPostgresSaver`. For testing or prototyping, use `InMemorySaver`.When invoking the agent, pass a `config` that includes the **thread ID** to associate execution with a conversation thread.
See the LangGraph interrupts documentation for details.

Configuration options

​

interrupt\_on

dict

required

Mapping of tool names to approval configs. Values can be `True` (interrupt with default config), `False` (auto-approve), or an `InterruptOnConfig` object.

description\_prefix

string

default:"Tool execution requires approval"

Prefix for action request descriptions

**`InterruptOnConfig` options:**

allowed\_decisions

list\[string\]

List of allowed decisions: `'approve'`, `'edit'`, or `'reject'`

description

string \| callable

Static string or callable function for custom description

## ​ Responding to interrupts

When you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in `interrupt_on`. In that case, the invocation result will include an `__interrupt__` field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.

from langgraph.types import Command

# Human-in-the-loop leverages LangGraph's persistence layer.
# You must provide a thread ID to associate the execution with a conversation thread,
# so the conversation can be paused and resumed (as is needed for human review).
config = {"configurable": {"thread_id": "some_id"}}
# Run the graph until the interrupt is hit.
result = agent.invoke(
{
"messages": [\
{\
"role": "user",\
"content": "Delete old records from the database",\
}\
]
},
config=config
)

# The interrupt contains the full HITL request with action_requests and review_configs
print(result['__interrupt__'])

# Resume with approval decision
agent.invoke(
Command(
resume={"decisions": [{"type": "approve"}]} # or "reject"
),
config=config # Same thread ID to resume the paused conversation
)

### ​ Decision types

- ✅ approve

- ✏️ edit

- ❌ reject

Use `approve` to approve the tool call as-is and execute it without changes.

agent.invoke(
Command(
# Decisions are provided as a list, one per action under review.
# The order of decisions must match the order of actions
# listed in the `__interrupt__` request.
resume={
"decisions": [\
{\
"type": "approve",\
}\
]
}
),
config=config # Same thread ID to resume the paused conversation
)

Use `edit` to modify the tool call before execution.
Provide the edited action with the new tool name and arguments.

agent.invoke(
Command(
resume={
"decisions": [\
{\
"type": "edit",\
# Edited action with tool name and args\
"edited_action": {\
# Tool name to call.\
# Will usually be the same as the original action.\
"name": "new_tool_name",\
# Arguments to pass to the tool.\
"args": {"key1": "new_value", "key2": "original_value"},\
}\
}\
]
}
),
config=config # Same thread ID to resume the paused conversation
)

Use `reject` to reject the tool call and provide feedback instead of execution.

agent.invoke(
Command(
resume={
"decisions": [\
{\
"type": "reject",\
# An explanation about why the action was rejected\
"message": "No, this is wrong because ..., instead do this ...",\
}\
]
}
),
config=config # Same thread ID to resume the paused conversation
)

The `message` is added to the conversation as feed Multiple decisions

When multiple actions are under review, provide a decision for each action in the same order as they appear in the interrupt:

{
"decisions": [\
{"type": "approve"},\
{\
"type": "edit",\
"edited_action": {\
"name": "tool_name",\
"args": {"param": "new_value"}\
}\
},\
{\
"type": "reject",\
"message": "This action is not allowed"\
}\
]
}

## ​ Streaming with human-in-the-loop

You can use `stream()` instead of `invoke()` to get real-time updates while the agent runs and handles interrupts. Use `stream_mode=['updates', 'messages']` to stream both agent progress and LLM tokens.

# Stream agent progress and LLM tokens until interrupt
for mode, chunk in agent.stream(
{"messages": [{"role": "user", "content": "Delete old records from the database"}]},
config=config,
stream_mode=["updates", "messages"],
):
if mode == "messages":
# LLM token
token, metadata = chunk
if token.content:
print(token.content, end="", flush=True)
elif mode == "updates":
# Check for interrupt
if "__interrupt__" in chunk:
print(f"\n\nInterrupt: {chunk['__interrupt__']}")

# Resume with streaming after human decision
for mode, chunk in agent.stream(
Command(resume={"decisions": [{"type": "approve"}]}),
config=config,
stream_mode=["updates", "messages"],
):
if mode == "messages":
token, metadata = chunk
if token.content:
print(token.content, end="", flush=True)

See the Streaming guide for more details on stream modes.

## ​ Execution lifecycle

The middleware defines an `after_model` hook that runs after the model generates a response but before any tool calls are executed:

1. The agent invokes the model to generate a response.
2. The middleware inspects the response for tool calls.
3. If any calls require human input, the middleware builds a `HITLRequest` with `action_requests` and `review_configs` and calls interrupt.
4. The agent waits for human decisions.
5. Based on the `HITLResponse` decisions, the middleware executes approved or edited calls, synthesizes ToolMessage’s for rejected calls, and resumes execution.

## ​ Custom HITL logic

For more specialized workflows, you can build custom HITL logic directly using the interrupt primitive and middleware abstraction.Review the execution lifecycle above to understand how to integrate interrupts into the agent’s operation.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Model Context Protocol (MCP)\\
\\
Previous Multi-agent\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/retrieval

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Retrieval

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Building a knowledge base
- From retrieval to RAG
- Retrieval Pipeline
- Building Blocks
- RAG Architectures
- 2-step RAG
- Agentic RAG
- Hybrid RAG

Large Language Models (LLMs) are powerful, but they have two key limitations:

- **Finite context** — they can’t ingest entire corpora at once.
- **Static knowledge** — their training data is frozen at a point in time.

Retrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of **Retrieval-Augmented Generation (RAG)**: enhancing an LLM’s answers with context-specific information.

## ​ Building a knowledge base

A **knowledge base** is a repository of documents or structured data used during retrieval.If you need a custom knowledge base, you can use LangChain’s document loaders and vector stores to build one from your own data.

If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do **not** need to rebuild it. You can:

- Connect it as a **tool** for an agent in Agentic RAG.
- Query it and supply the retrieved content as context to the LLM (2-Step RAG).

See the following tutorial to build a searchable knowledge base and minimal RAG workflow: **Tutorial: Semantic search** \\
\\
Learn how to create a searchable knowledge base from your own data using LangChain’s document loaders, embeddings, and vector stores.\\
In this tutorial, you’ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You’ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.\\
\\
Learn more

### ​ From retrieval to RAG

Retrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they **integrate retrieval with generation** to produce grounded, context-aware answers.This is the core idea behind **Retrieval-Augmented Generation (RAG)**. The retrieval pipeline becomes a foundation for a broader system that combines search with generation.

### ​ Retrieval Pipeline

A typical retrieval workflow looks like this:

Sources

(Google Drive, Slack, Notion, etc.)

Document Loaders

Documents

Split into chunks

Turn into embeddings

Vector Store

User Query

Query embedding

Retriever

LLM uses retrieved info

Answer

Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.

### ​ Building Blocks

**Document loaders**

Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized `Document` objects.

Learn more

**Text splitters** \\
\\
Break large docs into smaller chunks that will be retrievable individually and fit within a model’s context window.\\
\\
Learn more **Embedding models** \\
\\
An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\\
\\
Learn more **Vector stores** \\
\\
Specialized databases for storing and searching embeddings.\\
\\
Learn more **Retrievers** \\
\\
A retriever is an interface that returns documents given an unstructured query.\\
\\
Learn more

## ​ RAG Architectures

RAG can be implemented in multiple ways, depending on your system’s needs. We outline each type in the sections below.

| Architecture | Description | Control | Flexibility | Latency | Example Use Case |
| --- | --- | --- | --- | --- | --- |
| **2-Step RAG** | Retrieval always happens before generation. Simple and predictable | ✅ High | ❌ Low | ⚡ Fast | FAQs, documentation bots |
| **Agentic RAG** | An LLM-powered agent decides _when_ and _how_ to retrieve during reasoning | ❌ Low | ✅ High | ⏳ Variable | Research assistants with access to multiple tools |
| **Hybrid** | Combines characteristics of both approaches with validation steps | ⚖️ Medium | ⚖️ Medium | ⏳ Variable | Domain-specific Q&A with quality validation |

**Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.

### ​ 2-step RAG

In **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.

User Question

Retrieve Relevant Documents

Generate Answer

Return Answer to User

**Tutorial: Retrieval-Augmented Generation (RAG)** \\
\\
See how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\\
This tutorial walks through two approaches:\\
\\
- A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.\\
- A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.\\
\\
Learn more

### ​ Agentic RAG

**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.

The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.

Yes

No

User Input / Question

Agent (LLM)

Need external info?

Search using tool(s)

Enough to answer?

Generate final answer

Show Extended example: Agentic RAG for LangGraph's llms.txt

This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading llms.txt, which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user’s question.

Copy

import requests
from langchain.agents import create_agent
from langchain.messages import HumanMessage
from langchain.tools import tool
from markdownify import markdownify

ALLOWED_DOMAINS = ["https://langchain-ai.github.io/"]
LLMS_TXT = 'https://langchain-ai.github.io/langgraph/llms.txt'

@tool

"""Fetch and convert documentation from a URL"""
if not any(url.startswith(domain) for domain in ALLOWED_DOMAINS):
return (
"Error: URL not allowed. "
f"Must start with one of: {', '.join(ALLOWED_DOMAINS)}"
)
response = requests.get(url, timeout=10.0)
response.raise_for_status()
return markdownify(response.text)

# We will fetch the content of llms.txt, so this can
# be done ahead of time without requiring an LLM request.
llms_txt_content = requests.get(LLMS_TXT).text

# System prompt for the agent
system_prompt = f"""
You are an expert Python developer and technical assistant.
Your primary role is to help users with questions about LangGraph and related tools.

Instructions:

1. If a user asks a question you're unsure about — or one that likely involves API usage,
behavior, or configuration — you MUST use the `fetch_documentation` tool to consult the relevant docs.
2. When citing documentation, summarize clearly and include relevant context from the content.
3. Do not use any URLs outside of the allowed domain.
4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.

You can access official documentation from the following approved sources:

{llms_txt_content}

You MUST consult the documentation to get up to date documentation
before answering a user's question about LangGraph.

Your answers should be clear, concise, and technically accurate.
"""

tools = [fetch_documentation]

model = init_chat_model("claude-sonnet-4-0", max_tokens=32_000)

agent = create_agent(
model=model,
tools=tools,
system_prompt=system_prompt,
name="Agentic RAG",
)

response = agent.invoke({
'messages': [\
HumanMessage(content=(\
"Write a short example of a langgraph agent using the "\
"prebuilt create react agent. the agent should be able "\
"to look up stock pricing information."\
))\
]
})

print(response['messages'][-1].content)

### ​ Hybrid RAG

Hybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.Typical components include:

- **Query enhancement**: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.
- **Retrieval validation**: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.
- **Answer validation**: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.

The architecture often supports multiple iterations between these steps:

Query Enhancement

Retrieve Documents

Sufficient Info?

Refine Query

Answer Quality OK?

Try Different Approach?

Return Best Answer

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Custom workflow\\
\\
Previous Long-term memory\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/long-term-memory



---

# https://docs.langchain.com/oss/python/langchain/studio

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Agent development

LangSmith Studio

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Prerequisites
- Set up local Agent server
- 1\. Install the LangGraph CLI
- 2\. Prepare your agent
- 3\. Environment variables
- 4\. Create a LangGraph config file
- 5\. Install dependencies
- 6\. View your agent in Studio
- Video guide

When building agents with LangChain locally, it’s helpful to visualize what’s happening inside your agent, interact with it in real-time, and debug issues as they occur. **LangSmith Studio** is a free visual interface for developing and testing your LangChain agents from your local machine.Studio connects to your locally running agent to show you each step your agent takes: the prompts sent to the model, tool calls and their results, and the final output. You can test different inputs, inspect intermediate states, and iterate on your agent’s behavior without additional code or deployment.This pages describes how to set up Studio with your local LangChain agent.

## ​ Prerequisites

Before you begin, ensure you have the following:

- **A LangSmith account**: Sign up (for free) or log in at smith.langchain.com.
- **A LangSmith API key**: Follow the Create an API key guide.
- If you don’t want data traced to LangSmith, set `LANGSMITH_TRACING=false` in your application’s `.env` file. With tracing disabled, no data leaves your local server.

## ​ Set up local Agent server

### ​ 1\. Install the LangGraph CLI

The LangGraph CLI provides a local development server (also called Agent Server) that connects your agent to Studio.

Copy

pip install --upgrade "langgraph-cli[inmem]"

### ​ 2\. Prepare your agent

If you already have a LangChain agent, you can use it directly. This example uses a simple email agent:

agent.py

from langchain.agents import create_agent

def send_email(to: str, subject: str, body: str):
"""Send an email"""
email = {
"to": to,
"subject": subject,
"body": body
}
# ... email sending logic

return f"Email sent to {to}"

agent = create_agent(
"gpt-4o",
tools=[send_email],
system_prompt="You are an email assistant. Always use the send_email tool.",
)

### ​ 3\. Environment variables

Studio requires a LangSmith API key to connect your local agent. Create a `.env` file in the root of your project and add your API key from LangSmith.

Ensure your `.env` file is not committed to version control, such as Git.

.env

LANGSMITH_API_KEY=lsv2...

### ​ 4\. Create a LangGraph config file

The LangGraph CLI uses a configuration file to locate your agent and manage dependencies. Create a `langgraph.json` file in your app’s directory:

langgraph.json

{
"dependencies": ["."],
"graphs": {
"agent": "./src/agent.py:agent"
},
"env": ".env"
}

The `create_agent` function automatically returns a compiled LangGraph graph, which is what the `graphs` key expects in the configuration file.

For detailed explanations of each key in the JSON object of the configuration file, refer to the LangGraph configuration file reference.

At this point, the project structure will look like this:

my-app/
├── src
│ └── agent.py
├── .env
└── langgraph.json

### ​ 5\. Install dependencies

Install your project dependencies from the root directory:

pip

uv

pip install langchain langchain-openai

### ​ 6\. View your agent in Studio

Start the development server to connect your agent to Studio:

langgraph dev

Safari blocks `localhost` connections to Studio. To work around this, run the above command with `--tunnel` to access Studio via a secure tunnel.

Once the server is running, your agent is accessible both via API at `http://127.0.0.1:2024` and through the Studio UI at `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`:

With Studio connected to your local agent, you can iterate quickly on your agent’s behavior. Run a test input, inspect the full execution trace including prompts, tool arguments, return values, and token/latency metrics. When something goes wrong, Studio captures exceptions with the surrounding state to help you understand what happened.The development server supports hot-reloading—make changes to prompts or tool signatures in your code, and Studio reflects them immediately. Re-run conversation threads from any step to test your changes without starting over. This workflow scales from simple single-tool agents to complex multi-node graphs.For more information on how to run Studio, refer to the following guides in the LangSmith docs:

- Run application
- Manage assistants
- Manage threads
- Iterate on prompts
- Debug LangSmith traces
- Add node to dataset

## ​ Video guide

LangSmith Studio v2: The Ultimate Agent Development Environment - YouTube

Photo image of LangChain

LangChain

165K subscribers

LangSmith Studio v2: The Ultimate Agent Development Environment

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Watch on

0:00

0:00 / 8:09

•Live

•

For more information about local and deployed agents, see Set up local Agent Server and Deploy.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Long-term memory\\
\\
Previous Test\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/test

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Agent development

Test

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Unit Testing
- Mocking Chat Model
- InMemorySaver Checkpointer
- Integration Testing
- Installing AgentEvals
- Trajectory Match Evaluator
- LLM-as-Judge Evaluator
- Async Support
- LangSmith Integration
- Recording & Replaying HTTP Calls

Agentic applications let an LLM decide its own next steps to solve a problem. That flexibility is powerful, but the model’s black-box nature makes it hard to predict how a tweak in one part of your agent will affect the rest. To build production-ready agents, thorough testing is essential.There are a few approaches to testing your agents:

- Unit tests exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically.
- Integration tests test the agent using real network calls to confirm that components work together, credentials and schemas line up, and latency is acceptable.

Agentic applications tend to lean more on integration because they chain multiple components together and must deal with flakiness due to the nondeterministic nature of LLMs.

## ​ Unit Testing

### ​ Mocking Chat Model

For logic not requiring API calls, you can use an in-memory stub for mocking responses.LangChain provides `GenericFakeChatModel` for mocking text responses. It accepts an iterator of responses (AIMessages or strings) and returns one per invocation. It supports both regular and streaming usage.

Copy

from langchain_core.language_models.fake_chat_models import GenericFakeChatModel

model = GenericFakeChatModel(messages=iter([\
AIMessage(content="", tool_calls=[ToolCall(name="foo", args={"bar": "baz"}, id="call_1")]),\
"bar"\
]))

model.invoke("hello")
# AIMessage(content='', ..., tool_calls=[{'name': 'foo', 'args': {'bar': 'baz'}, 'id': 'call_1', 'type': 'tool_call'}])

If we invoke the model again, it will return the next item in the iterator:

model.invoke("hello, again!")
# AIMessage(content='bar', ...)

### ​ InMemorySaver Checkpointer

To enable persistence during testing, you can use the `InMemorySaver` checkpointer. This allows you to simulate multiple turns to test state-dependent behavior:

from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
model,
tools=[],
checkpointer=InMemorySaver()
)

# First invocation
agent.invoke(HumanMessage(content="I live in Sydney, Australia."))

# Second invocation: the first message is persisted (Sydney location), so the model returns GMT+10 time
agent.invoke(HumanMessage(content="What's my local time?"))

## ​ Integration Testing

Many agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain’s `agentevals` package provides evaluators specifically designed for testing agent trajectories with live models.AgentEvals lets you easily evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a **trajectory match** or by using an **LLM judge**: **Trajectory match** \\
\\
Hard-code a reference trajectory for a given input and validate the run via a step-by-step comparison.Ideal for testing well-defined workflows where you know the expected behavior. Use when you have specific expectations about which tools should be called and in what order. This approach is deterministic, fast, and cost-effective since it doesn’t require additional LLM calls. **LLM-as-judge** \\
\\
Use a LLM to qualitatively validate your agent’s execution trajectory. The “judge” LLM reviews the agent’s decisions against a prompt rubric (which can include a reference trajectory).More flexible and can assess nuanced aspects like efficiency and appropriateness, but requires an LLM call and is less deterministic. Use when you want to evaluate the overall quality and reasonableness of the agent’s trajectory without strict tool call or ordering requirements.

### ​ Installing AgentEvals

pip install agentevals

Or, clone the AgentEvals repository directly.

### ​ Trajectory Match Evaluator

AgentEvals offers the `create_trajectory_match_evaluator` function to match your agent’s trajectory against a reference trajectory. There are four modes to choose from:

| Mode | Description | Use Case |
| --- | --- | --- |
| `strict` | Exact match of messages and tool calls in the same order | Testing specific sequences (e.g., policy lookup before authorization) |
| `unordered` | Same tool calls allowed in any order | Verifying information retrieval when order doesn’t matter |
| `subset` | Agent calls only tools from reference (no extras) | Ensuring agent doesn’t exceed expected scope |
| `superset` | Agent calls at least the reference tools (extras allowed) | Verifying minimum required actions are taken |

Strict match

The `strict` mode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action.

from langchain.agents import create_agent
from langchain.tools import tool
from langchain.messages import HumanMessage, AIMessage, ToolMessage
from agentevals.trajectory.match import create_trajectory_match_evaluator

@tool
def get_weather(city: str):
"""Get weather information for a city."""
return f"It's 75 degrees and sunny in {city}."

agent = create_agent("gpt-4o", tools=[get_weather])

evaluator = create_trajectory_match_evaluator(
trajectory_match_mode="strict",
)

def test_weather_tool_called_strict():
result = agent.invoke({
"messages": [HumanMessage(content="What's the weather in San Francisco?")]
})

reference_trajectory = [\
HumanMessage(content="What's the weather in San Francisco?"),\
AIMessage(content="", tool_calls=[\
{"id": "call_1", "name": "get_weather", "args": {"city": "San Francisco"}}\
]),\
ToolMessage(content="It's 75 degrees and sunny in San Francisco.", tool_call_id="call_1"),\
AIMessage(content="The weather in San Francisco is 75 degrees and sunny."),\
]

evaluation = evaluator(
outputs=result["messages"],
reference_outputs=reference_trajectory
)
# {
# 'key': 'trajectory_strict_match',
# 'score': True,
# 'comment': None,
# }
assert evaluation["score"] is True

Unordered match

The `unordered` mode allows the same tool calls in any order, which is helpful when you want to verify that specific information was retrieved but don’t care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn’t matter.

@tool
def get_events(city: str):
"""Get events happening in a city."""
return f"Concert at the park in {city} tonight."

agent = create_agent("gpt-4o", tools=[get_weather, get_events])

evaluator = create_trajectory_match_evaluator(
trajectory_match_mode="unordered",
)

def test_multiple_tools_any_order():
result = agent.invoke({
"messages": [HumanMessage(content="What's happening in SF today?")]
})

# Reference shows tools called in different order than actual execution
reference_trajectory = [\
HumanMessage(content="What's happening in SF today?"),\
AIMessage(content="", tool_calls=[\
{"id": "call_1", "name": "get_events", "args": {"city": "SF"}},\
{"id": "call_2", "name": "get_weather", "args": {"city": "SF"}},\
]),\
ToolMessage(content="Concert at the park in SF tonight.", tool_call_id="call_1"),\
ToolMessage(content="It's 75 degrees and sunny in SF.", tool_call_id="call_2"),\
AIMessage(content="Today in SF: 75 degrees and sunny with a concert at the park tonight."),\
]

evaluation = evaluator(
outputs=result["messages"],
reference_outputs=reference_trajectory,
)
# 'key': 'trajectory_unordered_match',

Subset and superset match

The `superset` and `subset` modes match partial trajectories. The `superset` mode verifies that the agent called at least the tools in the reference trajectory, allowing additional tool calls. The `subset` mode ensures the agent did not call any tools beyond those in the reference.

@tool
def get_detailed_forecast(city: str):
"""Get detailed weather forecast for a city."""
return f"Detailed forecast for {city}: sunny all week."

agent = create_agent("gpt-4o", tools=[get_weather, get_detailed_forecast])

evaluator = create_trajectory_match_evaluator(
trajectory_match_mode="superset",
)

def test_agent_calls_required_tools_plus_extra():
result = agent.invoke({
"messages": [HumanMessage(content="What's the weather in Boston?")]
})

# Reference only requires get_weather, but agent may call additional tools
reference_trajectory = [\
HumanMessage(content="What's the weather in Boston?"),\
AIMessage(content="", tool_calls=[\
{"id": "call_1", "name": "get_weather", "args": {"city": "Boston"}},\
]),\
ToolMessage(content="It's 75 degrees and sunny in Boston.", tool_call_id="call_1"),\
AIMessage(content="The weather in Boston is 75 degrees and sunny."),\
]

# 'key': 'trajectory_superset_match',
evaluation = evaluator(
outputs=result["messages"],
reference_outputs=reference_trajectory,
)
assert evaluation["score"] is True

You can also set the `tool_args_match_mode` property and/or `tool_args_match_overrides` to customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the repository for more details.

### ​ LLM-as-Judge Evaluator

You can also use an LLM to evaluate the agent’s execution path with the `create_trajectory_llm_as_judge` function. Unlike the trajectory match evaluators, it doesn’t require a reference trajectory, but one can be provided if available.

Without reference trajectory

from langchain.agents import create_agent
from langchain.tools import tool
from langchain.messages import HumanMessage, AIMessage, ToolMessage
from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT

evaluator = create_trajectory_llm_as_judge(
model="openai:o3-mini",
prompt=TRAJECTORY_ACCURACY_PROMPT,
)

def test_trajectory_quality():
result = agent.invoke({
"messages": [HumanMessage(content="What's the weather in Seattle?")]
})

evaluation = evaluator(
outputs=result["messages"],
)
# 'key': 'trajectory_accuracy',
# 'comment': 'The provided agent trajectory is reasonable...'

With reference trajectory

If you have a reference trajectory, you can add an extra variable to your prompt and pass in the reference trajectory. Below, we use the prebuilt `TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE` prompt and configure the `reference_outputs` variable:

evaluator = create_trajectory_llm_as_judge(
model="openai:o3-mini",
prompt=TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,
)
evaluation = judge_with_reference(
outputs=result["messages"],
reference_outputs=reference_trajectory,
)

For more configurability over how the LLM evaluates the trajectory, visit the repository.

### ​ Async Support

All `agentevals` evaluators support Python asyncio. For evaluators that use factory functions, async versions are available by adding `async` after `create_` in the function name.

Async judge and evaluator example

from agentevals.trajectory.llm import create_async_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT
from agentevals.trajectory.match import create_async_trajectory_match_evaluator

async_judge = create_async_trajectory_llm_as_judge(
model="openai:o3-mini",
prompt=TRAJECTORY_ACCURACY_PROMPT,
)

async_evaluator = create_async_trajectory_match_evaluator(
trajectory_match_mode="strict",
)

async def test_async_evaluation():
result = await agent.ainvoke({
"messages": [HumanMessage(content="What's the weather?")]
})

evaluation = await async_judge(outputs=result["messages"])
assert evaluation["score"] is True

## ​ LangSmith Integration

For tracking experiments over time, you can log evaluator results to LangSmith, a platform for building production-grade LLM applications that includes tracing, evaluation, and experimentation tools.First, set up LangSmith by setting the required environment variables:

export LANGSMITH_API_KEY="your_langsmith_api_key"
export LANGSMITH_TRACING="true"

LangSmith offers two main approaches for running evaluations: pytest integration and the `evaluate` function.

Using pytest integration

import pytest
from langsmith import testing as t
from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT

trajectory_evaluator = create_trajectory_llm_as_judge(
model="openai:o3-mini",
prompt=TRAJECTORY_ACCURACY_PROMPT,
)

@pytest.mark.langsmith
def test_trajectory_accuracy():
result = agent.invoke({
"messages": [HumanMessage(content="What's the weather in SF?")]
})

reference_trajectory = [\
HumanMessage(content="What's the weather in SF?"),\
AIMessage(content="", tool_calls=[\
{"id": "call_1", "name": "get_weather", "args": {"city": "SF"}},\
]),\
ToolMessage(content="It's 75 degrees and sunny in SF.", tool_call_id="call_1"),\
AIMessage(content="The weather in SF is 75 degrees and sunny."),\
]

# Log inputs, outputs, and reference outputs to LangSmith
t.log_inputs({})
t.log_outputs({"messages": result["messages"]})
t.log_reference_outputs({"messages": reference_trajectory})

trajectory_evaluator(
outputs=result["messages"],
reference_outputs=reference_trajectory
)

Run the evaluation with pytest:

pytest test_trajectory.py --langsmith-output

Results will be automatically logged to LangSmith.

Using the evaluate function

Alternatively, you can create a dataset in LangSmith and use the `evaluate` function:

from langsmith import Client
from agentevals.trajectory.llm import create_trajectory_llm_as_judge, TRAJECTORY_ACCURACY_PROMPT

client = Client()

def run_agent(inputs):
"""Your agent function that returns trajectory messages."""
return agent.invoke(inputs)["messages"]

experiment_results = client.evaluate(
run_agent,
data="your_dataset_name",
evaluators=[trajectory_evaluator]
)

To learn more about evaluating your agent, see the LangSmith docs.

## ​ Recording & Replaying HTTP Calls

Integration tests that call real LLM APIs can be slow and expensive, especially when run frequently in CI/CD pipelines. We recommend using a library for recording HTTP requests and responses, then replaying them in subsequent runs without making actual network calls.You can use `vcrpy` to achieve this. If you’re using `pytest`, the `pytest-recording` plugin provides a simple way to enable this with minimal configuration. Request/responses are recorded in cassettes, which are then used to mock the real network calls in subsequent runs.Set up your `conftest.py` file to filter out sensitive information from the cassettes:

conftest.py

import pytest

@pytest.fixture(scope="session")
def vcr_config():
return {
"filter_headers": [\
("authorization", "XXXX"),\
("x-api-key", "XXXX"),\
# ... other headers you want to mask\
],
"filter_query_parameters": [\
("api_key", "XXXX"),\
("key", "XXXX"),\
],
}

Then configure your project to recognise the `vcr` marker:

pytest.ini

pyproject.toml

[pytest]
markers =
vcr: record/replay HTTP via VCR
addopts = --record-mode=once

The `--record-mode=once` option records HTTP interactions on the first run and replays them on subsequent runs.

Now, simply decorate your tests with the `vcr` marker:

@pytest.mark.vcr()
def test_agent_trajectory():
# ...

The first time you run this test, your agent will make real network calls and pytest will generate a cassette file `test_agent_trajectory.yaml` in the `tests/cassettes` directory. Subsequent runs will use that cassette to mock the real network calls, granted the agent’s requests don’t change from the previous run. If they do, the test will fail and you’ll need to delete the cassette and rerun the test to record fresh interactions.

When you modify prompts, add new tools, or change expected trajectories, your saved cassettes will become outdated and your existing tests **will fail**. You should delete the corresponding cassette files and rerun the tests to record fresh interactions.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Studio\\
\\
Previous Agent Chat UI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/ui

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Agent development

Agent Chat UI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Quick start
- Local development
- Connect to your agent

Agent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI works seamlessly with agents created using `create_agent` and provides interactive experiences for your agents with minimal setup, whether you’re running locally or in a deployed context (such as LangSmith).Agent Chat UI is open source and can be adapted to your application needs.

Introducing Agent Chat UI - YouTube

Photo image of LangChain

LangChain

165K subscribers

Introducing Agent Chat UI

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Why am I seeing this?

Watch on

0:00

0:00 / 5:59

•Live

•

You can use generative UI in the Agent Chat UI. For more information, see Implement generative user interfaces with LangGraph.

### ​ Quick start

The fastest way to get started is using the hosted version:

1. **Visit Agent Chat UI**
2. **Connect your agent** by entering your deployment URL or local server address
3. **Start chatting** \- the UI will automatically detect and render tool calls and interrupts

### ​ Local development

For customization or local development, you can run Agent Chat UI locally:

Use npx

Clone repository

Copy

# Create a new Agent Chat UI project
npx create-agent-chat-app --project-name my-chat-ui
cd my-chat-ui

# Install dependencies and start
pnpm install
pnpm dev

### ​ Connect to your agent

Agent Chat UI can connect to both local and deployed agents.After starting Agent Chat UI, you’ll need to configure it to connect to your agent:

1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)
2. **Deployment URL**: Your Agent server’s endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent’s URL)
3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you’re using a local Agent server)

Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.

Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see Hiding Messages in the Chat.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Test\\
\\
Previous LangSmith Deployment\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/deploy

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Deploy with LangSmith

LangSmith Deployment

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Prerequisites
- Deploy your agent
- 1\. Create a repository on GitHub
- 2\. Deploy to LangSmith
- 3\. Test your application in Studio
- 4\. Get the API URL for your deployment
- 5\. Test the API

When you’re ready to deploy your LangChain agent to production, LangSmith provides a managed hosting platform designed for agent workloads. Traditional hosting platforms are built for stateless, short-lived web applications, while LangGraph is **purpose-built for stateful, long-running agents** that require persistent state and background execution. LangSmith handles the infrastructure, scaling, and operational concerns so you can deploy directly from your repository.

## ​ Prerequisites

Before you begin, ensure you have the following:

- A GitHub account
- A LangSmith account (free to sign up)

## ​ Deploy your agent

### ​ 1\. Create a repository on GitHub

Your application’s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide. Then, push your code to the repository.

### ​ 2\. Deploy to LangSmith

1

Navigate to LangSmith Deployment

Log in to LangSmith. In the left sidebar, select **Deployments**.

2

Create new deployment

Click the **\+ New Deployment** button. A pane will open where you can fill in the required fields.

3

Link repository

If you are a first time user or adding a private repository that has not been previously connected, click the **Add new account** button and follow the instructions to connect your GitHub account.

4

Deploy repository

Select your application’s repository. Click **Submit** to deploy. This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.

### ​ 3\. Test your application in Studio

Once your application is deployed:

1. Select the deployment you just created to view more details.
2. Click the **Studio** button in the top right corner. Studio will open to display your graph.

### ​ 4\. Get the API URL for your deployment

1. In the **Deployment details** view in LangGraph, click the **API URL** to copy it to your clipboard.
2. Click the `URL` to copy it to the clipboard.

### ​ 5\. Test the API

You can now test the API:

- Python

- Rest API

1. Install LangGraph Python:

Copy

pip install langgraph-sdk

2. Send a message to the agent:

from langgraph_sdk import get_sync_client # or get_client for async

client = get_sync_client(url="your-deployment-url", api_key="your-langsmith-api-key")

for chunk in client.runs.stream(
None, # Threadless run
"agent", # Name of agent. Defined in langgraph.json.
input={
"messages": [{\
"role": "human",\
"content": "What is LangGraph?",\
}],
},
stream_mode="updates",
):
print(f"Receiving new event of type: {chunk.event}...")
print(chunk.data)
print("\n\n")

curl -s --request POST \

--header 'Content-Type: application/json' \

--data "{
\"assistant_id\": \"agent\", `# Name of agent. Defined in langgraph.json.`
\"input\": {
\"messages\": [\
{\
\"role\": \"human\",\
\"content\": \"What is LangGraph?\"\
}\
]
},
\"stream_mode\": \"updates\"
}"

LangSmith offers additional hosting options, including self-hosted and hybrid. For more information, please see the Platform setup overview.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Agent Chat UI\\
\\
Previous LangSmith Observability\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/observability



---

# https://docs.langchain.com/oss/python/integrations/providers/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/learn)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Contributing to documentation LangSmith Python SDK

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/reference/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith reference Contributing to documentation Reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/contributing/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Contribute

- Documentation
- Code
- Integrations

404

# Page not found

We couldn’t find the page you were looking for.

Contributing to documentation Trace with LangChain (Python and JS/TS) Contributing integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/install)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Install LangChain Trace with LangChain (Python and JS/TS) Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/releases/changelog)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

Changelog Release policy Agent Server changelog

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/philosophy)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/agents)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview Build a voice agent with LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/models)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/messages)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/tools)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration LangChain integrations packages

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/short-term-memory)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/streaming)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/structured-output)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Structured output Models

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/middleware/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/middleware/built-in)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Built-in middleware Trace with LangChain (Python and JS/TS) How to add custom middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/middleware/custom)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

How to add custom middleware Trace with LangChain (Python and JS/TS) Built-in middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/guardrails)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Guardrails Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/runtime)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Install LangChain Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/context-engineering)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/mcp)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Model Context Protocol (MCP) Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/human-in-the-loop)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/retrieval)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/long-term-memory)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Long-term memory Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/studio)



---

# https://docs.langchain.com/oss/python/langchain/test)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/ui)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/deploy)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Install LangChain Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/observability)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/overview).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/overview),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK LangSmith reference LangGraph overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/platform-setup

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Overview

Set up LangSmith

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

This section covers how to host and manage LangSmith infrastructure. You can set up LangSmith for observability, evaluation, and prompt engineering, or use the full platform experience with LangSmith Deployment to also deploy and manage your applications through the UI.

**Start here if you’re setting up or maintaining LangSmith infrastructure.**If you want to deploy an agent application, the Deployment section covers application structure and deployment configuration.

## ​ Choose how to set up LangSmith

You can deploy LangSmith in one of three modes:

- **Cloud**: fully managed by LangChain
- **Hybrid**: LangChain manages the control plane; you host the data plane
- **Self-hosted**: you manage the full stack within your infrastructure

**Cloud** \\
\\
Fully managed observability, evaluation, prompt engineering, and application deployment. Deploy from GitHub with automated CI/CD.\\
\\
Get started **Hybrid**\\
\\
**(Enterprise)** Observability, evaluation, prompt engineering, and application deployment with your applications running in your infrastructure.\\
\\
Set up Hybrid **Self-hosted**\\
\\
**(Enterprise)** Full control with observability, evaluation, and prompt engineering. Enable the full platform experience with LangSmith Deployment or run standalone servers.\\
\\
Run self-hosted

### ​ Comparison

Refer to the following table for a comparison:

| Feature | **Cloud** | **Hybrid** | **Self-Hosted** |
| --- | --- | --- | --- |
| **Infrastructure location** | LangChain’s cloud | Split: Control plane in LangChain cloud, data plane in your cloud | Your cloud |
| **Who manages updates** | LangChain | LangChain (control plane), You (data plane) | You |
| **Who manages CI/CD for your apps** | LangChain | You | You |
| **Can deploy applications?** | ✅ Yes | ✅ Yes | ✅ Yes (with LangSmith Deployment enabled) |
| **Observability data location** | LangChain cloud | LangChain cloud | Your cloud |
| **Pricing** | Plus tier | Enterprise | Enterprise |
| **Best for** | Quick setup, managed infrastructure | Data residency requirements + managed control plane | Full control, data isolation |

You can run an Agent Server locally for free for testing and development.

### ​ Related

- Plans
- Pricing
- Observability

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Cloud\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/reference

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

LangSmith reference

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

The following sections provide API references and SDK documentation for LangSmith:

**Python SDK** \\
\\
Reference documentation for the LangSmith Python SDK. **JavaScript/TypeScript SDK** \\
\\
Reference documentation for the LangSmith JavaScript/TypeScript SDK. **LangGraph Python SDK** \\
\\
Reference documentation for deploying LangGraph applications with Python. **LangGraph JS/TS SDK** \\
\\
Reference documentation for deploying LangGraph applications with JavaScript/TypeScript. **LangSmith API** \\
\\
Complete REST API reference for LangSmith platform features. **Deployment APIs** \\
\\
API references for self-hosted and hybrid LangSmith deployments.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Python SDK\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability-quickstart

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Tracing quickstart

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Prerequisites
- 1\. Create a directory and install dependencies
- 2\. Set up environment variables
- 3\. Define your application
- 4\. Trace LLM calls
- 5\. Trace an entire application
- Next steps
- Video guide

_Observability_ is a critical requirement for applications built with Large Language Models (LLMs). LLMs are non-deterministic, which means that the same prompt can produce different responses. This behavior makes debugging and monitoring more challenging than with traditional software.LangSmith addresses this by providing end-to-end visibility into how your application handles a request. Each request generates a _trace_, which captures the full record of what happened. Within a trace are individual _runs_, the specific operations your application performed, such as an LLM call or a retrieval step. Tracing runs allows you to inspect, debug, and validate your application’s behavior.In this quickstart, you will set up a minimal _Retrieval Augmented Generation (RAG)_ application and add tracing with LangSmith. You will:

1. Configure your environment.
2. Create an application that retrieves context and calls an LLM.
3. Enable tracing to capture both the retrieval step and the LLM call.
4. View the resulting traces in the LangSmith UI.

If you prefer to watch a video on getting started with tracing, refer to the quickstart Video guide.

## ​ Prerequisites

Before you begin, make sure you have:

- **A LangSmith account**: Sign up or log in at smith.langchain.com.
- **A LangSmith API key**: Follow the Create an API key guide.
- **An OpenAI API key**: Generate this from the OpenAI dashboard.

The example app in this quickstart will use OpenAI as the LLM provider. You can adapt the example for your app’s LLM provider.

If you’re building an application with LangChain or LangGraph, you can enable LangSmith tracing with a single environment variable. Get started by reading the guides for tracing with LangChain or tracing with LangGraph.

## ​ 1\. Create a directory and install dependencies

In your terminal, create a directory for your project and install the dependencies in your environment:

Python

TypeScript

Copy

mkdir ls-observability-quickstart && cd ls-observability-quickstart
python -m venv .venv && source .venv/bin/activate
python -m pip install --upgrade pip
pip install -U langsmith openai

## ​ 2\. Set up environment variables

Set the following environment variables:

- `LANGSMITH_TRACING`
- `LANGSMITH_API_KEY`
- `OPENAI_API_KEY` (or your LLM provider’s API key)
- (optional) `LANGSMITH_WORKSPACE_ID`: If your LangSmith API key is linked to multiple workspaces, set this variable to specify which workspace to use.

export LANGSMITH_TRACING=true

If you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.

## ​ 3\. Define your application

You can use the example app code outlined in this step to instrument a RAG application. Or, you can use your own application code that includes an LLM call.This is a minimal RAG app that uses the OpenAI SDK directly without any LangSmith tracing added yet. It has three main parts:

- **Retriever function**: Simulates document retrieval that always returns the same string.
- **OpenAI client**: Instantiates a plain OpenAI client to send a chat completion request.
- **RAG function**: Combines the retrieved documents with the user’s question to form a system prompt, calls the `chat.completions.create()` endpoint with `gpt-4o-mini`, and returns the assistant’s response.

Add the following code into your app file (e.g., `app.py` or `app.ts`):

from openai import OpenAI

def retriever(query: str):
# Minimal example retriever
return ["Harrison worked at Kensho"]

# OpenAI client call (no wrapping yet)
client = OpenAI()

docs = retriever(question)
system_message = (
"Answer the user's question using only the provided information below:\n"
+ "\n".join(docs)
)

# This call is not traced yet
resp = client.chat.completions.create(
model="gpt-4o-mini",
messages=[\
{"role": "system", "content": system_message},\
{"role": "user", "content": question},\
],
)
return resp.choices[0].message.content

if __name__ == "__main__":
print(rag("Where did Harrison work?"))

## ​ 4\. Trace LLM calls

To start, you’ll trace all your OpenAI calls. LangSmith provides wrappers:

- Python: `wrap_openai`
- TypeScript: `wrapOpenAI`

This snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith.

1. Include the highlighted lines in your app file:

from openai import OpenAI
from langsmith.wrappers import wrap_openai # traces openai calls

def retriever(query: str):
return ["Harrison worked at Kensho"]

client = wrap_openai(OpenAI()) # log traces by wrapping the model calls

docs = retriever(question)
system_message = (
"Answer the user's question using only the provided information below:\n"
+ "\n".join(docs)
)
resp = client.chat.completions.create(
model="gpt-4o-mini",
messages=[\
{"role": "system", "content": system_message},\
{"role": "user", "content": question},\
],
)
return resp.choices[0].message.content

2. Call your application:

python app.py

You’ll receive the following output:

Harrison worked at Kensho.

3. In the LangSmith UI, navigate to the **default** Tracing Project for your workspace (or the workspace you specified in Step 2). You’ll see the OpenAI call you just instrumented.

## ​ 5\. Trace an entire application

You can also use the `traceable` decorator for Python or TypeScript to trace your entire application instead of just the LLM calls.

1. Include the highlighted code in your app file:

from openai import OpenAI
from langsmith.wrappers import wrap_openai
from langsmith import traceable

client = wrap_openai(OpenAI()) # keep this to capture the prompt and response from the LLM

@traceable

2. Call the application again to create a run:

3. , navigate to the **default** Tracing Project for your workspace (or the workspace you specified in Step 2). You’ll find a trace of the entire app pipeline with the **rag** step and the **ChatOpenAI** LLM call.

## ​ Next steps

Here are some topics you might want to explore next:

- Tracing integrations provide support for various LLM providers and agent frameworks.
- Filtering traces can help you effectively navigate and analyze data in tracing projects that contain a significant amount of data.
- Trace a RAG application is a full tutorial, which adds observability to an application from development through to production.
- Sending traces to a specific project changes the destination project of your traces.

After logging traces, use **Polly** to analyze them and get AI-powered insights into your application’s performance.

## ​ Video guide

Getting Started with LangSmith (1/8): Tracing - YouTube

Photo image of LangChain

LangChain

165K subscribers

Getting Started with LangSmith (1/8): Tracing

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Why am I seeing this?

Watch on

0:00

0:00 / 8:21

•Live

•

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Observability\\
\\
Previous Observability concepts\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability-concepts

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Observability concepts

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Runs
- Traces
- Threads
- Projects
- Feedback
- Tags
- Metadata
- Data storage and retention
- Deleting traces from LangSmith

This page covers key concepts that are important to understand when logging traces to LangSmith.A _trace_ records the sequence of steps your application takes—from receiving an input, through intermediate processing, to producing a final output. Each step within a trace is represented by a _run_. Multiple traces are grouped together within a _project_, and traces from multi-turn conversations can be linked together as a _thread_.The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer.

## ​ Runs

A _run_ is a span representing a single unit of work or operation within your LLM application. This could be anything from a single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span.!Run

## ​ Traces

A _trace_ is a collection of runs for a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.!Trace

## ​ Threads

A _thread_ is a sequence of traces representing a single conversation. Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. Each turn in the conversation is represented as its own trace, but these traces are linked together by being part of the same thread. The most recent trace in a thread is the latest message exchange.To group traces into threads, you pass a special metadata key (`session_id`, `thread_id`, or `conversation_id`) with a unique identifier value that links the traces together.Learn how to configure threads.!Thread representing a sequence of traces in a multi-turn conversation.!Thread representing a sequence of traces in a multi-turn conversation.

Use **Polly** to analyze traces, runs, and threads. Polly helps you understand agent performance, debug issues, and gain insights from conversation threads without manually digging through data.

## ​ Projects

A _project_ is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.!Project

## ​ Feedback

_Feedback_ allows you to score an individual run based on certain criteria. Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID. Feedback can be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization.You can collect feedback on runs in a number of ways:

1. Sent up along with a trace from the LLM application.
2. Generated by a user in the app inline or in an annotation queue.
3. Generated by an automatic evaluator during offline evaluation.
4. Generated by an online evaluator.

To learn more about how feedback is stored in the application, refer to the Feedback data format guide.!Feedback

## ​ Tags

_Tags_ are collections of strings that can be attached to runs. You can use tags to do the following in the LangSmith UI:

- Categorize runs for easier search.
- Filter runs.
- Group runs together for analysis.

Learn how to attach tags to your traces.!Tags

## ​ Metadata

_Metadata_ is a collection of key-value pairs that you can attach to runs. You can use metadata to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similarly to tags, you can use metadata to filter runs in the LangSmith UI or group runs together for analysis.Learn how to add metadata to your traces.!Metadata

## ​ Data storage and retention

For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database.After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata retained for the purpose of showing accurate statistics, such as historic usage and cost.

If you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.

## ​ Deleting traces from LangSmith

If you need to remove a trace from LangSmith before its expiration date, you can do so by deleting the project that contains it.You can delete a project with one of the following ways:

- In the LangSmith UI, select the **Delete** option on the project’s overflow menu.
- With the `delete_tracer_sessions` API endpoint
- With the `delete_project()` ( Python) or `deleteProject()` ( JS/TS) in the LangSmith SDK.

LangSmith does not support self-service deletion of individual traces.If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, the account owner should contact support via LangSmith Support with the organization ID and trace IDs.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Tracing quickstart\\
\\
Previous Trace a RAG application tutorial\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability-llm-tutorial

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Trace a RAG application tutorial

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Prototyping
- Set up your environment
- Trace your LLM calls
- Trace the whole chain
- Beta Testing
- Collecting Feedback
- Logging Metadata
- Production
- Monitoring
- A/B Testing
- Drilldown
- Conclusion

In this tutorial, we’ll build a simple RAG application using the OpenAI SDK. We’ll add observability to the application at each stage of development, from prototyping to production.

Python

TypeScript

Copy

from openai import OpenAI
openai_client = OpenAI()

# This is the retriever we will use in RAG
# This is mocked out, but it could be anything we want
def retriever(query: str):
results = ["Harrison worked at Kensho"]
return results

# This is the end-to-end RAG chain.
# It does a retrieval step then calls OpenAI
def rag(question):
docs = retriever(question)
system_message = """Answer the users question using only the provided information below:
{docs}""".format(docs="\n".join(docs))

return openai_client.chat.completions.create(
messages=[\
{"role": "system", "content": system_message},\
{"role": "user", "content": question},\
],
model="gpt-4o-mini",
)

## ​ Prototyping

Having observability set up from the start can help you iterate **much** more quickly than you would otherwise be able to. It allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using. In this section we’ll walk through how to set up observability so you can have maximal observability as you are prototyping.

### ​ Set up your environment

First, create an API key by navigating to the settings page.Next, install the LangSmith SDK:

Python SDK

TypeScript SDK

pip install langsmith

Finally, set up the appropriate environment variables. This will log traces to the `default` project (though you can easily change that).

export LANGSMITH_TRACING=true

### ​ Trace your LLM calls

The first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We’ve tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like:

from openai import OpenAI
from langsmith.wrappers import wrap_openai
openai_client = wrap_openai(OpenAI())

Notice how we import `from langsmith.wrappers import wrap_openai` and use it to wrap the OpenAI client (`openai_client = wrap_openai(OpenAI())`).What happens if you call it in the following way?

rag("where did harrison work")

This will produce a trace of just the OpenAI call - it should look something like this!Tracing tutorial openai

### ​ Trace the whole chain

Great - we’ve traced the LLM call. But it’s often very informative to trace more than that. LangSmith is **built** for tracing the entire LLM pipeline - so let’s do that! We can do this by modifying the code to now look something like this:

from openai import OpenAI
from langsmith import traceable
from langsmith.wrappers import wrap_openai
openai_client = wrap_openai(OpenAI())

@traceable
def rag(question):
docs = retriever(question)
system_message = """Answer the users question using only the provided information below:
{docs}""".format(docs="\n".join(docs))

Notice how we import `from langsmith import traceable` and use it decorate the overall function (`@traceable`).What happens if you call it in the following way?

This will produce a trace of the entire RAG pipeline - it should look something like this!Tracing tutorial chain

## ​ Beta Testing

The next stage of LLM application development is beta testing your application. This is when you release it to a few initial users. Having good observability set up here is crucial as often you don’t know exactly how users will actually use your application, so this allows you get insights into how they do so. This also means that you probably want to make some changes to your tracing set up to better allow for that. This extends the observability you set up in the previous section

### ​ Collecting Feedback

A huge part of having good observability during beta testing is collecting feedback. What feedback you collect is often application specific - but at the very least a simple thumbs up/down is a good start. After logging that feedback, you need to be able to easily associate it with the run that caused that. Luckily LangSmith makes it easy to do that.First, you need to log the feedback from your app. An easy way to do this is to keep track of a run ID for each run, and then use that to log feedback. Keeping track of the run ID would look something like:

from langsmith import uuid7

run_id = str(uuid7())
rag(
"where did harrison work",
langsmith_extra={"run_id": run_id}
)

Associating feedback with that run would look something like:

from langsmith import Client
ls_client = Client()
ls_client.create_feedback(
run_id,
key="user-score",
score=1.0,
)

Once the feedback is logged, you can then see it associated with each run by clicking into the `Metadata` tab when inspecting the run. It should look something like this!Tracing tutorial feedbackYou can also query for all runs with positive (or negative) feedback by using the filtering logic in the runs table. You can do this by creating a filter like the following:!Tracing tutorial filtering

### ​ Logging Metadata

It is also a good idea to start logging metadata. This allows you to start keep track of different attributes of your app. This is important in allowing you to know what version or variant of your app was used to produce a given result.For this example, we will log the LLM used. Oftentimes you may be experimenting with different LLMs, so having that information as metadata can be useful for filtering. In order to do that, we can add it as such:

@traceable(run_type="retriever")
def retriever(query: str):
results = ["Harrison worked at Kensho"]
return results

@traceable(metadata={"llm": "gpt-4o-mini"})
def rag(question):
docs = retriever(question)
system_message = """Answer the users question using only the provided information below:
{docs}""".format(docs='\n'.join(docs))
return openai_client.chat.completions.create(messages = [\
{"role": "system", "content": system_message},\
{"role": "user", "content": question},\
], model="gpt-4o-mini")

Notice we added `@traceable(metadata={"llm": "gpt-4o-mini"})` to the `rag` function.Keeping track of metadata in this way assumes that it is known ahead of time. This is fine for LLM types, but less desirable for other types of information - like a User ID. In order to log information that, we can pass it in at run time with the run ID.

run_id = str(uuid7())
rag(
"where did harrison work",
langsmith_extra={"run_id": run_id, "metadata": {"user_id": "harrison"}}
)

Now that we’ve logged these two pieces of metadata, we should be able to see them both show up in the UI here.!Tracing tutorial metadataWe can filter for these pieces of information by constructing a filter like the following:!Tracing tutorial metadata filtering

## ​ Production

Great - you’ve used this newfound observability to iterate quickly and gain confidence that your app is performing well. Time to ship it to production! What new observability do you need to add?First of all, let’s note that the same observability you’ve already added will keep on providing value in production. You will continue to be able to drill down into particular runs.In production you likely have a LOT more traffic. So you don’t really want to be stuck looking at datapoints one at a time. Luckily, LangSmith has a set of tools to help with observability in production.

### ​ Monitoring

If you click on the `Monitor` tab in a project, you will see a series of monitoring charts. Here we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc. You can view these over time across a few different time bins.!Tracing tutorial monitor

### ​ A/B Testing

Group-by functionality for A/B testing requires at least 2 different values to exist for a given metadata key.

You can also use this tab to perform a version of A/B Testing. In the previous tutorial we starting tracking a few different metadata attributes - one of which was `llm`. We can group the monitoring charts by ANY metadata attribute, and instantly get grouped charts over time. This allows us to experiment with different LLMs (or prompts, or other) and track their performance over time.In order to do this, we just need to click on the `Metadata` button at the top. This will give us a drop down of options to choose from to group by:!Tracing tutorial monitor metadataOnce we select this, we will start to see charts grouped by this attribute:!Tracing tutorial monitor grouped

### ​ Drilldown

One of the awesome abilities that LangSmith provides is the ability to easily drilldown into datapoints that you identify as problematic while looking at monitoring charts. In order to do this, you can simply hover over a datapoint in the monitoring chart. When you do this, you will be able to click the datapoint. This will lead you

## ​ Conclusion

In this tutorial you saw how to set up your LLM application with best-in-class observability. No matter what stage your application is in, you will still benefit from observability.If you have more in-depth questions about observability, check out the how-to section for guides on topics like testing, prompt management, and more.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Observability concepts\\
\\
Previous Integrations\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/threads

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Tracing setup

Configure threads

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Group traces into threads
- Example
- View threads
- View a thread
- Thread overview
- Trace view
- View feedback
- Save thread level filter

Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the `Threads` feature in LangSmith.

## ​ Group traces into threads

A `Thread` is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.To associate traces together, you need to pass in a special `metadata` key where the value is the unique identifier for that thread. The key name should be one of:

- `session_id`
- `thread_id`
- `conversation_id`.

The value can be any string you want, but we recommend using UUIDs, such as `f47ac10b-58cc-4372-a567-0e02b2c3d479`. Check out this guide for instructions on adding metadata to your traces.

### ​ Example

This example demonstrates how to log and retrieve conversation history using a structured message format to maintain long-running chats.

Python

TypeScript

Copy

import os
from typing import List, Dict, Any, Optional

import openai
from langsmith import traceable, Client
import langsmith as ls
from langsmith.wrappers import wrap_openai

# Initialize clients
client = wrap_openai(openai.Client())
langsmith_client = Client()

# Configuration
LANGSMITH_PROJECT = "project-with-threads"
THREAD_ID = "thread-id-1"
langsmith_extra={"project_name": LANGSMITH_PROJECT, "metadata":{"session_id": THREAD_ID}}

# gets a history of all LLM calls in the thread to construct conversation history
def get_thread_history(thread_id: str, project_name: str):
# Filter runs by the specific thread and project
filter_string = f'and(in(metadata_key, ["session_id","conversation_id","thread_id"]), eq(metadata_value, "{thread_id}"))'
# Only grab the LLM runs
runs = [r for r in langsmith_client.list_runs(project_name=project_name, filter=filter_string, run_type="llm")]

# Sort by start time to get the most recent interaction
runs = sorted(runs, key=lambda run: run.start_time, reverse=True)

# Reconstruct the conversation state
latest_run = runs[0]
return latest_run.inputs['messages'] + [latest_run.outputs['choices'][0]['message']]

@traceable(name="Chat Bot")
def chat_pipeline(messages: list, get_chat_history: bool = False):
# Whether to continue an existing thread or start a new one
if get_chat_history:
run_tree = ls.get_current_run_tree()
# Get existing conversation history and append new messages
history_messages = get_thread_history(run_tree.extra["metadata"]["session_id"], run_tree.session_name)
all_messages = history_messages + messages
# Include the complete conversation in the input for tracing
input_messages = all_messages
else:
all_messages = messages
input_messages = messages

# Invoke the model
chat_completion = client.chat.completions.create(
model="gpt-4o-mini", messages=all_messages
)

# Return the complete conversation including input and response
response_message = chat_completion.choices[0].message
return {
"messages": input_messages + [response_message]
}

# Format message
messages = [\
{\
"content": "Hi, my name is Sally",\
"role": "user"\
}\
]
get_chat_history = False

# Call the chat pipeline
result = chat_pipeline(messages, get_chat_history, langsmith_extra=langsmith_extra)

After waiting a few seconds, you can make the following calls to continue the conversation. By passing `get_chat_history=True,`/`getChatHistory: true`,
you can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,
instead of just responding to the latest message.

# Continue the conversation.
messages = [\
{\
"content": "What is my name?",\
"role": "user"\
}\
]
get_chat_history = True

chat_pipeline(messages, get_chat_history, langsmith_extra=langsmith_extra)

Keep the conversation going. Since past messages are included, the LLM will remember the conversation.

messages = [\
{\
"content": "What was the first message I sent you?",\
"role": "user"\
}\
]
get_chat_history = True

## ​ View threads

You can view threads by clicking on the **Threads** tab in any project details page. You will then see a list of all threads, sorted by the most recent activity.

Use **Polly** in thread views to analyze conversation threads, understand user sentiment, identify pain points, and track whether issues were resolved.

### ​ View a thread

You can then click into a particular thread. This will open the history for a particular thread.

Threads can be viewed in two different ways:

- Thread overview
- Trace view

You can use the buttons at the top of the page to switch between the two views or use the keyboard shortcut `T` to toggle between the two views.

#### ​ Thread overview

The thread overview page shows you a chatbot-like UI where you can see the inputs and outputs for each turn of the conversation. You can configure which fields in the inputs and outputs are displayed in the overview, or show multiple fields by clicking the **Configure** button.The JSON path for the inputs and outputs supports negative indexing, so you can use `-1` to access the last element of an array. For example, `inputs.messages[-1].content` will access the last message in the `messages` array.

#### ​ Trace view

The trace view here is similar to the trace view when looking at a single run, except that you have easy access to all the runs for each turn in the thread.

### ​ View feedback

When viewing a thread, across the top of the page you will see a section called `Feedback`. This is where you can see the feedback for each of the runs that make up the thread. This feedback is aggregated, so if you evaluate each run of a thread for the same criteria, you will see the average score across all the runs displayed. You can also see thread level feedback left here.

### ​ Save thread level filter

Similar to saving filters at the project level, you can also save commonly used filters at the thread level. To save filters on the threads table, set a filter using the filters button and then click the **Save filter** button.You can open up the trace or annotate the trace in a side panel by clicking on `Annotate` and `Open trace`, respectively.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Upload files with traces\\
\\
Previous Log traces to a specific project\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/cost-tracking

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Configuration & troubleshooting

Cost tracking

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Viewing costs in the LangSmith UI
- Token and cost breakdowns
- Where to view token and cost breakdowns
- Cost tracking
- LLM calls: Automatically track costs based on token counts
- LLM calls: Sending costs directly
- Other runs: Sending costs

Building agents at scale introduces non-trivial, usage-based costs that can be difficult to track. LangSmith automatically records LLM token usage and costs for major providers, and also allows you to submit custom cost data for any additional components.This gives you a single, unified view of costs across your entire application, which makes it easy to monitor, understand, and debug your spend.This guide covers:

- Viewing costs in the LangSmith UI
- How cost tracking works
- How to send custom cost data

## ​ Viewing costs in the LangSmith UI

In the LangSmith UI, you can explore usage and spend in three main ways: first by understanding how tokens and costs are broken down, then by viewing those details within individual traces, and finally by inspecting aggregated metrics in project stats and dashboards.

### ​ Token and cost breakdowns

Token usage and costs are broken down into three categories:

- **Input**: Tokens in the prompt sent to the model. Subtypes include: cache reads, text tokens, image tokens, etc
- **Output**: Tokens generated in the response from the model. Subtypes include: reasoning tokens, text tokens, image tokens, etc
- **Other**: Costs from tool calls, retrieval steps or any custom runs.

You can view detailed breakdowns by hovering over cost sections in the UI. When available, each section is further categorized by subtype.!Cost tooltip!Cost tooltipYou can inspect these breakdowns throughout the LangSmith UI, described in the following section.

### ​ Where to view token and cost breakdowns

In the trace tree

The trace tree shows the most detailed view of token usage and cost (for a single trace). It displays the total usage for the entire trace, aggregated values for each parent run and token and cost breakdowns for each child run.Open any run inside a tracing project to view its trace tree.!Cost tooltip!Cost tooltip

In project stats

The project stats panel shows the total token usage and cost for all traces in a project.!Cost tracking chart!Cost tracking chart

In dashboards

Dashboards help you explore cost and token usage trends over time. The prebuilt dashboard for a tracing project shows total costs and a cost breakdown by input and output tokens.You may also configure custom cost tracking charts in custom dashboards.!Cost tracking chart!Cost tracking chart

## ​ Cost tracking

You can track costs in two ways:

1. Costs for LLM calls can be **automatically derived from token counts and model prices**
2. Cost for LLM calls or any other run type can be **manually specified as part of the run data**

The approach you use will depend on on what you’re tracking and how your model pricing is structured:

| Method | Run type: LLM | Run type: Other |
| --- | --- | --- |

| **Manually** | If LLM call costs are non-linear (eg. follow a custom cost function) | Send costs for any run types, e.g. tool calls, retrieval steps |

### ​ LLM calls: Automatically track costs based on token counts

To compute cost automatically from token usage, you need to provide **token counts**, the **model and provider** and the **model price**.

Follow the instructions below if you’re using model providers whose responses don’t follow the same patterns as one of OpenAI or Anthropic.These steps are **only required** if you are _not_:

- Calling LLMs with LangChain
- Using `@traceable` to trace LLM calls to OpenAI, Anthropic or models that follow an OpenAI-compliant format
- Using LangSmith wrappers for OpenAI or Anthropic.

**1\. Send token counts**Many models include token counts as part of the response. You must extract this information and include it in your run using one of the following methods:

A. Set a \`usage\_metadata\` field on the run’s metadata

Set a `usage_metadata` field on the run’s metadata. The advantage of this approach is that you do not need to change your traced function’s runtime outputs

Python

TypeScript

Copy

from langsmith import traceable, get_current_run_tree

inputs = [\
{"role": "system", "content": "You are a helpful assistant."},\
{"role": "user", "content": "I'd like to book a table for two."},\
]

@traceable(
run_type="llm",
metadata={"ls_provider": "my_provider", "ls_model_name": "my_model"}
)
def chat_model(messages: list):
# Imagine this is the real model output format your application expects
assistant_message = {
"role": "assistant",
"content": "Sure, what time would you like to book the table for?"
}

# Token usage you compute or receive from the provider
token_usage = {
"input_tokens": 27,
"output_tokens": 13,
"total_tokens": 40,
"input_token_details": {"cache_read": 10}
}

# Attach token usage to the LangSmith run
run = get_current_run_tree()
run.set(usage_metadata=token_usage)

return assistant_message

chat_model(inputs)

B. Return a \`usage\_metadata\` field in your traced function's outputs.

Include the `usage_metadata` key directly within the object returned by your traced function. LangSmith will extract it from the output.

from langsmith import traceable

inputs = [\
{"role": "system", "content": "You are a helpful assistant."},\
{"role": "user", "content": "I'd like to book a table for two."},\
]
output = {
"choices": [\
{\
"message": {\
"role": "assistant",\
"content": "Sure, what time would you like to book the table for?"\
}\
}\
],
"usage_metadata": {
"input_tokens": 27,
"output_tokens": 13,
"total_tokens": 40,
"input_token_details": {"cache_read": 10}
},
}

@traceable(
run_type="llm",
metadata={"ls_provider": "my_provider", "ls_model_name": "my_model"}
)
def chat_model(messages: list):
return output

In either case, the usage metadata should contain a subset of the following LangSmith-recognized fields:

Usage Metadata Schema and Cost Calculation

The following fields in the `usage_metadata` dict are recognized by LangSmith. You can view the full Python types or TypeScript interfaces directly.

​

input\_tokens

number

Number of tokens used in the model input. Sum of all input token types.

output\_tokens

Number of tokens used in the model response. Sum of all output token types.

total\_tokens

Number of tokens used in the input and output. Optional, can be inferred. Sum of input\_tokens + output\_tokens.

input\_token\_details

object

Breakdown of input token types. Keys are token-type strings, values are counts. Example `{"cache_read": 5}`.Known fields include: `audio`, `text`, `image`, `cache_read`, `cache_creation`. Additional fields are possible depending on the model or provider.

output\_token\_details

Breakdown of output token types. Keys are token-type strings, values are counts. Example `{"reasoning": 5}`.Known fields include: `audio`, `text`, `image`, `reasoning`. Additional fields are possible depending on the model or provider.

input\_cost

Cost of the input tokens.

output\_cost

Cost of the output tokens.

total\_cost

Cost of the tokens. Optional, can be inferred. Sum of input\_cost + output\_cost.

input\_cost\_details

Details of the input cost. Keys are token-type strings, values are cost amounts.

output\_cost\_details

Details of the output cost. Keys are token-type strings, values are cost amounts.

**Cost Calculations**The cost for a run is computed greedily from most-to-least specific token type. Suppose you set a price of $2 per 1M input tokens with a detailed price of $1 per 1M `cache_read` input tokens, and $3 per 1M output tokens. If you uploaded the following usage metadata:

{
"input_tokens": 20,
"input_token_details": {"cache_read": 5},
"output_tokens": 10,
"total_tokens": 30,
}

Then, the token costs would be computed as follows:

# Notice that LangSmith computes the cache_read cost and then for any
# remaining input_tokens, the default input price is applied.
input_cost = 5 * 1e-6 + (20 - 5) * 2e-6 # 3.5e-5
output_cost = 10 * 3e-6 # 3e-5
total_cost = input_cost + output_cost # 6.5e-5

**2\. Specify model name**When using a custom model, the following fields need to be specified in a run’s metadata in order to associate token counts with costs. It’s also helpful to provide these metadata fields to identify the model when viewing traces and when filtering.

- `ls_provider`: The provider of the model, e.g., “openai”, “anthropic”
- `ls_model_name`: The name of the model, e.g., “gpt-4o-mini”, “claude-3-opus-20240229”

**3\. Set model prices**A model pricing map is used to map model names to their per-token prices to compute costs from token counts. LangSmith’s model pricing table is used for this.

The table comes with pricing information for most OpenAI, Anthropic, and Gemini models. You can add prices for other models, or overwrite pricing for default models if you have custom pricing.

For models that have different pricing for different token types (e.g., multimodal or cached tokens), you can specify a breakdown of prices for each token type. Hovering over the `...` next to the input/output prices shows you the price breakdown by token type.!Model price map!Model price map

Updates to the model pricing map are not reflected in the costs for traces already logged. We do not currently support backfilling model pricing changes.

Create a new or modify an existing model price entry

To modify the default model prices, create a new entry with the same model, provider and match pattern as the default entry.To create a _new entry_ in the model pricing map, click on the `+ Model` button in the top right corner.!New price map entry interface!New price map entry interfaceHere, you can specify the following fields:

- **Model Name**: The human-readable name of the model.
- **Input Price**: The cost per 1M input tokens for the model. This number is multiplied by the number of tokens in the prompt to calculate the prompt cost.
- **Input Price Breakdown** (Optional): The breakdown of price for each different type of input token, e.g. `cache_read`, `video`, `audio`
- **Output Price**: The cost per 1M output tokens for the model. This number is multiplied by the number of tokens in the completion to calculate the completion cost.
- **Output Price Breakdown** (Optional): The breakdown of price for each different type of output token, e.g. `reasoning`, `image`, etc.
- **Model Activation Date** (Optional): The date from which the pricing is applicable. Only runs after this date will apply this model price.
- **Match Pattern**: A regex pattern to match the model name. This is used to match the value for `ls_model_name` in the run metadata.
- **Provider** (Optional): The provider of the model. If specified, this is matched against `ls_provider` in the run metadata.

Once you have set up the model pricing map, LangSmith will automatically calculate and aggregate the token-based costs for traces based on the token counts provided in the LLM invocations.

### ​ LLM calls: Sending costs directly

If your model follows a non-linear pricing scheme, we recommend calculating costs client-side and sending them to LangSmith as `usage_metadata`.

Gemini 3 Pro Preview and Gemini 2.5 Pro follow a pricing scheme with a stepwise cost function. We support this pricing scheme for Gemini by default. For any other models with non-linear pricing, you will need to follow these instructions to calculate costs.

@traceable(
run_type="llm",
metadata={"ls_provider": "my_provider", "ls_model_name": "my_model"}
)
def chat_model(messages: list):
llm_output = {
"choices": [\
{\
"message": {\
"role": "assistant",\
"content": "Sure, what time would you like to book the table for?"\
}\
}\
],
"usage_metadata": {
# Specify cost (in dollars) for the inputs and outputs
"input_cost": 1.1e-6,
"input_cost_details": {"cache_read": 2.3e-7},
"output_cost": 5.0e-6,
},
}
run = get_current_run_tree()
run.set(usage_metadata=llm_output["usage_metadata"])
return llm_output["choices"][0]["message"]

### ​ Other runs: Sending costs

You can also send cost information for any non-LLM runs, such as tool calls.The cost must be specified in the `total_cost` field under the runs `usage_metadata`.

A. Set a \`total\_cost\` field on the run’s usage\_metadata

Set a `total_cost` field on the run’s `usage_metadata`. The advantage of this approach is that you do not need to change your traced function’s runtime outputs

# Example tool: get_weather
@traceable(run_type="tool", name="get_weather")
def get_weather(city: str):
# Your tool logic goes here
result = {
"temperature_f": 68,
"condition": "sunny",
"city": city,
}

# Cost for this tool call (computed however you like)
tool_cost = 0.0015

# Attach usage metadata to the LangSmith run
run = get_current_run_tree()
run.set(usage_metadata={"total_cost": tool_cost})

# Return only the actual tool result (no usage info)
return result

tool_response = get_weather("San Francisco")

B. Return a \`total\_cost\` field in your traced function's outputs.

@traceable(run_type="tool", name="get_weather")
def get_weather(city: str):
result = {
"temperature_f": 68,
"condition": "sunny",
"city": city,
}

# Attach tool call costs here
return {
**result,
"usage_metadata": {
"total_cost": 0.0015, # <-- cost for this tool call
},
}

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Set a sampling rate for traces\\
\\
Previous Implement distributed tracing\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/filter-traces-in-application

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Viewing & managing traces

Filter traces

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Creating and Applying Filters
- Filtering by run attributes
- Filtering by time range
- Filter operators
- Specific Filtering Techniques
- Filter for intermediate runs (spans)
- Filter based on inputs and outputs
- Filter based on input / output key-value pairs
- Example: Filtering for tool calls
- Negative filtering on key-value pairs
- Save a filter
- Save a filter
- Use a saved filter
- Update a saved filter
- Delete a saved filter
- Copy a filter
- Filtering runs within the trace view
- Manually specify a raw query in LangSmith query language
- Use an AI Query to auto-generate a query (Experimental)
- Advanced filters
- Filter for intermediate runs (spans) on properties of the root
- Filter for runs (spans) whose child runs have some attribute
- Example: Filtering on all runs whose tree contains the tool call filter

**Recommended reading**: It might be helpful to read the Conceptual guide on tracing to gain familiarity with the concepts mentioned on this page.

Tracing projects can contain a significant amount of data. Filters are used for effectively navigating and analyzing this data, allowing you to:

- **Have focused investigations**: Quickly narrow down to specific runs for ad-hoc analysis
- **Debug and analyze**: Identify and examine errors, failed runs, and performance bottlenecks

This page contains a series of guides for how to filter runs in a tracing project. If you are programmatically exporting runs for analysis via the API or SDK, please refer to the exporting traces guide for more information.

## ​ Creating and Applying Filters

### ​ Filtering by run attributes

There are two ways to filter runs in a tracing project:

1. **Filters**: Located towards the top-left of the tracing projects page. This is where you construct and manage detailed filter criteria.

2. **Filter Shortcuts**: Positioned on the right sidebar of the tracing projects page. The filter shortcuts bar provides quick access to filters based on the most frequently occurring attributes in your project’s runs.

**Default filter**By default, the `IsTrace` is `true` filter is applied. This displays only top-level traces. Removing this filter will show all runs, including intermediate spans, in the project.

### ​ Filtering by time range

In addition to filtering by run attributes, you can also filter runs within a specific time range. This option is available towards the top-left of the tracing projects page.!Filtering on time

### ​ Filter operators

The available filter operators depend on the data type of the attribute you are filtering on. Here’s an overview of common operators:

- **is**: Exact match on the filter value
- **is not**: Negative match on the filter value
- **contains**: Partial match on the filter value
- **does not contain**: Negative partial match on the filter value
- **is one of**: Match on any of the values in the list

## ​ Specific Filtering Techniques

### ​ Filter for intermediate runs (spans)

In order to filter for intermediate runs (spans), you first need to remove the default `IsTrace` is `true` filter. For example, you would do this if you wanted to filter by `run name` for sub runs or filter by `run type`.Run metadata and tags are also powerful to filter on. These rely on good tagging across all parts of your pipeline. To learn more, you can check out this guide.

### ​ Filter based on inputs and outputs

You can filter runs based on the content in the inputs and outputs of the run.To filter either inputs or outputs, you can use the `Full-Text Search` filter which will match keywords in either field. For more targeted search, you can use the `Input` or `Output` filters which will only match content based on the respective field.

For performance, we index up to 250 characters of data for full-text search. If your search query exceeds this limit, we recommend using Input/Output key-value search instead.

You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided.Note that keyword search is done by splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common JSON keywords).!FilteringBased on the filters above, the system will search for `python` and `tensorflow` in either inputs or outputs, and `embedding` in the inputs along with `fine` and `tune` in the outputs.

### ​ Filter based on input / output key-value pairs

In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.

We index up to 100 unique keys to keep your data organized and searchable. Each key also has a character limit of 250 characters per value. If your data exceeds either of these limits, the text won’t be indexed. This helps us ensure fast, reliable performance.

To filter based on key-value pairs, select the `Input Key` or `Output Key` filter from the filters dropdown.For example, to match the following input:

Copy

{
"input": "What is the capital of France?"
}

Select `Filters`, `Add Filter` to bring up the filtering options. Then select `Input Key`, enter `input` as the key and enter `What is the capital of France?` as the value.!FilteringYou can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output:

{
"documents": [\
{\
"page_content": "The capital of France is Paris",\
"metadata": {},\
"type": "Document"\
}\
]
}

Select `Output Key`, enter `documents.page_content` as the key and enter `The capital of France is Paris` as the value. This will match the nested key `documents.page_content` with the specified value.!FilteringYou can add multiple key-value filters to create more complex queries. You can also use the `Filter Shortcuts` on the right side to quickly filter based on common key-value pairs as shown below:!Filtering

### ​ Example: Filtering for tool calls

It’s common to want to search for traces that contain specific tool calls. Tool calls are typically indicated in the output of an LLM run. To filter for tool calls, you would use the `Output Key` filter.While this example will show you how to filter for tool calls, the same logic can be applied to filter for any key-value pair in the output.In this case, let’s assume this is the output you want to filter for:

{
"generations": [\
[\
{\
"text": "",\
"type": "ChatGeneration",\
"message": {\
"lc": 1,\
"type": "constructor",\
"id": [],\
"kwargs": {\
"type": "ai",\
"id": "run-ca7f7531-f4de-4790-9c3e-960be7f8b109",\
"tool_calls": [\
{\
"name": "Plan",\
"args": {\
"steps": [\
"Research LangGraph's node configuration capabilities",\
"Investigate how to add a Python code execution node",\
"Find an example or create a sample implementation of a code execution node"\
]\
},\
"id": "toolu_01XexPzAVknT3gRmUB5PK5BP",\
"type": "tool_call"\
}\
]\
}\
}\
}\
]\
],
"llm_output": null,
"run": null,
"type": "LLMResult"
}

With the example above, the KV search will map each nested JSON path as a key-value pair that you can use to search and filter.LangSmith will break it into the following set of searchable key-value pairs:

| Key | Value |
| --- | --- |
| `generations.type` | `ChatGeneration` |
| `generations.message.type` | `constructor` |
| `generations.message.kwargs.type` | `ai` |
| `generations.message.kwargs.id` | `run-ca7f7531-f4de-4790-9c3e-960be7f8b109` |
| `generations.message.kwargs.tool_calls.name` | `Plan` |
| `generations.message.kwargs.tool_calls.args.steps` | `Research LangGraph's node configuration capabilities` |
| `generations.message.kwargs.tool_calls.args.steps` | `Investigate how to add a Python code execution node` |
| `generations.message.kwargs.tool_calls.args.steps` | `Find an example or create a sample implementation of a code execution node` |
| `generations.message.kwargs.tool_calls.id` | `toolu_01XexPzAVknT3gRmUB5PK5BP` |
| `generations.message.kwargs.tool_calls.type` | `tool_call` |
| `type` | `LLMResult` |

To search for a specific tool call, you can use the following Output Key search while removing the root runs filter:`generations.message.kwargs.tool_calls.name` = `Plan`This will match root and non-root runs where the `tool_calls` name is `Plan`.!Filtering

### ​ Negative filtering on key-value pairs

Different types of negative filtering can be applied to `Metadata`, `Input Key`, and `Output Key` fields to exclude specific runs from your results.For example, to find all runs where the metadata key `phone` is not equal to `1234567890`, set the `Metadata``Key` operator to `is` and `Key` field to `phone`, then set the `Value` operator to `is not` and the `Value` field to `1234567890`. This will match all runs that have a metadata key `phone` with any value except `1234567890`.!FilteringTo find runs that don’t have a specific metadata key, set the `Key` operator to `is not`. For example, setting the `Key` operator to `is not` with `phone` as the key will match all runs that don’t have a `phone` field in their metadata.!FilteringYou can also filter for runs that neither have a specific key nor a specific value. To find runs where the metadata has neither the key `phone` nor any field with the value `1234567890`, set the `Key` operator to `is not` with key `phone`, and the `Value` operator to `is not` with value `1234567890`.!FilteringFinally, you can also filter for runs that do not have a specific key but have a specific value. To find runs where there is no `phone` key but there is a value of `1234567890` for some other key, set the `Key` operator to `is not` with key `phone`, and the `Value` operator to `is` with value `1234567890`.!FilteringNote that you can use the `does not contain` operator instead of `is not` to perform a substring match.

## ​ Save a filter

Saving filters allows you to store and reuse frequently used filter configurations. Saved filters are specific to a tracing project.

#### ​ Save a filter

In the filter box, click the **Save filter** button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.!Filtering

#### ​ Use a saved filter

After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than three saved filters, only two will be displayed directly, with the rest accessible via a “more” menu. You can use the settings icon in the saved filter bar to optionally hide default saved filters.!Filtering

#### ​ Delete a saved filter

Click the settings icon in the saved filter bar, and delete a filter using the trash icon.

## ​ Copy a filter

You can copy a constructed filter to share it with colleagues, reuse it later, or query runs programmatically in the API or SDK.In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.This will give you a string representing the filter in the LangSmith query language. For example: `and(eq(is_root, true), and(eq(feedback_key, "user_score"), eq(feedback_score, 1)))`. For more information on the query language syntax, please refer to this reference.!Copy Filter

## ​ Filtering runs within the trace view

You can also apply filters directly within the trace view, which is useful for sifting through traces with a large number of runs. The same filters available in the main runs table view can be applied here.By default, only the runs that match the filters will be shown. To see the matched runs within the broader context of the trace tree, switch the view option from “Filtered Only” to “Show All” or “Most relevant”.!Filtering within trace view

## ​ Manually specify a raw query in LangSmith query language

If you have copied a previously constructed filter, you may want to manually apply this raw query in a future session.In order to do this, you can click on **Advanced filters** on the bottom of the filters popover. From there you can paste a raw query into the text box.Note that this will add that query to the existing queries, not overwrite it.!Raw Query

## ​ Use an AI Query to auto-generate a query (Experimental)

Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we’ve added an `AI Query` functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query.For example: “All runs longer than 10 seconds”!AI Query

## ​ Advanced filters

### ​ Filter for intermediate runs (spans) on properties of the root

A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it.In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Trace filters`. These filters will apply to the traces of all the parent runs of the individual runs you’ve already filtered for.!Filtering

### ​ Filter for runs (spans) whose child runs have some attribute

This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name `Foo`. This is useful when `Foo` is not always called, but you want to analyze the cases where it is.In order to do this, you can click on the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Tree filters`. This will make the rule you specify apply to all child runs of the individual runs you’ve already filtered for.!Filtering

### ​ Example: Filtering on all runs whose tree contains the tool call filter

Extending the tool call filtering example from above, if you would like to filter for all runs _whose tree contains_ the tool filter call, you can use the tree filter in the advanced filters setting:!Filtering

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Beta LangSmith Collector-Proxy\\
\\
Previous Query traces (SDK)\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/export-traces

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Viewing & managing traces

Query traces (SDK)

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Use filter arguments
- List all runs in a project
- List LLM and Chat runs in the last 24 hours
- List root runs in a project
- List runs without errors
- List runs by run ID
- Use filter query language
- List all root runs in a conversational thread
- List all runs called “extractor” whose root of the trace was assigned feedback “user\_score” score of 1
- List runs with “star\_rating” key whose score is greater than 4
- List runs that took longer than 5 seconds to complete
- List all runs that have “error” not equal to null
- List all runs where start\_time is greater than a specific timestamp
- List all runs that contain the string “substring”
- List all runs that are tagged with the git hash “2aa1cf4”
- List all runs that started after a specific timestamp and either have “error” not equal to null or a “Correctness” feedback score equal to 0
- Complex query: List all runs where tags include “experimental” or “beta” and latency is greater than 2 seconds
- Search trace trees by full text
- Check for presence of metadata
- Check for environment details in metadata
- Check for conversation ID in metadata
- Negative filtering on key-value pairs
- Combine multiple filters
- Tree Filter
- Advanced: export flattened trace view with child tool usage
- Advanced: export retriever IO for traces with feedback

**Recommended Reading**Before diving into this content, it might be helpful to read the following:

- Run (span) data format
- LangSmith trace query syntax

**If you are looking to export a large volume of traces, we recommend that you use the Bulk Data Export functionality, as it will better handle large data volumes and will support automatic retries and parallelization across partitions.**

The recommended way to query runs (the span data in LangSmith traces) is to use the `list_runs` method in the SDK or `/runs/query` endpoint in the API.LangSmith stores traces in a simple format that is specified in the Run (span) data format.

## ​ Use filter arguments

For simple queries, you don’t have to rely on our query syntax. You can use the filter arguments specified in the filter arguments reference.

**Prerequisites**Initialize the client before running the below code snippets.

Python

TypeScript

Copy

from langsmith import Client

client = Client()

Below are some examples of ways to list runs using keyword arguments:

### ​ List all runs in a project

### ​ List LLM and Chat runs in the last 24 hours

todays_llm_runs = client.list_runs(

start_time=datetime.now() - timedelta(days=1),
run_type="llm",
)

### ​ List root runs in a project

Root runs are runs that have no parents. These are assigned a value of `True` for `is_root`. You can use this to filter for root runs.

root_runs = client.list_runs(

is_root=True
)

### ​ List runs without errors

### ​ List runs by run ID

**Ignores Other Arguments**If you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like `project_name`, `run_type`, etc. and directly return the runs matching the given IDs.

If you have a list of run IDs, you can list them directly:

run_ids = ['a36092d2-4ad5-4fb4-9c0d-0dba9a2ed836','9398e6be-964f-4aa4-8ae9-ad78cd4b7074']
selected_runs = client.list_runs(id=run_ids)

## ​ Use filter query language

For more complex queries, you can use the query language described in the filter query language reference.

### ​ List all root runs in a conversational thread

This is the way to fetch runs in a conversational thread. For more information on setting up threads, refer to our how-to guide on setting up threads.
Threads are grouped by setting a shared thread ID. The LangSmith UI lets you use any one of the following three metadata keys: `session_id`, `conversation_id`, or `thread_id`. The session ID is also known as the tracing project ID. The following query matches on any of them.

filter_string = f'and(in(metadata_key, ["session_id","conversation_id","thread_id"]), eq(metadata_value, "{group_key}"))'
thread_runs = client.list_runs(

filter=filter_string,
is_root=True
)

### ​ List all runs called “extractor” whose root of the trace was assigned feedback “user\_score” score of 1

client.list_runs(

filter='eq(name, "extractor")',
trace_filter='and(eq(feedback_key, "user_score"), eq(feedback_score, 1))'
)

### ​ List runs with “star\_rating” key whose score is greater than 4

filter='and(eq(feedback_key, "star_rating"), gt(feedback_score, 4))'
)

### ​ List runs that took longer than 5 seconds to complete

### ​ List all runs that have “error” not equal to null

### ​ List all runs where start\_time is greater than a specific timestamp

### ​ List all runs that contain the string “substring”

### ​ List all runs that are tagged with the git hash “2aa1cf4”

### ​ List all runs that started after a specific timestamp and either have “error” not equal to null or a “Correctness” feedback score equal to 0

filter='and(gt(start_time, "2023-07-15T12:34:56Z"), or(neq(error, null), and(eq(feedback_key, "Correctness"), eq(feedback_score, 0.0))))'
)

### ​ Complex query: List all runs where tags include “experimental” or “beta” and latency is greater than 2 seconds

filter='and(or(has(tags, "experimental"), has(tags, "beta")), gt(latency, 2))'
)

### ​ Search trace trees by full text

You can use the `search()` function without any specific field to do a full text search across all string fields in a run. This allows you to quickly find traces that match a search term.

filter='search("image classification")'
)

### ​ Check for presence of metadata

If you want to check for the presence of metadata, you can use the `eq` operator, optionally with an `and` statement to match by value. This is useful if you want to log more structured information about your runs.

to_search = {
"user_id": ""
}

# Check for any run with the "user_id" metadata key
client.list_runs(
project_name="default",
filter="eq(metadata_key, 'user_id')"
)
# Check for runs with user_id=4070f233-f61e-44eb-bff1-da3c163895a3
client.list_runs(
project_name="default",
filter="and(eq(metadata_key, 'user_id'), eq(metadata_value, '4070f233-f61e-44eb-bff1-da3c163895a3'))"
)

### ​ Check for environment details in metadata

A common pattern is to add environment information to your traces via metadata. If you want to filter for runs containing environment metadata, you can use the same pattern as above:

client.list_runs(
project_name="default",
filter="and(eq(metadata_key, 'environment'), eq(metadata_value, 'production'))"
)

### ​ Check for conversation ID in metadata

Another common way to associate traces in the same conversation is by using a shared conversation ID. If you want to filter runs based on a conversation ID in this way, you can search for that ID in the metadata.

client.list_runs(
project_name="default",
filter="and(eq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))"
)

### ​ Negative filtering on key-value pairs

You can use negative filtering on metadata, input, and output key-value pairs to exclude specific runs from your results. Here are some examples for metadata key-value pairs but the same logic applies to input and output key-value pairs.

# Find all runs where the metadata does not contain a "conversation_id" key
client.list_runs(
project_name="default",
filter="and(neq(metadata_key, 'conversation_id'))"
)

# Find all runs where the conversation_id in metadata is not "a1b2c3d4-e5f6-7890"
client.list_runs(
project_name="default",
filter="and(eq(metadata_key, 'conversation_id'), neq(metadata_value, 'a1b2c3d4-e5f6-7890'))"
)

# Find all runs where there is no "conversation_id" metadata key and the "a1b2c3d4-e5f6-7890" value is not present
client.list_runs(
project_name="default",
filter="and(neq(metadata_key, 'conversation_id'), neq(metadata_value, 'a1b2c3d4-e5f6-7890'))"
)

# Find all runs where the conversation_id metadata key is not present but the "a1b2c3d4-e5f6-7890" value is present
client.list_runs(
project_name="default",
filter="and(neq(metadata_key, 'conversation_id'), eq(metadata_value, 'a1b2c3d4-e5f6-7890'))"
)

### ​ Combine multiple filters

If you want to combine multiple conditions to refine your search, you can use the `and` operator along with other filtering functions. Here’s how you can search for runs named “ChatOpenAI” that also have a specific `conversation_id` in their metadata:

client.list_runs(
project_name="default",
filter="and(eq(name, 'ChatOpenAI'), eq(metadata_key, 'conversation_id'), eq(metadata_value, '69b12c91-b1e2-46ce-91de-794c077e8151'))"
)

### ​ Tree Filter

List all runs named “RetrieveDocs” whose root run has a “user\_score” feedback of 1 and any run in the full trace is named “ExpandQuery”.This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace.

filter='eq(name, "RetrieveDocs")',
trace_filter='and(eq(feedback_key, "user_score"), eq(feedback_score, 1))',
tree_filter='eq(name, "ExpandQuery")'
)

### ​ Advanced: export flattened trace view with child tool usage

The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace.
This can be used to analyze the behavior of your agents across multiple traces.This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information.To optimize the query, the example:

1. Selects only the necessary fields when querying tool runs to reduce query time.
2. Fetches root runs in batches while processing tool runs concurrently.

from collections import defaultdict
from concurrent.futures import Future, ThreadPoolExecutor
from datetime import datetime, timedelta

from langsmith import Client
from tqdm.auto import tqdm

client = Client()
project_name = "my-project"
num_days = 30

# List all tool runs
tool_runs = client.list_runs(
project_name=project_name,
start_time=datetime.now() - timedelta(days=num_days),
run_type="tool",
# We don't need to fetch inputs, outputs, and other values that # may increase the query time
select=["trace_id", "name", "run_type"],
)

data = []
futures: list[Future] = []
trace_cursor = 0
trace_batch_size = 50

tool_runs_by_parent = defaultdict(lambda: defaultdict(set))
# Do not exceed rate limit
with ThreadPoolExecutor(max_workers=2) as executor:
# Group tool runs by parent run ID
for run in tqdm(tool_runs):
# Collect all tools invoked within a given trace
tool_runs_by_parent[run.trace_id]["tools_involved"].add(run.name)
# maybe send a batch of parent run IDs to the server
# this lets us query for the root runs in batches
# while still processing the tool runs
if len(tool_runs_by_parent) % trace_batch_size == 0:
if this_batch := list(tool_runs_by_parent.keys())[\
trace_cursor : trace_cursor + trace_batch_size\
]:
trace_cursor += trace_batch_size
futures.append(
executor.submit(
client.list_runs,
project_name=project_name,
run_ids=this_batch,
select=["name", "inputs", "outputs", "run_type"],
)
)
if this_batch := list(tool_runs_by_parent.keys())[trace_cursor:]:
futures.append(
executor.submit(
client.list_runs,
project_name=project_name,
run_ids=this_batch,
select=["name", "inputs", "outputs", "run_type"],
)
)

for future in tqdm(futures):
root_runs = future.result()
for root_run in root_runs:
root_data = tool_runs_by_parent[root_run.id]
data.append(
{
"run_id": root_run.id,
"run_name": root_run.name,
"run_type": root_run.run_type,
"inputs": root_run.inputs,
"outputs": root_run.outputs,
"tools_involved": list(root_data["tools_involved"]),
}
)

# (Optional): Convert to a pandas DataFrame
import pandas as pd

df = pd.DataFrame(data)
df.head()

### ​ Advanced: export retriever IO for traces with feedback

This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior.
The following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score.

import pandas as pd
from langsmith import Client
from tqdm.auto import tqdm

client = Client()
project_name = "your-project-name"
num_days = 1

retriever_runs = client.list_runs(
project_name=project_name,
start_time=datetime.now() - timedelta(days=num_days),
run_type="retriever",
# This time we do want to fetch the inputs and outputs, since they
# may be adjusted by query expansion steps.
select=["trace_id", "name", "run_type", "inputs", "outputs"],
trace_filter='eq(feedback_key, "user_score")',
)

retriever_runs_by_parent = defaultdict(lambda: defaultdict(list))
# Group retriever runs by parent run ID
with ThreadPoolExecutor(max_workers=2) as executor:
for run in tqdm(retriever_runs):
# Collect all retriever calls invoked within a given trace
for k, v in run.inputs.items():
retriever_runs_by_parent[run.trace_id][f"retriever.inputs.{k}"].append(v)
for k, v in (run.outputs or {}).items():
# Extend the docs
retriever_runs_by_parent[run.trace_id][f"retriever.outputs.{k}"].extend(v)
# while still processing the retriever runs
if len(retriever_runs_by_parent) % trace_batch_size == 0:
if this_batch := list(retriever_runs_by_parent.keys())[\
trace_cursor : trace_cursor + trace_batch_size\
]:
trace_cursor += trace_batch_size
futures.append(
executor.submit(
client.list_runs,
project_name=project_name,
run_ids=this_batch,
select=[\
"name",\
"inputs",\
"outputs",\
"run_type",\
"feedback_stats",\
],
)
)
if this_batch := list(retriever_runs_by_parent.keys())[trace_cursor:]:
futures.append(
executor.submit(
client.list_runs,
project_name=project_name,
run_ids=this_batch,
select=["name", "inputs", "outputs", "run_type"],
)
)

for future in tqdm(futures):
root_runs = future.result()
for root_run in root_runs:
root_data = retriever_runs_by_parent[root_run.id]
feedback = {
f"feedback.{k}": v.get("avg")
for k, v in (root_run.feedback_stats or {}).items()
}
inputs = {f"inputs.{k}": v for k, v in root_run.inputs.items()}
outputs = {f"outputs.{k}": v for k, v in (root_run.outputs or {}).items()}
data.append(
{
"run_id": root_run.id,
"run_name": root_run.name,
**inputs,
**outputs,
**feedback,
**root_data,
}
)

import pandas as pd
df = pd.DataFrame(data)
df.head()

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Filter traces\\
\\
Previous Compare traces\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/compare-traces

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Viewing & managing traces

Compare traces

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

To compare traces, click on the **Compare** button in the upper right hand side of any trace view.!Compare buttonThis will show the trace run table. Select the trace you want to compare against the original trace.!Select traceThe pane will open with both traces selected in a side by side comparison view.!Compare traceTo stop comparing, close the pane or click on **Stop comparing** in the upper right hand side of the pane.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Query traces (SDK)\\
\\
Previous Share or unshare a trace publicly\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/share-trace

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Viewing & managing traces

Share or unshare a trace publicly

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

**Sharing a trace publicly will make it accessible to anyone with the link. Make sure you’re not sharing sensitive information.**If your self-hosted or hybrid LangSmith deployment is within a VPC, then the public link is accessible only to members authenticated within your VPC. For enhanced security, we recommend configuring your instance with a private URL accessible only to users with access to your network.

To share a trace publicly, simply click on the **Share** button in the upper right hand side of any trace view.!Share traceThis will open a dialog where you can copy the link to the trace.Shared traces will be accessible to anyone with the link, even if they don’t have a LangSmith account. They will be able to view the trace, but not edit it.To “unshare” a trace, either:

1. Click on **Unshare** by clicking on **Public** in the upper right hand corner of any publicly shared trace, then **Unshare** in the dialog.!Unshare trace

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Compare traces\\
\\
Previous LangSmith Fetch\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/langsmith-fetch

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Viewing & managing traces

LangSmith Fetch

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Installation
- Setup
- Use with a coding agent
- Find project and trace IDs
- Usage
- Options
- Output formats
- Fetch a trace or thread
- Fetch multiple
- Include metadata and feedback
- Override the configured tracing project
- Export to files

LangSmith Fetch is a command-line interface (CLI) tool for retrieving trace data ( runs, traces, and threads) from your LangSmith projects. It allows you to use LangSmith’s tracing and debugging features directly in your terminal and development workflows.You can use LangSmith Fetch for the following use cases:

- Immediate debugging: Fetch the most recent trace of a failed or unexpected agent run with a single command.
- Bulk export for analysis: Export large numbers of traces or entire conversation threads to JSON files for offline analysis, building evaluation datasets, or regression tests.
- Terminal-based workflows: Integrate trace data into your existing tools; for example, piping output to Unix utilities like jq, or feeding traces into an AI coding assistant for automated analysis.

## ​ Installation

Copy

pip install langsmith-fetch

## ​ Setup

Set your LangSmith API key and project name:

export LANGSMITH_API_KEY=lsv2_...
export LANGSMITH_PROJECT=your-project-name

The CLI will automatically fetch traces or threads in `LANGSMITH_PROJECT`. Replace `your-project-name` with the name of your LangSmith project (if it doesn’t exist, it will be created automatically on first use).

`langsmith-fetch` only requires the `LANGSMITH_PROJECT` environment variable. It automatically looks up the project UUID and saves both to `~/.langsmith-cli/config.yaml`. You can also specify a project by its UUID via a CLI flag.

### ​ Use with a coding agent

After you’ve installed and set up `langsmith-fetch`, use your coding agent to ask questions like the following:

Use langsmith-fetch to analyze the last 3 threads from my LangSmith project for potential improvements

Many agents will use the `langsmith-fetch --help` command to understand how to use the CLI and complete your request.

## ​ Find project and trace IDs

In most cases, you won’t need to find IDs manually (the CLI uses your project name and latest traces by default). However, if you want to fetch a specific item by ID, you can find them in the LangSmith UI:

- **Project UUID**: Each project has a unique ID (UUID). You can find it in the project’s URL or by hovering over **ID** next to the project’s name. This UUID can be used with the `--project-uuid` flag on CLI commands

## ​ Usage

After installation and setup, you can use the `langsmith-fetch` command to retrieve traces or threads. The general usage is:

langsmith-fetch COMMAND [ARGUMENTS] [OPTIONS]

LangSmith Fetch provides the following commands to fetch either single items or in bulk:

| Command | Fetches | Output location |
| --- | --- | --- |

| `traces [directory]` | Recent traces from the project (multiple) | Saves each trace as a JSON file in the given directory, or prints to stdout if no directory is provided. **Tip:** Using a directory is recommended for bulk exports. |
| `threads [directory]` | Recent threads from the project (multiple) | Saves each thread as a JSON file in the given directory, or prints to stdout if no directory is provided. |

Traces are fetched chronologically with most recent first.

### ​ Options

The commands support additional flags to filter and format the output:

| Option / Flag | Applies to | Description | Default |
| --- | --- | --- | --- |

| `--include-metadata` | `traces` (bulk fetch) | Include run metadata in the output (such as tokens used, execution time, status, costs). This will add a `"metadata"` section to each trace’s JSON. | _Off by default_ |
| `--include-feedback` | `traces` (bulk fetch) | Include any feedback entries attached to the runs. Enabling this will make an extra API call for each trace to fetch feedback data. | _Off by default_ |

| `--no-progress` | `traces`, `threads` | Disable the progress bar output. By default a progress indicator is shown when fetching multiple items; use this flag to hide it (useful for non-interactive scripts). | Progress bar on |

### ​ Output formats

The `--format` option controls how the fetched data is displayed:

- `pretty` (default): A human-readable view with rich text formatting for easy inspection in the terminal. This format is great for quick debugging of a single trace or thread.By default:

Explicitly specify the format:

- `json`: Well-formatted JSON output with syntax highlighting. Use this if you want to examine the raw data structure or pipe it into JSON processing tools.

- `raw`: Compact JSON with no extra whitespace. This is useful for piping the output to other programs (e.g., using `jq` or saving directly) without extra formatting.

### ​ Fetch a trace or thread

You can fetch a single thread or trace with the ID. The command will output to the terminal by default:

### ​ Fetch multiple

For bulk fetches of traces or threads, we recommend specifying a target directory path. Each fetched trace or thread will be saved as a separate JSON file in that folder, making it easy to browse or process them later.

You can specify a destination directory for the bulk commands (`traces`/`threads`). For example, the following command will save the 10 most recent traces as JSON files in the `my-traces-data` directory:

langsmith-fetch traces ./my-traces-data --limit 10

langsmith-fetch threads ./my-thread-data --limit 10

If you omit the directory and `--limit`, the tool will output the results of the most recent, single trace to your terminal.When sending to a directory, files will be named in the following way:

- Default: Files named by trace ID (e.g., `3b0b15fe-1e3a-4aef-afa8-48df15879cfe.json`).
- Custom pattern: Use `--filename-pattern` with placeholders:

- `{trace_id}`: Trace ID (default: `{trace_id}.json`).
- `{index}` or `{idx}`: Sequential number starting from 1.
- Format specs supported: `{index:03d}` for zero-padded numbers.

### ​ Include metadata and feedback

You can include run metadata and any feedback associated with the trace:

langsmith-fetch traces --limit 1 --include-metadata --include-feedback

RUN METADATA

Status: success
Start Time: 2025-12-12T18:05:47.558274
End Time: 2025-12-12T18:05:48.811072
Duration: 1252ms

Token Usage:
Prompt: 15
Completion: 88
Total: 103

Costs:
Total: $0.00014
Prompt: $0.00001
Completion: $0.00013

Custom Metadata:
LANGSMITH_PROJECT: weather-demo
LANGSMITH_TRACING: true
ls_run_depth: 0

Feedback Stats:
correctness: {'n': 1, 'avg': 1.0, 'stdev': 0.0, 'errors': 0, 'show_feedback_arrow': False, 'comments': [''], 'sources':
['{"type":"app","metadata":null,"user_id":"d5ee8d42-a274-4f32-9c35-b765287fe5ec","ls_user_id":"ac375f5f-0da0-44c1-82a2-0ecfd6ecac27"}'],
'session_min_score': 1.0, 'session_max_score': 1.0, 'values': {}, 'contains_thread_feedback': False}
note: {'n': 0, 'avg': None, 'stdev': None, 'errors': 0, 'show_feedback_arrow': False, 'comments': ['The answer should be more specific on rainfall\
totals.'], 'sources':
['{"type":"app","metadata":null,"user_id":"d5ee8d42-a274-4f32-9c35-b765287fe5ec","ls_user_id":"ac375f5f-0da0-44c1-82a2-0ecfd6ecac27"}'],
'session_min_score': None, 'session_max_score': None, 'values': {}, 'contains_thread_feedback': False}
...

### ​ Override the configured tracing project

To fetch traces from a different project than the one configured with `LANGSMITH_PROJECT`, use the `--project-uuid` option:

Running this command will just fetch traces from that project, it will not modify the LangSmith project already configured in `~/.langsmith-cli/config.yaml`.

### ​ Export to files

You can fetch traces or full threads and export to a file:

langsmith-fetch threads ./my_threads --since 2025-12-01T00:00:00Z

This command retrieves all threads that have occurred since December 1, 2025, saving each conversation as a JSON file under `./my_threads`. This is useful for exporting chat transcripts or building regression tests on multi-turn conversations. You could also use `--limit` with threads to fetch a specific number of recent threads, and `--last-n-minutes` works here as well.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Share or unshare a trace publicly\\
\\
Previous View server logs for a trace\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/platform-logs

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Viewing & managing traces

View server logs for a trace

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Access server logs from trace view
- Server logs view
- Filtering logs by trace ID

When viewing a trace that was generated by a run in LangSmith, you can access the associated server logs directly from the trace view.

Viewing server logs for a trace only works with the Cloud SaaS and fully self-hosted deployment options.

## ​ Access server logs from trace view

In the trace view, use the **See Logs** button in the top right corner, next to the **Run in Studio** button.!View server logs buttonClicking this button will take you to the server logs view for the associated deployment in LangSmith.

## ​ Server logs view

The server logs view displays logs from both:

- **Agent Server’s own operational logs** \- Internal server operations, API calls, and system events
- **User application logs**\- Logs written in your graph with:

- Python: Use the `logging` or `structlog` libraries
- JavaScript: Use the re-exported Winston logger from `@langchain/langgraph-sdk/logging`:

Copy

import { getLogger } from "@langchain/langgraph-sdk/logging";

const logger = getLogger();
logger.info("Your log message");

## ​ Filtering logs by trace ID

When you navigate from the trace view, the **Filters** box will automatically pre-fill with the Trace ID from the trace you just viewed.This allows you to quickly filter the logs to see only those related to your specific trace execution.!Lgp server logs filters

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Fetch\\
\\
Previous Bulk Exporting Trace Data\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/data-export

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Viewing & managing traces

Bulk Exporting Trace Data

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Destinations
- Exporting Data
- Destinations - Providing a S3 bucket
- Preparing the Destination
- Credentials configuration
- AWS S3 bucket
- Google GCS XML S3 compatible bucket
- Create an export job
- Limiting exported fields
- Scheduled exports
- Monitoring the Export Job
- Monitor Export Status
- List Runs for an Export
- List All Exports
- Stop an Export
- Partitioning Scheme
- Importing Data into other systems
- BigQuery
- Snowflake
- RedShift
- Clickhouse
- DuckDB
- Error Handling
- Debugging Destination Errors
- Monitoring Runs
- Common Errors

**Plan restrictions apply**Please note that the Data Export functionality is only supported for LangSmith Plus or Enterprise tiers.

LangSmith’s bulk data export functionality allows you to export your traces into an external destination. This can be useful if you want to analyze the
data offline in a tool such as BigQuery, Snowflake, RedShift, Jupyter Notebooks, etc.An export can be launched to target a specific LangSmith project and date range. Once a batch export is launched, our system will handle the orchestration and resilience of the export process.
Please note that exporting your data may take some time depending on the size of your data. We also have a limit on how many of your exports can run at the same time.
Bulk exports also have a runtime timeout of 24 hours.

## ​ Destinations

Currently we support exporting to an S3 bucket or S3 API compatible bucket that you provide. The data will be exported in
Parquet columnar format. This format will allow you to easily import the data into
other systems. The data export will contain equivalent data fields as the Run data format.

## ​ Exporting Data

### ​ Destinations - Providing a S3 bucket

To export LangSmith data, you will need to provide an S3 bucket where the data will be exported to.The following information is needed for the export:

- **Bucket Name**: The name of the S3 bucket where the data will be exported to.
- **Prefix**: The root prefix within the bucket where the data will be exported to.
- **S3 Region**: The region of the bucket - this is needed for AWS S3 buckets.
- **Endpoint URL**: The endpoint URL for the S3 bucket - this is needed for S3 API compatible buckets.
- **Access Key**: The access key for the S3 bucket.
- **Secret Key**: The secret key for the S3 bucket.
- **Include Bucket in Prefix** (optional): Whether to include the bucket name as part of the path prefix. Defaults to `false` for new destinations or when the bucket name is already present in the path. Set to `true` for legacy compatibility or when using storage systems that require the bucket name in the path.

We support any S3 compatible bucket, for non AWS buckets such as GCS or MinIO, you will need to provide the endpoint URL.

### ​ Preparing the Destination

**For self-hosted and EU region deployments**Update the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below.
For the EU region, use `eu.api.smith.langchain.com`.

**Permissions required**Both the `backend` and `queue` services require write access to the destination bucket:

- The `backend` service attempts to write a test file to the destination bucket when the export destination is created.
It will delete the test file if it has permission to do so (delete access is optional).
- The `queue` service is responsible for bulk export execution and uploading the files to the bucket.

The following example demonstrates how to create a destination using cURL. Replace the placeholder values with your actual configuration details.
Note that credentials will be stored securely in an encrypted form in our system.

Copy

curl --request POST \
--url 'https://api.smith.langchain.com/api/v1/bulk-exports/destinations' \
--header 'Content-Type: application/json' \
--header 'X-API-Key: YOUR_API_KEY' \
--header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \
--data '{
"destination_type": "s3",
"display_name": "My S3 Destination",
"config": {
"bucket_name": "your-s3-bucket-name",
"prefix": "root_folder_prefix",
"region": "your aws s3 region",
"endpoint_url": "your endpoint url for s3 compatible buckets",
"include_bucket_in_prefix": true
},
"credentials": {
"access_key_id": "YOUR_S3_ACCESS_KEY_ID",
"secret_access_key": "YOUR_S3_SECRET_ACCESS_KEY"
}
}'

Use the returned `id` to reference this destination in subsequent bulk export operations.**If you receive an error while creating a destination, see debug destination errors for details on how to debug this.**

#### ​ Credentials configuration

We support the following additional credentials formats besides static `access_key_id` and `secret_access_key`:

- To use temporary credentials that include an AWS session token,
additionally provide the `credentials.session_token` key when creating the bulk export destination.
- (Self-hosted only): To use environment-based credentials such as with AWS IAM Roles for Service Accounts (IRSA),
omit the `credentials` key from the request when creating the bulk export destination.
In this case, the standard Boto3 credentials locations will be checked in the order defined by the library.

#### ​ AWS S3 bucket

For AWS S3, you can leave off the `endpoint_url` and supply the region that matches the region of your bucket.

curl --request POST \
--url 'https://api.smith.langchain.com/api/v1/bulk-exports/destinations' \
--header 'Content-Type: application/json' \
--header 'X-API-Key: YOUR_API_KEY' \
--header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \
--data '{
"destination_type": "s3",
"display_name": "My AWS S3 Destination",
"config": {
"bucket_name": "my_bucket",
"prefix": "data_exports",
"region": "us-east-1"
},
"credentials": {
"access_key_id": "YOUR_S3_ACCESS_KEY_ID",
"secret_access_key": "YOUR_S3_SECRET_ACCESS_KEY"
}
}'

#### ​ Google GCS XML S3 compatible bucket

When using Google’s GCS bucket, you need to use the XML S3 compatible API, and supply the `endpoint_url`
which is typically `https://storage.googleapis.com`.
Here is an example of the API request when using the GCS XML API which is compatible with S3:

curl --request POST \
--url 'https://api.smith.langchain.com/api/v1/bulk-exports/destinations' \
--header 'Content-Type: application/json' \
--header 'X-API-Key: YOUR_API_KEY' \
--header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \
--data '{
"destination_type": "s3",
"display_name": "My GCS Destination",
"config": {
"bucket_name": "my_bucket",
"prefix": "data_exports",
"endpoint_url": "https://storage.googleapis.com"
"include_bucket_in_prefix": true
},
"credentials": {
"access_key_id": "YOUR_S3_ACCESS_KEY_ID",
"secret_access_key": "YOUR_S3_SECRET_ACCESS_KEY"
}
}'

See Google documentation for more info

### ​ Create an export job

To export data, you will need to create an export job. This job will specify the destination, the project, the date range, and filter expression of the data to export. The filter expression is used to narrow down the set of runs exported and is optional. Not setting the filter field will export all runs. Refer to our filter query language and examples to determine the correct filter expression for your export.You can use the following cURL command to create the job:

curl --request POST \
--url 'https://api.smith.langchain.com/api/v1/bulk-exports' \
--header 'Content-Type: application/json' \
--header 'X-API-Key: YOUR_API_KEY' \
--header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \
--data '{
"bulk_export_destination_id": "your_destination_id",
"session_id": "project_uuid",
"start_time": "2024-01-01T00:00:00Z",
"end_time": "2024-01-02T23:59:59Z",
"filter": "and(eq(run_type, \"llm\"), eq(name, \"ChatOpenAI\"), eq(input_key, \"messages.content\"), like(input_value, \"%messages.content%\"))",
"format_version": "v2_beta"
}'

The `session_id` is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.

Use the returned `id` to reference this export in subsequent bulk export operations.

#### ​ Limiting exported fields

You can improve bulk export speed and reduce row size by limiting which fields are included in the exported Parquet files using the `export_fields` parameter. When `export_fields` is provided, only the specified fields are exported as columns in the Parquet files. When `export_fields` is not provided, all exportable fields are included.This is particularly useful when you want to exclude larger fields like `inputs` and `outputs`.The following example creates an export job that only includes specific fields:

curl --request POST \
--url 'https://api.smith.langchain.com/api/v1/bulk-exports' \
--header 'Content-Type: application/json' \
--header 'X-API-Key: YOUR_API_KEY' \
--header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \
--data '{
"bulk_export_destination_id": "your_destination_id",
"session_id": "project_uuid",
"start_time": "2024-01-01T00:00:00Z",
"end_time": "2024-01-02T23:59:59Z",
"export_fields": ["id", "name", "run_type", "start_time", "end_time", "status", "total_tokens", "total_cost"],
"format_version": "v2_beta"
}'

The `export_fields` parameter accepts an array of field names. Available fields include the Run data format fields as well as additional export-only fields:

- `tenant_id`
- `is_root`

**Performance tip**: Excluding `inputs` and `outputs` from your export can significantly improve export performance and reduce file sizes, especially for large runs. Only include these fields if you need them for your analysis.

### ​ Scheduled exports

Scheduled exports collect runs periodically and export to the configured destination.
To create a scheduled export, include `interval_hours` and remove `end_time`:

curl --request POST \
--url 'https://api.smith.langchain.com/api/v1/bulk-exports' \
--header 'Content-Type: application/json' \
--header 'X-API-Key: YOUR_API_KEY' \
--header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \
--data '{
"bulk_export_destination_id": "your_destination_id",
"session_id": "project_uuid",
"start_time": "2024-01-01T00:00:00Z",
"filter": "and(eq(run_type, \"llm\"), eq(name, \"ChatOpenAI\"), eq(input_key, \"messages.content\"), like(input_value, \"%messages.content%\"))",
"interval_hours": 1,
"format_version": "v2_beta"
}'

You can also use `export_fields` with scheduled exports to limit which fields are exported:

curl --request POST \
--url 'https://api.smith.langchain.com/api/v1/bulk-exports' \
--header 'Content-Type: application/json' \
--header 'X-API-Key: YOUR_API_KEY' \
--header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \
--data '{
"bulk_export_destination_id": "your_destination_id",
"session_id": "project_uuid",
"start_time": "2024-01-01T00:00:00Z",
"interval_hours": 1,
"export_fields": ["id", "name", "run_type", "start_time", "end_time", "status", "total_tokens", "total_cost"],
"format_version": "v2_beta"
}'

**Details**

- `interval_hours` must be between 1 hour and 168 hours (1 week) inclusive.
- For spawned exports, the first time range exported is `start_time=(scheduled_export_start_time), end_time=(start_time + interval_hours)`.
Then `start_time=(previous_export_end_time), end_time=(this_export_start_time + interval_hours)`, and so on.
- `end_time` must be omitted for scheduled exports. `end_time` is still required for non-scheduled exports.
- Scheduled exports can be stopped by cancelling the export.

- Exports that have been spawned by a scheduled export have the `source_bulk_export_id` attribute filled.
- If desired, these spawned bulk exports must be canceled separately from the source scheduled bulk export -
canceling the source bulk export **does not** cancel the spawned bulk exports.
- Spawned exports run at `end_time + 10 minutes` to account for any runs that are submitted with `end_time` in the recent past.
- `format_version` (optional): The format version to use for the parquet files. `"v2_beta"` has (1) enhanced datatypes for the columns and (2) a Hive-compliant folder structure.

**Example**If a scheduled bulk export is created with `start_time=2025-07-16T00:00:00Z` and `interval_hours=6`:

| Export | Start Time | End Time | Runs At |
| --- | --- | --- | --- |
| 1 | 2025-07-16T00:00:00Z | 2025-07-16T06:00:00Z | 2025-07-16T06:10:00Z |
| 2 | 2025-07-16T06:00:00Z | 2025-07-16T12:00:00Z | 2025-07-16T12:10:00Z |
| 3 | 2025-07-16T12:00:00Z | 2025-07-16T18:00:00Z | 2025-07-16T18:10:00Z |

## ​ Monitoring the Export Job

### ​ Monitor Export Status

To monitor the status of an export job, use the following cURL command:

curl --request GET \
--url 'https://api.smith.langchain.com/api/v1/bulk-exports/{export_id}' \
--header 'Content-Type: application/json' \
--header 'X-API-Key: YOUR_API_KEY' \
--header 'X-Tenant-Id: YOUR_WORKSPACE_ID'

Replace `{export_id}` with the ID of the export you want to monitor. This command retrieves the current status of the specified export job.

### ​ List Runs for an Export

An export is typically broken up into multiple runs which correspond to a specific date partition to export.
To list all runs associated with a specific export, use the following cURL command:

curl --request GET \
--url 'https://api.smith.langchain.com/api/v1/bulk-exports/{export_id}/runs' \
--header 'Content-Type: application/json' \
--header 'X-API-Key: YOUR_API_KEY' \
--header 'X-Tenant-Id: YOUR_WORKSPACE_ID'

This command fetches all runs related to the specified export, providing details such as run ID, status, creation time, rows exported, etc.

### ​ List All Exports

To retrieve a list of all export jobs, use the following cURL command:

curl --request GET \
--url 'https://api.smith.langchain.com/api/v1/bulk-exports' \
--header 'Content-Type: application/json' \
--header 'X-API-Key: YOUR_API_KEY' \
--header 'X-Tenant-Id: YOUR_WORKSPACE_ID'

This command returns a list of all export jobs along with their current statuses and creation timestamps.

### ​ Stop an Export

To stop an existing export, use the following cURL command:

curl --request PATCH \
--url 'https://api.smith.langchain.com/api/v1/bulk-exports/{export_id}' \
--header 'Content-Type: application/json' \
--header 'X-API-Key: YOUR_API_KEY' \
--header 'X-Tenant-Id: YOUR_WORKSPACE_ID' \
--data '{
"status": "Cancelled"
}'

Replace `{export_id}` with the ID of the export you wish to cancel. Note that a job cannot be restarted once it has been cancelled,
you will need to create a new export job instead.

## ​ Partitioning Scheme

Data will be exported into your bucket into the follow Hive partitioned format:

## ​ Importing Data into other systems

Importing data from S3 and Parquet format is commonly supported by the majority of analytical systems. See below for documentation links:

### ​ BigQuery

To import your data into BigQuery, see Loading Data from Parquet and also
Hive Partitioned loads.

### ​ Snowflake

You can load data into Snowflake from S3 by following the Load from Cloud Document.

### ​ RedShift

You can COPY data from S3 or Parquet into Amazon Redshift by following the AWS COPY command documentation.

### ​ Clickhouse

You can directly query data in S3 / Parquet format in Clickhouse. As an example, if using GCS, you can query the data as follows:

'access_key_id', 'access_secret', 'Parquet')

See Clickhouse S3 Integration Documentation for more information.

### ​ DuckDB

You can query the data from S3 in-memory with SQL using DuckDB. See S3 import Documentation.

## ​ Error Handling

### ​ Debugging Destination Errors

The destinations API endpoint will validate that the destination and credentials are valid and that write access is
is present for the bucket.If you receive an error, and would like to debug this error, you can use the AWS CLI
to test the connectivity to the bucket. You should be able to write a file with the CLI using the same
data that you supplied to the destinations API above.**AWS S3:**

aws configure

# set the same access key credentials and region as you used for the destination
> AWS Access Key ID: <access_key_id>
> AWS Secret Access Key: <secret_access_key>
> Default region name [us-east-1]: <region>

# List buckets
aws s3 ls /

# test write permissions
touch ./test.txt

**GCS Compatible Buckets:**You will need to supply the endpoint\_url with `--endpoint-url` option.
For GCS, the `endpoint_url` is typically `https://storage.googleapis.com`:

### ​ Monitoring Runs

You can monitor your runs using the List Runs API. If this is a known error, this will be added to the `errors` field of the run.

### ​ Common Errors

Here are some common errors:

| Error | Description |
| --- | --- |
| Access denied | The blob store credentials or bucket are not valid. This error occurs when the provided access key and secret key combination doesn’t have the necessary permissions to access the specified bucket or perform the required operations. |
| Bucket is not valid | The specified blob store bucket is not valid. This error is thrown when the bucket doesn’t exist or there is not enough access to perform writes on the bucket. |
| Key ID you provided does not exist | The blob store credentials provided are not valid. This error occurs when the access key ID used for authentication is not a valid key. |
| Invalid endpoint | The endpoint\_url provided is invalid. This error is raised when the specified endpoint is an invalid endpoint. Only S3 compatible endpoints are supported, for example `https://storage.googleapis.com` for GCS, `https://play.min.io` for minio, etc. If using AWS, you should omit the endpoint\_url. |

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

View server logs for a trace\\
\\
Previous Set up automation rules\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/rules

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Automations

Set up automation rules

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- View automation rules
- Create a rule
- 1\. Navigate to rule creation
- 2\. Name your rule
- 3\. Create a filter
- 4\. Configure a sampling rate
- 5\. (Optional) Apply rule to past runs
- 6\. Select an action to trigger when the rule is applied.
- View logs for your automations
- Video guide

While you can manually sift through and process production logs from your LLM application, it often becomes difficult as your application scales to more users.
LangSmith provides a powerful feature called **Automations** that allow you to trigger certain actions on your trace data.
At a high level, automations are defined by a **filter**, **sampling rate**, and **action**.Automation rules can trigger actions such as: adding traces to a dataset, adding to an annotation queue, triggering a webhook (e.g. for remote evaluations) or extending data retention. Some examples of automations you can set up:

- Send all traces with negative feed page.

If an automation rule matches any run within a trace, the trace will be auto-upgraded to extended data retention. This upgrade will impact trace pricing, but ensures that traces meeting your automation criteria (typically those most valuable for analysis) are preserved for investigation.

## ​ View automation rules

Head to the **Tracing Projects** tab and select a tracing project. To view existing automation rules for that tracing project, click on the **Automations** tab.!View automation rules

## ​ Create a rule

#### ​ 1\. Navigate to rule creation

Head to the **Tracing Projects** tab and select a tracing project. Click on **\+ New** in the top right corner of the tracing project page, then click on **New Automation**.

#### ​ 3\. Create a filter

Automation rule filters work the same way as filters applied to traces in the project. For more information on filters, you can refer to this guide

#### ​ 4\. Configure a sampling rate

Configure a sampling rate to control the percentage of filtered runs that trigger the automation action.You can specify a sampling rate between 0 and 1 for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action.

#### ​ 5\. (Optional) Apply rule to past runs

Apply rule to past runs by toggling the **Apply to past runs** and entering a “Backfill from” date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately. In order to track progress of the backfill, you can view logs for your automations

#### ​ 6\. Select an action to trigger when the rule is applied.

There are four actions you can take with an automation rule:

- **Add to dataset**: Add the inputs and outputs of the trace to a dataset.
- **Add to annotation queue**: Add the trace to an annotation queue.
- **Trigger webhook**: Trigger a webhook with the trace data. For more information on webhooks, you can refer to this guide.
- **Extend data retention**: Extends the data retention period on matching traces that use base retention (see data retention docs for more details).
Note that all other rules will also extend data retention on matching traces through the
auto-upgrade mechanism described in the aforementioned data retention docs,
but this rule takes no additional action.

## ​ View logs for your automations

Logs allow you to gain confidence that your rules are working as expected. You can view logs for your automations by heading to the **Automations** tab within a tracing project and clicking the **Logs** button for the rule you created.The logs tab allows you to:

- View all runs processed by a given rule for the time period selected
- If a particular rule execution has triggered an error, you can view the error message by hovering over the error icon
- You can monitor the progress of a backfill job by filtering to the rule’s creation timestamp. This is because the backfill starts from when the rule was created.
- Inspect the run that the automation rule applied to using the **View run** button. For rules that add runs as examples to datasets, you can view the example produced.

## ​ Video guide

Getting Started with LangSmith (7/8): Automations & Online Evaluation - YouTube

Tap to unmute

Getting Started with LangSmith (7/8): Automations & Online Evaluation LangChain

LangChain165K subscribers

Watch on

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Bulk Exporting Trace Data\\
\\
Previous Configure webhook notifications for rules\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/webhooks

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Automations

Configure webhook notifications for rules

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Webhook payload
- Security
- Webhook custom HTTP headers
- Webhook Delivery
- Example with Modal
- Setup
- Secrets
- Service
- Hooking it up

When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs.!Webhook

## ​ Webhook payload

The payload we send to your webhook endpoint contains:

- `"rule_id"`: this is the ID of the automation that sent this payload
- `"start_time"` and `"end_time"`: these are the time boundaries where we found matching runs
- `"runs"`: this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.
- `"feedback_stats"`: this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below.

Copy

"feedback_stats": {
"about_langchain": {
"n": 1,
"avg": 0.0,
"show_feedback_arrow": true,
"values": {}
},
"category": {
"n": 0,
"avg": null,
"show_feedback_arrow": true,
"values": {
"CONCEPTUAL": 1
}
},
"user_score": {
"n": 2,
"avg": 0.0,
"show_feedback_arrow": false,
"values": {}
},
"vagueness": {
"n": 1,
"avg": 0.0,
"show_feedback_arrow": true,
"values": {}
}
}

**fetching from S3 URLs**Depending on how recent your runs are, the `inputs_s3_urls` and `outputs_s3_urls` fields may contain S3 URLs to the actual data instead of the data itself.The `inputs` and `outputs` can be fetched by the `ROOT.presigned_url` provided in `inputs_s3_urls` and `outputs_s3_urls` respectively.

This is an example of the entire payload we send to your webhook endpoint:

{
"rule_id": "d75d7417-0c57-4655-88fe-1db3cda3a47a",
"start_time": "2024-04-05T01:28:54.734491+00:00",
"end_time": "2024-04-05T01:28:56.492563+00:00",
"runs": [\
{\
"status": "success",\
"is_root": true,\
"trace_id": "6ab80f10-d79c-4fa2-b441-922ed6feb630",\
"dotted_order": "20230505T051324571809Z6ab80f10-d79c-4fa2-b441-922ed6feb630",\
"run_type": "tool",\
"modified_at": "2024-04-05T01:28:54.145062",\
"tenant_id": "2ebda79f-2946-4491-a9ad-d642f49e0815",\
"end_time": "2024-04-05T01:28:54.085649",\
"name": "Search",\
"start_time": "2024-04-05T01:28:54.085646",\
"id": "6ab80f10-d79c-4fa2-b441-922ed6feb630",\
"session_id": "6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5",\
"parent_run_ids": [],\
"child_run_ids": null,\
"direct_child_run_ids": null,\
"total_tokens": 0,\
"completion_tokens": 0,\
"prompt_tokens": 0,\
"total_cost": null,\
"completion_cost": null,\
"prompt_cost": null,\
"first_token_time": null,\
"app_path": "/o/2ebda79f-2946-4491-a9ad-d642f49e0815/projects/p/6a3be6a2-9a8c-4fc8-b4c6-a8983b286cc5/r/6ab80f10-d79c-4fa2-b441-922ed6feb630?trace_id=6ab80f10-d79c-4fa2-b441-922ed6feb630&start_time=2023-05-05T05:13:24.571809",\
"in_dataset": false,\
"last_queued_at": null,\
"inputs": null,\
"inputs_s3_urls": null,\
"outputs": null,\
"outputs_s3_urls": null,\
"extra": null,\
"events": null,\
"feedback_stats": null,\
"serialized": null,\
"share_token": null\
}\
]
}

## ​ Security

We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications.An example would be

### ​ Webhook custom HTTP headers

If you’d like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the `Headers` option next to the URL field and add your headers.

Headers are stored in encrypted format.

### ​ Webhook Delivery

When delivering events to your webhook endpoint we follow these guidelines

- If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.
- If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .
- If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.
- If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.
- Anything your endpoint returns in the body will be ignored

## ​ Example with Modal

### ​ Setup

For an example of how to set this up, we will use Modal. Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We’ll focus on the web endpoints here.First, create a Modal account. Then, locally install the Modal SDK:

pip

uv

pip install modal

To finish setting up your account, run the command:

modal setup

and follow the instructions

### ​ Secrets

Next, you will need to set up some secrets in Modal.First, LangSmith will need to authenticate to Modal by passing in a secret.
The easiest way to do this is to pass in a secret in the query parameters.
To validate this secret, we will need to add a secret in _Modal_ to validate it.
We will do that by creating a Modal secret.
You can see instructions for secrets here.
For this purpose, let’s call our secret `ls-webhook` and have it set an environment variable with the name `LS_WEBHOOK`.We can also set up a LangSmith secret - luckily there is already an integration template for this!!LangSmith Modal Template

### ​ Service

After that, you can create a Python file that will serve as your endpoint.
An example is below, with comments explaining what is going on:

from fastapi import HTTPException, status, Request, Query
from modal import Secret, Stub, web_endpoint, Image

stub = Stub("auth-example", image=Image.debian_slim().pip_install("langsmith"))

@stub.function(
secrets=[Secret.from_name("ls-webhook"), Secret.from_name("my-langsmith-secret")]
)
# We want this to be a `POST` endpoint since we will post data here
@web_endpoint(method="POST")
# We set up a `secret` query parameter
def f(data: dict, secret: str = Query(...)):
# You can import dependencies you don't have locally inside Modal functions
from langsmith import Client

# First, we validate the secret key we pass
import os

if secret != os.environ["LS_WEBHOOK"]:
raise HTTPException(
status_code=status.HTTP_401_UNAUTHORIZED,
detail="Incorrect bearer token",
headers={"WWW-Authenticate": "Bearer"},
)

# This is where we put the logic for what should happen inside this webhook
ls_client = Client()
runs = data["runs"]
ids = [r["id"] for r in runs]
feedback = list(ls_client.list_feedback(run_ids=ids))
for r, f in zip(runs, feedback):
try:
ls_client.create_example(
inputs=r["inputs"],
outputs={"output": f.correction},
dataset_name="classifier-github-issues",
)
except Exception:
raise ValueError(f"{r} and {f}")
# Function body
return "success!"

We can now deploy this easily with `modal deploy ...` (see docs here).You should now get something like:

✓ Created objects.
├── 🔨 Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py
├── 🔨 Created mount PythonPackage:langsmith

✓ App deployed! 🎉

View Deployment:

The important thing to remember is `https://hwchase17--auth-example-f.modal.run` \- the function we created to run.
NOTE: this is NOT the final deployment URL, make sure not to accidentally use that.

### ​ Hooking it up

We can now take the function URL we create above and add it as a webhook.
We have to remember to also pass in the secret key as a query parameter.
Putting it all together, it should look something like:

Replace `{SECRET}` with the secret key you created to access the Modal service.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Set up automation rules\\
\\
Previous Log user feedback using the SDK\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/attach-user-feedback

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Feedback & evaluation

Log user feedback using the SDK

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Use create\_feedback() / createFeedback()

**Key concepts**

- Conceptual guide on tracing and feedback
- Reference guide on feedback data format

LangSmith makes it easy to attach feed Use create\_feedback() / createFeedback()

Here we’ll walk through how to log feedback using the SDK.

**Child runs**
You can attach user feed.
This is essential for low-latency environments, where you want to make sure your application isn’t blocked on feedback creation.

Python

TypeScript

Copy

from langsmith import trace, traceable, Client

@traceable
def foo(x):
return {"y": x * 2}

@traceable
def bar(y):
return {"z": y - 1}

client = Client()

inputs = {"x": 1}
with trace(name="foobar", inputs=inputs) as root_run:
result = foo(**inputs)
result = bar(**result)
root_run.outputs = result
trace_id = root_run.id
child_runs = root_run.child_runs

# Provide feedback for a trace (a.k.a. a root run)
client.create_feedback(
key="user_feedback",
score=1,
trace_id=trace_id,
comment="the user said that ..."
)

# Provide feedback for a child run
foo_run_id = [run for run in child_runs if run.name == "foo"][0].id
client.create_feedback(
key="correctness",
score=0,
run_id=foo_run_id,
# trace_id= is optional but recommended to enable batched and backgrounded
# feedback ingestion.
trace_id=trace_id,
)

You can even log feedback for in-progress runs using `create_feedback() / createFeedback()`. See this guide for how to get the run ID of an in-progress run.To learn more about how to filter traces based on various attributes, including user feedback, see this guide.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Configure webhook notifications for rules\\
\\
Previous Set up online evaluators\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/online-evaluations

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Feedback & evaluation

Set up online evaluators

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- View online evaluators
- Configure online evaluators
- 1\. Navigate to online evaluators
- 2\. Name your evaluator
- 3\. Create a filter
- 4\. (Optional) Configure a sampling rate
- 5\. (Optional) Apply rule to past runs
- 6\. Select evaluator type
- Configure a LLM-as-a-judge online evaluator
- Configure a custom code evaluator
- Write your evaluation function
- Test and save your evaluation function
- Video guide
- Configure multi-turn online evaluators
- Prerequisites
- Configuration
- Limits
- Troubleshooting

**Recommended Reading**Before diving into this content, it might be helpful to read the following:

- Running online evaluations

Online evaluations provide real-time feedback on your production traces. This is useful to continuously monitor the performance of your application—to identify issues, measure improvements, and ensure consistent quality over time.There are two types of online evaluations supported in LangSmith:

- **LLM-as-a-judge**: Use an LLM to evaluate traces as a scalable substitute for human-like judgment (e.g., toxicity, hallucinations, correctness). Supports two different levels of granularity:

- **Run level**: Evaluate a single run.
- **Thread level**: Evaluate all traces in a thread.
- **Custom Code**: Write an evaluator in Python directly in LangSmith. Often used for validating structure or statistical properties of your data.

When an online evaluator runs on any run within a trace, the trace will be auto-upgraded to extended data retention. This upgrade will impact trace pricing, but ensures that traces meeting your evaluation criteria (typically those most valuable for analysis) are preserved for investigation.

## ​ View online evaluators

Head to the **Tracing Projects** tab and select a tracing project. To view existing online evaluators for that project, click on the **Evaluators** tab.!View online evaluators

## ​ Configure online evaluators

#### ​ 1\. Navigate to online evaluators

Head to the **Tracing Projects** tab and select a tracing project. Click on **\+ New** in the top right corner of the tracing project page, then click on **New Evaluator**. Select the evaluator you want to configure.

#### ​ 3\. Create a filter

For example, you may want to apply specific evaluators based on:

- Runs where a user left feedback indicating the response was unsatisfactory.
- Runs that invoke a specific tool call. See filtering for tool calls for more information.
- Runs that match a particular piece of metadata (e.g. if you log traces with a `plan_type` and only want to run evaluations on traces from your enterprise customers). See adding metadata to your traces for more information.

Filters on evaluators work the same way as when you’re filtering traces in a project. For more information on filters, you can refer to this guide.

It’s often helpful to inspect runs as you’re creating a filter for your evaluator. With the evaluator configuration panel open, you can inspect runs and apply filters to them. Any filters you apply to the runs table will automatically be reflected in filters on your evaluator.

#### ​ 4\. (Optional) Configure a sampling rate

Configure a sampling rate to control the percentage of filtered runs that trigger the automation action. For example, to control costs, you may want to set a filter to only apply the evaluator to 10% of traces. In order to do this, you would set the sampling rate to 0.1.

#### ​ 5\. (Optional) Apply rule to past runs

Apply rule to past runs by toggling the **Apply to past runs** and entering a “Backfill from” date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately.In order to track progress of the backfill, you can view logs for your evaluator by heading to the **Evaluators** tab within a tracing project and clicking the Logs button for the evaluator you created. Online evaluator logs are similar to automation rule logs.

- Add an evaluator name
- Optionally filter runs that you would like to apply your evaluator on or configure a sampling rate.
- Select **Apply Evaluator**

#### ​ 6\. Select evaluator type

- Configuring LLM-as-a-judge evaluators
- Configuring custom code evaluators

### ​ Configure a LLM-as-a-judge online evaluator

View this guide to configure an LLM-as-a-judge evaluator.

### ​ Configure a custom code evaluator

Select **custom code** evaluator.

#### ​ Write your evaluation function

**Custom code evaluators restrictions.****Allowed Libraries**: You can import all standard library functions, as well as the following public packages:

Copy

numpy (v2.2.2): "numpy"
pandas (v1.5.2): "pandas"
jsonschema (v4.21.1): "jsonschema"
scipy (v1.14.1): "scipy"
sklearn (v1.26.4): "scikit-learn"

**Network Access**: You cannot access the internet from a custom code evaluator.

Custom code evaluators must be written inline. We recommend testing locally before setting up your custom code evaluator in LangSmith.In the UI, you will see a panel that lets you write your code inline, with some starter code:!Online eval custom codeCustom code evaluators take in one argument:

- A `Run` ( reference). This represents the sampled run to evaluate.

They return a single value:

- Feedback(s) Dictionary: A dictionary whose keys are the type of feedback you want to return, and values are the score you will give for that feedback key. For example, `{"correctness": 1, "silliness": 0}` would create two types of feedback on the run, one saying it is correct, and the other saying it is not silly.

In the below screenshot, you can see an example of a simple function that validates that each run in the experiment has a known json field:

Python

JavaScript

import json

def perform_eval(run):
output_to_validate = run['outputs']
is_valid_json = 0

# assert you can serialize/deserialize as json
try:
json.loads(json.dumps(output_to_validate))
except Exception as e:
return { "formatted": False }

# assert output facts exist
if "facts" not in output_to_validate:
return { "formatted": False }

# assert required fields exist
if "years_mentioned" not in output_to_validate["facts"]:
return { "formatted": False }

return {"formatted": True}

#### ​ Test and save your evaluation function

Before saving, you can test your evaluator function on a recent run by clicking **Test Code** to make sure that your code executes properly.Once you **Save**, your online evaluator will run over newly sampled runs (or backfilled ones too if you chose the backfill option).If you prefer a video tutorial, check out the Online Evaluations video from the Introduction to LangSmith Course.

### ​ Video guide

Getting Started with LangSmith (7/8): Automations & Online Evaluation - YouTube

Photo image of LangChain

LangChain

165K subscribers

Getting Started with LangSmith (7/8): Automations & Online Evaluation

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Why am I seeing this?

Watch on

0:00

0:00 / 5:23

•Live

•

## ​ Configure multi-turn online evaluators

Multi-turn online evaluators allow you to evaluate entire conversations between a human and an agent — not just individual exchanges. They measure end-to-end interaction quality across all turns in a thread.You can use multi-turn evaluations to measure:

1. Semantic Intent: What the user was trying to do.
2. Semantic Outcome: What actually happened, did the task succeed.
3. Trajectory: How the conversation unfolded, including trajectory of tool calls.

Running multi-turn online evals will auto-upgrade each trace within a thread to extended data retention. This upgrade will impact trace pricing, but ensures that traces meeting your evaluation criteria (typically those most valuable for analysis) are preserved for investigation.

### ​ Prerequisites

- Your tracing project must be using threads.
- The top-level inputs and outputs of each trace in a thread must have a `messages` key that contains a list of messages. We support messages in LangChain, OpenAI Chat Completions, and Anthropic Messages formats.

- If the top-level inputs and outputs of each trace only contain the latest message in the conversation, LangSmith will automatically combine messages across turns into a thread.
- If the top-level inputs and outputs of each trace contain the full conversation history, LangSmith will use that directly.

If your traces don’t follow the format above, thread level evaluators won’t work. You’ll need to update how you trace to LangSmith to ensure each trace’s top-level inputs and outputs contain a list of `messages`.Please refer to the troubleshooting section for more information.

### ​ Configuration

1. Navigate to the **Tracing Projects** tab and select a tracing project.

3. **Name your evaluator**.
4. **Apply filters or a sampling rate**.

Use filters or sampling to control evaluator cost. For example, evaluate only threads under _N_ turns or sample 10% of all threads.
5. **Configure an idle time**.

The first time you configure a thread level evaluator, you’ll define the idle time — the amount of time after the last trace in a thread before it’s considered complete and ready for evaluation. This value should reflect the expected length of user interactions in your app. It applies across all evaluators in the project.

When first testing your evaluator, use a short idle time so you can see results quickly. Once validated, increase it to match the expected length of user interactions.

6. **Configure your model.**

Select the provider and model you want to use for your evaluator. Threads tend to get long, so you should use a model with a higher context window in order to avoid running into limits. For example, OpenAI’s GPT-4.1 mini or Gemini 2.5 Flash are good options as they both have 1M+ token context windows.
7. **Configure your LLM-as-a-judge prompt.**

Define what you want to evaluate. This prompt will be used to evaluate the thread. You can also configure which parts of the `messages` list are passed to the evaluator to control the content it receives: - All messages: Send the full message list.
- Human and AI pairs: Send only user and assistant messages (excluding system messages, tool calls, etc.).
- First human and last AI: Send only the first user message and the last assistant reply.
8. **Set up your feedback configuration**.

Configure a name for the feedback key, the format for the feedback you want to collect and optionally enable reasoning on the feedback.

We don’t recommend using the same feedback key for a thread-level evaluator and a run-level evaluator as it can be hard to distinguish between the two.

8. **Save your evaluator.**

After saving, your evaluator will appear in the **Evaluators** tab. You can test it once the idle time has passed for any new threads created after saving.

### ​ Limits

These are the current limits for multi-turn online evaluators (subject to change). Please reach out if you are running into any of these limits.

- **Runs must be less than one week old**: When a thread becomes idle, only runs within the past 7 days are eligible for evaluation.
- **Maximum of 500 threads evaluated at once**: If you have more than 500 threads marked as idle in a five minute period, we will automatically sample beyond 500.
- **Maximum of 10 multi-turn online evaluators per workspace**

### ​ Troubleshooting

**Checking the status of your evaluator**

You can check when your evaluator was last run by heading to the **Evaluators** tab within a tracing project and clicking the **Logs** button for the evaluator you created to view its run history.**Inspect the data sent to the evaluator**

Inspect the data sent to the evaluator by heading to the **Evaluators** tab within a tracing project, clicking on the evaluator you created and clicking the **Evaluator traces** tab.In this tab, you can see the inputs passed into the LLM-as-a-judge evaluator. If your messages are not being passed in correctly, you will see blank values in the inputs. This can happen if your messages are not formatted in one of the expected formats.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Log user feedback using the SDK\\
\\
Previous Monitor projects with dashboards\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/dashboards

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Monitoring & alerting

Monitor projects with dashboards

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Prebuilt dashboards
- Dashboard sections
- Group by
- Custom Dashboards
- Creating a new dashboard
- Adding charts to your dashboard
- Chart configuration
- Select tracing projects and filter runs
- Pick a metric
- Split the data
- Pick a chart type
- Save and manage charts
- Linking to a dashboard from a tracing project
- Example: user-journey monitoring
- Video guide

Dashboards give you high-level insights into your trace data, helping you spot trends and monitor the health of your applications. Dashboards are available in the **Monitoring** tab in the left sidebar.LangSmith offers two dashboard types:

- **Prebuilt dashboards**: Automatically generated for every tracing project.
- **Custom dashboards**: Fully configurable collections of charts tailored to your needs.

## ​ Prebuilt dashboards

Prebuilt dashboards are created automatically for each project and cover essential metrics, such as trace count, error rates, token usage, and more. By default, the prebuilt dashboard for your tracing project can be accessed using the **Dashboard** button on the top right of the tracing project page.!prebuilt

**You cannot modify a prebuilt dashboard. In the future, we plan to allow you to clone a default dashboard in order to have a starting point to customize it.**

### ​ Dashboard sections

Prebuilt dashboards are broken down into the following sections:

| Section | What it shows |
| --- | --- |
| Traces | Trace count, latency and error rates. A trace is a collection of runs related to a single operation. For example, if a user request triggers an agent, all runs for that agent invocation would be part of the same trace. |
| LLM Calls | LLM call count and latency. Includes all runs where run type is “llm”. |
| Cost & Tokens | Total and per-trace token counts and costs, broken down by token type. Costs are measured using LangSmith’s cost tracking. |
| Tools | Run counts, error rates, and latency stats for tool runs broken down by tool name. Includes runs where run type is “tool”. Limits to top 5 most frequently occurring tools. |
| Run Types | Run counts, error rates, and latency stats for runs that are immediate children of the root run. This helps in understanding the high-level execution path of agents. Limits to top 5 most frequently occurring run names. Refer to the image following this table. |
| Feedback Scores | Aggregate stats for the top 5 most frequently occurring types of feedback. Charts show average score for numerical feedback and category counts for categorical feedback. |

For example, for the following trace, the following runs have a depth of 1:!Run depth explained

### ​ Group by

Group by run tag or metadata can be used to split data over attributes that are important to your application. The global group by setting appears on the top right hand side of the dashboard. Note that the Tool and Run Type charts already have a group by applied, so the global group by won’t take effect; the global group by will apply to all other charts.

When adding metadata to runs, we recommend having the same metadata on the trace, as well as the specific run (e.g. LLM call). Metadata and tags are not propagated from parent to child runs, or vice versa. So, if you want to see e.g. both your trace charts and your LLM call charts grouped on some metadata key then both your traces (root runs) and your LLM runs need to have that metadata attached.

## ​ Custom Dashboards

Create tailored collections of charts for tracking metrics that matter most for your application.

### ​ Creating a new dashboard

1. Navigate to the **Monitor** tab in the left sidebar.
2. Click on the **\+ New Dashboard** button.
3. Give your dashboard a name and a description.
4. Click on **Create**.

### ​ Adding charts to your dashboard

1. Within a dashboard, click on the **\+ New Chart** button to open up the chart creation pane.
2. Give your chart a name and a description.
3. Configure the chart.

#### ​ Select tracing projects and filter runs

- Select one or more tracing projects to track metrics for.
- Use the **Chart filters** section to refine the matching runs. This filter applies to all data series in the chart. For more information on filtering traces, view our guide on filtering traces in application.

#### ​ Pick a metric

- Choose a metric from the dropdown menu to set the y-axis of your chart. With a project and a metric selected, you’ll see a preview of your chart and the matching runs.
- For certain metrics (such as latency, token usage, cost), we support comparing multiple metrics with the same unit. For example, you may want one chart where you can see prompt tokens and completion tokens. Each metric appears as a separate line.

#### ​ Split the data

There are two ways to create multiple series in a chart (i.e. create multiple lines in a chart):

1. **Group by**: Group runs by run tag or metadata, run name, or run type. Group by automatically splits the data into multiple series based on the field selected. Note that group by is limited to the top 5 elements by frequency.
2. **Data series**: Manually define multiple series with individual filters. This is useful for comparing granular data within a single metric.

#### ​ Pick a chart type

- Choose between a line chart and a bar chart for visualizing

### ​ Save and manage charts

- Click `Save` to save your chart to the dashboard.
- Edit or delete a chart by clicking the triple dot button in the top right of the chart.
- Clone a chart by clicking the triple line button in the top right of the chart and selecting **\+ Clone**. This will open a new chart creation pane with the same configurations as the original.

## ​ Linking to a dashboard from a tracing project

You can link to any dashboard directly from a tracing project. By default, the prebuilt dashboard for your tracing project is selected. If you have a custom dashboard that you would like to link instead:

1. In your tracing project, click the three dots next to the **Dashboard** button.
2. Choose a dashboard to set as the new default.

## ​ Example: user-journey monitoring

Use monitoring charts for mapping the decisions made by an agent at a particular node.Consider an email assistant agent. At a particular node it makes a decision about an email to:

- send an email back
- notify the user
- no response needed

We can create a chart to track and visualize the breakdown of these decisions.**Creating the chart**

1. **Metric Selection**: Select the metric `Run count`.
2. **Chart Filters**: Add a tree filter to include all of the traces with name `triage_input`. This means we only include traces that hit the `triage_input` node. Also add a chart filter for `Is Root` is `true`, so our count is not inflated by the number of nodes in the trace.!Decision at node
3. **Data Series**: Create a data series for each decision made at the `triage_input` node. The output of the decision is stored in the `triage.response` field of the output object, and the value of the decision is either `no`, `email`, or `notify`. Each of these decisions generates a separate data series in the chart.!Decision at node

Now we can visualize the decisions made at the `triage_input` node over time.

## ​ Video guide

Getting Started with LangSmith (8/8): Dashboards - YouTube

Photo image of LangChain

LangChain

165K subscribers

Getting Started with LangSmith (8/8): Dashboards

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Watch on

0:00

0:00 / 6:19

•Live

•

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Set up online evaluators\\
\\
Previous Alerts in LangSmith\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/alerts

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Monitoring & alerting

Alerts in LangSmith

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Overview
- Configuring an alert
- Step 1: Navigate to create alert
- Step 2: Select metric type
- Step 2: Define alert conditions
- Step 3: Configure notification channel
- Best practices

**Self-hosted Version Requirement**Access to alerts requires Helm chart version **0.10.3** or later.

## ​ Overview

Effective observability in LLM applications requires proactive detection of failures, performance degradations, and regressions. LangSmith’s alerts feature helps identify critical issues such as:

- API rate limit violations from model providers
- Latency increases for your application
- Application changes that affect feedback scores reflecting end-user experience

Alerts in LangSmith are project-scoped, requiring separate configuration for each monitored project.

## ​ Configuring an alert

### ​ Step 1: Navigate to create alert

First navigate to the Tracing project that you would like to configure alerts for. Click the Alerts icon on the top right hand corner of the page to view existing alerts for that project and set up a new alert.

### ​ Step 2: Select metric type

LangSmith offers threshold-based alerting on three core metrics:

| Metric Type | Description | Use Case |
| --- | --- | --- |
| **Errored Runs** | Track runs with an error status | Monitors for failures in an application. |
| **Feedback Score** | Measures the average feedback score | Track feedback from end users or online evaluation results to alert on regressions. |
| **Latency** | Measures average run execution time | Tracks the latency of your application to alert on spikes and performance bottlenecks. |

Additionally, for **Errored Runs** and **Run Latency**, you can define filters to narrow down the runs that trigger alerts. For example, you might create an error alert filter for all `llm` runs tagged with `support_agent` that encounter a `RateLimitExceeded` error.

### ​ Step 2: Define alert conditions

Alert conditions consist of several components:

- **Aggregation Method**: Average, Percentage, or Count

- **Threshold Value**: Numerical value triggering the alert
- **Aggregation Window**: Time period for metric calculation (currently choose between 5 or 15 minutes)
- **Feedback Key** (Feedback Score alerts only): Specific feedback metric to monitor

**Example:** The configuration shown above would generate an alert when more than 5% of runs within the past 5 minutes result in errors.You can preview alert behavior over a historical time window to understand how many datapoints—and which ones—would have triggered an alert at a chosen threshold (indicated in red). For example, setting an average latency threshold of 60 seconds for a project lets you visualize potential alerts, as shown in the image below.

### ​ Step 3: Configure notification channel

LangSmith supports the following notification channels:

1. PagerDuty integration
2. Webhook notifications

Select the appropriate channel to ensure notifications reach the responsible team members.

## ​ Best practices

- Adjust sensitivity based on application criticality
- Start with broader thresholds and refine based on observed patterns
- Ensure alert routing reaches appropriate on-call personnel

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Monitor projects with dashboards\\
\\
Previous Configure webhook notifications for LangSmith alerts\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/alerts-webhook

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Monitoring & alerting

Configure webhook notifications for LangSmith alerts

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Overview
- Prerequisites
- Integration Configuration
- Step 1: Prepare Your Receiving Endpoint
- Step 2: Configure Webhook Parameters
- Step 3: Test the Webhook
- Troubleshooting
- Security Considerations
- Sending alerts to Slack using a webhook
- Prerequisites
- Step 1: Create a Slack App
- Step 2: Configure Bot Permissions
- Step 3: Install the App to Your Workspace
- Step 4: Add the Bot to a Slack Channel
- Step 5: Configure the Webhook Alert in LangSmith
- Step 6: Test the Integration
- (Optional) Step 7: Link to the Alert Preview in the Request Body
- Additional Resources

## ​ Overview

This guide details the process for setting up webhook notifications for LangSmith alerts. Before proceeding, make sure you have followed the steps leading up to the notification step of creating the alert by following this guide. Webhooks enable integration with custom services and third-party platforms by sending HTTP POST requests when alert conditions are triggered. Use webhooks to forward alert data to ticketing systems, chat applications, or custom monitoring solutions.

## ​ Prerequisites

- An endpoint that can receive HTTP POST requests
- Appropriate authentication credentials for your receiving service (if required)

## ​ Integration Configuration

### ​ Step 1: Prepare Your Receiving Endpoint

Before configuring the webhook in LangSmith, ensure your receiving endpoint:

- Accepts HTTP POST requests
- Can process JSON payloads
- Is accessible from external services
- Has appropriate authentication mechanisms (if required)

Additionally, if on a custom deployment of LangSmith, make sure there are no firewall settings blocking egress traffic from LangSmith services.

### ​ Step 2: Configure Webhook Parameters

- **URL**: The complete URL of your receiving endpoint

- Example: `https://api.example.com/incident-webhook`

**Optional Fields**

- **Headers**: JSON Key-value pairs sent with the webhook request - Common headers include: - `Authorization`: For authentication tokens
- `Content-Type`: Usually set to `application/json` (default)
- `X-Source`: To identify the source as LangSmith
- If no headers, then simply use `{}`
- **Request Body Template**: Customize the JSON payload sent to your endpoint - Default: LangSmith sends the payload defined and the following additonal key-value pairs appended to the payload: - `project_name`: Name of the triggered alert
- `alert_rule_id`: A UUID to identify the LangSmith alert. This can be used as a de-duplication key in the webhook service.
- `alert_rule_name`: The name of the alert rule.
- `alert_rule_type`: The type of alert (as of 04/01/2025 all alerts are of type `threshold`).
- `alert_rule_attribute`: The attribute associated with the alert rule - `error_count`, `feedback_score` or `latency`.
- `triggered_metric_value`: The value of the metric at the time the threshold was triggered.
- `triggered_threshold`: The threshold that triggered the alert.
- `timestamp`: The timestamp that triggered the alert.

### ​ Step 3: Test the Webhook

Click **Send Test Alert** to send the webhook notification to ensure the notification works as intended.

## ​ Troubleshooting

If webhook notifications aren’t being delivered:

- Verify the webhook URL is correct and accessible
- Ensure any authentication headers are properly formatted
- Check that your receiving endpoint accepts POST requests
- Examine your endpoint’s logs for received but rejected requests
- Verify your custom payload template is valid JSON format

## ​ Security Considerations

- Use HTTPS for your webhook endpoints
- Implement authentication for your webhook endpoint
- Consider adding a shared secret in your headers to verify webhook sources
- Validate incoming webhook requests before processing them

## ​ Sending alerts to Slack using a webhook

Here is an example for configuring LangSmith alerts to send notifications to Slack channels using the `chat.postMessage` API.

### ​ Prerequisites

- Access to a Slack workspace
- A LangSmith project to set up alerts
- Permissions to create Slack applications

### ​ Step 1: Create a Slack App

1. Visit the Slack API Applications page
2. Click **Create New App**
3. Select **From scratch**
4. Provide an **App Name** (e.g., “LangSmith Alerts”)
5. Select the workspace where you want to install the app
6. Click **Create App**

### ​ Step 2: Configure Bot Permissions

1. In the left sidebar of your Slack app configuration, click **OAuth & Permissions**
2. Scroll down to **Bot Token Scopes** under **Scopes** and click **Add an OAuth Scope**
3. Add the following scopes: - `chat:write` (Send messages as the app)
- `chat:write.public` (Send messages to channels the app isn’t in)
- `channels:read` (View basic channel information)

### ​ Step 3: Install the App to Your Workspace

1. Scroll up to the top of the **OAuth & Permissions** page
2. Click **Install to Workspace**
3. Review the permissions and click **Allow**
4. Copy the **Bot User OAuth Token** that appears (begins with `xoxb-`)

### ​ Step 5: Configure the Webhook Alert in LangSmith

1. In LangSmith, navigate to your project
2. Select **Alerts → Create Alert**
3. Define your alert metrics and conditions
4. In the notification section, select **Webhook**
5. Configure the webhook with the following settings:

**Webhook URL**

Copy

**Headers**

Replace `xoxb-your-token-here` with your Bot’s User OAuth Token

{
"Content-Type": "application/json",
"Authorization": "Bearer xoxb-your-token-here"
}

**Request Body Template**

It is required to fill in the `{channel_id}` from the value found in Step 4.

The remaining fields: `alert_name`, `project_name` and `project_url` optionally add additional context to the alert message. You can find your `project_url` in the browser’s URL bar. Copy the portion up to but not including any query parameters.

{
"channel": "{channel_id}",
"text": "{alert_name} triggered for {project_name}",
"blocks": [\
{\
"type": "section",\
"text": {\
"type": "mrkdwn",\
"text": "🚨{alert_name} has been triggered"\
}\
},\
{\
"type": "section",\
"text": {\
"type": "mrkdwn",\
"text": "Please check the following link for more information:"\
}\
},\
{\
"type": "section",\
"text": {\
"type": "mrkdwn",\

}\
}\
]
}

6. Click **Save** to activate the webhook configuration

### ​ Step 6: Test the Integration

1. In the LangSmith alert configuration, click **Test Alert**
2. Check your specified Slack channel for the test notification
3. Verify that the message contains the expected alert information

### ​ (Optional) Step 7: Link to the Alert Preview in the Request Body

After creating an alert, you can optionally link to its preview in the webhook’s request body.!Alert Preview PaneTo configure this:

1. Save your alert
2. Find your saved alert in the alerts table and click it
3. Copy the displayed URL
4. Click “Edit Alert”
5. Replace the existing project URL with the copied alert preview URL

## ​ Additional Resources

- LangSmith Alerts Documentation
- Slack chat.postMessage API Documentation
- Slack Block Kit Builder

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Alerts in LangSmith\\
\\
Previous Discover errors and usage patterns with the Insights Agent\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/insights

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Monitoring & alerting

Discover errors and usage patterns with the Insights Agent

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Prerequisites
- Generate your first Insights Report
- From the LangSmith UI
- From the LangSmith SDK
- Understand the results
- Executive summary
- Top-level categories
- Subcategories
- Individual traces
- Configure a job
- Autogenerating a config
- Choose a model provider
- Using a prebuilt config
- Building a config from scratch
- Select traces
- Categories
- Summary prompt
- Attributes
- Filter attributes
- Save your config

The Insights Agent automatically analyzes your traces to detect usage patterns, common agent behaviors and failure modes — without requiring you to manually review thousands of traces.Insights uses hierarchical categorization to make sense of your data and highlight actionable trends.

Insights is available for LangSmith Plus and Enterprise plans and is only available for LangSmith SaaS deployments.

## ​ Prerequisites

- An OpenAI API key (generate one here) or an Anthropic API key (generate one here)
- Permissions to create rules in LangSmith (required to generate new Insights Reports)
- Permissions to view tracing projects LangSmith (required to view existing Insights Reports)

## ​ Generate your first Insights Report

Auto configuration flow for Insights Agent

#### ​ From the LangSmith UI

1. Navigate to **Tracing Projects** in the left-hand menu and select a tracing project.
2. Click **+New** in the top right corner then **New Insights Report** to generate new insights over the project.
3. Enter a name for your job.
4. Click the icon in the top right of the job creation pane to set your OpenAI (or Anthropic) API key as a workspace secret. If your workspace already has an OpenAI API key set, you can skip this step.
5. Answer the guided questions to focus your Insights Report on what you want to learn about your agent, then click **Run job**.

Toggle to Manual mode to try prebuilt configs for common use cases or build your own.

This will kick off a background Insights Report. Reports can take up to 30 minutes to complete.

#### ​ From the LangSmith SDK

You can generate Insights Reports over data stored outside LangSmith using the Python SDK. This allows you to analyze chat histories from your production systems, logs, or other sources.When you call `generate_insights()`, the SDK will:

1. Upload your chat histories as traces to a new LangSmith project
2. Generate an Insights Report over those uploaded traces
3. Return a link to your results in the LangSmith UI

Python

Copy

import os
from langsmith import Client

client = Client()

chat_histories = [\
[\
{"role": "user", "content": "how are you"},\
{"role": "assistant", "content": "good!"},\
],\
[\
{"role": "user", "content": "do you like art"},\
{"role": "assistant", "content": "only Tarkovsky"},\
],\
]

report = client.generate_insights(
chat_histories=chat_histories,
name="Customer Support Topics - March 2024",
instructions="What are the main topics and questions users are asking about?",
openai_api_key=os.environ["OPENAI_API_KEY"], # optional if already set as workspace secret
)

# client.poll_insights(report=report)

Generating insights over 1,000 threads typically costs $1.00-$2.00 with OpenAI models and $3.00-$4.00 with current Anthropic models. The cost scales with the number of threads sampled and the size of each thread.

## ​ Understand the results

Once your job has completed, you can navigate to the **Insights** tab where you’ll see a table of Insights Report. Each Report contains insights generated over a specific sample of traces from the tracing project.

Insights Reports for a single tracing project

Click into your job to see traces organized into a set of auto-generated categories.You can drill down through categories and subcategories to view the underlying traces, feedback, and run statistics.

Common topics of conversations with the chatbot

### ​ Executive summary

At the top of each report, you’ll find an executive summary that surfaces the most important patterns discovered in your traces. This includes:

- Key findings with percentages showing how often each pattern appears.
- Clickable references (e.g., #1, #2, #3) to traces the agent identified as exceptionally relevant to your question.

Executive summary showing key patterns with trace references

### ​ Top-level categories

Your traces are automatically grouped into top-level categories that represent the broadest patterns in your data.The distribution bars show how frequently each pattern occurs, making it easy to spot behaviors that happen more or less than expected.Each category has a brief description and displays aggregated metrics over the traces it contains, including:

- Typical trace stats (like error rates, latency, cost)
- Feedback scores from your evaluators
- Attributes extracted as part of the job

### ​ Subcategories

Clicking on any category shows a breakdown into subcategories, which gives you a more granular understanding of interaction patterns in that category of traces.In the Chat Langchain example pictured above, under “Data & Retrieval” there are subcategories like “Vector Stores” and “Data Ingestion”.

### ​ Individual traces

You can view the traces assigned to each category or subcategory by clicking through to see the traces table. From there, you can click into any trace to see the full conversation details.

## ​ Configure a job

You can create an Insights Report three ways. Start with the auto-generated flow to spin up a baseline, then iterate with saved or manual configs as you refine.

### ​ Autogenerating a config

1. Open **New Insights** and make sure the **Auto** toggle is active.
2. Answer the natural-language questions about your agent’s purpose, what you want to learn, and how traces are structured. Insights will translate your answers into a draft config (job name, summary prompt, attributes, and sampling defaults).
3. Choose a provider, then click **Generate config** to preview or **Run job** to launch immediately.

**Providing useful context**For best results, write a sentence or two for each prompt that gives the agent the context it needs—what you’re trying to learn, which signals or fields matter most, and anything you already know isn’t useful. The clearer you are about what your agent does and how its traces are structured, the more the Insights Agent can group examples in a way that’s specific, actionable, and aligned with how you reason about your data.**Describing your traces**Explain how your data is organized—are these single runs or multi-turn conversations? Which inputs and outputs contain the key information? This helps the Insights Agent generate summary prompts and attributes that focus on what matters. You can also directly specify variables from the summary prompt section if needed.

### ​ Choose a model provider

You can select either OpenAI or Anthropic models to power the agent. You must have the corresponding workspace secret set for whichever provider you choose (`OPENAI_API_KEY` or `ANTHROPIC_API_KEY`).Note that using current Anthropic models costs ~3x as much as using OpenAI models.

### ​ Using a prebuilt config

Prebuilt configs in Manual mode

Use the **Saved configurations** dropdown to load presets for common jobs like **Usage Patterns** or **Error Analysis**. Run them directly for a fast start, or adjust filters, prompts, and providers before saving your customized version. To learn more about what you can customize, read the section below.

### ​ Building a config from scratch

Building your own config helps when you need more control—for example, predefining categories you want your data to be grouped into or targeting traces that match specific feedback scores and filters.

#### ​ Select traces

- **Sample size**: The maximum number of traces to analyze. Currently capped at 1,000
- **Time range**: Traces are sampled from this time range
- **Filters**: Additional trace filters. As you adjust filters, you’ll see how many traces match your criteria

#### ​ Categories

By default, top-level categories are automatically generated bottom-up from the underlying traces.In some instances, you know specific categories you’re interested in upfront and want the job to bucket traces into those predefined categories.The **Categories** section of the config lets you do this by enumerating the names and descriptions of the top-level categories you want to be used.Subcategories are still auto-generated by the algorithm within the predefined top-level categories.

#### ​ Summary prompt

The first step of the job is to create a brief summary of every trace — it is these summaries that are then categorized.Extracting the right information in the summary is essential for getting useful categories.The prompt used to generate these summaries can be edited.The two things to think about when editing the prompt are:

- Summarization instructions: Any information that isn’t in the trace summary won’t affect the categories that get generated, so make sure to provide clear instructions on what information is important to extract from each trace.
- Trace content: Use mustache formatting to specify which parts of each trace are passed to the summarizer. Large traces with lots of inputs and outputs can be expensive and noisy. Reducing the prompt to only include the most relevant parts of the trace can improve your results.

The Insights Agent analyzes threads \- groups of related traces that represent multi-turn conversations. You must specify what parts of the thread to send to the summarizer using at least one of these template variables:

| Variable | Best for | Example |
| --- | --- | --- |
| `run.*` | Access data from the most recent root run (i.e. final turn) in a thread | `{{run.inputs}}``{{run.outputs}}``{{run.error}}` |

You can also access nested fields using dot notation. For example, the prompt `"Summarize this: {{run.inputs.foo.bar}}"` will include only the “bar” value within the “foo” value of the last run’s inputs.

#### ​ Attributes

Along with a summary, you can define additional categorical, numerical, and boolean attributes to be extracted from each trace.
These attributes will influence the categorization step — traces with similar attribute values will tend to be categorized together.
You can also see aggregations of these attributes per category.As an example, you might want to extract the attribute `user_satisfied: boolean` from each trace to steer the algorithm towards categories that split up positive and negative user experiences, and to see the average user satisfaction per category.

#### ​ Filter attributes

You can use the `filter_by` parameter on boolean attributes to pre-filter traces before generating insights. When enabled, only traces where the attribute evaluates to `true` are included in the analysis.This is useful when you want to focus your Insights Report on a specific subset of traces—for example, only analyzing errors, only examining English-language conversations, or only including traces that meet certain quality criteria.

Using filter attributes to generate Insights only on traces with agent errors

**How it works:**

- Add `"filter_by": true` to any boolean attribute when creating a config for the Insights Agent
- The LLM evaluates each trace against the attribute description during summarization
- Traces where the attribute is `false` or missing are excluded before insights are generated

## ​ Save your config

You can optionally save configs for future reuse using the ‘save as’ button.
This is especially useful if you want to compare Insights Reports over time to identify changes in user and agent behavior.Select from previously saved configs in the dropdown in the top-left corner of the pane when creating a new Insights Report.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Configure webhook notifications for LangSmith alerts\\
\\
Previous Run (span) data format\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/run-data-format

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Data type reference

Run (span) data format

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- What is dotted\_order?

Before diving into this content, it might be helpful to read the following:

- Conceptual guide on tracing and runs

LangSmith stores and processes trace data in a simple format that is easy to export and import.Many of these fields are optional or not important to know about but are included for completeness.

| Field Name | Type | Description |
| --- | --- | --- |
| **`id`** | UUID | Unique identifier for the span. |
| **`name`** | string | The name associated with the run. |
| **`inputs`** | object | A map or set of inputs provided to the run. |
| **`run_type`** | string | Type of run, e.g., `'llm'`, `'chain'`, `'tool'`. |
| **`start_time`** | datetime | Start time of the run. |
| **`end_time`** | datetime | End time of the run. |
| **`extra`** | object | Any extra information run. |
| **`error`** | string | Error message if the run encountered an error. |
| **`outputs`** | object | A map or set of outputs generated by the run. |
| **`events`** | array of objects | A list of event objects associated with the run. This is relevant for runs executed with streaming. |
| **`tags`** | array of strings | Tags or labels associated with the run. |
| **`trace_id`** | UUID | Unique identifier for the trace the run is a part of. This is also the `id` field of the root run of the trace |

| **`status`** | string | Current status of the run execution, e.g., `'error'`, `'pending'`, `'success'` |
| **`child_run_ids`** | array of UUIDs | List of IDs for all child runs. |
| **`direct_child_run_ids`** | array of UUIDs | List of IDs for direct children of this run. |
| **`parent_run_ids`** | array of UUIDs | List of IDs for all parent runs. |
| **`feedback_stats`** | object | Aggregations of feedback statistics for this run |
| **`reference_example_id`** | UUID | ID of a reference example associated with the run. This is usually only present for evaluation runs. |
| **`total_tokens`** | integer | Total number of tokens processed by the run. |
| **`prompt_tokens`** | integer | Number of tokens in the prompt of the run. |
| **`completion_tokens`** | integer | Number of tokens in the completion of the run. |
| **`total_cost`** | decimal | Total cost associated with processing the run. |
| **`prompt_cost`** | decimal | Cost associated with the prompt part of the run. |
| **`completion_cost`** | decimal | Cost associated with the completion of the run. |
| **`first_token_time`** | datetime | Time when the first token of a model output was generated. Only applies for runs with `run_type="llm"` and streaming enabled. |
| **`session_id`** | string | Session identifier for the run, also known as the tracing project ID. |
| **`in_dataset`** | boolean | Indicates whether the run is included in a dataset. |
| **`parent_run_id`** | UUID | Unique identifier of the parent run. |
| `execution_order` (deprecated) | integer | The order in which this run was executed within the trace. |
| `serialized` | object | Serialized state of the object executing the run if applicable. |
| `manifest_id` (deprecated) | UUID | Identifier for a manifest associated with the span. |
| `manifest_s3_id` | UUID | S3 identifier for the manifest. |
| `inputs_s3_urls` | object | S3 URLs for the inputs. |
| `outputs_s3_urls` | object | S3 URLs for the outputs. |
| `price_model_id` | UUID | Identifier for the pricing model applied to the run. |
| `app_path` | string | Application (UI) path for this run. |
| `last_queued_at` | datetime | Last time the span was queued. |
| `share_token` | string | Token for sharing access to the run’s data. |

Here is an example of a JSON representation of a run in the above format:

Copy

{
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",
"name": "string",
"inputs": {},
"run_type": "llm",
"start_time": "2024-04-29T00:49:12.090000",
"end_time": "2024-04-29T00:49:12.459000",
"extra": {},
"error": "string",
"execution_order": 1,
"serialized": {},
"outputs": {},
"parent_run_id": "f8faf8c1-9778-49a4-9004-628cdb0047e5",
"manifest_id": "82825e8e-31fc-47d5-83ce-cd926068341e",
"manifest_s3_id": "0454f93b-7eb6-4b9d-a203-f1261e686840",
"events": [{}],
"tags": ["foo"],
"inputs_s3_urls": {},
"outputs_s3_urls": {},
"trace_id": "df570c03-5a03-4cea-8df0-c162d05127ac",
"dotted_order": "20240429T004912090000Z497f6eca-6276-4993-bfeb-53cbbbba6f08",
"status": "string",
"child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"],
"direct_child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"],
"parent_run_ids": ["f8faf8c1-9778-49a4-9004-628cdb0047e5"],
"feedback_stats": {
"correctness": {
"n": 1,
"avg": 1.0
}
},
"reference_example_id": "9fb06aaa-105f-4c87-845f-47d62ffd7ee6",
"total_tokens": 0,
"prompt_tokens": 0,
"completion_tokens": 0,
"total_cost": 0.0,
"prompt_cost": 0.0,
"completion_cost": 0.0,
"price_model_id": "0b5d9575-bec3-4256-b43a-05893b8b8440",
"first_token_time": null,
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",
"app_path": "string",
"last_queued_at": null,
"in_dataset": true,
"share_token": "d0430ac3-04a1-4e32-a7ea-57776ad22c1c"
}

#### ​ What is `dotted_order`?

A run’s dotted order is a sortable key that fully specifies its location within the tracing hierarchy.Take the following example:

import langsmith as ls

@ls.traceable
def grandchild():
p("grandchild")

@ls.traceable
def child():
grandchild()

@ls.traceable
def parent():
child()

If you print out the IDs at each stage, you may get the following:

parent	run_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7	trace_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7 parent_run_id=null	dotted_order=20240919T171648521691Z0e01bf50-474d-4536-810f-67d3ee7ea3e7
child	run_id=a8024e23-5b82-47fd-970e-f6a5ba3f5097	trace_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7 parent_run_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7	dotted_order=20240919T171648521691Z0e01bf50-474d-4536-810f-67d3ee7ea3e7.20240919T171648523407Za8024e23-5b82-47fd-970e-f6a5ba3f5097
grandchild	run_id=0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6	trace_id=0e01bf50-474d-4536-810f-67d3ee7ea3e7 parent_run_id=a8024e23-5b82-47fd-970e-f6a5ba3f5097	dotted_order=20240919T171648521691Z0e01bf50-474d-4536-810f-67d3ee7ea3e7.20240919T171648523407Za8024e23-5b82-47fd-970e-f6a5ba3f5097.20240919T171648523563Z0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6

Note a few invariants:

- The `id` is equal to the last 36 characters of the dotted order (the suffix after the final `'Z'`). See `0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6` for example in the grandchild.
- The `trace_id` is equal to the first UUID in the dotted order (i.e., `dotted_order.split('.')[0].split('Z')[1]`)
- If `parent_run_id` exists, it is the penultimate UUID in the dotted order. See `a8024e23-5b82-47fd-970e-f6a5ba3f5097` in the grandchild, for an example.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Discover errors and usage patterns with the Insights Agent\\
\\
Previous Feedback data format\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/feedback-data-format

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Data type reference

Feedback data format

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

Before diving into this content, it might be helpful to read the following:

- Conceptual guide on tracing and feedback

**Feedback** is LangSmith’s way of storing the criteria and scores from evaluation on a particular trace or intermediate run (span). Feedback can be produced from a variety of ways, such as:

1. Sent up along with a trace from the LLM application
2. Generated by a user in the app inline or in an annotation queue
3. Generated by an automatic evaluator during offline evaluation
4. Generated by an online evaluator

Feedback is stored in a simple format with the following fields:

| Field Name | Type | Description |
| --- | --- | --- |
| `id` | UUID | Unique identifier for the record itself |
| `created_at` | datetime | Timestamp when the record was created |
| `modified_at` | datetime | Timestamp when the record was last modified |
| `session_id` | UUID | Unique identifier for the experiment or tracing project the run was a part of |
| `run_id` | UUID | Unique identifier for a specific run within a session |
| `key` | string | A key describing the criteria of the feedback, e.g. `'correctness'` |
| `score` | number | Numerical score associated with the feedback key |
| `value` | string | Reserved for storing a value associated with the score. Useful for categorical feedback. |
| `comment` | string | Any comment or annotation associated with the record. This can be a justification for the score given. |
| `correction` | object | Reserved for storing correction details, if any |
| `feedback_source` | object | Object containing information about the feedback source |
| `feedback_source.type` | string | The type of source where the feedback originated, e.g. `'api'`, `'app'`, `'evaluator'` |
| `feedback_source.metadata` | object | Reserved for additional metadata, currently |
| `feedback_source.user_id` | UUID | Unique identifier for the user providing feedback |

Here is an example JSON representation of a feedback record in the above format:

Copy

{
"created_at": "2024-05-05T23:23:11.077838",
"modified_at": "2024-05-05T23:23:11.232962",
"session_id": "c919298b-0af2-4517-97a2-0f98ed4a48f8",
"run_id": "e26174e5-2190-4566-b970-7c3d9a621baa",
"key": "correctness",
"score": 1.0,
"value": null,
"comment": "I gave this score because the answer was correct.",
"correction": null,
"id": "62104630-c7f5-41dc-8ee2-0acee5c14224",
"feedback_source": {
"type": "app",
"metadata": null,
"user_id": "ad52b092-1346-42f4-a934-6e5521562fab"
}
}

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Run (span) data format\\
\\
Previous Trace query syntax\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/trace-query-syntax

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Data type reference

Trace query syntax

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Filter arguments
- Filter query language

Using the method in the SDK or endpoint in the API, you can filter runs to analyze and export.

## ​ Filter arguments

| Keys | Description |
| --- | --- |
| `project_id` / `project_name` | The project(s) to fetch runs from - can be a single project or a list of projects. |
| `trace_id` | Fetch runs that are part of a specific trace. |
| `run_type` | The type of run to get, such as `llm`, `chain`, `tool`, `retriever`, etc. |
| `dataset_name` / `dataset_id` | Fetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset. |
| `reference_example_id` | Fetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input. |
| `parent_run_id` | Fetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory. |
| `error` | Fetch runs that errored or did not error. |
| `run_ids` | Fetch runs with a given list of run ids. Note: **This will ignore all other filtering arguments.** |
| `filter` | Fetch runs that match a given structured filter statement. See the guide below for more information. |
| `trace_filter` | Filter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of the root run within a trace. |
| `tree_filter` | Filter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of any run within a trace. |
| `is_root` | Only return root runs. |
| `select` | Select the fields to return in the response. By default, all fields are returned. See run data format for available fields. |
| `query` ( _experimental_) | Natural language query, which translates your query into a filter statement. |

**Performance tip**: Passing the `select` parameter and excluding `inputs` and `outputs` from the list can significantly improve query performance and reduce response sizes, especially for large runs.

## ​ Filter query language

LangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs.The filtering grammar is based on common comparators on fields in the run object. Supported comparators include:

- `gte` (greater than or equal to)
- `gt` (greater than)
- `lte` (less than or equal to)
- `lt` (less than)
- `eq` (equal to)
- `neq` (not equal to)
- `has` (check if run contains a tag or metadata json blob)
- `search` (search for a substring in a string field)

Additionally, you can combine multiple comparisons through the `and` operator.These can be applied on fields of the run object, such as its `id`, `name`, `run_type`, `start_time` / `end_time`, `latency`, `total_tokens`, `error`, `execution_order`, `tags`, and any associated feedback through `feedback_key` and `feedback_score`.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Feedback data format\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/polly

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Additional resources

LangSmith Polly

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- Tracing page
- Thread view
- Prompt Playground
- What’s next

**Polly is in beta.** Your feedback on Polly is invaluable as the team refines its capabilities.

**LangSmith Polly** is an AI assistant embedded directly in your LangSmith workspace to help you analyze and understand your application data.Polly helps you gain insight from your traces, conversation threads, and prompts without having to dig through data manually. By asking natural language questions, you can quickly understand agent performance, debug issues, and analyze user sentiment.!LangSmith Polly icon Polly appears in the right-hand bottom corner of the following locations within LangSmith UI, optimized for different use cases:

- Trace pages
- Thread views
- Prompt Playground

### ​ Tracing page

On an individual trace, Polly pulls in the context of the page and analyzes the run. Polly reads the run data and trajectory to help you understand what happened and identify areas for improvement.To ask Polly about your tracing:

1. In your **Tracing Projects**, click on a trace to view its details page.
2. Select a run in the trace.
3. Open Polly in the right-hand corner of the page to ask questions relating to this run.
4. Ask Polly a question about your data. You can use the sample questions or you might ask questions like: - “Is there anything that the agent could have done better here?”
- “Why did this run fail?”
- “What took the most time in this trace?”
- “What errors occurred during this run?”
- “Summarize what happened in this trace”

When analyzing runs, Polly will examine the full trace context, including run metadata, inputs, outputs, intermediate steps, and configuration to provide actionable insights. This helps you diagnose issues without manually expanding each step in the trace tree or cross-referencing multiple runs.

### ​ Thread view

Under the **Threads** tab, Polly analyzes conversation threads by pulling in relevant information about the user interaction. This helps you understand user sentiment and conversation outcomes.To ask Polly about your threads:

1. Select a thread.
2. Open Polly in the right-hand corner of the page to ask questions relating to this thread.
3. Ask Polly a question about the conversation thread. You might ask questions like: - “Did the user seem frustrated?”
- “What issues is the user experiencing?”
- “How did this conversation resolve?”
- “Was the user’s problem solved?”
- “What was the main topic of this thread?”

Use Polly in thread view to gain insights into how users are interacting with your application. Understand conversation outcomes and whether issues were resolved, identify common user pain points, and track user sentiment through thread analysis. This helps you improve user experience by understanding what’s working and what needs improvement in your application’s responses.

### ​ Prompt Playground

When you open a prompt in the Playground, Polly can help you edit and improve your prompts based on your instructions. Polly reads the prompt and makes suggested edits.To ask Polly about your prompt:

1. Enter the **Playground** from the left-hand navigation or trace view.
2. Select a prompt to experiment with.
3. Open Polly in the right-hand corner of the page to work on this prompt.
4. You can use one of the automated options that Polly suggests:

- **Optimize prompt**: Polly will analyze the current prompt and make edits to the prompt with a summary of the changes.
- **Generate a tool**: Give details to Polly on the tool you would like to add. It will generate a tool for your prompt template. It can also help you modify existing tooling or system messages about tooling. Then, have Polly test tool configurations with reviews of sample output from the model using the tool.
- **Generate an output schema**: Polly will create a JSON schema that defines the structure of the output you want the model to generate. This is useful when you need the model to return data in a specific format. Select this option, and then provide Polly with the type of data, fields/properties, and any other constraints you might need.

Or, you might ask your own questions, like:

- “Make it respond in Italian”
- “Add more context about the user’s role”
- “Make the tone more professional”
- “Simplify the instructions”
- “Add examples to the prompt”

## ​ What’s next

Learn more about the features that Polly helps you explore:

**Observability** \\
\\
Learn more about tracing and monitoring your LLM applications **Threads** \\
\\
Understand how threads work in LangSmith **Prompt Engineering** \\
\\
Create and iterate on prompts in the playground **Evaluation** \\
\\
Evaluate and test your applications systematically

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

User management\\
\\
Previous Data storage and privacy\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/platform-setup)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/reference)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith reference Agent Server API reference for LangSmith Deployment Control plane API reference for LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability-quickstart)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability-concepts)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability Observability concepts

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability-llm-tutorial)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/threads)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Fetch LangSmith docs LangSmith Polly

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/cost-tracking)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Cost tracking LangSmith docs Metadata parameters reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/filter-traces-in-application)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Prevent logging of sensitive data in traces LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/export-traces)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Export LangSmith telemetry to your observability backend LangSmith Fetch Bulk Exporting Trace Data

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/compare-traces)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability LangSmith Fetch

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/share-trace)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability Share or unshare a trace publicly Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/langsmith-fetch)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Fetch LangSmith docs LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/platform-logs)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/data-export)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Export LangSmith telemetry to your observability backend Bulk Exporting Trace Data LangSmith Fetch

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/rules)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith docs LangSmith Deployment Self-hosted LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/webhooks)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Configure webhook notifications for LangSmith alerts Use webhooks LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/attach-user-feedback)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Log user feedback using the SDK LangSmith Polly Annotate traces and runs inline

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/online-evaluations)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Evaluation Evaluation types Set up online evaluators

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/dashboards)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith docs Monitor projects with dashboards LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/alerts)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Configure webhook notifications for LangSmith alerts Alerts in LangSmith LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/alerts-webhook)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Configure webhook notifications for LangSmith alerts Use webhooks LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/insights)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Discover errors and usage patterns with the Insights Agent LangSmith Polly LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/run-data-format)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith data plane LangSmith Fetch Self-hosted LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/feedback-data-format)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Feedback data format Log user feedback using the SDK LangSmith Polly

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/trace-query-syntax)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Query traces (SDK) Trace query syntax LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability-concepts).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability Observability concepts

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/polly)**,

Skip to main content**,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Polly LangSmith docs LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/integrations/providers/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integration Packages

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

- All providers

##### Popular Providers

- OpenAI

- Anthropic

- Google

- AWS

- Microsoft

##### General integrations

- Chat models
- Tools and Toolkits
- LLMs
- Middleware
- Key-value stores
- Document transformers
- Model caches
- Callbacks

##### RAG integrations

- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

LangChain integrates with a wide variety of chat & embedding models, tools & toolkits, document loaders, vector stores, and more.These providers have standalone `langchain-provider` packages for improved versioning, dependency management, and testing.

## ​ Popular providers

| Provider | Package | Downloads | Latest |
| --- | --- | --- | --- |
| Anthropic | `@langchain/anthropic` | !Downloads | !NPM |
| Azure CosmosDB | `@langchain/azure-cosmosdb` | !Downloads | !NPM |
| Cerebras | `@langchain/cerebras` | !Downloads | !NPM |
| Cloudflare | `@langchain/cloudflare` | !Downloads | !NPM |
| Cohere | `@langchain/cohere` | !Downloads | !NPM |
| Exa | `langchain-exa` | !Downloads | !NPM |
| Google GenAI | `@langchain/google-genai` | !Downloads | !NPM |
| Google VertexAI | `@langchain/google-vertexai` | !Downloads | !NPM |
| Google VertexAI (Web Environments) | `@langchain/google-vertexai-web` | !Downloads | !NPM |
| Groq | `@langchain/groq` | !Downloads | !NPM |
| MistralAI | `@langchain/mistralai` | !Downloads | !NPM |
| MongoDB | `@langchain/mongodb` | !Downloads | !NPM |
| Nomic | `@langchain/nomic` | !Downloads | !NPM |
| OpenAI | `@langchain/openai` | !Downloads | !NPM |
| Pinecone | `@langchain/pinecone` | !Downloads | !NPM |
| Qdrant | `@langchain/qdrant` | !Downloads | !NPM |
| Tavily | `@langchain/tavily` | !Downloads | !NPM |
| Weaviate | `@langchain/weaviate` | !Downloads | !NPM |
| xAI | `@langchain/xai` | !Downloads | !NPM |
| Yandex | `@langchain/yandex` | !Downloads | !NPM |

## ​ All providers

See all providers or search for a provider using the search field.

If you’d like to contribute an integration, see Contributing integrations.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

All integrations\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/learn

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Learn

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Router: Knowledge base
- LangGraph

- Custom RAG agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### LangChain Academy

- LangChain Academy

##### Additional resources

- Case studies
- Get help

On this page

- Use Cases
- LangChain
- LangGraph
- Multi-agent
- Conceptual Overviews
- Additional Resources

In the **Learn** section of the documentation, you’ll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph.

## ​ Use Cases

Below are tutorials for common use cases, organized by framework.

### ​ LangChain

LangChain agent implementations make it easy to get started for most use cases. **Semantic Search** \\
\\
Build a semantic search engine over a PDF with LangChain components. **RAG Agent** \\
\\
Create a Retrieval Augmented Generation (RAG) agent. **SQL Agent** \\
\\
Build a SQL agent to interact with databases with human-in-the-loop review. **Voice Agent** \\
\\
Build an agent you can speak and listen to.

### ​ LangGraph

LangChain’s agent implementations use LangGraph primitives.
If deeper customization is required, agents can be implemented directly in LangGraph. **Custom RAG Agent** \\
\\
Build a RAG agent using LangGraph primitives for fine-grained control.

### ​ Multi-agent

These tutorials demonstrate multi-agent patterns, blending LangChain agents with LangGraph workflows.

## ​ Conceptual Overviews

These guides explain the core concepts and APIs underlying LangChain and LangGraph. **Memory** \\
\\
Understand persistence of interactions within and across threads. **Context engineering** \\
\\
Learn methods for providing AI applications the right information and tools to accomplish a task. **Graph API** \\
\\
Explore LangGraph’s declarative graph-building API. **Functional API** \\
\\
Build agents as a single function.

## ​ Additional Resources

**LangChain Academy** \\
\\
Courses and exercises to level up your LangChain skills. **Case Studies** \\
\\
See how teams are using LangChain and LangGraph in production.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a semantic search engine with LangChain\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/reference/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Reference

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

On this page

- Reference sites

Comprehensive API reference documentation for the LangChain and LangGraph Python and TypeScript libraries.

## ​ Reference sites

**LangChain** \\
\\
Complete API reference for LangChain JavaScript/TypeScript, including chat models, tools, agents, and more. **LangGraph** \\
\\
Complete API reference for LangGraph JavaScript/TypeScript, including graph APIs, state management, checkpointing, and more.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangChain SDK\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/contributing/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Contributing

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Contribute

- Documentation
- Code
- Integrations

On this page

- Ways to Contribute
- Acceptable uses of LLMs

**Welcome! Thank you for your interest in contributing.**LangChain has helped form the largest developer community in generative AI, and we’re always open to new contributors. Whether you’re fixing bugs, adding features, improving documentation, or sharing feedback, your involvement helps make LangChain and LangGraph better for everyone 🦜❤️

## ​ Ways to Contribute

Report bugs

Found a bug? Please help us fix it by following these steps:

1

Search

Check if the issue already exists in our GitHub Issues for the respective repo:

**LangChain** \\
\\
Issues **LangGraph** \\
\\
Issues

2

Create issue

If no issue exists, create a new one. When writing, be sure to follow the template provided and to include a minimal, reproducible, example. Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.

3

Wait

A project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.

If you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please link them rather than combining them. For example,

Copy

This issue is blocked by #123 and related to #456.

Suggest features

Have an idea for a new feature or enhancement?

Search the issues for the respective repository for existing feature requests:

Discuss

If no requests exist, start a new discussion under the relevant category so that project maintainers and the community can provide feedback.

Describe

Be sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.

Improve documentation

Documentation improvements are always welcome! We strive to keep our docs clear and comprehensive, and your perspective can make a big difference. **How to propose changes to the documentation** \\
\\
Guide

Contribute code

With a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help! **How to make your first Pull Request** \\
\\
Guide If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work.If you are looking for something to work on, check out the issues labeled “good first issue” or “help wanted” in our repos:

**LangChain** \\
\\
Labels **LangGraph** \\
\\
Labels

Add a new integration

**LangChain** \\
\\
Guide to adding a new LangChain integration

## ​ Acceptable uses of LLMs

Generative AI can be a useful tool for contributors, but like any tool should be used with critical thinking and good judgement.We struggle when contributors’ entire work (code changes, documentation update, pull request descriptions) are LLM-generated. These drive-by contributions often mean well but often miss the mark in terms of contextual relevance, accuracy, and quality.We will close those pull requests and issues that are unproductive, so we can focus our maintainer capacity elsewhere.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Contributing to documentation\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/install

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Install LangGraph

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

To install the base LangGraph package:

npm

pnpm

yarn

bun

Copy

npm install @langchain/langgraph @langchain/core

To use LangGraph you will usually want to access LLMs and define tools.
You can do this however you see fit.One way to do this (which we will use in the docs) is to use LangChain.Install LangChain with:

npm install langchain

To work with specific LLM provider packages, you will need install them separately.Refer to the integrations page for provider-specific installation instructions.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangGraph overview\\
\\
Previous Quickstart\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/quickstart

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Quickstart

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- 1\. Define tools and model
- 2\. Define state
- 3\. Define model node
- 4\. Define tool node
- 5\. Define end logic
- 6\. Build and compile the agent

This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API.

- Use the Graph API if you prefer to define your agent as a graph of nodes and edges.
- Use the Functional API if you prefer to define your agent as a single function.

For conceptual information, see Graph API overview and Functional API overview.

For this example, you will need to set up a Claude (Anthropic) account and get an API key. Then, set the `ANTHROPIC_API_KEY` environment variable in your terminal.

- Use the Graph API

- Use the Functional API

## ​ 1\. Define tools and model

In this example, we’ll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.

Copy

import { ChatAnthropic } from "@langchain/anthropic";
import { tool } from "@langchain/core/tools";
import * as z from "zod";

const model = new ChatAnthropic({
model: "claude-sonnet-4-5-20250929",
temperature: 0,
});

// Define tools

name: "add",
description: "Add two numbers",
schema: z.object({
a: z.number().describe("First number"),
b: z.number().describe("Second number"),
}),
});

name: "multiply",
description: "Multiply two numbers",
schema: z.object({
a: z.number().describe("First number"),
b: z.number().describe("Second number"),
}),
});

name: "divide",
description: "Divide two numbers",
schema: z.object({
a: z.number().describe("First number"),
b: z.number().describe("Second number"),
}),
});

// Augment the LLM with tools
const toolsByName = {
[add.name]: add,
[multiply.name]: multiply,
[divide.name]: divide,
};
const tools = Object.values(toolsByName);
const modelWithTools = model.bindTools(tools);

## ​ 2\. Define state

The graph’s state is used to store the messages and the number of LLM calls.

import { StateGraph, START, END, MessagesAnnotation, Annotation } from "@langchain/langgraph";

const MessagesState = Annotation.Root({
...MessagesAnnotation.spec,

}),
});

// Extract the state type for function signatures
type MessagesStateType = typeof MessagesState.State;

## ​ 3\. Define model node

The model node is used to call the LLM and decide whether to call a tool or not.

import { SystemMessage } from "@langchain/core/messages";
async function llmCall(state: MessagesStateType) {
return {
messages: [await modelWithTools.invoke([\
new SystemMessage(\
"You are a helpful assistant tasked with performing arithmetic on a set of inputs."\
),\
...state.messages,\
])],
llmCalls: 1,
};
}

## ​ 4\. Define tool node

The tool node is used to call the tools and return the results.

import { AIMessage, ToolMessage } from "@langchain/core/messages";
async function toolNode(state: MessagesStateType) {
const lastMessage = state.messages.at(-1);

if (lastMessage == null || !AIMessage.isInstance(lastMessage)) {
return { messages: [] };
}

const result: ToolMessage[] = [];
for (const toolCall of lastMessage.tool_calls ?? []) {
const tool = toolsByName[toolCall.name];
const observation = await tool.invoke(toolCall);
result.push(observation);
}

return { messages: result };
}

## ​ 5\. Define end logic

The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.

async function shouldContinue(state: MessagesStateType) {
const lastMessage = state.messages.at(-1);

// Check if it's an AIMessage before accessing tool_calls
if (!lastMessage || !AIMessage.isInstance(lastMessage)) {
return END;
}

// If the LLM makes a tool call, then perform an action
if (lastMessage.tool_calls?.length) {
return "toolNode";
}

// Otherwise, we stop (reply to the user)
return END;
}

## ​ 6\. Build and compile the agent

The agent is built using the `StateGraph` class and compiled using the `compile` method.

const agent = new StateGraph(MessagesState)
.addNode("llmCall", llmCall)
.addNode("toolNode", toolNode)
.addEdge(START, "llmCall")
.addConditionalEdges("llmCall", shouldContinue, ["toolNode", END])
.addEdge("toolNode", "llmCall")
.compile();

// Invoke
import { HumanMessage } from "@langchain/core/messages";
const result = await agent.invoke({
messages: [new HumanMessage("Add 3 and 4.")],
});

for (const message of result.messages) {
console.log(`[${message.type}]: ${message.text}`);
}

To learn how to trace your agent with LangSmith, see the LangSmith documentation.

Congratulations! You’ve built your first agent using the LangGraph Graph API.

Full code example

// Step 1: Define tools and model

// Step 2: Define state

// Step 3: Define model node

import { SystemMessage } from "@langchain/core/messages";

async function llmCall(state: MessagesStateType) {
return {
messages: [await modelWithTools.invoke([\
new SystemMessage(\
"You are a helpful assistant tasked with performing arithmetic on a set of inputs."\
),\
...state.messages,\
])],
llmCalls: 1,
};
}

// Step 4: Define tool node

import { AIMessage, ToolMessage } from "@langchain/core/messages";

async function toolNode(state: MessagesStateType) {
const lastMessage = state.messages.at(-1);

// Step 5: Define logic to determine whether to end

// Step 6: Build and compile the agent

## ​ 2\. Define model node

import { task, entrypoint } from "@langchain/langgraph";
import { SystemMessage } from "@langchain/core/messages";

return modelWithTools.invoke([\
new SystemMessage(\
"You are a helpful assistant tasked with performing arithmetic on a set of inputs."\
),\
...messages,\
]);
});

## ​ 3\. Define tool node

import type { ToolCall } from "@langchain/core/messages/tool";

const tool = toolsByName[toolCall.name];
return tool.invoke(toolCall);
});

## ​ 4\. Define agent

import { addMessages } from "@langchain/langgraph";
import { type BaseMessage } from "@langchain/core/messages";

let modelResponse = await callLlm(messages);

while (true) {
if (!modelResponse.tool_calls?.length) {
break;
}

// Execute tools
const toolResults = await Promise.all(

);
messages = addMessages(messages, [modelResponse, ...toolResults]);
modelResponse = await callLlm(messages);
}

return messages;
});

// Invoke
import { HumanMessage } from "@langchain/core/messages";

const result = await agent.invoke([new HumanMessage("Add 3 and 4.")]);

for (const message of result) {
console.log(`[${message.getType()}]: ${message.text}`);
}

Congratulations! You’ve built your first agent using the LangGraph Functional API.

// Step 2: Define model node

// Step 3: Define tool node

// Step 4: Define agent
import { addMessages } from "@langchain/langgraph";
import { type BaseMessage } from "@langchain/core/messages";

// Invoke
import { HumanMessage } from "@langchain/core/messages";
const result = await agent.invoke([new HumanMessage("Add 3 and 4.")]);

for (const message of result) {
console.log(`[${message.type}]: ${message.text}`);
}

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Install LangGraph\\
\\
Previous Run a local server\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/local-server

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Run a local server

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Prerequisites
- 1\. Install the LangGraph CLI
- 2\. Create a LangGraph app
- 3\. Install dependencies
- 4\. Create a .env file
- 5\. Launch Agent server
- 6\. Test your application in Studio
- 7\. Test the API
- Next steps

This guide shows you how to run a LangGraph application locally.

## ​ Prerequisites

Before you begin, ensure you have the following:

- An API key for LangSmith \- free to sign up

## ​ 1\. Install the LangGraph CLI

Copy

npx @langchain/langgraph-cli

## ​ 2\. Create a LangGraph app

Create a new app from the `new-langgraph-project-js` template. This template demonstrates a single-node application you can extend with your own logic.

npm create langgraph

## ​ 3\. Install dependencies

In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:

cd path/to/your/app
npm install

## ​ 4\. Create a `.env` file

You will find a `.env.example` in the root of your new LangGraph app. Create a `.env` file in the root of your new LangGraph app and copy the contents of the `.env.example` file into it, filling in the necessary API keys:

LANGSMITH_API_KEY=lsv2...

## ​ 5\. Launch Agent server

Start the LangGraph API server locally:

npx @langchain/langgraph-cli dev

Sample output:

INFO:langgraph_api.cli:

Welcome to

╦ ┌─┐┌┐┌┌─┐╔═╗┬─┐┌─┐┌─┐┬ ┬
║ ├─┤││││ ┬║ ╦├┬┘├─┤├─┘├─┤
╩═╝┴ ┴┘└┘└─┘╚═╝┴└─┴ ┴┴ ┴ ┴

- 🚀 API:
- 🎨 Studio UI:
- 📚 API Docs:

This in-memory server is designed for development and testing.
For production use, please use LangSmith Deployment.

The `langgraph dev` command starts Agent Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy Agent Server with access to a persistent storage backend. For more information, see the Platform setup overview.

## ​ 6\. Test your application in Studio

Studio is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of the `langgraph dev` command:

For an Agent Server running on a custom host/port, update the `baseUrl` query parameter in the URL. For example, if your server is running on `http://myhost:3000`:

Safari compatibility

Use the `--tunnel` flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:

langgraph dev --tunnel

## ​ 7\. Test the API

- Javascript SDK

- Rest API

1. Install the LangGraph JS SDK:

npm install @langchain/langgraph-sdk

2. Send a message to the assistant (threadless run):

const { Client } = await import("@langchain/langgraph-sdk");

// only set the apiUrl if you changed the default port when calling langgraph dev
const client = new Client({ apiUrl: "http://localhost:2024"});

const streamResponse = client.runs.stream(
null, // Threadless run
"agent", // Assistant ID
{
input: {
"messages": [\
{ "role": "user", "content": "What is LangGraph?"}\
]
},
streamMode: "messages-tuple",
}
);

for await (const chunk of streamResponse) {
console.log(`Receiving new event of type: ${chunk.event}...`);
console.log(JSON.stringify(chunk.data));
console.log("\n\n");
}

curl -s --request POST \
--url "http://localhost:2024/runs/stream" \
--header 'Content-Type: application/json' \
--data "{
\"assistant_id\": \"agent\",
\"input\": {
\"messages\": [\
{\
\"role\": \"human\",\
\"content\": \"What is LangGraph?\"\
}\
]
},
\"stream_mode\": \"messages-tuple\"
}"

## ​ Next steps

Now that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:

- Deployment quickstart: Deploy your LangGraph app using LangSmith.
- LangSmith: Learn about foundational LangSmith concepts.
- SDK Reference: Explore the SDK API Reference.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Quickstart\\
\\
Previous Changelog\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/releases/changelog

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Releases

Changelog

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

FiltersClear

langchain@langchain/openai@langchain/anthropic@langchain/ollama@langchain/community@langchain/xai@langchain/tavily@langchain/mongodb@langchain/mcp-adapters@langchain/google-common@langchain/corelanggraph

**Subscribe**: Our changelog includes an RSS feed that can integrate with Slack, email, Discord bots like Readybot or RSS Feeds to Discord Bot, and other subscription tools.

​

Dec 12, 2025

langchain@langchain/openai@langchain/anthropic@langchain/ollama@langchain/community@langchain/xai@langchain/tavily@langchain/mongodb@langchain/mcp-adapters@langchain/google-common@langchain/core

## ​ v1.2.0

### ​ `langchain`

- Structured output: Added ability to manually set `strict` mode when using `providerStrategy` for structured output.

### ​ `@langchain/openai`

- **New provider built-in tools:** Support for file search, web search, code interpreter, image generation, computer use, shell, and MCP connector tools.
- **Content moderation:** New `moderateContent` option on `ChatOpenAI` for detecting and handling unsafe content.
- Prefer responses API for GPT-5.2 Pro model.

## ​ v1.3.0

### ​ `@langchain/anthropic`

- **New provider built-in tools:** Support for text editor, web fetch, computer use, tool search, and MCP toolset tools.
- Exposed `ChatAnthropicInput` type for improved type safety.

## ​ v1.1.0

### ​ `@langchain/ollama`

- **Native structured outputs:** Added support for native structured output via `withStructuredOutput`.
- Support for custom `baseUrl` configuration.

## ​ v1.0.0

### ​ `@langchain/community`

- Jira document loader updated to use v3 API.
- LanceDB: Added `similaritySearch()` and `similaritySearchWithScore()` support.
- Elasticsearch hybrid search support.
- New `GoogleCalendarDeleteTool`.
- Various bug fixes for LlamaCppEmbeddings, PrismaVectorStore, IBM WatsonX, and security improvements.

### ​ Other packages

- **@langchain/xai:** Native Live Search support.
- **@langchain/tavily:** Added Tavily’s research endpoint.
- **@langchain/mongodb:** New MongoDB LLM cache.
- **@langchain/mcp-adapters:** Added `onConnectionError` option.
- **@langchain/google-common:**`jsonSchema` method support in `withStructuredOutput`.
- **@langchain/core:** Security fixes, better subgraph nesting in Mermaid graphs, UUID7 for run IDs.

Nov 25, 2025

langchain

- Model profiles: Chat models now expose supported features and capabilities through a `.profile` getter. These data are derived from models.dev, an open source project providing model capability data.
- Model retry middleware: New middleware for automatically retrying failed model calls with configurable exponential backoff, improving agent reliability.
- Content moderation middleware: OpenAI content moderation middleware for detecting and handling unsafe content in agent interactions. Supports checking user input, model output, and tool results.
- Summarization middleware: Updated to support flexible trigger points using model profiles for context-aware summarization.
- Structured output: `ProviderStrategy` support (native structured output) can now be inferred from model profiles.
- `SystemMessage` for `createAgent`: Support for passing `SystemMessage` instances directly to `createAgent`’s `systemPrompt` parameter and a new `concat` method for extending system messages. Enables advanced features like cache control and structured content blocks.
- Dynamic system prompt middleware: Return values from `dynamicSystemPromptMiddleware` are now purely additive. When returning a `SystemMessage` or `string`, they are merged with existing system messages rather than replacing them, making it easier to compose multiple middleware that modify the prompt.
- **Compatibility improvements:** Fixed error handling for Zod v4 validation errors in structured output and tool schemas, ensuring detailed error messages are properly displayed.

Oct 20, 2025

langchainlanggraph

### ​ `langchain`

- Release notes
- Migration guide

### ​ `langgraph`

If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Versioning\\
\\
Previous What's new in LangChain v1\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/thinking-in-langgraph

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Thinking in LangGraph

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Start with the process you want to automate
- Step 1: Map out your workflow as discrete steps
- Step 2: Identify what each step needs to do
- LLM steps
- Data steps
- Action steps
- User input steps
- Step 3: Design your state
- What belongs in state?
- Keep state raw, format prompts on-demand
- Step 4: Build your nodes
- Handle errors appropriately
- Implementing our email agent nodes
- Step 5: Wire it together
- Try out your agent
- Summary and next steps
- Key Insights
- Advanced considerations
- Where to go from here

When you build an agent with LangGraph, you will first break it apart into discrete steps called **nodes**. Then, you will describe the different decisions and transitions from each of your nodes. Finally, you connect nodes together through a shared **state** that each node can read from and write to.In this walkthrough, we’ll guide you through the thought process of building a customer support email agent with LangGraph.

## ​ Start with the process you want to automate

Imagine that you need to build an AI agent that handles customer support emails. Your product team has given you these requirements:

Copy

The agent should:

- Read incoming customer emails
- Classify them by urgency and topic
- Search relevant documentation to answer questions
- Draft appropriate responses
- Escalate complex issues to human agents
- Schedule follow-ups when needed

Example scenarios to handle:

1. Simple product question: "How do I reset my password?"
2. Bug report: "The export feature crashes when I select PDF format"
3. Urgent billing issue: "I was charged twice for my subscription!"
4. Feature request: "Can you add dark mode to the mobile app?"
5. Complex technical issue: "Our API integration fails intermittently with 504 errors"

To implement an agent in LangGraph, you will usually follow the same five steps.

## ​ Step 1: Map out your workflow as discrete steps

Start by identifying the distinct steps in your process. Each step will become a **node** (a function that does one specific thing). Then, sketch how these steps connect to each other.

START

Read Email

Classify Intent

Doc Search

Bug Track

Human Review

Draft Reply

Send Reply

END

The arrows in this diagram show possible paths, but the actual decision of which path to take happens inside each node.Now that we’ve identified the components in our workflow, let’s understand what each node needs to do:

- `Read Email`: Extract and parse the email content
- `Classify Intent`: Use an LLM to categorize urgency and topic, then route to appropriate action
- `Doc Search`: Query your knowledge base for relevant information
- `Bug Track`: Create or update issue in tracking system
- `Draft Reply`: Generate an appropriate response
- `Human Review`: Escalate to human agent for approval or handling
- `Send Reply`: Dispatch the email response

Notice that some nodes make decisions about where to go next (`Classify Intent`, `Draft Reply`, `Human Review`), while others always proceed to the same next step (`Read Email` always goes to `Classify Intent`, `Doc Search` always goes to `Draft Reply`).

## ​ Step 2: Identify what each step needs to do

For each node in your graph, determine what type of operation it represents and what context it needs to work properly.

**LLM steps** \\
\\
Use when you need to understand, analyze, generate text, or make reasoning decisions **Data steps** \\
\\
Use when you need to retrieve information from external sources **Action steps** \\
\\
Use when you need to perform external actions **User input steps** \\
\\
Use when you need human intervention

### ​ LLM steps

When a step needs to understand, analyze, generate text, or make reasoning decisions:

Classify intent

- Static context (prompt): Classification categories, urgency definitions, response format
- Dynamic context (from state): Email content, sender information
- Desired outcome: Structured classification that determines routing

Draft reply

- Static context (prompt): Tone guidelines, company policies, response templates
- Dynamic context (from state): Classification results, search results, customer history
- Desired outcome: Professional email response ready for review

### ​ Data steps

When a step needs to retrieve information from external sources:

Document search

- Parameters: Query built from intent and topic
- Retry strategy: Yes, with exponential backoff for transient failures
- Caching: Could cache common queries to reduce API calls

Customer history lookup

- Parameters: Customer email or ID from state
- Retry strategy: Yes, but with fall Action steps

When a step needs to perform an external action:

Send reply

- When to execute node: After approval (human or automated)
- Retry strategy: Yes, with exponential backoff for network issues
- Should not cache: Each send is a unique action

Bug track

- When to execute node: Always when intent is “bug”
- Retry strategy: Yes, critical to not lose bug reports
- Returns: Ticket ID to include in response

### ​ User input steps

When a step needs human intervention:

Human review node

- Context for decision: Original email, draft response, urgency, classification
- Expected input format: Approval boolean plus optional edited response
- When triggered: High urgency, complex issues, or quality concerns

## ​ Step 3: Design your state

State is the shared memory accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.

### ​ What belongs in state?

Ask yourself these questions about each piece of data:

## Include in state

Does it need to persist across steps? If yes, it goes in state.

## Don't store

Can you derive it from other data? If yes, compute it when needed instead of storing it in state.

For our email agent, we need to track:

- The original email and sender info (can’t reconstruct these later)
- Classification results (needed by multiple later/downstream nodes)
- Search results and customer data (expensive to re-fetch)
- The draft response (needs to persist through review)
- Execution metadata (for debugging and recovery)

### ​ Keep state raw, format prompts on-demand

A key principle: your state should store raw data, not formatted text. Format prompts inside nodes when you need them.

This separation means:

- Different nodes can format the same data differently for their needs
- You can change prompt templates without modifying your state schema
- Debugging is clearer – you see exactly what data each node received
- Your agent can evolve without breaking existing state

Let’s define our state:

import * as z from "zod";

// Define the structure for email classification
const EmailClassificationSchema = z.object({
intent: z.enum(["question", "bug", "billing", "feature", "complex"]),
urgency: z.enum(["low", "medium", "high", "critical"]),
topic: z.string(),
summary: z.string(),
});

const EmailAgentState = z.object({
// Raw email data
emailContent: z.string(),
senderEmail: z.string(),
emailId: z.string(),

// Classification result
classification: EmailClassificationSchema.optional(),

// Raw search/API results
searchResults: z.array(z.string()).optional(), // List of raw document chunks
customerHistory: z.record(z.any()).optional(), // Raw customer data from CRM

// Generated content
responseText: z.string().optional(),
});

Notice that the state contains only raw data – no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM.

## ​ Step 4: Build your nodes

Now we implement each step as a function. A node in LangGraph is just a JavaScript function that takes the current state and returns updates to it.

### ​ Handle errors appropriately

Different errors need different handling strategies:

| Error Type | Who Fixes It | Strategy | When to Use |
| --- | --- | --- | --- |
| Transient errors (network issues, rate limits) | System (automatic) | Retry policy | Temporary failures that usually resolve on retry |
| LLM-recoverable errors (tool failures, parsing issues) | LLM | Store error in state and loop back | LLM can see the error and adjust its approach |
| User-fixable errors (missing information, unclear instructions) | Human | Pause with `interrupt()` | Need user input to proceed |
| Unexpected errors | Developer | Let them bubble up | Unknown issues that need debugging |

- Transient errors

- LLM-recoverable

- User-fixable

- Unexpected

Add a retry policy to automatically retry network issues and rate limits:

import type { RetryPolicy } from "@langchain/langgraph";

workflow.addNode(
"searchDocumentation",
searchDocumentation,
{
retryPolicy: { maxAttempts: 3, initialInterval: 1.0 },
},
);

Store the error in state and loop back so the LLM can see what went wrong and try again:

import { Command } from "@langchain/langgraph";

async function executeTool(state: State) {
try {
const result = await runTool(state.toolCall);
return new Command({
update: { toolResult: result },
goto: "agent",
});
} catch (error) {
// Let the LLM see what went wrong and try again
return new Command({
update: { toolResult: `Tool error: ${error}` },
goto: "agent"
});
}
}

Pause and collect information from the user when needed (like account IDs, order numbers, or clarifications):

import { Command, interrupt } from "@langchain/langgraph";

async function lookupCustomerHistory(state: State) {
if (!state.customerId) {
const userInput = interrupt({
message: "Customer ID needed",
request: "Please provide the customer's account ID to look up their subscription history",
});
return new Command({
update: { customerId: userInput.customerId },
goto: "lookupCustomerHistory",
});
}
// Now proceed with the lookup
const customerData = await fetchCustomerHistory(state.customerId);
return new Command({
update: { customerHistory: customerData },
goto: "draftResponse",
});
}

Let them bubble up for debugging. Don’t catch what you can’t handle:

try {
await emailService.send(state.responseText);
} catch (error) {
throw error; // Surface unexpected errors
}
}

### ​ Implementing our email agent nodes

We’ll implement each node as a simple function. Remember: nodes take state, do work, and return updates.

Read and classify nodes

import { StateGraph, START, END, Command } from "@langchain/langgraph";
import { HumanMessage } from "@langchain/core/messages";
import { ChatAnthropic } from "@langchain/anthropic";

const llm = new ChatAnthropic({ model: "claude-sonnet-4-5-20250929" });

async function readEmail(state: EmailAgentStateType) {
// Extract and parse email content
// In production, this would connect to your email service
console.log(`Processing email: ${state.emailContent}`);
return {};
}

async function classifyIntent(state: EmailAgentStateType) {
// Use LLM to classify email intent and urgency, then route accordingly

// Create structured LLM that returns EmailClassification object
const structuredLlm = llm.withStructuredOutput(EmailClassificationSchema);

// Format the prompt on-demand, not stored in state
const classificationPrompt = `
Analyze this customer email and classify it:

Email: ${state.emailContent}
From: ${state.senderEmail}

Provide classification including intent, urgency, topic, and summary.
`;

// Get structured response directly as object
const classification = await structuredLlm.invoke(classificationPrompt);

// Determine next node based on classification
let nextNode: "searchDocumentation" | "humanReview" | "draftResponse" | "bugTracking";

if (classification.intent === "billing" || classification.urgency === "critical") {
nextNode = "humanReview";
} else if (classification.intent === "question" || classification.intent === "feature") {
nextNode = "searchDocumentation";
} else if (classification.intent === "bug") {
nextNode = "bugTracking";
} else {
nextNode = "draftResponse";
}

// Store classification as a single object in state
return new Command({
update: { classification },
goto: nextNode,
});
}

Search and tracking nodes

async function searchDocumentation(state: EmailAgentStateType) {
// Search knowledge base for relevant information

// Build search query from classification
const classification = state.classification!;
const query = `${classification.intent} ${classification.topic}`;

let searchResults: string[];

try {
// Implement your search logic here
// Store raw search results, not formatted text
searchResults = [\

"Password must be at least 12 characters",\
"Include uppercase, lowercase, numbers, and symbols",\
];
} catch (error) {
// For recoverable search errors, store error and continue
searchResults = [`Search temporarily unavailable: ${error}`];
}

return new Command({
update: { searchResults }, // Store raw results or error
goto: "draftResponse",
});
}

async function bugTracking(state: EmailAgentStateType) {
// Create or update bug tracking ticket

// Create ticket in your bug tracking system
const ticketId = "BUG-12345"; // Would be created via API

return new Command({
update: { searchResults: [`Bug ticket ${ticketId} created`] },
goto: "draftResponse",
});
}

Response nodes

async function draftResponse(state: EmailAgentStateType) {
// Generate response using context and route based on quality

const classification = state.classification!;

// Format context from raw state data on-demand
const contextSections: string[] = [];

if (state.searchResults) {
// Format search results for the prompt

contextSections.push(`Relevant documentation:\n${formattedDocs}`);
}

if (state.customerHistory) {
// Format customer data for the prompt
contextSections.push(`Customer tier: ${state.customerHistory.tier ?? "standard"}`);
}

// Build the prompt with formatted context
const draftPrompt = `
Draft a response to this customer email:
${state.emailContent}

Email intent: ${classification.intent}
Urgency level: ${classification.urgency}

${contextSections.join("\n\n")}

Guidelines:
- Be professional and helpful
- Address their specific concern
- Use the provided documentation when relevant
`;

const response = await llm.invoke([new HumanMessage(draftPrompt)]);

// Determine if human review needed based on urgency and intent
const needsReview = (
classification.urgency === "high" ||
classification.urgency === "critical" ||
classification.intent === "complex"
);

// Route to appropriate next node
const nextNode = needsReview ? "humanReview" : "sendReply";

return new Command({
update: { responseText: response.content.toString() }, // Store only the raw response
goto: nextNode,
});
}

async function humanReview(state: EmailAgentStateType) {
// Pause for human review using interrupt and route based on decision
const classification = state.classification!;

// interrupt() must come first - any code before it will re-run on resume
const humanDecision = interrupt({
emailId: state.emailId,
originalEmail: state.emailContent,
draftResponse: state.responseText,
urgency: classification.urgency,
intent: classification.intent,
action: "Please review and approve/edit this response",
});

// Now process the human's decision
if (humanDecision.approved) {
return new Command({
update: { responseText: humanDecision.editedResponse || state.responseText },
goto: "sendReply",
});
} else {
// Rejection means human will handle directly
return new Command({ update: {}, goto: END });
}
}

// Send the email response
// Integrate with email service
console.log(`Sending reply: ${state.responseText!.substring(0, 100)}...`);
return {};
}

## ​ Step 5: Wire it together

Now we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges.To enable human-in-the-loop with `interrupt()`, we need to compile with a checkpointer to save state between runs:

Graph compilation code

import { MemorySaver, RetryPolicy } from "@langchain/langgraph";

// Create the graph
const workflow = new StateGraph(EmailAgentState)
// Add nodes with appropriate error handling
.addNode("readEmail", readEmail)
.addNode("classifyIntent", classifyIntent)
// Add retry policy for nodes that might have transient failures
.addNode(
"searchDocumentation",
searchDocumentation,
{ retryPolicy: { maxAttempts: 3 } },
)
.addNode("bugTracking", bugTracking)
.addNode("draftResponse", draftResponse)
.addNode("humanReview", humanReview)
.addNode("sendReply", sendReply)
// Add only the essential edges
.addEdge(START, "readEmail")
.addEdge("readEmail", "classifyIntent")
.addEdge("sendReply", END);

// Compile with checkpointer for persistence
const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });

The graph structure is minimal because routing happens inside nodes through `Command` objects. Each node declares where it can go, making the flow explicit and traceable.

### ​ Try out your agent

Let’s run our agent with an urgent billing issue that needs human review:

Testing the agent

// Test with an urgent billing issue
const initialState: EmailAgentStateType = {
emailContent: "I was charged twice for my subscription! This is urgent!",
senderEmail: "customer@example.com",
emailId: "email_123"
};

// Run with a thread_id for persistence
const config = { configurable: { thread_id: "customer_123" } };
const result = await app.invoke(initialState, config);
// The graph will pause at human_review
console.log(`Draft ready for review: ${result.responseText?.substring(0, 100)}...`);

// When ready, provide human input to resume
import { Command } from "@langchain/langgraph";

const humanResponse = new Command({
resume: {
approved: true,
editedResponse: "We sincerely apologize for the double charge. I've initiated an immediate refund...",
}
});

// Resume execution
const finalResult = await app.invoke(humanResponse, config);
console.log("Email sent successfully!");

The graph pauses when it hits `interrupt()`, saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The `thread_id` ensures all state for this conversation is preserved together.

## ​ Summary and next steps

### ​ Key Insights

Building this email agent has shown us the LangGraph way of thinking:

**Break into discrete steps** \\
\\
Each node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps. **State is shared memory** \\
\\
Store raw data, not formatted text. This lets different nodes use the same information in different ways. **Nodes are functions** \\
\\
They take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination. **Errors are part of the flow** \\
\\
Transient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging. **Human input is first-class** \\
\\
The `interrupt()` function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first. **Graph structure emerges naturally** \\
\\
You define the essential connections, and your nodes handle their own routing logic. This keeps control flow explicit and traceable - you can always understand what your agent will do next by looking at the current node.

### ​ Advanced considerations

Node granularity trade-offs

This section explores the trade-offs in node granularity design. Most applications can skip this and use the patterns shown above.

You might wonder: why not combine `Read Email` and `Classify Intent` into one node?Or why separate Doc Search from Draft Reply?The answer involves trade-offs between resilience and observability.**The resilience consideration:** LangGraph’s durable execution creates checkpoints at node boundaries. When a workflow resumes after an interruption or failure, it starts from the beginning of the node where execution stopped. Smaller nodes mean more frequent checkpoints, which means less work to repeat if something goes wrong. If you combine multiple operations into one large node, a failure near the end means re-executing everything from the start of that node.Why we chose this breakdown for the email agent:

- **Isolation of external services:** Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others.
- **Intermediate visibility:** Having `Classify Intent` as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring—you can see exactly when and why the agent routes to human review.
- **Different failure modes:** LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently.
- **Reusability and testing:** Smaller nodes are easier to test in isolation and reuse in other workflows.

A different valid approach: You could combine `Read Email` and `Classify Intent` into a single node. You’d lose the ability to inspect the raw email before classification and would repeat both operations on any failure in that node. For most applications, the observability and debugging benefits of separate nodes are worth the trade-off.Application-level concerns: The caching discussion in Step 2 (whether to cache search results) is an application-level decision, not a LangGraph framework feature. You implement caching within your node functions based on your specific requirements—LangGraph doesn’t prescribe this.Performance considerations: More nodes doesn’t mean slower execution. LangGraph writes checkpoints in the background by default ( async durability mode), so your graph continues running without waiting for checkpoints to complete. This means you get frequent checkpoints with minimal performance impact. You can adjust this behavior if needed—use `"exit"` mode to checkpoint only at completion, or `"sync"` mode to block execution until each checkpoint is written.

### ​ Where to go from here

This was an introduction to thinking about building agents with LangGraph. You can extend this foundation with:

**Human-in-the-loop patterns** \\
\\
Learn how to add tool approval before execution, batch approval, and other patterns **Subgraphs** \\
\\
Create subgraphs for complex multi-step operations **Streaming** \\
\\
Add streaming to show real-time progress to users **Observability** \\
\\
Add observability with LangSmith for debugging and monitoring **Tool Integration** \\
\\
Integrate more tools for web search, database queries, and API calls **Retry Logic** \\
\\
Implement retry logic with exponential backoff for failed operations

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Changelog\\
\\
Previous Workflows and agents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/workflows-agents

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Workflows and agents

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Setup
- LLMs and augmentations
- Prompt chaining
- Parallelization
- Routing
- Orchestrator-worker
- Creating workers in LangGraph
- Evaluator-optimizer
- Agents

This guide reviews common workflow and agent patterns.

- Workflows have predetermined code paths and are designed to operate in a certain order.
- Agents are dynamic and define their own processes and tool usage.

## ​ Setup

To build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:

1. Install dependencies

npm

pnpm

yarn

bun

Copy

npm install @langchain/langgraph @langchain/core

2. Initialize the LLM:

import { ChatAnthropic } from "@langchain/anthropic";

const llm = new ChatAnthropic({
model: "claude-sonnet-4-5-20250929",

});

## ​ LLMs and augmentations

Workflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.!LLM augmentations

import * as z from "zod";
import { tool } from "langchain";

// Schema for structured output
const SearchQuery = z.object({
search_query: z.string().describe("Query that is optimized web search."),
justification: z
.string()
.describe("Why this query is relevant to the user's request."),
});

// Augment the LLM with schema for structured output
const structuredLlm = llm.withStructuredOutput(SearchQuery);

// Invoke the augmented LLM
const output = await structuredLlm.invoke(
"How does Calcium CT score relate to high cholesterol?"
);

// Define a tool
const multiply = tool(

return a * b;
},
{
name: "multiply",
description: "Multiply two numbers",
schema: z.object({
a: z.number(),
b: z.number(),
}),
}
);

// Augment the LLM with tools
const llmWithTools = llm.bindTools([multiply]);

// Invoke the LLM with input that triggers the tool call
const msg = await llmWithTools.invoke("What is 2 times 3?");

// Get the tool call
console.log(msg.tool_calls);

## ​ Prompt chaining

Prompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:

- Translating documents into different languages
- Verifying generated content for consistency

Graph API

Functional API

import { StateGraph, Annotation } from "@langchain/langgraph";

// Graph state
const StateAnnotation = Annotation.Root({

// Define node functions

// First LLM call to generate initial joke
async function generateJoke(state: typeof StateAnnotation.State) {
const msg = await llm.invoke(`Write a short joke about ${state.topic}`);
return { joke: msg.content };
}

// Gate function to check if the joke has a punchline
function checkPunchline(state: typeof StateAnnotation.State) {
// Simple check - does the joke contain "?" or "!"
if (state.joke?.includes("?") || state.joke?.includes("!")) {
return "Pass";
}
return "Fail";
}

// Second LLM call to improve the joke
async function improveJoke(state: typeof StateAnnotation.State) {
const msg = await llm.invoke(
`Make this joke funnier by adding wordplay: ${state.joke}`
);
return { improvedJoke: msg.content };
}

// Third LLM call for final polish
async function polishJoke(state: typeof StateAnnotation.State) {
const msg = await llm.invoke(
`Add a surprising twist to this joke: ${state.improvedJoke}`
);
return { finalJoke: msg.content };
}

// Build workflow
const chain = new StateGraph(StateAnnotation)
.addNode("generateJoke", generateJoke)
.addNode("improveJoke", improveJoke)
.addNode("polishJoke", polishJoke)
.addEdge("__start__", "generateJoke")
.addConditionalEdges("generateJoke", checkPunchline, {
Pass: "improveJoke",
Fail: "__end__"
})
.addEdge("improveJoke", "polishJoke")
.addEdge("polishJoke", "__end__")
.compile();

// Invoke
const state = await chain.invoke({ topic: "cats" });
console.log("Initial joke:");
console.log(state.joke);
console.log("\n--- --- ---\n");
if (state.improvedJoke !== undefined) {
console.log("Improved joke:");
console.log(state.improvedJoke);
console.log("\n--- --- ---\n");

console.log("Final joke:");
console.log(state.finalJoke);
} else {
console.log("Joke failed quality gate - no punchline detected!");
}

## ​ Parallelization

With parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:

- Split up subtasks and run them in parallel, which increases speed
- Run tasks multiple times to check for different outputs, which increases confidence

Some examples include:

- Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors
- Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources

// Nodes
// First LLM call to generate initial joke
async function callLlm1(state: typeof StateAnnotation.State) {
const msg = await llm.invoke(`Write a joke about ${state.topic}`);
return { joke: msg.content };
}

// Second LLM call to generate story
async function callLlm2(state: typeof StateAnnotation.State) {
const msg = await llm.invoke(`Write a story about ${state.topic}`);
return { story: msg.content };
}

// Third LLM call to generate poem
async function callLlm3(state: typeof StateAnnotation.State) {
const msg = await llm.invoke(`Write a poem about ${state.topic}`);
return { poem: msg.content };
}

// Combine the joke, story and poem into a single output
async function aggregator(state: typeof StateAnnotation.State) {
const combined = `Here's a story, joke, and poem about ${state.topic}!\n\n` +
`STORY:\n${state.story}\n\n` +
`JOKE:\n${state.joke}\n\n` +
`POEM:\n${state.poem}`;
return { combinedOutput: combined };
}

// Build workflow
const parallelWorkflow = new StateGraph(StateAnnotation)
.addNode("callLlm1", callLlm1)
.addNode("callLlm2", callLlm2)
.addNode("callLlm3", callLlm3)
.addNode("aggregator", aggregator)
.addEdge("__start__", "callLlm1")
.addEdge("__start__", "callLlm2")
.addEdge("__start__", "callLlm3")
.addEdge("callLlm1", "aggregator")
.addEdge("callLlm2", "aggregator")
.addEdge("callLlm3", "aggregator")
.addEdge("aggregator", "__end__")
.compile();

// Invoke
const result = await parallelWorkflow.invoke({ topic: "cats" });
console.log(result.combinedOutput);

## ​ Routing

Routing workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.!routing.png

import { StateGraph, Annotation } from "@langchain/langgraph";
import * as z from "zod";

// Schema for structured output to use as routing logic
const routeSchema = z.object({
step: z.enum(["poem", "story", "joke"]).describe(
"The next step in the routing process"
),
});

// Augment the LLM with schema for structured output
const router = llm.withStructuredOutput(routeSchema);

// Nodes
// Write a story
async function llmCall1(state: typeof StateAnnotation.State) {
const result = await llm.invoke([{\
role: "system",\
content: "You are an expert storyteller.",\
}, {\
role: "user",\
content: state.input\
}]);
return { output: result.content };
}

// Write a joke
async function llmCall2(state: typeof StateAnnotation.State) {
const result = await llm.invoke([{\
role: "system",\
content: "You are an expert comedian.",\
}, {\
role: "user",\
content: state.input\
}]);
return { output: result.content };
}

// Write a poem
async function llmCall3(state: typeof StateAnnotation.State) {
const result = await llm.invoke([{\
role: "system",\
content: "You are an expert poet.",\
}, {\
role: "user",\
content: state.input\
}]);
return { output: result.content };
}

async function llmCallRouter(state: typeof StateAnnotation.State) {
// Route the input to the appropriate node
const decision = await router.invoke([\
{\
role: "system",\
content: "Route the input to story, joke, or poem based on the user's request."\
},\
{\
role: "user",\
content: state.input\
},\
]);

return { decision: decision.step };
}

// Conditional edge function to route to the appropriate node
function routeDecision(state: typeof StateAnnotation.State) {
// Return the node name you want to visit next
if (state.decision === "story") {
return "llmCall1";
} else if (state.decision === "joke") {
return "llmCall2";
} else if (state.decision === "poem") {
return "llmCall3";
}
}

// Build workflow
const routerWorkflow = new StateGraph(StateAnnotation)
.addNode("llmCall1", llmCall1)
.addNode("llmCall2", llmCall2)
.addNode("llmCall3", llmCall3)
.addNode("llmCallRouter", llmCallRouter)
.addEdge("__start__", "llmCallRouter")
.addConditionalEdges(
"llmCallRouter",
routeDecision,
["llmCall1", "llmCall2", "llmCall3"],
)
.addEdge("llmCall1", "__end__")
.addEdge("llmCall2", "__end__")
.addEdge("llmCall3", "__end__")
.compile();

// Invoke
const state = await routerWorkflow.invoke({
input: "Write me a joke about cats"
});
console.log(state.output);

## ​ Orchestrator-worker

In an orchestrator-worker configuration, the orchestrator:

- Breaks down tasks into subtasks
- Delegates subtasks to workers
- Synthesizes worker outputs into a final result

type SectionSchema = {
name: string;
description: string;
}
type SectionsSchema = {
sections: SectionSchema[];
}

// Augment the LLM with schema for structured output
const planner = llm.withStructuredOutput(sectionsSchema);

### ​ Creating workers in LangGraph

Orchestrator-worker workflows are common and LangGraph has built-in support for them. The `Send` API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the `Send` API to send a section to each worker.

import { Annotation, StateGraph, Send } from "@langchain/langgraph";

}),

// Worker state
const WorkerStateAnnotation = Annotation.Root({

}),
});

// Nodes
async function orchestrator(state: typeof StateAnnotation.State) {
// Generate queries
const reportSections = await planner.invoke([\
{ role: "system", content: "Generate a plan for the report." },\
{ role: "user", content: `Here is the report topic: ${state.topic}` },\
]);

return { sections: reportSections.sections };
}

async function llmCall(state: typeof WorkerStateAnnotation.State) {
// Generate section
const section = await llm.invoke([\
{\
role: "system",\
content: "Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.",\
},\
{\
role: "user",\
content: `Here is the section name: ${state.section.name} and description: ${state.section.description}`,\
},\
]);

// Write the updated section to completed sections
return { completedSections: [section.content] };
}

async function synthesizer(state: typeof StateAnnotation.State) {
// List of completed sections
const completedSections = state.completedSections;

// Format completed section to str to use as context for final sections
const completedReportSections = completedSections.join("\n\n---\n\n");

return { finalReport: completedReportSections };
}

// Conditional edge function to create llm_call workers that each write a section of the report
function assignWorkers(state: typeof StateAnnotation.State) {
// Kick off section writing in parallel via Send() API

);
}

// Build workflow
const orchestratorWorker = new StateGraph(StateAnnotation)
.addNode("orchestrator", orchestrator)
.addNode("llmCall", llmCall)
.addNode("synthesizer", synthesizer)
.addEdge("__start__", "orchestrator")
.addConditionalEdges(
"orchestrator",
assignWorkers,
["llmCall"]
)
.addEdge("llmCall", "synthesizer")
.addEdge("synthesizer", "__end__")
.compile();

// Invoke
const state = await orchestratorWorker.invoke({
topic: "Create a report on LLM scaling laws"
});
console.log(state.finalReport);

## ​ Evaluator-optimizer

In evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.Evaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.!evaluator_optimizer.png

import * as z from "zod";
import { Annotation, StateGraph } from "@langchain/langgraph";

// Schema for structured output to use in evaluation
const feedbackSchema = z.object({
grade: z.enum(["funny", "not funny"]).describe(
"Decide if the joke is funny or not."
),
feedback: z.string().describe(
"If the joke is not funny, provide feedback on how to improve it."
),
});

// Augment the LLM with schema for structured output
const evaluator = llm.withStructuredOutput(feedbackSchema);

// Nodes
async function llmCallGenerator(state: typeof StateAnnotation.State) {
// LLM generates a joke
let msg;
if (state.feedback) {
msg = await llm.invoke(
`Write a joke about ${state.topic} but take into account the feedback: ${state.feedback}`
);
} else {
msg = await llm.invoke(`Write a joke about ${state.topic}`);
}
return { joke: msg.content };
}

async function llmCallEvaluator(state: typeof StateAnnotation.State) {
// LLM evaluates the joke
const grade = await evaluator.invoke(`Grade the joke ${state.joke}`);
return { funnyOrNot: grade.grade, feedback: grade.feedback };
}

// Conditional edge function to route Agents

Agents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.!agent.png

To get started with agents, see the quickstart or read more about how they work in LangChain.

Using tools

import { tool } from "@langchain/core/tools";
import * as z from "zod";

// Define tools
const multiply = tool(

return a * b;
},
{
name: "multiply",
description: "Multiply two numbers together",
schema: z.object({
a: z.number().describe("first number"),
b: z.number().describe("second number"),
}),
}
);

const add = tool(

return a + b;
},
{
name: "add",
description: "Add two numbers together",
schema: z.object({
a: z.number().describe("first number"),
b: z.number().describe("second number"),
}),
}
);

const divide = tool(

return a / b;
},
{
name: "divide",
description: "Divide two numbers",
schema: z.object({
a: z.number().describe("first number"),
b: z.number().describe("second number"),
}),
}
);

// Augment the LLM with tools
const tools = [add, multiply, divide];

const llmWithTools = llm.bindTools(tools);

import { MessagesAnnotation, StateGraph } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import {
SystemMessage,
ToolMessage
} from "@langchain/core/messages";

// Nodes
async function llmCall(state: typeof MessagesAnnotation.State) {
// LLM decides whether to call a tool or not
const result = await llmWithTools.invoke([\
{\
role: "system",\
content: "You are a helpful assistant tasked with performing arithmetic on a set of inputs."\
},\
...state.messages\
]);

return {
messages: [result]
};
}

const toolNode = new ToolNode(tools);

// Conditional edge function to route to the tool node or end
function shouldContinue(state: typeof MessagesAnnotation.State) {
const messages = state.messages;
const lastMessage = messages.at(-1);

// If the LLM makes a tool call, then perform an action
if (lastMessage?.tool_calls?.length) {
return "toolNode";
}
// Otherwise, we stop (reply to the user)
return "__end__";
}

// Build workflow
const agentBuilder = new StateGraph(MessagesAnnotation)
.addNode("llmCall", llmCall)
.addNode("toolNode", toolNode)
// Add edges to connect nodes
.addEdge("__start__", "llmCall")
.addConditionalEdges(
"llmCall",
shouldContinue,
["toolNode", "__end__"]
)
.addEdge("toolNode", "llmCall")
.compile();

// Invoke
const messages = [{\
role: "user",\
content: "Add 3 and 4."\
}];
const result = await agentBuilder.invoke({ messages });
console.log(result.messages);

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Thinking in LangGraph\\
\\
Previous Persistence\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/persistence

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Persistence

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Threads
- Checkpoints
- Get state
- Get state history
- Replay
- Update state
- config
- values
- as\_node
- Memory Store
- Basic Usage
- Semantic Search
- Using in LangGraph
- Checkpointer libraries
- Checkpointer interface
- Serializer
- Capabilities
- Human-in-the-loop
- Memory
- Time Travel
- Fault-tolerance
- Pending writes

LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a `checkpoint` of the graph state at every super-step. Those checkpoints are saved to a `thread`, which can be accessed after graph execution. Because `threads` allow access to graph’s state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we’ll discuss each of these concepts in more detail.!Checkpoints

**Agent Server handles checkpointing automatically**
When using the Agent Server, you don’t need to implement or configure checkpointers manually. The server handles all persistence infrastructure for you behind the scenes.

## ​ Threads

A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread.When invoking a graph with a checkpointer, you **must** specify a `thread_id` as part of the `configurable` portion of the config:

Copy

{
configurable: {
thread_id: "1";
}
}

A thread’s current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the API reference for more details.The checkpointer uses `thread_id` as the primary key for storing and retrieving checkpoints. Without it, the checkpointer cannot save state or resume execution after an interrupt, since the checkpointer uses `thread_id` to load the saved state.

## ​ Checkpoints

The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by `StateSnapshot` object with the following key properties:

- `config`: Config associated with this checkpoint.
- `metadata`: Metadata associated with this checkpoint.
- `values`: Values of the state channels at this point in time.
- `next` A tuple of the node names to execute next in the graph.
- `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts.

Checkpoints are persisted and can be used to restore the state of a thread at a later time.Let’s see what checkpoints are saved when a simple graph is invoked as follows:

import { StateGraph, START, END, MemorySaver } from "@langchain/langgraph";
import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";

const State = z.object({
foo: z.string(),
bar: z.array(z.string()).register(registry, {
reducer: {

},

}),
});

const workflow = new StateGraph(State)

return { foo: "a", bar: ["a"] };
})

return { foo: "b", bar: ["b"] };
})
.addEdge(START, "nodeA")
.addEdge("nodeA", "nodeB")
.addEdge("nodeB", END);

const checkpointer = new MemorySaver();
const graph = workflow.compile({ checkpointer });

const config = { configurable: { thread_id: "1" } };
await graph.invoke({ foo: "", bar: [] }, config);

const config = { configurable: { thread_id: "1" } };
await graph.invoke({ foo: "" }, config);

After we run the graph, we expect to see exactly 4 checkpoints:

- Empty checkpoint with `START` as the next node to be executed
- Checkpoint with the user input `{'foo': '', 'bar': []}` and `nodeA` as the next node to be executed
- Checkpoint with the outputs of `nodeA``{'foo': 'a', 'bar': ['a']}` and `nodeB` as the next node to be executed
- Checkpoint with the outputs of `nodeB``{'foo': 'b', 'bar': ['a', 'b']}` and no next nodes to be executed

Note that the `bar` channel values contain outputs from both nodes as we have a reducer for the `bar` channel.

### ​ Get state

When interacting with the saved graph state, you **must** specify a thread identifier. You can view the _latest_ state of the graph by calling `graph.getState(config)`. This will return a `StateSnapshot` object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.

// get the latest state snapshot
const config = { configurable: { thread_id: "1" } };
await graph.getState(config);

// get a state snapshot for a specific checkpoint_id
const config = {
configurable: {
thread_id: "1",
checkpoint_id: "1ef663ba-28fe-6528-8002-5a559208592c",
},
};
await graph.getState(config);

In our example, the output of `getState` will look like this:

StateSnapshot {
values: { foo: 'b', bar: ['a', 'b'] },
next: [],
config: {
configurable: {
thread_id: '1',
checkpoint_ns: '',
checkpoint_id: '1ef663ba-28fe-6528-8002-5a559208592c'
}
},
metadata: {
source: 'loop',
writes: { nodeB: { foo: 'b', bar: ['b'] } },
step: 2
},
createdAt: '2024-08-29T19:19:38.821749+00:00',
parentConfig: {
configurable: {
thread_id: '1',
checkpoint_ns: '',
checkpoint_id: '1ef663ba-28f9-6ec4-8001-31981c2c39f8'
}
},
tasks: []
}

### ​ Get state history

You can get the full history of the graph execution for a given thread by calling `graph.getStateHistory(config)`. This will return a list of `StateSnapshot` objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / `StateSnapshot` being the first in the list.

const config = { configurable: { thread_id: "1" } };
for await (const state of graph.getStateHistory(config)) {
console.log(state);
}

In our example, the output of `getStateHistory` will look like this:

[\
StateSnapshot {\
values: { foo: 'b', bar: ['a', 'b'] },\
next: [],\
config: {\
configurable: {\
thread_id: '1',\
checkpoint_ns: '',\
checkpoint_id: '1ef663ba-28fe-6528-8002-5a559208592c'\
}\
},\
metadata: {\
source: 'loop',\
writes: { nodeB: { foo: 'b', bar: ['b'] } },\
step: 2\
},\
createdAt: '2024-08-29T19:19:38.821749+00:00',\
parentConfig: {\
configurable: {\
thread_id: '1',\
checkpoint_ns: '',\
checkpoint_id: '1ef663ba-28f9-6ec4-8001-31981c2c39f8'\
}\
},\
tasks: []\
},\
StateSnapshot {\
values: { foo: 'a', bar: ['a'] },\
next: ['nodeB'],\
config: {\
configurable: {\
thread_id: '1',\
checkpoint_ns: '',\
checkpoint_id: '1ef663ba-28f9-6ec4-8001-31981c2c39f8'\
}\
},\
metadata: {\
source: 'loop',\
writes: { nodeA: { foo: 'a', bar: ['a'] } },\
step: 1\
},\
createdAt: '2024-08-29T19:19:38.819946+00:00',\
parentConfig: {\
configurable: {\
thread_id: '1',\
checkpoint_ns: '',\
checkpoint_id: '1ef663ba-28f4-6b4a-8000-ca575a13d36a'\
}\
},\
tasks: [\
PregelTask {\
id: '6fb7314f-f114-5413-a1f3-d37dfe98ff44',\
name: 'nodeB',\
error: null,\
interrupts: []\
}\
]\
},\
StateSnapshot {\
values: { foo: '', bar: [] },\
next: ['node_a'],\
config: {\
configurable: {\
thread_id: '1',\
checkpoint_ns: '',\
checkpoint_id: '1ef663ba-28f4-6b4a-8000-ca575a13d36a'\
}\
},\
metadata: {\
source: 'loop',\
writes: null,\
step: 0\
},\
createdAt: '2024-08-29T19:19:38.817813+00:00',\
parentConfig: {\
configurable: {\
thread_id: '1',\
checkpoint_ns: '',\
checkpoint_id: '1ef663ba-28f0-6c66-bfff-6723431e8481'\
}\
},\
tasks: [\
PregelTask {\
id: 'f1b14528-5ee5-579c-949b-23ef9bfbed58',\
name: 'node_a',\
error: null,\
interrupts: []\
}\
]\
},\
StateSnapshot {\
values: { bar: [] },\
next: ['__start__'],\
config: {\
configurable: {\
thread_id: '1',\
checkpoint_ns: '',\
checkpoint_id: '1ef663ba-28f0-6c66-bfff-6723431e8481'\
}\
},\
metadata: {\
source: 'input',\
writes: { foo: '' },\
step: -1\
},\
createdAt: '2024-08-29T19:19:38.816205+00:00',\
parentConfig: null,\
tasks: [\
PregelTask {\
id: '6d27aa2e-d72b-5504-a36f-8620e54a76dd',\
name: '__start__',\
error: null,\
interrupts: []\
}\
]\
}\
]

### ​ Replay

It’s also possible to play-back a prior graph execution. If we `invoke` a graph with a `thread_id` and a `checkpoint_id`, then we will _re-play_ the previously executed steps _before_ a checkpoint that corresponds to the `checkpoint_id`, and only execute the steps _after_ the checkpoint.

- `thread_id` is the ID of a thread.
- `checkpoint_id` is an identifier that refers to a specific checkpoint within a thread.

You must pass these when invoking the graph as part of the `configurable` portion of the config:

const config = {
configurable: {
thread_id: "1",
checkpoint_id: "0c62ca34-ac19-445d-bbb0-5b4984975b2a",
},
};
await graph.invoke(null, config);

Importantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply _re-plays_ that particular step in the graph and does not re-execute the step, but only for the steps _before_ the provided `checkpoint_id`. All of the steps _after_`checkpoint_id` will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying.!Replay

### ​ Update state

In addition to re-playing the graph from specific `checkpoints`, we can also _edit_ the graph state. We do this using `graph.updateState()`. This method accepts three different arguments:

#### ​ `config`

The config should contain `thread_id` specifying which thread to update. When only the `thread_id` is passed, we update (or fork) the current state. Optionally, if we include `checkpoint_id` field, then we fork that selected checkpoint.

#### ​ `values`

These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that `update_state` does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let’s walk through an example.Let’s assume you have defined the state of your graph with the following schema (see full example above):

import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";

const State = z.object({
foo: z.number(),
bar: z.array(z.string()).register(registry, {
reducer: {

Let’s now assume the current state of the graph is

{ foo: 1, bar: ["a"] }

If you update the state as below:

await graph.updateState(config, { foo: 2, bar: ["b"] });

Then the new state of the graph will be:

{ foo: 2, bar: ["a", "b"] }

The `foo` key (channel) is completely changed (because there is no reducer specified for that channel, so `updateState` overwrites it). However, there is a reducer specified for the `bar` key, and so it appends `"b"` to the state of `bar`.

#### ​ `as_node`

The final thing you can optionally specify when calling `updateState` is `asNode`. If you provide it, the update will be applied as if it came from node `asNode`. If `asNode` is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state.!Update

## ​ Memory Store

**LangGraph API handles stores automatically**
When using the LangGraph API, you don’t need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.

### ​ Basic Usage

First, let’s showcase this in isolation without using LangGraph.

import { MemoryStore } from "@langchain/langgraph";

const memoryStore = new MemoryStore();

const userId = "1";
const namespaceForMemory = [userId, "memories"];

We use the `store.put` method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (`memory_id`) and the value (a dictionary) is the memory itself.

import { v4 as uuidv4 } from "uuid";

const memoryId = uuidv4();
const memory = { food_preference: "I like pizza" };
await memoryStore.put(namespaceForMemory, memoryId, memory);

We can read out memories in our namespace using the `store.search` method, which will return all memories for a given user as a list. The most recent memory is the last in the list.

const memories = await memoryStore.search(namespaceForMemory);
memories[memories.length - 1];

// {
// value: { food_preference: 'I like pizza' },
// key: '07e0caf4-1631-47b7-b15f-65515d4c1843',
// namespace: ['1', 'memories'],
// createdAt: '2024-10-02T17:22:31.590602+00:00',
// updatedAt: '2024-10-02T17:22:31.590605+00:00'
// }

The attributes it has are:

- `value`: The value of this memory
- `key`: A unique key for this memory in this namespace
- `namespace`: A list of strings, the namespace of this memory type
- `createdAt`: Timestamp for when this memory was created
- `updatedAt`: Timestamp for when this memory was updated

### ​ Semantic Search

Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:

import { OpenAIEmbeddings } from "@langchain/openai";

const store = new InMemoryStore({
index: {
embeddings: new OpenAIEmbeddings({ model: "text-embedding-3-small" }),
dims: 1536,
fields: ["food_preference", "$"], // Fields to embed
},
});

Now when searching, you can use natural language queries to find relevant memories:

// Find memories about food preferences
// (This can be done after putting memories into the store)
const memories = await store.search(namespaceForMemory, {
query: "What does the user like to eat?",
limit: 3, // Return top 3 matches
});

You can control which parts of your memories get embedded by configuring the `fields` parameter or by specifying the `index` parameter when storing memories:

// Store with specific fields to embed
await store.put(
namespaceForMemory,
uuidv4(),
{
food_preference: "I love Italian cuisine",
context: "Discussing dinner plans",
},
{ index: ["food_preference"] } // Only embed "food_preferences" field
);

// Store without embedding (still retrievable, but not searchable)
await store.put(
namespaceForMemory,
uuidv4(),
{ system_info: "Last updated: 2024-01-01" },
{ index: false }
);

### ​ Using in LangGraph

With this all in place, we use the `memoryStore` in LangGraph. The `memoryStore` works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the `memoryStore` allows us to store arbitrary information for access _across_ threads. We compile the graph with both the checkpointer and the `memoryStore` as follows.

import { MemorySaver } from "@langchain/langgraph";

// We need this because we want to enable threads (conversations)
const checkpointer = new MemorySaver();

// ... Define the graph ...

// Compile the graph with the checkpointer and store
const graph = workflow.compile({ checkpointer, store: memoryStore });

We invoke the graph with a `thread_id`, as before, and also with a `user_id`, which we’ll use to namespace our memories to this particular user as we showed above.

// Invoke the graph
const userId = "1";
const config = { configurable: { thread_id: "1", user_id: userId } };

// First let's just say hi to the AI
for await (const update of await graph.stream(
{ messages: [{ role: "user", content: "hi" }] },
{ ...config, streamMode: "updates" }
)) {
console.log(update);
}

We can access the `memoryStore` and the `user_id` in _any node_ by accessing `config` and `store` as node arguments. Here’s how we might use semantic search in a node to find relevant memories:

import { MessagesZodMeta, Runtime } from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages";
import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";

const MessagesZodState = z.object({
messages: z

.register(registry, MessagesZodMeta),
});

const updateMemory = async (

// Get the user id from the config
const userId = runtime.context?.user_id;
if (!userId) throw new Error("User ID is required");

// Namespace the memory
const namespace = [userId, "memories"];

// ... Analyze conversation and create a new memory

// Create a new memory ID
const memoryId = uuidv4();

// We create a new memory
await runtime.store?.put(namespace, memoryId, { memory });
};

As we showed above, we can also access the store in any node and use the `store.search` method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.

memories[memories.length - 1];
// {
// value: { food_preference: 'I like pizza' },
// key: '07e0caf4-1631-47b7-b15f-65515d4c1843',
// namespace: ['1', 'memories'],
// createdAt: '2024-10-02T17:22:31.590602+00:00',
// updatedAt: '2024-10-02T17:22:31.590605+00:00'
// }

We can access the memories and use them in our model call.

const callModel = async (

config: LangGraphRunnableConfig,
store: BaseStore

// Get the user id from the config
const userId = config.configurable?.user_id;

// Search based on the most recent message
const memories = await store.search(namespace, {
query: state.messages[state.messages.length - 1].content,
limit: 3,
});

// ... Use memories in the model call
};

If we create a new thread, we can still access the same memories so long as the `user_id` is the same.

// Invoke the graph
const config = { configurable: { thread_id: "2", user_id: "1" } };

// Let's say hi again
for await (const update of await graph.stream(
{ messages: [{ role: "user", content: "hi, tell me about my memories" }] },
{ ...config, streamMode: "updates" }
)) {
console.log(update);
}

When we use the LangSmith, either locally (e.g., in Studio) or hosted with LangSmith, the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you **do** need to configure the indexing settings in your `langgraph.json` file. For example:

{
...
"store": {
"index": {
"embed": "openai:text-embeddings-3-small",
"dims": 1536,
"fields": ["$"]
}
}
}

See the deployment guide for more details and configuration options.

## ​ Checkpointer libraries

Under the hood, checkpointing is powered by checkpointer objects that conform to `BaseCheckpointSaver` interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:

- `@langchain/langgraph-checkpoint`: The base interface for checkpointer savers ( `BaseCheckpointSaver`) and serialization/deserialization interface ( `SerializerProtocol`). Includes in-memory checkpointer implementation ( `MemorySaver`) for experimentation. LangGraph comes with `@langchain/langgraph-checkpoint` included.
- `@langchain/langgraph-checkpoint-sqlite`: An implementation of LangGraph checkpointer that uses SQLite database ( `SqliteSaver`). Ideal for experimentation and local workflows. Needs to be installed separately.
- `@langchain/langgraph-checkpoint-postgres`: An advanced checkpointer that uses Postgres database ( `PostgresSaver`), used in LangSmith. Ideal for using in production. Needs to be installed separately.

### ​ Checkpointer interface

Each checkpointer conforms to the `BaseCheckpointSaver` interface and implements the following methods:

- `.put` \- Store a checkpoint with its configuration and metadata.
- `.putWrites` \- Store intermediate writes linked to a checkpoint (i.e. pending writes).
- `.getTuple` \- Fetch a checkpoint tuple using for a given configuration (`thread_id` and `checkpoint_id`). This is used to populate `StateSnapshot` in `graph.getState()`.
- `.list` \- List checkpoints that match a given configuration and filter criteria. This is used to populate state history in `graph.getStateHistory()`

### ​ Serializer

When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.`@langchain/langgraph-checkpoint` defines protocol for implementing serializers and provides a default implementation that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.

## ​ Capabilities

### ​ Human-in-the-loop

First, checkpointers facilitate human-in-the-loop workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See the how-to guides for examples.

### ​ Memory

Second, checkpointers allow for “memory” between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See Add memory for information on how to add and manage conversation memory using checkpointers.

### ​ Time Travel

Third, checkpointers allow for “time travel”, allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.

### ​ Fault-tolerance

Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don’t re-run the successful nodes.

#### ​ Pending writes

Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don’t re-run the successful nodes.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Workflows and agents\\
\\
Previous Durable execution\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/durable-execution

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Durable execution

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Requirements
- Determinism and Consistent Replay
- Durability modes
- Using tasks in nodes
- Resuming Workflows
- Starting Points for Resuming Workflows

**Durable execution** is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require human-in-the-loop, where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps — even after a significant delay (e.g., a week later).LangGraph’s built-in persistence layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted — whether by a system failure or for human-in-the-loop interactions — it can be resumed from its last recorded state.

If you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.
To make the most of durable execution, ensure that your workflow is designed to be deterministic and idempotent and wrap any side effects or non-deterministic operations inside tasks. You can use tasks from both the StateGraph (Graph API) and the Functional API.

## ​ Requirements

To leverage durable execution in LangGraph, you need to:

1. Enable persistence in your workflow by specifying a checkpointer that will save workflow progress.
2. Specify a thread identifier when executing a workflow. This will track the execution history for a particular instance of the workflow.
3. Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside tasks to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see Determinism and Consistent Replay.

## ​ Determinism and Consistent Replay

When you resume a workflow run, the code does **NOT** resume from the **same line of code** where execution stopped; instead, it will identify an appropriate starting point from which to pick up where it left off. This means that the workflow will replay all steps from the starting point until it reaches the point where it was stopped.As a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside tasks or nodes.To ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:

- **Avoid Repeating Work**: If a node contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate **task**. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.
- **Encapsulate Non-Deterministic Operations:** Wrap any code that might yield non-deterministic results (e.g., random number generation) inside **tasks** or **nodes**. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.
- **Use Idempotent Operations**: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a **task** starts but fails to complete successfully, the workflow’s resumption will re-run the **task**, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.

For some examples of pitfalls to avoid, see the Common Pitfalls section in the functional API, which shows
how to structure your code using **tasks** to avoid these issues. The same principles apply to the StateGraph (Graph API).

## ​ Durability modes

LangGraph supports three durability modes that allow you to balance performance and data consistency based on your application’s requirements. A higher durability mode adds more overhead to the workflow execution. You can specify the durability mode when calling any graph execution method:

Copy

graph.stream(
{"input": "test"},
durability="sync"
)

The durability modes, from least to most durable, are as follows:

- `"exit"`: Changes are persisted only when graph execution exits (either successfully, with an error, or due to an interrupt). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from system failures (e.g., process crashes) that occur mid-execution.
- `"async"`: Changes are persisted asynchronously while the next step executes. This provides good performance and durability, but there’s a small risk that checkpoints might not be written if the process crashes during execution.
- `"sync"`: Changes are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.

## ​ Using tasks in nodes

If a node contains multiple operations, you may find it easier to convert each operation into a **task** rather than refactor the operations into individual nodes.

- Original

- With task

import { StateGraph, START, END } from "@langchain/langgraph";
import { MemorySaver } from "@langchain/langgraph";
import { v4 as uuidv4 } from "uuid";
import * as z from "zod";

// Define a Zod schema to represent the state
const State = z.object({
url: z.string(),
result: z.string().optional(),
});

const response = await fetch(state.url);
const text = await response.text();
const result = text.slice(0, 100); // Side-effect
return {
result,
};
};

// Create a StateGraph builder and add a node for the callApi function
const builder = new StateGraph(State)
.addNode("callApi", callApi)
.addEdge(START, "callApi")
.addEdge("callApi", END);

// Specify a checkpointer
const checkpointer = new MemorySaver();

// Compile the graph with the checkpointer
const graph = builder.compile({ checkpointer });

// Define a config with a thread ID.
const threadId = uuidv4();
const config = { configurable: { thread_id: threadId } };

// Invoke the graph
await graph.invoke({ url: "https://www.example.com" }, config);

import { StateGraph, START, END } from "@langchain/langgraph";
import { MemorySaver } from "@langchain/langgraph";
import { task } from "@langchain/langgraph";
import { v4 as uuidv4 } from "uuid";
import * as z from "zod";

// Define a Zod schema to represent the state
const State = z.object({
urls: z.array(z.string()),
results: z.array(z.string()).optional(),
});

const response = await fetch(url);
const text = await response.text();
return text.slice(0, 100);
});

const results = await Promise.all(requests);
return {
results,
};
};

// Invoke the graph
await graph.invoke({ urls: ["https://www.example.com"] }, config);

## ​ Resuming Workflows

Once you have enabled durable execution in your workflow, you can resume execution for the following scenarios:

- **Pausing and Resuming Workflows:** Use the interrupt function to pause a workflow at specific points and the `Command` primitive to resume it with updated state. See **Interrupts** for more details.
- **Recovering from Failures:** Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a `null` as the input value (see this example with the functional API).

## ​ Starting Points for Resuming Workflows

- If you’re using a StateGraph (Graph API), the starting point is the beginning of the **node** where execution stopped.
- If you’re making a subgraph call inside a node, the starting point will be the **parent** node that called the subgraph that was halted.
Inside the subgraph, the starting point will be the specific **node** where execution stopped.
- If you’re using the Functional API, the starting point is the beginning of the **entrypoint** where execution stopped.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Persistence\\
\\
Previous Streaming\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/streaming

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Streaming

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Supported stream modes
- Basic usage example
- Stream multiple modes
- Stream graph state
- Stream subgraph outputs
- Debugging
- LLM tokens
- Filter by LLM invocation
- Filter by node
- Stream custom data
- Use with any LLM
- Disable streaming for specific chat models

LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.What’s possible with LangGraph streaming:

- **Stream graph state** — get state updates / values with `updates` and `values` modes.
- **Stream subgraph outputs** — include outputs from both the parent graph and any nested subgraphs.
- **Stream LLM tokens** — capture token streams from anywhere: inside nodes, subgraphs, or tools.
- **Stream custom data** — send custom updates or progress signals directly from tool functions.
- **Use multiple streaming modes** — choose from `values` (full state), `updates` (state deltas), `messages` (LLM tokens + metadata), `custom` (arbitrary user data), or `debug` (detailed traces).

## ​ Supported stream modes

Pass one or more of the following stream modes as a list to the `stream` method:

| Mode | Description |
| --- | --- |
| `values` | Streams the full value of the state after each step of the graph. |
| `updates` | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |
| `custom` | Streams custom data from inside your graph nodes. |
| `messages` | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked. |
| `debug` | Streams as much information as possible throughout the execution of the graph. |

## ​ Basic usage example

LangGraph graphs expose the `stream` method to yield streamed outputs as iterators.

Copy

for await (const chunk of await graph.stream(inputs, {
streamMode: "updates",
})) {
console.log(chunk);
}

Extended example: streaming updates

import { StateGraph, START, END } from "@langchain/langgraph";
import * as z from "zod";

const State = z.object({
topic: z.string(),
joke: z.string(),
});

const graph = new StateGraph(State)

return { topic: state.topic + " and cats" };
})

return { joke: `This is a joke about ${state.topic}` };
})
.addEdge(START, "refineTopic")
.addEdge("refineTopic", "generateJoke")
.addEdge("generateJoke", END)
.compile();

for await (const chunk of await graph.stream(
{ topic: "ice cream" },
// Set streamMode: "updates" to stream only the updates to the graph state after each node
// Other stream modes are also available. See supported stream modes for details
{ streamMode: "updates" }
)) {
console.log(chunk);
}

{'refineTopic': {'topic': 'ice cream and cats'}}
{'generateJoke': {'joke': 'This is a joke about ice cream and cats'}}

## ​ Stream multiple modes

You can pass an array as the `streamMode` parameter to stream multiple modes at once.The streamed outputs will be tuples of `[mode, chunk]` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

for await (const [mode, chunk] of await graph.stream(inputs, {
streamMode: ["updates", "custom"],
})) {
console.log(chunk);
}

## ​ Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

- `updates` streams the **updates** to the state after each step of the graph.
- `values` streams the **full value** of the state after each step of the graph.

- updates

- values

Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.

for await (const chunk of await graph.stream(
{ topic: "ice cream" },
{ streamMode: "updates" }
)) {
console.log(chunk);
}

Use this to stream the **full state** of the graph after each step.

for await (const chunk of await graph.stream(
{ topic: "ice cream" },
{ streamMode: "values" }
)) {
console.log(chunk);
}

## ​ Stream subgraph outputs

for await (const chunk of await graph.stream(
{ foo: "foo" },
{
// Set subgraphs: true to stream outputs from subgraphs
subgraphs: true,
streamMode: "updates",
}
)) {
console.log(chunk);
}

Extended example: streaming from subgraphs

import { StateGraph, START } from "@langchain/langgraph";
import * as z from "zod";

// Define subgraph
const SubgraphState = z.object({
foo: z.string(), // note that this key is shared with the parent graph state
bar: z.string(),
});

const subgraphBuilder = new StateGraph(SubgraphState)

return { bar: "bar" };
})

return { foo: state.foo + state.bar };
})
.addEdge(START, "subgraphNode1")
.addEdge("subgraphNode1", "subgraphNode2");
const subgraph = subgraphBuilder.compile();

// Define parent graph
const ParentState = z.object({
foo: z.string(),
});

const builder = new StateGraph(ParentState)

return { foo: "hi! " + state.foo };
})
.addNode("node2", subgraph)
.addEdge(START, "node1")
.addEdge("node1", "node2");
const graph = builder.compile();

for await (const chunk of await graph.stream(
{ foo: "foo" },
{
streamMode: "updates",
// Set subgraphs: true to stream outputs from subgraphs
subgraphs: true,
}
)) {
console.log(chunk);
}

[[], {'node1': {'foo': 'hi! foo'}}]
[['node2:dfddc4ba-c3c5-6887-5012-a243b5b377c2'], {'subgraphNode1': {'bar': 'bar'}}]
[['node2:dfddc4ba-c3c5-6887-5012-a243b5b377c2'], {'subgraphNode2': {'foo': 'hi! foobar'}}]
[[], {'node2': {'foo': 'hi! foobar'}}]

**Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.

### ​ Debugging

Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.

for await (const chunk of await graph.stream(
{ topic: "ice cream" },
{ streamMode: "debug" }
)) {
console.log(chunk);
}

## ​ LLM tokens

Use the `messages` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.The streamed output from `messages` mode is a tuple `[message_chunk, metadata]` where:

- `message_chunk`: the token or message segment from the LLM.

import { ChatOpenAI } from "@langchain/openai";
import { StateGraph, START } from "@langchain/langgraph";
import * as z from "zod";

const MyState = z.object({
topic: z.string(),
joke: z.string().default(""),
});

const model = new ChatOpenAI({ model: "gpt-4o-mini" });

// Call the LLM to generate a joke about a topic
// Note that message events are emitted even when the LLM is run using .invoke rather than .stream
const modelResponse = await model.invoke([\
{ role: "user", content: `Generate a joke about ${state.topic}` },\
]);
return { joke: modelResponse.content };
};

const graph = new StateGraph(MyState)
.addNode("callModel", callModel)
.addEdge(START, "callModel")
.compile();

// The "messages" stream mode returns an iterator of tuples [messageChunk, metadata]
// where messageChunk is the token streamed by the LLM and metadata is a dictionary
// with information about the graph node where the LLM was called and other information
for await (const [messageChunk, metadata] of await graph.stream(
{ topic: "ice cream" },
{ streamMode: "messages" }
)) {
if (messageChunk.content) {
console.log(messageChunk.content + "|");
}
}

#### ​ Filter by LLM invocation

You can associate `tags` with LLM invocations to filter the streamed tokens by LLM invocation.

import { ChatOpenAI } from "@langchain/openai";

// model1 is tagged with "joke"
const model1 = new ChatOpenAI({
model: "gpt-4o-mini",
tags: ['joke']
});
// model2 is tagged with "poem"
const model2 = new ChatOpenAI({
model: "gpt-4o-mini",
tags: ['poem']
});

const graph = // ... define a graph that uses these LLMs

// The streamMode is set to "messages" to stream LLM tokens
// The metadata contains information about the LLM invocation, including the tags
for await (const [msg, metadata] of await graph.stream(
{ topic: "cats" },
{ streamMode: "messages" }
)) {
// Filter the streamed tokens by the tags field in the metadata to only include
// the tokens from the LLM invocation with the "joke" tag
if (metadata.tags?.includes("joke")) {
console.log(msg.content + "|");
}
}

Extended example: filtering by tags

// The jokeModel is tagged with "joke"
const jokeModel = new ChatOpenAI({
model: "gpt-4o-mini",
tags: ["joke"]
});
// The poemModel is tagged with "poem"
const poemModel = new ChatOpenAI({
model: "gpt-4o-mini",
tags: ["poem"]
});

const State = z.object({
topic: z.string(),
joke: z.string(),
poem: z.string(),
});

const topic = state.topic;
console.log("Writing joke...");

const jokeResponse = await jokeModel.invoke([\
{ role: "user", content: `Write a joke about ${topic}` }\
]);

console.log("\n\nWriting poem...");
const poemResponse = await poemModel.invoke([\
{ role: "user", content: `Write a short poem about ${topic}` }\
]);

return {
joke: jokeResponse.content,
poem: poemResponse.content
};
})
.addEdge(START, "callModel")
.compile();

#### ​ Filter by node

To stream tokens only from specific nodes, use `stream_mode="messages"` and filter the outputs by the `langgraph_node` field in the streamed metadata:

// The "messages" stream mode returns a tuple of [messageChunk, metadata]
// where messageChunk is the token streamed by the LLM and metadata is a dictionary
// with information about the graph node where the LLM was called and other information
for await (const [msg, metadata] of await graph.stream(
inputs,
{ streamMode: "messages" }
)) {
// Filter the streamed tokens by the langgraph_node field in the metadata
// to only include the tokens from the specified node
if (msg.content && metadata.langgraph_node === "some_node_name") {
// ...
}
}

Extended example: streaming LLM tokens from specific nodes

const topic = state.topic;
const jokeResponse = await model.invoke([\
{ role: "user", content: `Write a joke about ${topic}` }\
]);
return { joke: jokeResponse.content };
})

const topic = state.topic;
const poemResponse = await model.invoke([\
{ role: "user", content: `Write a short poem about ${topic}` }\
]);
return { poem: poemResponse.content };
})
// write both the joke and the poem concurrently
.addEdge(START, "writeJoke")
.addEdge(START, "writePoem")
.compile();

// The "messages" stream mode returns a tuple of [messageChunk, metadata]
// where messageChunk is the token streamed by the LLM and metadata is a dictionary
// with information about the graph node where the LLM was called and other information
for await (const [msg, metadata] of await graph.stream(
{ topic: "cats" },
{ streamMode: "messages" }
)) {
// Filter the streamed tokens by the langgraph_node field in the metadata
// to only include the tokens from the writePoem node
if (msg.content && metadata.langgraph_node === "writePoem") {
console.log(msg.content + "|");
}
}

## ​ Stream custom data

To send **custom user-defined data** from inside a LangGraph node or tool, follow these steps:

1. Use the `writer` parameter from the `LangGraphRunnableConfig` to emit custom data.
2. Set `streamMode: "custom"` when calling `.stream()` to get the custom data in the stream. You can combine multiple modes (e.g., `["updates", "custom"]`), but at least one must be `"custom"`.

- node

- tool

import { StateGraph, START, LangGraphRunnableConfig } from "@langchain/langgraph";
import * as z from "zod";

const State = z.object({
query: z.string(),
answer: z.string(),
});

// Use the writer to emit a custom key-value pair (e.g., progress update)
config.writer({ custom_key: "Generating custom data inside node" });
return { answer: "some data" };
})
.addEdge(START, "node")
.compile();

const inputs = { query: "example" };

// Set streamMode: "custom" to receive the custom data in the stream
for await (const chunk of await graph.stream(inputs, { streamMode: "custom" })) {
console.log(chunk);
}

import { tool } from "@langchain/core/tools";
import { LangGraphRunnableConfig } from "@langchain/langgraph";
import * as z from "zod";

const queryDatabase = tool(

// Use the writer to emit a custom key-value pair (e.g., progress update)
config.writer({ data: "Retrieved 0/100 records", type: "progress" });
// perform query
// Emit another custom key-value pair
config.writer({ data: "Retrieved 100/100 records", type: "progress" });
return "some-answer";
},
{
name: "query_database",
description: "Query the database.",
schema: z.object({
query: z.string().describe("The query to execute."),
}),
}
);

const graph = // ... define a graph that uses this tool

## ​ Use with any LLM

You can use `streamMode: "custom"` to stream data from **any LLM API** — even if that API does **not** implement the LangChain chat model interface.This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.

import { LangGraphRunnableConfig } from "@langchain/langgraph";

const callArbitraryModel = async (
state: any,
config: LangGraphRunnableConfig

// Example node that calls an arbitrary model and streams the output
// Assume you have a streaming client that yields chunks
// Generate LLM tokens using your custom streaming client
for await (const chunk of yourCustomStreamingClient(state.topic)) {
// Use the writer to send custom data to the stream
config.writer({ custom_llm_chunk: chunk });
}
return { result: "completed" };
};

const graph = new StateGraph(State)
.addNode("callArbitraryModel", callArbitraryModel)
// Add other nodes and edges as needed
.compile();

// Set streamMode: "custom" to receive the custom data in the stream
for await (const chunk of await graph.stream(
{ topic: "cats" },
{ streamMode: "custom" }
)) {
// The chunk will contain the custom data streamed from the llm
console.log(chunk);
}

Extended example: streaming arbitrary chat model

import { StateGraph, START, MessagesZodMeta, LangGraphRunnableConfig } from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages";
import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";
import OpenAI from "openai";

const openaiClient = new OpenAI();
const modelName = "gpt-4o-mini";

async function* streamTokens(modelName: string, messages: any[]) {
const response = await openaiClient.chat.completions.create({
messages,
model: modelName,
stream: true,
});

let role: string | null = null;
for await (const chunk of response) {
const delta = chunk.choices[0]?.delta;

if (delta?.role) {
role = delta.role;
}

if (delta?.content) {
yield { role, content: delta.content };
}
}
}

// this is our tool
const getItems = tool(

let response = "";
for await (const msgChunk of streamTokens(
modelName,
[\
{\
role: "user",\
content: `Can you tell me what kind of items i might find in the following place: '${input.place}'. List at least 3 such items separating them by a comma. And include a brief description of each item.`,\
},\
]
)) {
response += msgChunk.content;
config.writer?.(msgChunk);
}
return response;
},
{
name: "get_items",
description: "Use this tool to list items one might find in a place you're asked about.",
schema: z.object({
place: z.string().describe("The place to look up items for."),
}),
}
);

const State = z.object({
messages: z

.register(registry, MessagesZodMeta),
});

const graph = new StateGraph(State)
// this is the tool-calling graph node

const aiMessage = state.messages.at(-1);
const toolCall = aiMessage.tool_calls?.at(-1);

const functionName = toolCall?.function?.name;
if (functionName !== "get_items") {
throw new Error(`Tool ${functionName} not supported`);
}

const functionArguments = toolCall?.function?.arguments;
const args = JSON.parse(functionArguments);

const functionResponse = await getItems.invoke(args);
const toolMessage = {
tool_call_id: toolCall.id,
role: "tool",
name: functionName,
content: functionResponse,
};
return { messages: [toolMessage] };
})
.addEdge(START, "callTool")
.compile();

Let’s invoke the graph with an `AIMessage` that includes a tool call:

const inputs = {
messages: [\
{\
content: null,\
role: "assistant",\
tool_calls: [\
{\
id: "1",\
function: {\
arguments: '{"place":"bedroom"}',\
name: "get_items",\
},\
type: "function",\
}\
],\
}\
]
};

for await (const chunk of await graph.stream(
inputs,
{ streamMode: "custom" }
)) {
console.log(chunk.content + "|");
}

## ​ Disable streaming for specific chat models

If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for
models that do not support it.Set `streaming: false` when initializing the model.

const model = new ChatOpenAI({
model: "o1-preview",
// Set streaming: false to disable streaming for the chat model
streaming: false,
});

Not all chat model integrations support the `streaming` parameter. If your model doesn’t support it, use `disableStreaming: true` instead. This parameter is available on all chat models via the base class.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Durable execution\\
\\
Previous Interrupts\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/interrupts

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Interrupts

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Pause using interrupt
- Resuming interrupts
- Common patterns
- Approve or reject
- Review and edit state
- Interrupts in tools
- Validating human input
- Rules of interrupts
- Do not wrap interrupt calls in try/catch
- Do not reorder interrupt calls within a node
- Do not return complex values in interrupt calls
- Side effects called before interrupt must be idempotent
- Using with subgraphs called as functions
- Debugging with interrupts
- Using LangGraph Studio

Interrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its persistence layer and waits indefinitely until you resume execution.Interrupts work by calling the `interrupt()` function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you’re ready to continue, you resume execution by re-invoking the graph using `Command`, which then becomes the return value of the `interrupt()` call from inside the node.Unlike static breakpoints (which pause before or after specific nodes), interrupts are **dynamic**—they can be placed anywhere in your code and can be conditional based on your application logic.

- **Checkpointing keeps your place:** the checkpointer writes the exact graph state so you can resume later, even when in an error state.
- **`thread_id` is your pointer:** use `{ configurable: { thread_id: ... } }` as options to the `invoke` method to tell the checkpointer which state to load.
- **Interrupt payloads surface as `__interrupt__`:** the values you pass to `interrupt()` Pause using `interrupt`

The `interrupt` function pauses graph execution and returns a value to the caller. When you call `interrupt` within a node, LangGraph saves the current graph state and waits for you to resume execution with input.To use `interrupt`, you need:

1. A **checkpointer** to persist the graph state (use a durable checkpointer in production)
2. A **thread ID** in your config so the runtime knows which state to resume from
3. To call `interrupt()` where you want to pause (payload must be JSON-serializable)

Copy

import { interrupt } from "@langchain/langgraph";

async function approvalNode(state: State) {
// Pause and ask for approval
const approved = interrupt("Do you approve this action?");

// Command({ resume: ... }) provides the value returned into this variable
return { approved };
}

When you call `interrupt`, here’s what happens:

1. **Graph execution gets suspended** at the exact point where `interrupt` is called
2. **State is saved** using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)
3. **Value is returned** to the caller under `__interrupt__`; it can be any JSON-serializable value (string, object, array, etc.)
4. **Graph waits indefinitely** until you resume execution with a response
5. **Response is passed back** into the node when you resume, becoming the return value of the `interrupt()` call

## ​ Resuming interrupts

After an interrupt pauses execution, you resume the graph by invoking it again with a `Command` that contains the resume value. The resume value is passed ;
// [{ value: 'Do you approve this action?', ... }]

// Resume with the human's response
// Command({ resume }) returns that value from interrupt() in the node
await graph.invoke(new Command({ resume: true }), config);

**Key points about resuming:**

- You must use the **same thread ID** when resuming that was used when the interrupt occurred
- The value passed to `Command(resume=...)` becomes the return value of the `interrupt` call
- The node restarts from the beginning of the node where the `interrupt` was called when resumed, so any code before the `interrupt` runs again
- You can pass any JSON-serializable value as the resume value

## ​ Common patterns

The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:

- Approval workflows: Pause before executing critical actions (API calls, database changes, financial transactions)
- Review and edit: Let humans review and modify LLM outputs or tool calls before continuing
- Interrupting tool calls: Pause before executing tool calls to review and edit the tool call before execution
- Validating human input: Pause before proceeding to the next step to validate human input

### ​ Approve or reject

One of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.

import { interrupt, Command } from "@langchain/langgraph";

function approvalNode(state: State): Command {
// Pause execution; payload surfaces in result.__interrupt__
const isApproved = interrupt({
question: "Do you want to proceed?",
details: state.actionDetails
});

// Route based on the response
if (isApproved) {
return new Command({ goto: "proceed" }); // Runs after the resume payload is provided
} else {
return new Command({ goto: "cancel" });
}
}

When you resume the graph, pass `true` to approve or `false` to reject:

// To approve
await graph.invoke(new Command({ resume: true }), config);

// To reject
await graph.invoke(new Command({ resume: false }), config);

Full example

import {
Command,
MemorySaver,
START,
END,
StateGraph,
interrupt,
} from "@langchain/langgraph";
import * as z from "zod";

const State = z.object({
actionDetails: z.string(),
status: z.enum(["pending", "approved", "rejected"]).nullable(),
});

const graphBuilder = new StateGraph(State)

// Expose details so the caller can render them in a UI
const decision = interrupt({
question: "Approve this action?",
details: state.actionDetails,
});
return new Command({ goto: decision ? "proceed" : "cancel" });
}, { ends: ['proceed', 'cancel'] })

.addEdge(START, "approval")
.addEdge("proceed", END)
.addEdge("cancel", END);

// Use a more durable checkpointer in production
const checkpointer = new MemorySaver();
const graph = graphBuilder.compile({ checkpointer });

const config = { configurable: { thread_id: "approval-123" } };
const initial = await graph.invoke(
{ actionDetails: "Transfer $500", status: "pending" },
config,
);
console.log(initial.__interrupt__);
// [{ value: { question: ..., details: ... } }]

// Resume with the decision; true routes to proceed, false to cancel
const resumed = await graph.invoke(new Command({ resume: true }), config);

### ​ Review and edit state

Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.

function reviewNode(state: State) {
// Pause and show the current content for review (surfaces in result.__interrupt__)
const editedContent = interrupt({
instruction: "Review and edit this content",
content: state.generatedText
});

// Update the state with the edited version
return { generatedText: editedContent };
}

When resuming, provide the edited content:

await graph.invoke(
new Command({ resume: "The edited and improved text" }), // Value becomes the return from interrupt()
config
);

const State = z.object({
generatedText: z.string(),
});

const builder = new StateGraph(State)

// Ask a reviewer to edit the generated content
const updated = interrupt({
instruction: "Review and edit this content",
content: state.generatedText,
});
return { generatedText: updated };
})
.addEdge(START, "review")
.addEdge("review", END);

const checkpointer = new MemorySaver();
const graph = builder.compile({ checkpointer });

const config = { configurable: { thread_id: "review-42" } };
const initial = await graph.invoke({ generatedText: "Initial draft" }, config);
console.log(initial.__interrupt__);
// [{ value: { instruction: ..., content: ... } }]

// Resume with the edited text from the reviewer
const finalState = await graph.invoke(
new Command({ resume: "Improved draft after review" }),
config,
);

### ​ Interrupts in tools

You can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it’s called, and allows for human review and editing of the tool call before it is executed.First, define a tool that uses `interrupt`:

import { tool } from "@langchain/core/tools";
import { interrupt } from "@langchain/langgraph";
import * as z from "zod";

const sendEmailTool = tool(

// Pause before sending; payload surfaces in result.__interrupt__
const response = interrupt({
action: "send_email",
to,
subject,
body,
message: "Approve sending this email?",
});

if (response?.action === "approve") {
// Resume value can override inputs before executing
const finalTo = response.to ?? to;
const finalSubject = response.subject ?? subject;
const finalBody = response.body ?? body;
return `Email sent to ${finalTo} with subject '${finalSubject}'`;
}
return "Email cancelled by user";
},
{
name: "send_email",
description: "Send an email to a recipient",
schema: z.object({
to: z.string(),
subject: z.string(),
body: z.string(),
}),
},
);

This approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.

import { tool } from "@langchain/core/tools";
import { ChatAnthropic } from "@langchain/anthropic";
import {
Command,
MemorySaver,
START,
END,
StateGraph,
interrupt,
} from "@langchain/langgraph";
import * as z from "zod";

if (response?.action === "approve") {
const finalTo = response.to ?? to;
const finalSubject = response.subject ?? subject;
const finalBody = response.body ?? body;
console.log("[sendEmailTool]", finalTo, finalSubject, finalBody);
return `Email sent to ${finalTo}`;
}
return "Email cancelled by user";
},
{
name: "send_email",
description: "Send an email to a recipient",
schema: z.object({
to: z.string(),
subject: z.string(),
body: z.string(),
}),
},
);

const model = new ChatAnthropic({ model: "claude-sonnet-4-5-20250929" }).bindTools([sendEmailTool]);

const Message = z.object({
role: z.enum(["user", "assistant", "tool"]),
content: z.string(),
});

const State = z.object({
messages: z.array(Message),
});

// LLM may decide to call the tool; interrupt pauses before sending
const response = await model.invoke(state.messages);
return { messages: [...state.messages, response] };
})
.addEdge(START, "agent")
.addEdge("agent", END);

const checkpointer = new MemorySaver();
const graph = graphBuilder.compile({ checkpointer });

const config = { configurable: { thread_id: "email-workflow" } };
const initial = await graph.invoke(
{
messages: [\
{ role: "user", content: "Send an email to alice@example.com about the meeting" },\
],
},
config,
);

// Resume with approval and optionally edited arguments
const resumed = await graph.invoke(
new Command({
resume: { action: "approve", subject: "Updated subject" },
}),
config,
);

### ​ Validating human input

Sometimes you need to validate input from humans and ask again if it’s invalid. You can do this using multiple `interrupt` calls in a loop.

function getAgeNode(state: State) {
let prompt = "What is your age?";

while (true) {
const answer = interrupt(prompt); // payload surfaces in result.__interrupt__

// Validate the input

// Valid input - continue
return { age: answer };
} else {
// Invalid input - ask again with a more specific prompt
prompt = `'${answer}' is not a valid age. Please enter a positive number.`;
}
}
}

Each time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.

const State = z.object({
age: z.number().nullable(),
});

let prompt = "What is your age?";

return { age: answer };
}

prompt = `'${answer}' is not a valid age. Please enter a positive number.`;
}
})
.addEdge(START, "collectAge")
.addEdge("collectAge", END);

const config = { configurable: { thread_id: "form-1" } };
const first = await graph.invoke({ age: null }, config);

// Provide invalid data; the node re-prompts
const retry = await graph.invoke(new Command({ resume: "thirty" }), config);

// Provide valid data; loop exits and state updates
const final = await graph.invoke(new Command({ resume: 30 }), config);

## ​ Rules of interrupts

When you call `interrupt` within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.When execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning—it does not resume from the exact line where `interrupt` was called. This means any code that ran before the `interrupt` will execute again. Because of this, there’s a few important rules to follow when working with interrupts to ensure they behave as expected.

### ​ Do not wrap `interrupt` calls in try/catch

The way that `interrupt` pauses execution at the point of the call is by throwing a special exception. If you wrap the `interrupt` call in a try/catch block, you will catch this exception and the interrupt will not be passed calls from error-prone code
- ✅ Conditionally catch errors if needed

Separating logic

Conditionally handling errors

async function nodeA(state: State) {
// ✅ Good: interrupting first, then handling error conditions separately
const name = interrupt("What's your name?");
try {
await fetchData(); // This can fail
} catch (err) {
console.error(error);
}
return state;
}

- 🔴 Do not wrap `interrupt` calls in bare try/catch blocks

async function nodeA(state: State) {
// ❌ Bad: wrapping interrupt in bare try/catch will catch the interrupt exception
try {
const name = interrupt("What's your name?");
} catch (err) {
console.error(error);
}
return state;
}

### ​ Do not reorder `interrupt` calls within a node

It’s common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.When a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task’s resume list. Matching is **strictly index-based**, so the order of interrupt calls within the node is important.

- ✅ Keep `interrupt` calls consistent across node executions

async function nodeA(state: State) {
// ✅ Good: interrupt calls happen in the same order every time
const name = interrupt("What's your name?");
const age = interrupt("What's your age?");
const city = interrupt("What's your city?");

return {
name,
age,
city
};
}

- 🔴 Do not conditionally skip `interrupt` calls within a node
- 🔴 Do not loop `interrupt` calls using logic that isn’t deterministic across executions

Skipping interrupts

Looping interrupts

async function nodeA(state: State) {
// ❌ Bad: conditionally skipping interrupts changes the order
const name = interrupt("What's your name?");

// On first run, this might skip the interrupt
// On resume, it might not skip it - causing index mismatch
if (state.needsAge) {
const age = interrupt("What's your age?");
}

const city = interrupt("What's your city?");

return { name, city };
}

### ​ Do not return complex values in `interrupt` calls

Depending on which checkpointer is used, complex values may not be serializable (e.g. you can’t serialize a function). To make your graphs adaptable to any deployment, it’s best practice to only use values that can be reasonably serialized.

- ✅ Pass simple, JSON-serializable types to `interrupt`
- ✅ Pass dictionaries/objects with simple values

Simple values

Structured data

async function nodeA(state: State) {
// ✅ Good: passing simple types that are serializable
const name = interrupt("What's your name?");
const count = interrupt(42);
const approved = interrupt(true);

return { name, count, approved };
}

- 🔴 Do not pass functions, class instances, or other complex objects to `interrupt`

Functions

Class instances

function validateInput(value: string): boolean {

}

async function nodeA(state: State) {
// ❌ Bad: passing a function to interrupt
// The function cannot be serialized
const response = interrupt({
question: "What's your name?",
validator: validateInput // This will fail
});
return { name: response };
}

### ​ Side effects called before `interrupt` must be idempotent

Because interrupts work by re-running the nodes they were called from, side effects called before `interrupt` should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.As an example, you might have an API call to update a record inside of a node. If `interrupt` is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.

- ✅ Use idempotent operations before `interrupt`
- ✅ Place side effects after `interrupt` calls
- ✅ Separate side effects into separate nodes when possible

Idempotent operations

Side effects after interrupt

Separating into different nodes

async function nodeA(state: State) {
// ✅ Good: using upsert operation which is idempotent
// Running this multiple times will have the same result
await db.upsertUser({
userId: state.userId,
status: "pending_approval"
});

const approved = interrupt("Approve this change?");

return { approved };
}

- 🔴 Do not perform non-idempotent operations before `interrupt`
- 🔴 Do not create new records without checking if they exist

Creating records

Appending to arrays

async function nodeA(state: State) {
// ❌ Bad: creating a new record before interrupt
// This will create duplicate records on each resume
const auditId = await db.createAuditLog({
userId: state.userId,
action: "pending_approval",
timestamp: new Date()
});

return { approved, auditId };
}

## ​ Using with subgraphs called as functions

When invoking a subgraph within a node, the parent graph will resume execution from the **beginning of the node** where the subgraph was invoked and the `interrupt` was triggered. Similarly, the **subgraph** will also resume from the beginning of the node where `interrupt` was called.

async function nodeInParentGraph(state: State) {
someCode(); // <-- This will re-execute when resumed
// Invoke a subgraph as a function.
// The subgraph contains an `interrupt` call.
const subgraphResult = await subgraph.invoke(someInput);
// ...
}

async function nodeInSubgraph(state: State) {
someOtherCode(); // <-- This will also re-execute when resumed
const result = interrupt("What's your name?");
// ...
}

## ​ Debugging with interrupts

To debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying `interruptBefore` and `interruptAfter` when compiling the graph.

Static interrupts are **not** recommended for human-in-the-loop workflows. Use the `interrupt` function instead.

- At compile time

- At run time

const graph = builder.compile({
interruptBefore: ["node_a"],
interruptAfter: ["node_b", "node_c"],
checkpointer,
});

// Pass a thread ID to the graph
const config = {
configurable: {
thread_id: "some_thread"
}
};

// Run the graph until the breakpoint
await graph.invoke(inputs, config);# [!code highlight]

await graph.invoke(null, config); # [!code highlight]

1. The breakpoints are set during `compile` time.
2. `interruptBefore` specifies the nodes where execution should pause before the node is executed.
3. `interruptAfter` specifies the nodes where execution should pause after the node is executed.
4. A checkpointer is required to enable breakpoints.
5. The graph is run until the first breakpoint is hit.
6. The graph is resumed by passing in `null` for the input. This will run the graph until the next breakpoint is hit.

// Run the graph until the breakpoint
graph.invoke(inputs, {
interruptBefore: ["node_a"],
interruptAfter: ["node_b", "node_c"],
configurable: {
thread_id: "some_thread"
}
});

// Resume the graph
await graph.invoke(null, config);

1. `graph.invoke` is called with the `interruptBefore` and `interruptAfter` parameters. This is a run-time configuration and can be changed for every invocation.
2. `interruptBefore` specifies the nodes where execution should pause before the node is executed.
3. `interruptAfter` specifies the nodes where execution should pause after the node is executed.
4. The graph is run until the first breakpoint is hit.
5. The graph is resumed by passing in `null` for the input. This will run the graph until the next breakpoint is hit.

### ​ Using LangGraph Studio

You can use LangGraph Studio to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.!image

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Streaming\\
\\
Previous Use time-travel\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/use-time-travel

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Use time-travel

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- In a workflow
- Setup
- 1\. Run the graph
- 2\. Identify a checkpoint
- 3\. Update the state
- 4\. Resume execution from the checkpoint

When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:

1. **Understand reasoning**: Analyze the steps that led to a successful result.
2. **Debug mistakes**: Identify where and why errors occurred.
3. **Explore alternatives**: Test different paths to uncover better solutions.

LangGraph provides time travel functionality to support these use cases. Specifically, you can resume execution from a prior checkpoint — either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.To use time-travel in LangGraph:

1. Run the graph with initial inputs using `invoke` or `stream` methods.
2. Identify a checkpoint in an existing thread: Use the `getStateHistory` method to retrieve the execution history for a specific `thread_id` and locate the desired `checkpoint_id`.
Alternatively, set a breakpoint before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that breakpoint.
3. Update the graph state (optional): Use the `updateState` method to modify the graph’s state at the checkpoint and resume execution from alternative state.
4. Resume execution from the checkpoint: Use the `invoke` or `stream` methods with an input of `null` and a configuration containing the appropriate `thread_id` and `checkpoint_id`.

For a conceptual overview of time-travel, see Time travel.

## ​ In a workflow

This example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes.

### ​ Setup

First we need to install the packages required

Copy

npm install @langchain/langgraph @langchain/anthropic

Next, we need to set API keys for Anthropic (the LLM we will use)

process.env.ANTHROPIC_API_KEY = "YOUR_API_KEY";

Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.

import { v4 as uuidv4 } from "uuid";
import * as z from "zod";
import { StateGraph, START, END } from "@langchain/langgraph";
import { ChatAnthropic } from "@langchain/anthropic";
import { MemorySaver } from "@langchain/langgraph";

const State = z.object({
topic: z.string().optional(),
joke: z.string().optional(),
});

const model = new ChatAnthropic({
model: "claude-sonnet-4-5-20250929",
temperature: 0,
});

// Build workflow
const workflow = new StateGraph(State)
// Add nodes

// LLM call to generate a topic for the joke
const msg = await model.invoke("Give me a funny topic for a joke");
return { topic: msg.content };
})

// LLM call to write a joke based on the topic
const msg = await model.invoke(`Write a short joke about ${state.topic}`);
return { joke: msg.content };
})
// Add edges to connect nodes
.addEdge(START, "generateTopic")
.addEdge("generateTopic", "writeJoke")
.addEdge("writeJoke", END);

// Compile
const checkpointer = new MemorySaver();
const graph = workflow.compile({ checkpointer });

### ​ 1\. Run the graph

const config = {
configurable: {
thread_id: uuidv4(),
},
};

const state = await graph.invoke({}, config);

console.log(state.topic);
console.log();
console.log(state.joke);

**Output:**

How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all!

# The Secret Life of Socks in the Dryer

I finally discovered where all my missing socks go after the dryer. Turns out they're not missing at all—they've just eloped with someone else's socks from the laundromat to start new lives together.

My blue argyle is now living in Bermuda with a red polka dot, posting vacation photos on Sockstagram and sending me lint as alimony.

### ​ 2\. Identify a checkpoint

// The states are returned in reverse chronological order.
const states = [];
for await (const state of graph.getStateHistory(config)) {
states.push(state);
}

for (const state of states) {
console.log(state.next);
console.log(state.config.configurable?.checkpoint_id);
console.log();
}

[]
1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e

['writeJoke']
1f02ac4a-ce2a-6494-8001-cb2e2d651227

['generateTopic']
1f02ac4a-a4e0-630d-8000-b73c254ba748

['__start__']
1f02ac4a-a4dd-665e-bfff-e6c8c44315d9

// This is the state before last (states are listed in chronological order)
const selectedState = states[1];
console.log(selectedState.next);
console.log(selectedState.values);

['writeJoke']
{'topic': 'How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\'t know about? There\\'s a lot of comedic potential in the everyday mystery that unites us all!'}

### ​ 3\. Update the state

`updateState` will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.

const newConfig = await graph.updateState(selectedState.config, {
topic: "chickens",
});
console.log(newConfig);

{'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}

### ​ 4\. Resume execution from the checkpoint

await graph.invoke(null, newConfig);

{
'topic': 'chickens',
'joke': 'Why did the chicken join a band?\n\nBecause it had excellent drumsticks!'
}

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Interrupts\\
\\
Previous Memory\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/add-memory

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Memory

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Add short-term memory
- Use in production
- Use in subgraphs
- Add long-term memory
- Use in production
- Use semantic search
- Manage short-term memory
- Trim messages
- Delete messages
- Summarize messages
- Manage checkpoints
- View thread state
- View the history of the thread
- Delete all checkpoints for a thread
- Database management

AI applications need memory to share context across multiple interactions. In LangGraph, you can add two types of memory:

- Add short-term memory as a part of your agent’s state to enable multi-turn conversations.
- Add long-term memory to store user-specific or application-level data across sessions.

## ​ Add short-term memory

**Short-term** memory (thread-level persistence) enables agents to track multi-turn conversations. To add short-term memory:

Copy

import { MemorySaver, StateGraph } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const builder = new StateGraph(...);
const graph = builder.compile({ checkpointer });

await graph.invoke(
{ messages: [{ role: "user", content: "hi! i am Bob" }] },
{ configurable: { thread_id: "1" } }
);

### ​ Use in production

In production, use a checkpointer backed by a database:

import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres";

const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";
const checkpointer = PostgresSaver.fromConnString(DB_URI);

Example: using Postgres checkpointer

npm install @langchain/langgraph-checkpoint-postgres

You need to call `checkpointer.setup()` the first time you’re using Postgres checkpointer

import { ChatAnthropic } from "@langchain/anthropic";
import { StateGraph, MessagesZodMeta, START } from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages";
import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";
import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres";

const MessagesZodState = z.object({
messages: z

.register(registry, MessagesZodMeta),
});

const model = new ChatAnthropic({ model: "claude-haiku-4-5-20251001" });

const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";
const checkpointer = PostgresSaver.fromConnString(DB_URI);
// await checkpointer.setup();

const builder = new StateGraph(MessagesZodState)

const response = await model.invoke(state.messages);
return { messages: [response] };
})
.addEdge(START, "call_model");

const graph = builder.compile({ checkpointer });

const config = {
configurable: {
thread_id: "1"
}
};

for await (const chunk of await graph.stream(
{ messages: [{ role: "user", content: "hi! I'm bob" }] },
{ ...config, streamMode: "values" }
)) {
console.log(chunk.messages.at(-1)?.content);
}

for await (const chunk of await graph.stream(
{ messages: [{ role: "user", content: "what's my name?" }] },
{ ...config, streamMode: "values" }
)) {
console.log(chunk.messages.at(-1)?.content);
}

### ​ Use in subgraphs

If your graph contains subgraphs, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.

import { StateGraph, START, MemorySaver } from "@langchain/langgraph";
import * as z from "zod";

const State = z.object({ foo: z.string() });

const subgraphBuilder = new StateGraph(State)

return { foo: state.foo + "bar" };
})
.addEdge(START, "subgraph_node_1");
const subgraph = subgraphBuilder.compile();

const builder = new StateGraph(State)
.addNode("node_1", subgraph)
.addEdge(START, "node_1");

const checkpointer = new MemorySaver();
const graph = builder.compile({ checkpointer });

If you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories.

const subgraphBuilder = new StateGraph(...);
const subgraph = subgraphBuilder.compile({ checkpointer: true });

## ​ Add long-term memory

Use long-term memory to store user-specific or application-specific data across conversations.

import { InMemoryStore, StateGraph } from "@langchain/langgraph";

const store = new InMemoryStore();

const builder = new StateGraph(...);
const graph = builder.compile({ store });

### ​ Use in production

In production, use a store backed by a database:

import { PostgresStore } from "@langchain/langgraph-checkpoint-postgres/store";

const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";
const store = PostgresStore.fromConnString(DB_URI);

Example: using Postgres store

You need to call `store.setup()` the first time you’re using Postgres store

import { ChatAnthropic } from "@langchain/anthropic";
import { StateGraph, MessagesZodMeta, START, LangGraphRunnableConfig } from "@langchain/langgraph";
import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres";
import { PostgresStore } from "@langchain/langgraph-checkpoint-postgres/store";
import { BaseMessage } from "@langchain/core/messages";
import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";
import { v4 as uuidv4 } from "uuid";

const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";

const store = PostgresStore.fromConnString(DB_URI);
const checkpointer = PostgresSaver.fromConnString(DB_URI);
// await store.setup();
// await checkpointer.setup();

const callModel = async (

config: LangGraphRunnableConfig,

const userId = config.configurable?.userId;
const namespace = ["memories", userId];
const memories = await config.store?.search(namespace, { query: state.messages.at(-1)?.content });

const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`;

// Store new memories if the user asks the model to remember
const lastMessage = state.messages.at(-1);
if (lastMessage?.content?.toLowerCase().includes("remember")) {
const memory = "User name is Bob";
await config.store?.put(namespace, uuidv4(), { data: memory });
}

const response = await model.invoke([\
{ role: "system", content: systemMsg },\
...state.messages\
]);
return { messages: [response] };
};

const builder = new StateGraph(MessagesZodState)
.addNode("call_model", callModel)
.addEdge(START, "call_model");

const graph = builder.compile({
checkpointer,
store,
});

const config = {
configurable: {
thread_id: "1",
userId: "1",
}
};

for await (const chunk of await graph.stream(
{ messages: [{ role: "user", content: "Hi! Remember: my name is Bob" }] },
{ ...config, streamMode: "values" }
)) {
console.log(chunk.messages.at(-1)?.content);
}

const config2 = {
configurable: {
thread_id: "2",
userId: "1",
}
};

for await (const chunk of await graph.stream(
{ messages: [{ role: "user", content: "what is my name?" }] },
{ ...config2, streamMode: "values" }
)) {
console.log(chunk.messages.at(-1)?.content);
}

### ​ Use semantic search

Enable semantic search in your graph’s memory store to let graph agents search for items in the store by semantic similarity.

import { OpenAIEmbeddings } from "@langchain/openai";
import { InMemoryStore } from "@langchain/langgraph";

// Create store with semantic search enabled
const embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small" });
const store = new InMemoryStore({
index: {
embeddings,
dims: 1536,
},
});

await store.put(["user_123", "memories"], "1", { text: "I love pizza" });
await store.put(["user_123", "memories"], "2", { text: "I am a plumber" });

const items = await store.search(["user_123", "memories"], {
query: "I'm hungry",
limit: 1,
});

Long-term memory with semantic search

import { OpenAIEmbeddings, ChatOpenAI } from "@langchain/openai";
import { StateGraph, START, MessagesZodMeta, InMemoryStore } from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages";
import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";

const model = new ChatOpenAI({ model: "gpt-4o-mini" });

// Create store with semantic search enabled
const embeddings = new OpenAIEmbeddings({ model: "text-embedding-3-small" });
const store = new InMemoryStore({
index: {
embeddings,
dims: 1536,
}
});

// Search based on user's last message
const items = await config.store.search(
["user_123", "memories"],
{ query: state.messages.at(-1)?.content, limit: 2 }
);

const memoriesText = memories ? `## Memories of user\n${memories}` : "";

const response = await model.invoke([\
{ role: "system", content: `You are a helpful assistant.\n${memoriesText}` },\
...state.messages,\
]);

return { messages: [response] };
};

const builder = new StateGraph(MessagesZodState)
.addNode("chat", chat)
.addEdge(START, "chat");
const graph = builder.compile({ store });

for await (const [message, metadata] of await graph.stream(
{ messages: [{ role: "user", content: "I'm hungry" }] },
{ streamMode: "messages" }
)) {
if (message.content) {
console.log(message.content);
}
}

## ​ Manage short-term memory

With short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:

- Trim messages: Remove first or last N messages (before calling LLM)
- Delete messages from LangGraph state permanently
- Summarize messages: Summarize earlier messages in the history and replace them with a summary
- Manage checkpoints to store and retrieve message history
- Custom strategies (e.g., message filtering, etc.)

This allows the agent to keep track of the conversation without exceeding the LLM’s context window.

### ​ Trim messages

Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `maxTokens`) to use for handling the boundary.To trim message history, use the `trimMessages` function:

import { trimMessages } from "@langchain/core/messages";

const messages = trimMessages(state.messages, {
strategy: "last",
maxTokens: 128,
startOn: "human",
endOn: ["human", "tool"],
});
const response = await model.invoke(messages);
return { messages: [response] };
};

const builder = new StateGraph(MessagesZodState)
.addNode("call_model", callModel);
// ...

Full example: trim messages

import { trimMessages, BaseMessage } from "@langchain/core/messages";
import { ChatAnthropic } from "@langchain/anthropic";
import { StateGraph, START, MessagesZodMeta, MemorySaver } from "@langchain/langgraph";
import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";

const model = new ChatAnthropic({ model: "claude-3-5-sonnet-20241022" });

const messages = trimMessages(state.messages, {
strategy: "last",
maxTokens: 128,
startOn: "human",
endOn: ["human", "tool"],
tokenCounter: model,
});
const response = await model.invoke(messages);
return { messages: [response] };
};

const checkpointer = new MemorySaver();
const builder = new StateGraph(MessagesZodState)
.addNode("call_model", callModel)
.addEdge(START, "call_model");
const graph = builder.compile({ checkpointer });

const config = { configurable: { thread_id: "1" } };
await graph.invoke({ messages: [{ role: "user", content: "hi, my name is bob" }] }, config);
await graph.invoke({ messages: [{ role: "user", content: "write a short poem about cats" }] }, config);
await graph.invoke({ messages: [{ role: "user", content: "now do the same but for dogs" }] }, config);
const finalResponse = await graph.invoke({ messages: [{ role: "user", content: "what's my name?" }] }, config);

console.log(finalResponse.messages.at(-1)?.content);

Your name is Bob, as you mentioned when you first introduced yourself.

### ​ Delete messages

You can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.To delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with `messagesStateReducer` reducer, like `MessagesZodState`.To remove specific messages:

import { RemoveMessage } from "@langchain/core/messages";

const messages = state.messages;

// remove the earliest two messages
return {
messages: messages
.slice(0, 2)

};
}
};

When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:

- Some providers expect message history to start with a `user` message
- Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.

Full example: delete messages

import { RemoveMessage, BaseMessage } from "@langchain/core/messages";
import { ChatAnthropic } from "@langchain/anthropic";
import { StateGraph, START, MemorySaver, MessagesZodMeta } from "@langchain/langgraph";
import * as z from "zod";
import { registry } from "@langchain/langgraph/zod";

// remove the earliest two messages

}
return {};
};

const response = await model.invoke(state.messages);
return { messages: [response] };
};

const builder = new StateGraph(MessagesZodState)
.addNode("call_model", callModel)
.addNode("delete_messages", deleteMessages)
.addEdge(START, "call_model")
.addEdge("call_model", "delete_messages");

const checkpointer = new MemorySaver();
const app = builder.compile({ checkpointer });

const config = { configurable: { thread_id: "1" } };

for await (const event of await app.stream(
{ messages: [{ role: "user", content: "hi! I'm bob" }] },
{ ...config, streamMode: "values" }
)) {

}

for await (const event of await app.stream(
{ messages: [{ role: "user", content: "what's my name?" }] },
{ ...config, streamMode: "values" }
)) {

[['human', "hi! I'm bob"]]
[['human', "hi! I'm bob"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?']]
[['human', "hi! I'm bob"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'], ['human', "what's my name?"]]
[['human', "hi! I'm bob"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'], ['human', "what's my name?"], ['ai', 'Your name is Bob.']]
[['human', "what's my name?"], ['ai', 'Your name is Bob.']]

### ​ Summarize messages

The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.!SummaryPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can include a `summary` key in the state alongside the `messages` key:

import { BaseMessage } from "@langchain/core/messages";
import { MessagesZodMeta } from "@langchain/langgraph";
import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";

const State = z.object({
messages: z

.register(registry, MessagesZodMeta),
summary: z.string().optional(),
});

Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This `summarizeConversation` node can be called after some number of messages have accumulated in the `messages` state key.

import { RemoveMessage, HumanMessage } from "@langchain/core/messages";

// First, we get any existing summary
const summary = state.summary || "";

// Create our summarization prompt
let summaryMessage: string;
if (summary) {
// A summary already exists
summaryMessage =
`This is a summary of the conversation to date: ${summary}\n\n` +
"Extend the summary by taking into account the new messages above:";
} else {
summaryMessage = "Create a summary of the conversation above:";
}

// Add prompt to our history
const messages = [\
...state.messages,\
new HumanMessage({ content: summaryMessage })\
];
const response = await model.invoke(messages);

// Delete all but the 2 most recent messages
const deleteMessages = state.messages
.slice(0, -2)

return {
summary: response.content,
messages: deleteMessages
};
};

Full example: summarize messages

import { ChatAnthropic } from "@langchain/anthropic";
import {
SystemMessage,
HumanMessage,
RemoveMessage,
type BaseMessage
} from "@langchain/core/messages";
import {
MessagesZodMeta,
StateGraph,
START,
END,
MemorySaver,
} from "@langchain/langgraph";
import { BaseMessage } from "@langchain/core/messages";
import { registry } from "@langchain/langgraph/zod";
import * as z from "zod";
import { v4 as uuidv4 } from "uuid";

const memory = new MemorySaver();

// We will add a `summary` attribute (in addition to `messages` key,
// which MessagesZodState already has)
const GraphState = z.object({
messages: z

.register(registry, MessagesZodMeta),
summary: z.string().default(""),
});

// We will use this model for both the conversation and the summarization
const model = new ChatAnthropic({ model: "claude-haiku-4-5-20251001" });

// Define the logic to call the model

// If a summary exists, we add this in as a system message
const { summary } = state;
let { messages } = state;
if (summary) {
const systemMessage = new SystemMessage({
id: uuidv4(),
content: `Summary of conversation earlier: ${summary}`,
});
messages = [systemMessage, ...messages];
}
const response = await model.invoke(messages);
// We return an object, because this will get added to the existing state
return { messages: [response] };
};

// We now define the logic for determining whether to end or summarize the conversation

const messages = state.messages;
// If there are more than six messages, then we summarize the conversation

return "summarize_conversation";
}
// Otherwise we can just end
return END;
};

// First, we summarize the conversation
const { summary, messages } = state;
let summaryMessage: string;
if (summary) {
// If a summary already exists, we use a different system prompt
// to summarize it than if one didn't
summaryMessage =
`This is summary of the conversation to date: ${summary}\n\n` +
"Extend the summary by taking into account the new messages above:";
} else {
summaryMessage = "Create a summary of the conversation above:";
}

const allMessages = [\
...messages,\
new HumanMessage({ id: uuidv4(), content: summaryMessage }),\
];

const response = await model.invoke(allMessages);

// We now need to delete messages that we no longer want to show up
// I will delete all but the last two messages, but you can change this
const deleteMessages = messages
.slice(0, -2)

if (typeof response.content !== "string") {
throw new Error("Expected a string response from the model");
}

return { summary: response.content, messages: deleteMessages };
};

// Define a new graph
const workflow = new StateGraph(GraphState)
// Define the conversation node and the summarize node
.addNode("conversation", callModel)
.addNode("summarize_conversation", summarizeConversation)
// Set the entrypoint as conversation
.addEdge(START, "conversation")
// We now add a conditional edge
.addConditionalEdges(
// First, we define the start node. We use `conversation`.
// This means these are the edges taken after the `conversation` node is called.
"conversation",
// Next, we pass in the function that will determine which node is called next.
shouldContinue,
)
// We now add a normal edge from `summarize_conversation` to END.
// This means that after `summarize_conversation` is called, we end.
.addEdge("summarize_conversation", END);

// Finally, we compile it!
const app = workflow.compile({ checkpointer: memory });

### ​ Manage checkpoints

You can view and delete the information stored by the checkpointer.

#### ​ View thread state

const config = {
configurable: {
thread_id: "1",
// optionally provide an ID for a specific checkpoint,
// otherwise the latest checkpoint is shown
// checkpoint_id: "1f029ca3-1f5b-6704-8004-820c16b69a5a"
},
};
await graph.getState(config);

{
values: { messages: [HumanMessage(...), AIMessage(...), HumanMessage(...), AIMessage(...)] },
next: [],
config: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1f5b-6704-8004-820c16b69a5a' } },
metadata: {
source: 'loop',
writes: { call_model: { messages: AIMessage(...) } },
step: 4,
parents: {},
thread_id: '1'
},
createdAt: '2025-05-05T16:01:24.680462+00:00',
parentConfig: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1790-6b0a-8003-baf965b6a38f' } },
tasks: [],
interrupts: []
}

#### ​ View the history of the thread

const config = {
configurable: {
thread_id: "1",
},
};

const history = [];
for await (const state of graph.getStateHistory(config)) {
history.push(state);
}

#### ​ Delete all checkpoints for a thread

const threadId = "1";
await checkpointer.deleteThread(threadId);

## ​ Database management

If you are using any database-backed persistence implementation (such as Postgres or Redis) to store short and/or long-term memory, you will need to run migrations to set up the required schema before you can use it with your database.By convention, most database-specific libraries define a `setup()` method on the checkpointer or store instance that runs the required migrations. However, you should check with your specific implementation of `BaseCheckpointSaver` or `BaseStore` to confirm the exact method name and usage.We recommend running migrations as a dedicated deployment step, or you can ensure they’re run as part of server startup.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Use time-travel\\
\\
Previous Subgraphs\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/use-subgraphs

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Subgraphs

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Setup
- Invoke a graph from a node
- Add a graph as a node
- Add persistence
- View subgraph state
- Stream subgraph outputs

This guide explains the mechanics of using subgraphs. A subgraph is a graph that is used as a node in another graph.Subgraphs are useful for:

- Building multi-agent systems
- Re-using a set of nodes in multiple graphs
- Distributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph

When adding subgraphs, you need to define how the parent graph and the subgraph communicate:

- Invoke a graph from a node — subgraphs are called from inside a node in the parent graph
- Add a graph as a node — a subgraph is added directly as a node in the parent and **shares state keys** with the parent

## ​ Setup

Copy

npm install @langchain/langgraph

**Set up LangSmith for LangGraph development**
Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started here.

## ​ Invoke a graph from a node

A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have **completely different schemas** from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a multi-agent system.If that’s the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results ;
}

1. Transform the state to the subgraph state
2. Transform response

[[], { node1: { foo: 'hi! foo' } }]
[['node2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7'], { subgraphNode1: { baz: 'baz' } }]
[['node2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7'], { subgraphNode2: { bar: 'hi! foobaz' } }]
[[], { node2: { foo: 'hi! foobaz' } }]

Full example: different state schemas (two levels of subgraphs)

import { StateGraph, START, END } from "@langchain/langgraph";
import * as z from "zod";

// Grandchild graph
const GrandChildState = z.object({
myGrandchildKey: z.string(),
});

const grandchild = new StateGraph(GrandChildState)

// NOTE: child or parent keys will not be accessible here
return { myGrandchildKey: state.myGrandchildKey + ", how are you" };
})
.addEdge(START, "grandchild1")
.addEdge("grandchild1", END);

const grandchildGraph = grandchild.compile();

// Child graph
const ChildState = z.object({
myChildKey: z.string(),
});

const child = new StateGraph(ChildState)

// NOTE: parent or grandchild keys won't be accessible here
const grandchildGraphInput = { myGrandchildKey: state.myChildKey };
const grandchildGraphOutput = await grandchildGraph.invoke(grandchildGraphInput);
return { myChildKey: grandchildGraphOutput.myGrandchildKey + " today?" };
})
.addEdge(START, "child1")
.addEdge("child1", END);

const childGraph = child.compile();

// Parent graph
const ParentState = z.object({
myKey: z.string(),
});

const parent = new StateGraph(ParentState)

// NOTE: child or grandchild keys won't be accessible here
return { myKey: "hi " + state.myKey };
})

const childGraphInput = { myChildKey: state.myKey };
const childGraphOutput = await childGraph.invoke(childGraphInput);
return { myKey: childGraphOutput.myChildKey };
})

return { myKey: state.myKey + " bye!" };
})
.addEdge(START, "parent1")
.addEdge("parent1", "child")
.addEdge("child", "parent2")
.addEdge("parent2", END);

const parentGraph = parent.compile();

for await (const chunk of await parentGraph.stream(
{ myKey: "Bob" },
{ subgraphs: true }
)) {
console.log(chunk);
}

1. We’re transforming the state from the child state channels (`myChildKey`) to the grandchild state channels (`myGrandchildKey`)
2. We’re transforming the state from the grandchild state channels (`myGrandchildKey`)

[[], { parent1: { myKey: 'hi Bob' } }]
[['child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child1:781bb3b1-3971-84ce-810b-acf819a03f9c'], { grandchild1: { myGrandchildKey: 'hi Bob, how are you' } }]
[['child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b'], { child1: { myChildKey: 'hi Bob, how are you today?' } }]
[[], { child: { myKey: 'hi Bob, how are you today?' } }]
[[], { parent2: { myKey: 'hi Bob, how are you today? bye!' } }]

## ​ Add a graph as a node

When the parent graph and subgraph can communicate over a shared state key (channel) in the schema, you can add a graph as a node in another graph. For example, in multi-agent systems, the agents often communicate over a shared messages key.!SQL agent graphIf your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:

1. Define the subgraph workflow (`subgraphBuilder` in the example below) and compile it
2. Pass compiled subgraph to the `.addNode` method when defining the parent graph workflow

import { StateGraph, START } from "@langchain/langgraph";
import * as z from "zod";

const State = z.object({
foo: z.string(),
});

// Subgraph
const subgraphBuilder = new StateGraph(State)

return { foo: "hi! " + state.foo };
})
.addEdge(START, "subgraphNode1");

const subgraph = subgraphBuilder.compile();

// Parent graph
const builder = new StateGraph(State)
.addNode("node1", subgraph)
.addEdge(START, "node1");

const graph = builder.compile();

Full example: shared state schemas

// Define subgraph
const SubgraphState = z.object({
foo: z.string(),
bar: z.string(),
});

const subgraphBuilder = new StateGraph(SubgraphState)

return { bar: "bar" };
})

// note that this node is using a state key ('bar') that is only available in the subgraph
// and is sending update on the shared state key ('foo')
return { foo: state.foo + state.bar };
})
.addEdge(START, "subgraphNode1")
.addEdge("subgraphNode1", "subgraphNode2");

// Define parent graph
const ParentState = z.object({
foo: z.string(),
});

const builder = new StateGraph(ParentState)

return { foo: "hi! " + state.foo };
})
.addNode("node2", subgraph)
.addEdge(START, "node1")
.addEdge("node1", "node2");

for await (const chunk of await graph.stream({ foo: "foo" })) {
console.log(chunk);
}

1. This key is shared with the parent graph state
2. This key is private to the `SubgraphState` and is not visible to the parent graph

{ node1: { foo: 'hi! foo' } }
{ node2: { foo: 'hi! foobar' } }

## ​ Add persistence

You only need to **provide the checkpointer when compiling the parent graph**. LangGraph will automatically propagate the checkpointer to the child subgraphs.

import { StateGraph, START, MemorySaver } from "@langchain/langgraph";
import * as z from "zod";

return { foo: state.foo + "bar" };
})
.addEdge(START, "subgraphNode1");

const checkpointer = new MemorySaver();
const graph = builder.compile({ checkpointer });

If you want the subgraph to **have its own memory**, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories:

const subgraphBuilder = new StateGraph(...)
const subgraph = subgraphBuilder.compile({ checkpointer: true });

## ​ View subgraph state

When you enable persistence, you can inspect the graph state (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.You can inspect the graph state via `graph.getState(config)`. To view the subgraph state, you can use `graph.getState(config, { subgraphs: true })`.

**Available only when interrupted**
Subgraph state can only be viewed **when the subgraph is interrupted**. Once you resume the graph, you won’t be able to access the subgraph state.

View interrupted subgraph state

import { StateGraph, START, MemorySaver, interrupt, Command } from "@langchain/langgraph";
import * as z from "zod";

const value = interrupt("Provide value:");
return { foo: state.foo + value };
})
.addEdge(START, "subgraphNode1");

const config = { configurable: { thread_id: "1" } };

await graph.invoke({ foo: "" }, config);
const parentState = await graph.getState(config);
const subgraphState = (await graph.getState(config, { subgraphs: true })).tasks[0].state;

// resume the subgraph
await graph.invoke(new Command({ resume: "bar" }), config);

## ​ Stream subgraph outputs

To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

for await (const chunk of await graph.stream(
{ foo: "foo" },
{
subgraphs: true,
streamMode: "updates",
}
)) {
console.log(chunk);
}

1. Set `subgraphs: true` to stream outputs from subgraphs.

Stream from subgraphs

for await (const chunk of await graph.stream(
{ foo: "foo" },
{
streamMode: "updates",
subgraphs: true,
}
)) {
console.log(chunk);
}

[[], { node1: { foo: 'hi! foo' } }]
[['node2:e58e5673-a661-ebb0-70d4-e298a7fc28b7'], { subgraphNode1: { bar: 'bar' } }]
[['node2:e58e5673-a661-ebb0-70d4-e298a7fc28b7'], { subgraphNode2: { foo: 'hi! foobar' } }]
[[], { node2: { foo: 'hi! foobar' } }]

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Memory\\
\\
Previous Application structure\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/application-structure

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

Application structure

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Key Concepts
- File structure
- Configuration file
- Examples
- Dependencies
- Graphs
- Environment variables

A LangGraph application consists of one or more graphs, a configuration file (`langgraph.json`), a file that specifies dependencies, and an optional `.env` file that specifies environment variables.This guide shows a typical structure of an application and shows you how to provide the required configuration to deploy an application with LangSmith Deployment.

LangSmith Deployment is a managed hosting platform for deploying and scaling LangGraph agents. It handles the infrastructure, scaling, and operational concerns so you can deploy your stateful, long-running agents directly from your repository. Learn more in the Deployment documentation.

## ​ Key Concepts

To deploy using the LangSmith, the following information should be provided:

1. A LangGraph configuration file (`langgraph.json`) that specifies the dependencies, graphs, and environment variables to use for the application.
2. The graphs that implement the logic of the application.
3. A file that specifies dependencies required to run the application.
4. Environment variables that are required for the application to run.

## ​ File structure

Below are examples of directory structures for applications:

Copy

my-app/
├── src # all project code lies within here
│ ├── utils # optional utilities for your graph
│ │ ├── tools.ts # tools for your graph
│ │ ├── nodes.ts # node functions for your graph
│ │ └── state.ts # state definition of your graph
│ └── agent.ts # code for constructing your graph
├── package.json # package dependencies
├── .env # environment variables
└── langgraph.json # configuration file for LangGraph

The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.

## ​ Configuration file

The `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.See the LangGraph configuration file reference for details on all supported keys in the JSON file.

The LangGraph CLI defaults to using the configuration file `langgraph.json` in the current directory.

### ​ Examples

- The dependencies will be loaded from a dependency file in the local directory (e.g., `package.json`).
- A single graph will be loaded from the file `./your_package/your_file.js` with the function `agent`.
- The environment variable `OPENAI_API_KEY` is set inline.

{
"dependencies": ["."],
"graphs": {
"my_agent": "./your_package/your_file.js:agent"
},
"env": {
"OPENAI_API_KEY": "secret-key"
}
}

## ​ Dependencies

A LangGraph application may depend on other TypeScript/JavaScript libraries.You will generally need to specify the following information for dependencies to be set up correctly:

1. A file in the directory that specifies the dependencies (e.g. `package.json`).
2. A `dependencies` key in the LangGraph configuration file that specifies the dependencies required to run the LangGraph application.
3. Any additional binaries or system libraries can be specified using `dockerfile_lines` key in the LangGraph configuration file.

## ​ Graphs

Use the `graphs` key in the LangGraph configuration file to specify which graphs will be available in the deployed LangGraph application.You can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.

## ​ Environment variables

If you’re working with a deployed LangGraph application locally, you can configure environment variables in the `env` key of the LangGraph configuration file.For a production deployment, you will typically want to configure the environment variables in the deployment environment.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Subgraphs\\
\\
Previous Test\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/test

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

Test

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Prerequisites
- Getting started
- Testing individual nodes and edges
- Partial execution

After you’ve prototyped your LangGraph agent, a natural next step is to add tests. This guide covers some useful patterns you can use when writing unit tests.Note that this guide is LangGraph-specific and covers scenarios around graphs with custom structures - if you are just getting started, check out this section that uses LangChain’s built-in `createAgent` instead.

## ​ Prerequisites

First, make sure you have `vitest` installed:

Copy

$ npm install -D vitest

## ​ Getting started

Because many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance.The below example shows how this works with a simple, linear graph that progresses through `node1` and `node2`. Each node updates the single state key `my_key`:

import { test, expect } from 'vitest';
import {
StateGraph,
START,
END,
MemorySaver,
} from '@langchain/langgraph';
import { z } from "zod/v4";

const State = z.object({
my_key: z.string(),
});

return new StateGraph(State)

.addEdge(START, 'node1')
.addEdge('node1', 'node2')
.addEdge('node2', END);
};

const uncompiledGraph = createGraph();
const checkpointer = new MemorySaver();
const compiledGraph = uncompiledGraph.compile({ checkpointer });
const result = await compiledGraph.invoke(
{ my_key: 'initial_value' },
{ configurable: { thread_id: '1' } }
);
expect(result.my_key).toBe('hello from node2');
});

## ​ Testing individual nodes and edges

Compiled LangGraph agents expose references to each individual node as `graph.nodes`. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:

const uncompiledGraph = createGraph();
// Will be ignored in this example
const checkpointer = new MemorySaver();
const compiledGraph = uncompiledGraph.compile({ checkpointer });
// Only invoke node 1
const result = await compiledGraph.nodes['node1'].invoke(
{ my_key: 'initial_value' },
);
expect(result.my_key).toBe('hello from node1');
});

## ​ Partial execution

For agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to restructure these sections as subgraphs, which you can invoke in isolation as normal.However, if you do not wish to make changes to your agent graph’s overall structure, you can use LangGraph’s persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:

1. Compile your agent with a checkpointer (the in-memory checkpointer `MemorySaver` will suffice for testing).
2. Call your agent’s `update_state` method with an `asNode` parameter set to the name of the node _before_ the one you want to start your test.
3. Invoke your agent with the same `thread_id` you used to update the state and an `interruptBefore` parameter set to the name of the node you want to stop at.

Here’s an example that executes only the second and third nodes in a linear graph:

.addEdge(START, 'node1')
.addEdge('node1', 'node2')
.addEdge('node2', 'node3')
.addEdge('node3', 'node4')
.addEdge('node4', END);
};

const uncompiledGraph = createGraph();
const checkpointer = new MemorySaver();
const compiledGraph = uncompiledGraph.compile({ checkpointer });
await compiledGraph.updateState(
{ configurable: { thread_id: '1' } },
// The state passed into node 2 - simulating the state at
// the end of node 1
{ my_key: 'initial_value' },
// Update saved state as if it came from node 1
// Execution will resume at node 2
'node1',
);
const result = await compiledGraph.invoke(
// Resume execution by passing None
null,
{
configurable: { thread_id: '1' },
// Stop after node 3 so that node 4 doesn't run
interruptAfter: ['node3']
},
);
expect(result.my_key).toBe('hello from node3');
});

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Application structure\\
\\
Previous LangSmith Studio\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/studio

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

LangSmith Studio

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Prerequisites
- Set up local Agent server
- 1\. Install the LangGraph CLI
- 2\. Prepare your agent
- 3\. Environment variables
- 4\. Create a LangGraph config file
- 5\. Install dependencies
- 6\. View your agent in Studio
- Video guide

When building agents with LangChain locally, it’s helpful to visualize what’s happening inside your agent, interact with it in real-time, and debug issues as they occur. **LangSmith Studio** is a free visual interface for developing and testing your LangChain agents from your local machine.Studio connects to your locally running agent to show you each step your agent takes: the prompts sent to the model, tool calls and their results, and the final output. You can test different inputs, inspect intermediate states, and iterate on your agent’s behavior without additional code or deployment.This pages describes how to set up Studio with your local LangChain agent.

## ​ Prerequisites

Before you begin, ensure you have the following:

- **A LangSmith account**: Sign up (for free) or log in at smith.langchain.com.
- **A LangSmith API key**: Follow the Create an API key guide.
- If you don’t want data traced to LangSmith, set `LANGSMITH_TRACING=false` in your application’s `.env` file. With tracing disabled, no data leaves your local server.

## ​ Set up local Agent server

### ​ 1\. Install the LangGraph CLI

The LangGraph CLI provides a local development server (also called Agent Server) that connects your agent to Studio.

Copy

npx @langchain/langgraph-cli

### ​ 2\. Prepare your agent

If you already have a LangChain agent, you can use it directly. This example uses a simple email agent:

agent.ts

import { createAgent } from "@langchain/langgraph";

function sendEmail(to: string, subject: string, body: string): string {
// Send an email
const email = {
to: to,
subject: subject,
body: body
};
// ... email sending logic

return `Email sent to ${to}`;
}

const agent = createAgent({
model: "gpt-4o",
tools: [sendEmail],
systemPrompt: "You are an email assistant. Always use the sendEmail tool.",
});

export { agent };

### ​ 3\. Environment variables

Studio requires a LangSmith API key to connect your local agent. Create a `.env` file in the root of your project and add your API key from LangSmith.

Ensure your `.env` file is not committed to version control, such as Git.

.env

LANGSMITH_API_KEY=lsv2...

### ​ 4\. Create a LangGraph config file

The LangGraph CLI uses a configuration file to locate your agent and manage dependencies. Create a `langgraph.json` file in your app’s directory:

langgraph.json

{
"dependencies": ["."],
"graphs": {
"agent": "./src/agent.ts:agent"
},
"env": ".env"
}

The @\[`create_agent`\] function automatically returns a compiled LangGraph graph, which is what the `graphs` key expects in the configuration file.

For detailed explanations of each key in the JSON object of the configuration file, refer to the LangGraph configuration file reference.

At this point, the project structure will look like this:

my-app/
├── src
│ └── agent.ts
├── .env
├── package.json
└── langgraph.json

### ​ 5\. Install dependencies

yarn install

### ​ 6\. View your agent in Studio

Start the development server to connect your agent to Studio:

npx @langchain/langgraph-cli dev

Safari blocks `localhost` connections to Studio. To work around this, run the above command with `--tunnel` to access Studio via a secure tunnel.

Once the server is running, your agent is accessible both via API at `http://127.0.0.1:2024` and through the Studio UI at `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`:

With Studio connected to your local agent, you can iterate quickly on your agent’s behavior. Run a test input, inspect the full execution trace including prompts, tool arguments, return values, and token/latency metrics. When something goes wrong, Studio captures exceptions with the surrounding state to help you understand what happened.The development server supports hot-reloading—make changes to prompts or tool signatures in your code, and Studio reflects them immediately. Re-run conversation threads from any step to test your changes without starting over. This workflow scales from simple single-tool agents to complex multi-node graphs.For more information on how to run Studio, refer to the following guides in the LangSmith docs:

- Run application
- Manage assistants
- Manage threads
- Iterate on prompts
- Debug LangSmith traces
- Add node to dataset

## ​ Video guide

LangSmith Studio v2: The Ultimate Agent Development Environment - YouTube

Photo image of LangChain

LangChain

165K subscribers

LangSmith Studio v2: The Ultimate Agent Development Environment

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Watch on

0:00

0:00 / 8:09

•Live

•

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Test\\
\\
Previous Agent Chat UI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/ui

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

Agent Chat UI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Quick start
- Local development
- Connect to your agent

Agent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI works seamlessly with agents created using `create_agent` and provides interactive experiences for your agents with minimal setup, whether you’re running locally or in a deployed context (such as LangSmith).Agent Chat UI is open source and can be adapted to your application needs.

Introducing Agent Chat UI - YouTube

Photo image of LangChain

LangChain

165K subscribers

Introducing Agent Chat UI

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Why am I seeing this?

Watch on

0:00

0:00 / 5:59

•Live

•

You can use generative UI in the Agent Chat UI. For more information, see Implement generative user interfaces with LangGraph.

### ​ Quick start

The fastest way to get started is using the hosted version:

1. **Visit Agent Chat UI**
2. **Connect your agent** by entering your deployment URL or local server address
3. **Start chatting** \- the UI will automatically detect and render tool calls and interrupts

### ​ Local development

For customization or local development, you can run Agent Chat UI locally:

Use npx

Clone repository

Copy

# Create a new Agent Chat UI project
npx create-agent-chat-app --project-name my-chat-ui
cd my-chat-ui

# Install dependencies and start
pnpm install
pnpm dev

### ​ Connect to your agent

Agent Chat UI can connect to both local and deployed agents.After starting Agent Chat UI, you’ll need to configure it to connect to your agent:

1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)
2. **Deployment URL**: Your Agent server’s endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent’s URL)
3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you’re using a local Agent server)

Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.

Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see Hiding Messages in the Chat.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Studio\\
\\
Previous LangSmith Deployment\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/deploy

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

LangSmith Deployment

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Prerequisites
- Deploy your agent
- 1\. Create a repository on GitHub
- 2\. Deploy to LangSmith
- 3\. Test your application in Studio
- 4\. Get the API URL for your deployment
- 5\. Test the API

This guide shows you how to deploy your agent to **LangSmith Cloud**, a fully managed hosting platform designed for agent workloads. With Cloud deployment, you can deploy directly from your GitHub repository—LangSmith handles the infrastructure, scaling, and operational concerns.Traditional hosting platforms are built for stateless, short-lived web applications. LangSmith Cloud is **purpose-built for stateful, long-running agents** that require persistent state and background execution.

LangSmith offers multiple deployment options beyond Cloud, including deploying with a control plane (hybrid/self-hosted) or as standalone servers. For more information, refer to the Deployment overview.

## ​ Prerequisites

Before you begin, ensure you have the following:

- A GitHub account
- A LangSmith account (free to sign up)

## ​ Deploy your agent

### ​ 1\. Create a repository on GitHub

Your application’s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide. Then, push your code to the repository.

### ​ 2\. Deploy to LangSmith

1

Navigate to LangSmith Deployment

Log in to LangSmith. In the left sidebar, select **Deployments**.

2

Create new deployment

Click the **\+ New Deployment** button. A pane will open where you can fill in the required fields.

3

Link repository

If you are a first time user or adding a private repository that has not been previously connected, click the **Add new account** button and follow the instructions to connect your GitHub account.

4

Deploy repository

Select your application’s repository. Click **Submit** to deploy. This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.

### ​ 3\. Test your application in Studio

Once your application is deployed:

1. Select the deployment you just created to view more details.
2. Click the **Studio** button in the top right corner. Studio will open to display your graph.

### ​ 4\. Get the API URL for your deployment

1. In the **Deployment details** view in LangGraph, click the **API URL** to copy it to your clipboard.
2. Click the `URL` to copy it to the clipboard.

### ​ 5\. Test the API

You can now test the API:

- JavaScript

- Rest API

1. Install LangGraph JS:

Copy

npm install @langchain/langgraph-sdk

2. Send a message to the agent:

const { Client } = await import("@langchain/langgraph-sdk");

const client = new Client({ apiUrl: "your-deployment-url", apiKey: "your-langsmith-api-key" });

const streamResponse = client.runs.stream(
null, // Threadless run
"agent", // Name of agent. Defined in langgraph.json.
{
input: {
"messages": [\
{ "role": "user", "content": "What is LangGraph?"}\
]
},
streamMode: "messages",
}
);

for await (const chunk of streamResponse) {
console.log(`Receiving new event of type: ${chunk.event}...`);
console.log(JSON.stringify(chunk.data));
console.log("\n\n");
}

curl -s --request POST \

--header 'Content-Type: application/json' \

--data "{
\"assistant_id\": \"agent\", `# Name of agent. Defined in langgraph.json.`
\"input\": {
\"messages\": [\
{\
\"role\": \"human\",\
\"content\": \"What is LangGraph?\"\
}\
]
},
\"stream_mode\": \"updates\"
}"

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Agent Chat UI\\
\\
Previous LangSmith Observability\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/observability

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

LangSmith Observability

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Prerequisites
- Enable tracing
- Trace selectively
- Log to a project
- Add metadata to traces
- Use anonymizers to prevent logging of sensitive data in traces

Traces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use LangSmith to visualize these execution steps. To use it, enable tracing for your application. This enables you to do the following:

- Debug a locally running application.
- Evaluate the application performance.
- Monitor the application.

## ​ Prerequisites

Before you begin, ensure you have the following:

- **A LangSmith account**: Sign up (for free) or log in at smith.langchain.com.
- **A LangSmith API key**: Follow the Create an API key guide.

## ​ Enable tracing

To enable tracing for your application, set the following environment variables:

Copy

export LANGSMITH_TRACING=true

By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see Log to a project.For more information, see Trace with LangGraph.

## ​ Trace selectively

You may opt to trace specific invocations or parts of your application using LangSmith’s `tracing_context` context manager:

import { LangChainTracer } from "@langchain/core/tracers/tracer_langchain";

// This WILL be traced
const tracer = new LangChainTracer();
await agent.invoke(
{
messages: [{role: "user", content: "Send a test email to alice@example.com"}]
},
{ callbacks: [tracer] }
);

// This will NOT be traced (if LANGSMITH_TRACING is not set)
await agent.invoke(
{
messages: [{role: "user", content: "Send another email"}]
}
);

## ​ Log to a project

Statically

You can set a custom project name for your entire application by setting the `LANGSMITH_PROJECT` environment variable:

export LANGSMITH_PROJECT=my-agent-project

Dynamically

You can set the project name programmatically for specific operations:

const tracer = new LangChainTracer({ projectName: "email-agent-test" });
await agent.invoke(
{
messages: [{role: "user", content: "Send a test email to alice@example.com"}]
},
{ callbacks: [tracer] }
);

## ​ Add metadata to traces

You can annotate your traces with custom metadata and tags:

const tracer = new LangChainTracer({ projectName: "email-agent-test" });
await agent.invoke(
{
messages: [{role: "user", content: "Send a test email to alice@example.com"}]
},
config: {
tags: ["production", "email-assistant", "v1.0"],
metadata: {
userId: "user123",
sessionId: "session456",
environment: "production"
}
},
);

This custom metadata and tags will be attached to the trace in LangSmith.

To learn more about how to use traces to debug, evaluate, and monitor your agents, see the LangSmith documentation.

## ​ Use anonymizers to prevent logging of sensitive data in traces

You may want to mask sensitive data to prevent it from being logged to LangSmith. You can create anonymizers and apply them to
your graph using configuration. This example will redact anything matching the Social Security Number format XXX-XX-XXXX from traces sent to LangSmith.

import { StateGraph } from "@langchain/langgraph";
import { LangChainTracer } from "@langchain/core/tracers/tracer_langchain";
import { StateAnnotation } from "./state.js";
import { createAnonymizer } from "langsmith/anonymizer"
import { Client } from "langsmith"

const anonymizer = createAnonymizer([\
# Matches SSNs\

])

const langsmithClient = new Client({ anonymizer })
const tracer = new LangChainTracer({
client: langsmithClient,
});

export const graph = new StateGraph(StateAnnotation)
.compile()
.withConfig({
callbacks: [tracer],
});

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Deployment\\
\\
Previous Choosing between Graph and Functional APIs\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/pregel

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangGraph APIs

LangGraph runtime

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Overview
- Actors
- Channels
- Examples
- High-level API

## ​ Overview

In LangGraph, Pregel combines **actors** and **channels** into a single application. **Actors** read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the **Pregel Algorithm**/ **Bulk Synchronous Parallel** model.Each step consists of three phases:

- **Plan**: Determine which **actors** to execute in this step. For example, in the first step, select the **actors** that subscribe to the special **input** channels; in subsequent steps, select the **actors** that subscribe to channels updated in the previous step.
- **Execution**: Execute all selected **actors** in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.
- **Update**: Update the channels with the values written by the **actors** in this step.

Repeat until no **actors** are selected for execution, or a maximum number of steps is reached.

## ​ Actors

An **actor** is a `PregelNode`. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an **actor** in the Pregel algorithm. `PregelNodes` implement LangChain’s Runnable interface.

## ​ Channels

Channels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function – which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:

- `LastValue`: The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.
- `Topic`: A configurable PubSub Topic, useful for sending multiple values between **actors**, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.
- `BinaryOperatorAggregate`: stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,`total = BinaryOperatorAggregate(int, operator.add)`

## ​ Examples

While most users will interact with Pregel through the StateGraph API or the entrypoint decorator, it is possible to interact with Pregel directly.Below are a few different examples to give you a sense of the Pregel API.

- Single node

- Multiple nodes

- Topic

- BinaryOperatorAggregate

- Cycle

Copy

import { EphemeralValue } from "@langchain/langgraph/channels";
import { Pregel, NodeBuilder } from "@langchain/langgraph/pregel";

const node1 = new NodeBuilder()
.subscribeOnly("a")

.writeTo("b");

const app = new Pregel({
nodes: { node1 },
channels: {

},
inputChannels: ["a"],
outputChannels: ["b"],
});

await app.invoke({ a: "foo" });

{ b: 'foofoo' }

import { LastValue, EphemeralValue } from "@langchain/langgraph/channels";
import { Pregel, NodeBuilder } from "@langchain/langgraph/pregel";

const node2 = new NodeBuilder()
.subscribeOnly("b")

.writeTo("c");

const app = new Pregel({
nodes: { node1, node2 },
channels: {

},
inputChannels: ["a"],
outputChannels: ["b", "c"],
});

{ b: 'foofoo', c: 'foofoofoofoo' }

import { EphemeralValue, Topic } from "@langchain/langgraph/channels";
import { Pregel, NodeBuilder } from "@langchain/langgraph/pregel";

.writeTo("b", "c");

const node2 = new NodeBuilder()
.subscribeTo("b")

},
inputChannels: ["a"],
outputChannels: ["c"],
});

{ c: ['foofoo', 'foofoofoofoo'] }

This example demonstrates how to use the `BinaryOperatorAggregate` channel to implement a reducer.

import { EphemeralValue, BinaryOperatorAggregate } from "@langchain/langgraph/channels";
import { Pregel, NodeBuilder } from "@langchain/langgraph/pregel";

if (current) {
return current + " | " + update;
} else {
return update;
}
};

This example demonstrates how to introduce a cycle in the graph, by having
a chain write to a channel it subscribes to. Execution will continue
until a `null` value is written to the channel.

import { EphemeralValue } from "@langchain/langgraph/channels";
import { Pregel, NodeBuilder, ChannelWriteEntry } from "@langchain/langgraph/pregel";

const exampleNode = new NodeBuilder()
.subscribeOnly("value")

.writeTo(new ChannelWriteEntry("value", { skipNone: true }));

const app = new Pregel({
nodes: { exampleNode },
channels: {

},
inputChannels: ["value"],
outputChannels: ["value"],
});

await app.invoke({ value: "a" });

{ value: 'aaaaaaaaaaaaaaaa' }

## ​ High-level API

LangGraph provides two high-level APIs for creating a Pregel application: the StateGraph (Graph API) and the Functional API.

- StateGraph (Graph API)

The StateGraph (Graph API) is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.

import { START, StateGraph } from "@langchain/langgraph";

interface Essay {
topic: string;
content?: string;
score?: number;
}

return {
content: `Essay about ${essay.topic}`,
};
};

return {
score: 10
};
};

channels: {
topic: null,
content: null,
score: null,
}
})
.addNode("writeEssay", writeEssay)
.addNode("scoreEssay", scoreEssay)
.addEdge(START, "writeEssay")
.addEdge("writeEssay", "scoreEssay");

// Compile the graph.
// This will return a Pregel instance.
const graph = builder.compile();

The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them.

console.log(graph.nodes);

You will see something like this:

{
__start__: PregelNode { ... },
writeEssay: PregelNode { ... },
scoreEssay: PregelNode { ... }
}

console.log(graph.channels);

You should see something like this

{
topic: LastValue { ... },
content: LastValue { ... },
score: LastValue { ... },
__start__: EphemeralValue { ... },
writeEssay: EphemeralValue { ... },
scoreEssay: EphemeralValue { ... },
'branch:__start__:__self__:writeEssay': EphemeralValue { ... },
'branch:__start__:__self__:scoreEssay': EphemeralValue { ... },
'branch:writeEssay:__self__:writeEssay': EphemeralValue { ... },
'branch:writeEssay:__self__:scoreEssay': EphemeralValue { ... },
'branch:scoreEssay:__self__:writeEssay': EphemeralValue { ... },
'branch:scoreEssay:__self__:scoreEssay': EphemeralValue { ... },
'start:writeEssay': EphemeralValue { ... }
}

In the Functional API, you can use an `entrypoint` to create a Pregel application. The `entrypoint` decorator allows you to define a function that takes input and returns output.

import { MemorySaver } from "@langchain/langgraph";
import { entrypoint } from "@langchain/langgraph/func";

const checkpointer = new MemorySaver();

const writeEssay = entrypoint(
{ checkpointer, name: "writeEssay" },

return {
content: `Essay about ${essay.topic}`,
};
}
);

console.log("Nodes: ");
console.log(writeEssay.nodes);
console.log("Channels: ");
console.log(writeEssay.channels);

Nodes:
{ writeEssay: PregelNode { ... } }
Channels:
{
__start__: EphemeralValue { ... },
__end__: LastValue { ... },
__previous__: LastValue { ... }
}

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Use the functional API\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/models

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Models

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Basic usage
- Initialize a model
- Key methods
- Parameters
- Invocation
- Invoke
- Stream
- Batch
- Tool calling
- Structured output
- Supported models
- Advanced topics
- Model profiles
- Multimodal
- Reasoning
- Local models
- Prompt caching
- Server-side tool use
- Base URL or proxy
- Log probabilities
- Token usage
- Invocation config

LLMs are powerful AI tools that can interpret and generate text like humans. They’re versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.In addition to text generation, many models support:

- Tool calling \- calling external tools (like databases queries or API calls) and use results in their responses.
- Structured output \- where the model’s response is constrained to follow a defined format.
- Multimodality \- process and return data other than text, such as images, audio, and video.
- Reasoning \- models perform multi-step reasoning to arrive at a conclusion.

Models are the reasoning engine of agents. They drive the agent’s decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.The quality and capabilities of the model you choose directly impact your agent’s baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.LangChain’s standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.

For provider-specific integration information and capabilities, see the provider’s chat model page.

## ​ Basic usage

Models can be utilized in two ways:

1. **With agents** \- Models can be dynamically specified when creating an agent.
2. **Standalone** \- Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.

The same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.

### ​ Initialize a model

The easiest way to get started with a standalone model in LangChain is to use `initChatModel` to initialize one from a chat model provider of your choice (examples below):

- OpenAI

- Anthropic

- Azure

- Google Gemini

- Bedrock Converse

👉 Read the OpenAI chat model integration docs

npm

pnpm

yarn

bun

Copy

npm install @langchain/openai

initChatModel

Model Class

import { initChatModel } from "langchain";

process.env.OPENAI_API_KEY = "your-api-key";

const model = await initChatModel("gpt-4.1");

👉 Read the Anthropic chat model integration docs

npm install @langchain/anthropic

process.env.ANTHROPIC_API_KEY = "your-api-key";

const model = await initChatModel("claude-sonnet-4-5-20250929");

👉 Read the Azure chat model integration docs

npm install @langchain/azure

process.env.AZURE_OPENAI_API_KEY = "your-api-key";
process.env.AZURE_OPENAI_ENDPOINT = "your-endpoint";
process.env.OPENAI_API_VERSION = "your-api-version";

const model = await initChatModel("azure_openai:gpt-4.1");

👉 Read the Google GenAI chat model integration docs

npm install @langchain/google-genai

process.env.GOOGLE_API_KEY = "your-api-key";

const model = await initChatModel("google-genai:gemini-2.5-flash-lite");

👉 Read the AWS Bedrock chat model integration docs

npm install @langchain/aws

// Follow the steps here to configure your credentials:
//

const model = await initChatModel("bedrock:gpt-4.1");

const response = await model.invoke("Why do parrots talk?");

See `initChatModel` for more detail, including information on how to pass model parameters.

### ​ Key methods

**Invoke** \\
\\
The model takes messages as input and outputs messages after generating a complete response. **Stream** \\
\\
Invoke the model, but stream the output as it is generated in real-time. **Batch** \\
\\
Send multiple requests to a model in a batch for more efficient processing.

In addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the integrations page for details.

## ​ Parameters

A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:

​

model

string

required

The name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the ’:’ format, for example, ‘openai:o1’.

apiKey

The key required for authenticating with the model’s provider. This is usually issued when you sign up for access to the model. Often accessed by setting an environment variable.

temperature

number

Controls the randomness of the model’s output. A higher number makes responses more creative; lower ones make them more deterministic.

maxTokens

Limits the total number of tokens in the response, effectively controlling how long the output can be.

timeout

The maximum time (in seconds) to wait for a response from the model before canceling the request.

maxRetries

The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.

Using `initChatModel`, pass these parameters as inline parameters:

Initialize using model parameters

const model = await initChatModel(
"claude-sonnet-4-5-20250929",
{ temperature: 0.7, timeout: 30, max_tokens: 1000 }
)

Each chat model integration may have additional params used to control provider-specific functionality.For example, `ChatOpenAI` has `use_responses_api` to dictate whether to use the OpenAI Responses or Completions API.To find all the parameters supported by a given chat model, head to the chat model integrations page.

* * *

## ​ Invocation

A chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.

### ​ Invoke

The most straightforward way to call a model is to use `invoke()` with a single message or a list of messages.

Single message

const response = await model.invoke("Why do parrots have colorful feathers?");
console.log(response);

A list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.See the messages guide for more detail on roles, types, and content.

Object format

const conversation = [\
{ role: "system", content: "You are a helpful assistant that translates English to French." },\
{ role: "user", content: "Translate: I love programming." },\
{ role: "assistant", content: "J'adore la programmation." },\
{ role: "user", content: "Translate: I love building applications." },\
];

const response = await model.invoke(conversation);
console.log(response); // AIMessage("J'adore créer des applications.")

Message objects

import { HumanMessage, AIMessage, SystemMessage } from "langchain";

const conversation = [\
new SystemMessage("You are a helpful assistant that translates English to French."),\
new HumanMessage("Translate: I love programming."),\
new AIMessage("J'adore la programmation."),\
new HumanMessage("Translate: I love building applications."),\
];

If the return type of your invocation is a string, ensure that you are using a chat model as opposed to a LLM. Legacy, text-completion LLMs return strings directly. LangChain chat models are prefixed with “Chat”, e.g., `ChatOpenAI`(/oss/integrations/chat/openai).

### ​ Stream

Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.Calling `stream()` returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:

Basic text streaming

Stream tool calls, reasoning, and other content

const stream = await model.stream("Why do parrots have colorful feathers?");
for await (const chunk of stream) {
console.log(chunk.text)
}

As opposed to `invoke()`, which returns a single `AIMessage` after the model has finished generating its full response, `stream()` returns multiple `AIMessageChunk` objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:

Construct AIMessage

let full: AIMessageChunk | null = null;
for await (const chunk of stream) {
full = full ? full.concat(chunk) : chunk;
console.log(full.text);
}

// The
// The sky
// The sky is
// The sky is typically
// The sky is typically blue
// ...

console.log(full.contentBlocks);
// [{"type": "text", "text": "The sky is typically blue..."}]

The resulting message can be treated the same as a message that was generated with `invoke()` – for example, it can be aggregated into a message history and passed

Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn’t streaming-capable would be one that needs to store the entire output in memory before it can be processed.

Advanced streaming topics

Streaming events

LangChain chat models can also stream semantic events using
\[`streamEvents()`\]\[BaseChatModel.streamEvents\].This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.

const stream = await model.streamEvents("Hello");
for await (const event of stream) {
if (event.event === "on_chat_model_start") {
console.log(`Input: ${event.data.input}`);
}
if (event.event === "on_chat_model_stream") {
console.log(`Token: ${event.data.chunk.text}`);
}
if (event.event === "on_chat_model_end") {
console.log(`Full message: ${event.data.output.text}`);
}
}

Input: Hello
Token: Hi
Token: there
Token: !
Token: How
Token: can
Token: I
...
Full message: Hi there! How can I help today?

See the `streamEvents()` reference for event types and other details.

"Auto-streaming" chat models

LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you’re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.In LangGraph agents, for example, you can call `model.invoke()` within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.

#### ​ How it works

When you `invoke()` a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking `on_llm_new_token` events in LangChain’s callback system.Callback events allow LangGraph `stream()` and `streamEvents()` to surface the chat model’s output in real-time.

### ​ Batch

Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:

Batch

const responses = await model.batch([\
"Why do parrots have colorful feathers?",\
"How do airplanes fly?",\
"What is quantum computing?",\
"Why do parrots have colorful feathers?",\
"How do airplanes fly?",\
"What is quantum computing?",\
]);
for (const response of responses) {
console.log(response);
}

When processing a large number of inputs using `batch()`, you may want to control the maximum number of parallel calls. This can be done by setting the `maxConcurrency` attribute in the `RunnableConfig` dictionary.

Batch with max concurrency

model.batch(
listOfInputs,
{
maxConcurrency: 5, // Limit to 5 parallel calls
}
)

See the `RunnableConfig` reference for a full list of supported attributes.

For more details on batching, see the reference.

## ​ Tool calling

Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:

1. A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema)
2. A function or coroutine to execute.

You may hear the term “function calling”. We use this interchangeably with “tool calling”.

Here’s the basic tool calling flow between a user and a model:

ToolsModelUserToolsModelUserpar\[Parallel Tool Calls\]par\[Tool Execution\]"What's the weather in SF and NYC?"Analyze request & decide tools neededgetWeather("San Francisco")getWeather("New York")SF weather dataNYC weather dataProcess results & generate response"SF: 72°F sunny, NYC: 68°F cloudy"

To make tools that you have defined available for use by a model, you must bind them using `bindTools`. In subsequent invocations, the model can choose to call any of the bound tools as needed.Some model providers offer built-in tools that can be enabled via model or invocation parameters (e.g. `ChatOpenAI`, `ChatAnthropic`). Check the respective provider reference for details.

See the tools guide for details and other options for creating tools.

Binding user tools

import { tool } from "langchain";
import * as z from "zod";
import { ChatOpenAI } from "@langchain/openai";

const getWeather = tool(

{
name: "get_weather",
description: "Get the weather at a location.",
schema: z.object({
location: z.string().describe("The location to get the weather for"),
}),
},
);

const model = new ChatOpenAI({ model: "gpt-4o" });
const modelWithTools = model.bindTools([getWeather]);

const response = await modelWithTools.invoke("What's the weather like in Boston?");
const toolCalls = response.tool_calls || [];
for (const tool_call of toolCalls) {
// View tool calls made by the model
console.log(`Tool: ${tool_call.name}`);
console.log(`Args: ${tool_call.args}`);
}

When binding user-defined tools, the model’s response includes a **request** to execute a tool. When using a model separately from an agent, it is up to you to execute the requested tool and return the result , the agent loop will handle the tool execution loop for you.Below, we show some common ways you can use tool calling.

Tool execution loop

When a model returns tool calls, you need to execute the tools and pass the results abstractions that handle this orchestration for you.Here’s a simple example of how to do this:

// Bind (potentially multiple) tools to the model
const modelWithTools = model.bindTools([get_weather])

// Step 1: Model generates tool calls
const messages = [{"role": "user", "content": "What's the weather in Boston?"}]
const ai_msg = await modelWithTools.invoke(messages)
messages.push(ai_msg)

// Step 2: Execute tools and collect results
for (const tool_call of ai_msg.tool_calls) {
// Execute the tool with the generated arguments
const tool_result = await get_weather.invoke(tool_call)
messages.push(tool_result)
}

// Step 3: Pass results returned by the tool includes a `tool_call_id` that matches the original tool call, helping the model correlate results with requests.

Forcing tool calls

By default, the model has the freedom to choose which bound tool to use based on the user’s input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or **any** tool from a given list:

Force use of any tool

Force use of specific tools

const modelWithTools = model.bindTools([tool_1], { toolChoice: "any" })

Parallel tool calls

Many models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously.

const modelWithTools = model.bind_tools([get_weather])

const response = await modelWithTools.invoke(
"What's the weather in Boston and Tokyo?"
)

// The model may generate multiple tool calls
console.log(response.tool_calls)
// [\
// { name: 'get_weather', args: { location: 'Boston' }, id: 'call_1' },\
// { name: 'get_time', args: { location: 'Tokyo' }, id: 'call_2' }\
// ]

// Execute all tools (can be done in parallel with async)
const results = []
for (const tool_call of response.tool_calls || []) {
if (tool_call.name === 'get_weather') {
const result = await get_weather.invoke(tool_call)
results.push(result)
}
}

The model intelligently determines when parallel execution is appropriate based on the independence of the requested operations.

Most models supporting tool calling enable parallel tool calls by default. Some (including OpenAI and Anthropic) allow you to disable this feature. To do this, set `parallel_tool_calls=False`:

model.bind_tools([get_weather], parallel_tool_calls=False)

Streaming tool calls

When streaming responses, tool calls are progressively built through `ToolCallChunk`. This allows you to see tool calls as they’re being generated rather than waiting for the complete response.

const stream = await modelWithTools.stream(
"What's the weather in Boston and Tokyo?"
)
for await (const chunk of stream) {
// Tool call chunks arrive progressively
if (chunk.tool_call_chunks) {
for (const tool_chunk of chunk.tool_call_chunks) {
console.log(`Tool: ${tool_chunk.get('name', '')}`)
console.log(`Args: ${tool_chunk.get('args', '')}`)
}
}
}

// Output:
// Tool: get_weather
// Args:
// Tool:
// Args: {"loc
// Tool:
// Args: ation": "BOS"}
// Tool: get_time
// Args:
// Tool:
// Args: {"timezone": "Tokyo"}

You can accumulate chunks to build complete tool calls:

Accumulate tool calls

let full: AIMessageChunk | null = null
const stream = await modelWithTools.stream("What's the weather in Boston?")
for await (const chunk of stream) {
full = full ? full.concat(chunk) : chunk
console.log(full.contentBlocks)
}

## ​ Structured output

Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.

- Zod

- JSON Schema

A zod schema is the preferred method of defining an output schema. Note that when a zod schema is provided, the model output will also be validated against the schema using zod’s parse methods.

import * as z from "zod";

const Movie = z.object({
title: z.string().describe("The title of the movie"),
year: z.number().describe("The year the movie was released"),
director: z.string().describe("The director of the movie"),
rating: z.number().describe("The movie's rating out of 10"),
});

const modelWithStructure = model.withStructuredOutput(Movie);

const response = await modelWithStructure.invoke("Provide details about the movie Inception");
console.log(response);
// {
// title: "Inception",
// year: 2010,
// director: "Christopher Nolan",
// rating: 8.8,
// }

For maximum control or interoperability, you can provide a raw JSON Schema.

const jsonSchema = {
"title": "Movie",
"description": "A movie with details",
"type": "object",
"properties": {
"title": {
"type": "string",
"description": "The title of the movie",
},
"year": {
"type": "integer",
"description": "The year the movie was released",
},
"director": {
"type": "string",
"description": "The director of the movie",
},
"rating": {
"type": "number",
"description": "The movie's rating out of 10",
},
},
"required": ["title", "year", "director", "rating"],
}

const modelWithStructure = model.withStructuredOutput(
jsonSchema,
{ method: "jsonSchema" },
)

const response = await modelWithStructure.invoke("Provide details about the movie Inception")
console.log(response) // {'title': 'Inception', 'year': 2010, ...}

**Key considerations for structured output:**

- **Method parameter**: Some providers support different methods (`'jsonSchema'`, `'functionCalling'`, `'jsonMode'`)
- **Include raw**: Use `includeRaw: true` to get both the parsed output and the raw `AIMessage`
- **Validation**: Zod models provide automatic validation, while JSON Schema requires manual validation

Example: Message output alongside parsed structure

It can be useful to return the raw `AIMessage` object alongside the parsed representation to access response metadata such as token counts. To do this, set `include_raw=True` when calling `with_structured_output`:

const Movie = z.object({
title: z.string().describe("The title of the movie"),
year: z.number().describe("The year the movie was released"),
director: z.string().describe("The director of the movie"),
rating: z.number().describe("The movie's rating out of 10"),
title: z.string().describe("The title of the movie"),
year: z.number().describe("The year the movie was released"),
director: z.string().describe("The director of the movie"),
rating: z.number().describe("The movie's rating out of 10"),
});

const modelWithStructure = model.withStructuredOutput(Movie, { includeRaw: true });

const response = await modelWithStructure.invoke("Provide details about the movie Inception");
console.log(response);
// {
// raw: AIMessage { ... },
// parsed: { title: "Inception", ... }
// }

Example: Nested structures

Schemas can be nested:

const Actor = z.object({
name: str
role: z.string(),
});

const MovieDetails = z.object({
title: z.string(),
year: z.number(),
cast: z.array(Actor),
genres: z.array(z.string()),
budget: z.number().nullable().describe("Budget in millions USD"),
});

const modelWithStructure = model.withStructuredOutput(MovieDetails);

## ​ Supported models

LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the integrations page.

## ​ Advanced topics

### ​ Model profiles

This is a beta feature. The format of model profiles is subject to change.

LangChain chat models can expose a dictionary of supported features and capabilities through a `.profile` property:

model.profile;
// {
// maxInputTokens: 400000,
// imageInputs: true,
// reasoningOutput: true,
// toolCalling: true,
// ...
// }

Refer to the full set of fields in the API reference.Much of the model profile data is powered by the models.dev project, an open source initiative that provides model capability data. This data is augmented with additional fields for purposes of use with LangChain. These augmentations are kept aligned with the upstream project as it evolves.Model profile data allow applications to work around model capabilities dynamically. For example:

1. Summarization middleware can trigger summarization based on a model’s context window size.
2. Structured output strategies in `createAgent` can be inferred automatically (e.g., by checking support for native structured output features).
3. Model inputs can be gated based on supported modalities and maximum input tokens.

Modify profile data

Model profile data can be changed if it is missing, stale, or incorrect.**Option 1 (quick fix)**You can instantiate a chat model with any valid profile:

const customProfile = {
maxInputTokens: 100_000,
toolCalling: true,
structuredOutput: true,
// ...
};
const model = initChatModel("...", { profile: customProfile });

**Option 2 (fix data upstream)**The primary source for the data is the models.dev project. These data are merged with additional fields and overrides in LangChain integration packages and are shipped with those packages.Model profile data can be updated through the following process:

1. (If needed) update the source data at models.dev through a pull request to its repository on GitHub.

### ​ Multimodal

Certain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing content blocks.

All LangChain chat models with underlying multimodal capabilities support:

1. Data in the cross-provider standard format (see our messages guide)
2. OpenAI chat completions format
3. Any format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format)

See the multimodal section of the messages guide for details.Some models can return multimodal data as part of their response. If invoked to do so, the resulting `AIMessage` will have content blocks with multimodal types.

Multimodal output

const response = await model.invoke("Create a picture of a cat");
console.log(response.contentBlocks);
// [\
// { type: "text", text: "Here's a picture of a cat" },\
// { type: "image", data: "...", mimeType: "image/jpeg" },\
// ]

See the integrations page for details on specific providers.

### ​ Reasoning

Many models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.**If supported by the underlying model,** you can surface this reasoning process to better understand how the model arrived at its final answer.

Stream reasoning output

Complete reasoning output

const stream = model.stream("Why do parrots have colorful feathers?");
for await (const chunk of stream) {

}

Depending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical “tiers” of reasoning (e.g., `'low'` or `'high'`) or integer token budgets.For details, see the integrations page or reference for your respective chat model.

### ​ Local models

LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.Ollama is one of the easiest ways to run models locally. See the full list of local integrations on the integrations page.

### ​ Prompt caching

Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be **implicit** or **explicit**:

- **Implicit prompt caching:** providers will automatically pass on cost savings if a request hits a cache. Examples: OpenAI and Gemini.
- **Explicit caching:**providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples:

- `ChatOpenAI` (via `prompt_cache_key`)
- Anthropic’s `AnthropicPromptCachingMiddleware`
- Gemini.
- AWS Bedrock

Prompt caching is often only engaged above a minimum input token threshold. See provider pages for details.

Cache usage will be reflected in the usage metadata of the model response.

### ​ Server-side tool use

Some providers support server-side tool-calling loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.If a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the content blocks of the response will return the server-side tool calls and results in a provider-agnostic format:

const model = await initChatModel("gpt-4.1-mini");
const modelWithTools = model.bindTools([{ type: "web_search" }])

const message = await modelWithTools.invoke("What was a positive news story from today?");
console.log(message.contentBlocks);

This represents a single conversational turn; there are no associated ToolMessage objects that need to be passed in as in client-side tool-calling.See the integration page for your given provider for available tools and usage details.

### ​ Base URL or proxy

For many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.

Base URL

Many model providers offer OpenAI-compatible APIs (e.g., Together AI, vLLM). You can use `initChatModel` with these providers by specifying the appropriate `base_url` parameter:

model = initChatModel(
"MODEL_NAME",
{
modelProvider: "openai",
baseUrl: "BASE_URL",
apiKey: "YOUR_API_KEY",
}
)

When using direct chat model class instantiation, the parameter name may vary by provider. Check the respective reference for details.

### ​ Log probabilities

Certain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the `logprobs` parameter when initializing the model:

const model = new ChatOpenAI({
model: "gpt-4o",
logprobs: true,
});

const responseMessage = await model.invoke("Why do parrots talk?");

responseMessage.response_metadata.logprobs.content.slice(0, 5);

### ​ Token usage

A number of model providers return token usage information as part of the invocation response. When available, this information will be included on the `AIMessage` objects produced by the corresponding model. For more details, see the messages guide.

Some provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the streaming usage metadata section of the integration guide for details.

### ​ Invocation config

When invoking a model, you can pass additional configuration through the `config` parameter using a `RunnableConfig` object. This provides run-time control over execution behavior, callbacks, and metadata tracking.Common configuration options include:

Invocation with config

const response = await model.invoke(
"Tell me a joke",
{
runName: "joke_generation", // Custom name for this run
tags: ["humor", "demo"], // Tags for categorization
metadata: {"user_id": "123"}, // Custom metadata
callbacks: [my_callback_handler], // Callback handlers
}
)

These configuration values are particularly useful when:

- Debugging with LangSmith tracing
- Implementing custom logging or monitoring
- Controlling resource usage in production
- Tracking invocations across complex pipelines

Key configuration attributes

runName

Identifies this specific invocation in logs and traces. Not inherited by sub-calls.

tags

string\[\]

Labels inherited by all sub-calls for filtering and organization in debugging tools.

metadata

object

Custom key-value pairs for tracking additional context, inherited by all sub-calls.

maxConcurrency

Controls the maximum number of parallel calls when using `batch()`.

callbacks

CallbackHandler\[\]

Handlers for monitoring and responding to events during execution.

recursion\_limit

Maximum recursion depth for chains to prevent infinite loops in complex pipelines.

See full `RunnableConfig` reference for all supported attributes.

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Agents\\
\\
Previous Messages\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/tools

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Tools

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Create tools
- Basic tool definition
- Accessing Context
- Context
- Memory (Store)
- Stream Writer

Tools extend what agents can do—letting them fetch real-time data, execute code, query external databases, and take actions in the world.Under the hood, tools are callable functions with well-defined inputs and outputs that get passed to a chat model. The model decides when to invoke a tool based on the conversation context, and what input arguments to provide.

For details on how models handle tool calls, see Tool calling.

## ​ Create tools

### ​ Basic tool definition

The simplest way to create a tool is by importing the `tool` function from the `langchain` package. You can use zod to define the tool’s input schema:

Copy

import * as z from "zod"
import { tool } from "langchain"

const searchDatabase = tool(

{
name: "search_database",
description: "Search the customer database for records matching the query.",
schema: z.object({
query: z.string().describe("Search terms to look for"),
limit: z.number().describe("Maximum number of results to return"),
}),
}
);

**Server-side tool use**Some chat models (e.g., OpenAI, Anthropic, and Gemini) feature built-in tools that are executed server-side, such as web search and code interpreters. Refer to the provider overview to learn how to access these tools with your specific chat model.

## ​ Accessing Context

**Why this matters:** Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.The runtime context provides a structured way to supply runtime data, such as DB connections, user IDs, or config, into your tools. This avoids global state and keeps tools testable and reusable.

#### ​ Context

Tools can access an agent’s runtime context through the `config` parameter:

import * as z from "zod"
import { ChatOpenAI } from "@langchain/openai"
import { createAgent } from "langchain"

const getUserName = tool(

return config.context.user_name
},
{
name: "get_user_name",
description: "Get the user's name.",
schema: z.object({}),
}
);

const contextSchema = z.object({
user_name: z.string(),
});

const agent = createAgent({
model: new ChatOpenAI({ model: "gpt-4o" }),
tools: [getUserName],
contextSchema,
});

const result = await agent.invoke(
{
messages: [{ role: "user", content: "What is my name?" }]
},
{
context: { user_name: "John Smith" }
}
);

#### ​ Memory (Store)

Access persistent data across conversations using the store. The store is accessed via `config.store` and allows you to save and retrieve user-specific or application-specific data.

import * as z from "zod";
import { createAgent, tool } from "langchain";
import { InMemoryStore } from "@langchain/langgraph";
import { ChatOpenAI } from "@langchain/openai";

const store = new InMemoryStore();

// Access memory
const getUserInfo = tool(

const value = await store.get(["users"], user_id);
console.log("get_user_info", user_id, value);
return value;
},
{
name: "get_user_info",
description: "Look up user info.",
schema: z.object({
user_id: z.string(),
}),
}
);

// Update memory
const saveUserInfo = tool(

console.log("save_user_info", user_id, name, age, email);
await store.put(["users"], user_id, { name, age, email });
return "Successfully saved user info.";
},
{
name: "save_user_info",
description: "Save user info.",
schema: z.object({
user_id: z.string(),
name: z.string(),
age: z.number(),
email: z.string(),
}),
}
);

const agent = createAgent({
model: new ChatOpenAI({ model: "gpt-4o" }),
tools: [getUserInfo, saveUserInfo],
store,
});

// First session: save user info
await agent.invoke({
messages: [\
{\
role: "user",\
content: "Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev",\
},\
],
});

// Second session: get user info
const result = await agent.invoke({
messages: [\
{ role: "user", content: "Get user info for user with id 'abc123'" },\
],
});

console.log(result);
// Here is the user info for user with ID "abc123":
// - Name: Foo
// - Age: 25
// - Email: foo@langchain.dev

See all 70 lines

#### ​ Stream Writer

Stream custom updates from tools as they execute using `config.streamWriter`. This is useful for providing real-time feed or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Messages\\
\\
Previous Short-term memory\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/agents

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Agents

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Core components
- Model
- Static model
- Dynamic model
- Tools
- Defining tools
- Tool error handling
- Tool use in the ReAct loop
- System prompt
- Dynamic system prompt
- Invocation
- Advanced concepts
- Structured output
- Memory
- Streaming
- Middleware

Agents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.`createAgent()` provides a production-ready agent implementation.An LLM Agent runs tools in a loop to achieve a goal.
An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.

action

observation

finish

input

model

tools

output

`createAgent()` builds a **graph**-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.

## ​ Core components

### ​ Model

The model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.

#### ​ Static model

Static models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.To initialize a static model from a model identifier string:

Copy

import { createAgent } from "langchain";

const agent = createAgent({
model: "gpt-5",
tools: []
});

Model identifier strings use the format `provider:model` (e.g. `"openai:gpt-5"`). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:

import { createAgent } from "langchain";
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
model: "gpt-4o",
temperature: 0.1,
maxTokens: 1000,
timeout: 30
});

const agent = createAgent({
model,
tools: []
});

Model instances give you complete control over configuration. Use them when you need to set specific parameters like `temperature`, `max_tokens`, `timeouts`, or configure API keys, `base_url`, and other provider-specific settings. Refer to the API reference to see available params and methods on your model.

#### ​ Dynamic model

Dynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.To use a dynamic model, create middleware with `wrapModelCall` that modifies the model in the request:

import { ChatOpenAI } from "@langchain/openai";
import { createAgent, createMiddleware } from "langchain";

const basicModel = new ChatOpenAI({ model: "gpt-4o-mini" });
const advancedModel = new ChatOpenAI({ model: "gpt-4o" });

const dynamicModelSelection = createMiddleware({
name: "DynamicModelSelection",

// Choose model based on conversation complexity
const messageCount = request.messages.length;

return handler({
...request,

});
},
});

const agent = createAgent({
model: "gpt-4o-mini", // Base model (used when messageCount ≤ 10)
tools,
middleware: [dynamicModelSelection],
});

For more details on middleware and advanced patterns, see the middleware documentation.

For model configuration details, see Models. For dynamic model selection patterns, see Dynamic model in middleware.

### ​ Tools

Tools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:

- Multiple tool calls in sequence (triggered by a single prompt)
- Parallel tool calls when appropriate
- Dynamic tool selection based on previous results
- Tool retry logic and error handling
- State persistence across tool calls

For more information, see Tools.

#### ​ Defining tools

Pass a list of tools to the agent.

import * as z from "zod";
import { createAgent, tool } from "langchain";

const search = tool(

{
name: "search",
description: "Search for information",
schema: z.object({
query: z.string().describe("The query to search for"),
}),
}
);

const getWeather = tool(

{
name: "get_weather",
description: "Get weather information for a location",
schema: z.object({
location: z.string().describe("The location to get weather for"),
}),
}
);

const agent = createAgent({
model: "gpt-4o",
tools: [search, getWeather],
});

If an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.

#### ​ Tool error handling

To customize how tool errors are handled, use the `wrapToolCall` hook in a custom middleware:

import { createAgent, createMiddleware, ToolMessage } from "langchain";

const handleToolErrors = createMiddleware({
name: "HandleToolErrors",

try {
return await handler(request);
} catch (error) {
// Return a custom error message to the model
return new ToolMessage({
content: `Tool error: Please check your input and try again. (${error})`,
tool_call_id: request.toolCall.id!,
});
}
},
});

const agent = createAgent({
model: "gpt-4o",
tools: [\
/* ... */\
],
middleware: [handleToolErrors],
});

The agent will return a `ToolMessage` with the custom error message when a tool fails.

#### ​ Tool use in the ReAct loop

Agents follow the ReAct (“Reasoning + Acting”) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.

Example of ReAct loop

**Prompt:** Identify the current most popular wireless headphones and verify availability.

================================ Human Message =================================

Find the most popular wireless headphones right now and check if they're in stock

- **Reasoning**: “Popularity is time-sensitive, I need to use the provided search tool.”
- **Acting**: Call `search_products("wireless headphones")`

================================== Ai Message ==================================
Tool Calls:
search_products (call_abc123)
Call ID: call_abc123
Args:
query: wireless headphones

================================= Tool Message =================================

Found 5 products matching "wireless headphones". Top 5 results: WH-1000XM5, ...

- **Reasoning**: “I need to confirm availability for the top-ranked item before answering.”
- **Acting**: Call `check_inventory("WH-1000XM5")`

================================== Ai Message ==================================
Tool Calls:
check_inventory (call_def456)
Call ID: call_def456
Args:
product_id: WH-1000XM5

Product WH-1000XM5: 10 units in stock

- **Reasoning**: “I have the most popular model and its stock status. I can now answer the user’s question.”
- **Acting**: Produce final answer

================================== Ai Message ==================================

I found wireless headphones (model WH-1000XM5) with 10 units in stock...

To learn more about tools, see Tools.

### ​ System prompt

You can shape how your agent approaches tasks by providing a prompt. The `systemPrompt` parameter can be provided as a string:

const agent = createAgent({
model,
tools,
systemPrompt: "You are a helpful assistant. Be concise and accurate.",
});

When no `systemPrompt` is provided, the agent will infer its task from the messages directly.The `systemPrompt` parameter accepts either a `string` or a `SystemMessage`. Using a `SystemMessage` gives you more control over the prompt structure, which is useful for provider-specific features like Anthropic’s prompt caching:

import { createAgent } from "langchain";
import { SystemMessage, HumanMessage } from "@langchain/core/messages";

const literaryAgent = createAgent({
model: "anthropic:claude-sonnet-4-5",
systemPrompt: new SystemMessage({
content: [\
{\
type: "text",\
text: "You are an AI assistant tasked with analyzing literary works.",\
},\
{\
type: "text",\

cache_control: { type: "ephemeral" }\
}\
]
})
});

const result = await literaryAgent.invoke({
messages: [new HumanMessage("Analyze the major themes in 'Pride and Prejudice'.")]
});

The `cache_control` field with `{ type: "ephemeral" }` tells Anthropic to cache that content block, reducing latency and costs for repeated requests that use the same system prompt.

#### ​ Dynamic system prompt

For more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.

import * as z from "zod";
import { createAgent, dynamicSystemPromptMiddleware } from "langchain";

const contextSchema = z.object({
userRole: z.enum(["expert", "beginner"]),
});

const agent = createAgent({
model: "gpt-4o",
tools: [/* ... */],
contextSchema,
middleware: [\
dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {\
const userRole = runtime.context.userRole || "user";\
const basePrompt = "You are a helpful assistant.";\
\
if (userRole === "expert") {\
return `${basePrompt} Provide detailed technical responses.`;\
} else if (userRole === "beginner") {\
return `${basePrompt} Explain concepts simply and avoid jargon.`;\
}\
return basePrompt;\
}),\
],
});

// The system prompt will be set dynamically based on context
const result = await agent.invoke(
{ messages: [{ role: "user", content: "Explain machine learning" }] },
{ context: { userRole: "expert" } }
);

For more details on message types and formatting, see Messages. For comprehensive middleware documentation, see Middleware.

## ​ Invocation

You can invoke an agent by passing an update to its `State`. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:

await agent.invoke({
messages: [{ role: "user", content: "What's the weather in San Francisco?" }],
})

For streaming steps and / or tokens from the agent, refer to the streaming guide.Otherwise, the agent follows the LangGraph Graph API and supports all associated methods, such as `stream` and `invoke`.

## ​ Advanced concepts

### ​ Structured output

In some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with the `responseFormat` parameter.

import * as z from "zod";
import { createAgent } from "langchain";

const ContactInfo = z.object({
name: z.string(),
email: z.string(),
phone: z.string(),
});

const agent = createAgent({
model: "gpt-4o",
responseFormat: ContactInfo,
});

const result = await agent.invoke({
messages: [\
{\
role: "user",\
content: "Extract contact info from: John Doe, john@example.com, (555) 123-4567",\
},\
],
});

console.log(result.structuredResponse);
// {
// name: 'John Doe',
// email: 'john@example.com',
// phone: '(555) 123-4567'
// }

To learn about structured output, see Structured output.

### ​ Memory

Agents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.Information stored in the state can be thought of as the short-term memory of the agent:

import * as z from "zod";
import { MessagesZodState } from "@langchain/langgraph";
import { createAgent } from "langchain";
import { type BaseMessage } from "@langchain/core/messages";

const customAgentState = z.object({
messages: MessagesZodState.shape.messages,
userPreferences: z.record(z.string(), z.string()),
});

const CustomAgentState = createAgent({
model: "gpt-4o",
tools: [],
stateSchema: customAgentState,
});

To learn more about memory, see Memory. For information on implementing long-term memory that persists across sessions, see Long-term memory.

### ​ Streaming

We’ve seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.

const stream = await agent.stream(
{
messages: [{\
role: "user",\
content: "Search for AI news and summarize the findings"\
}],
},
{ streamMode: "values" }
);

for await (const chunk of stream) {
// Each chunk contains the full state at that point
const latestMessage = chunk.messages.at(-1);
if (latestMessage?.content) {
console.log(`Agent: ${latestMessage.content}`);
} else if (latestMessage?.tool_calls) {

console.log(`Calling tools: ${toolCallNames.join(", ")}`);
}
}

For more details on streaming, see Streaming.

### ​ Middleware

Middleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:

- Process state before the model is called (e.g., message trimming, context injection)
- Modify or validate the model’s response (e.g., guardrails, content filtering)
- Handle tool execution errors with custom logic
- Implement dynamic model selection based on state or context
- Add custom logging, monitoring, or analytics

Middleware integrates seamlessly into the agent’s execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.

For comprehensive middleware documentation including hooks like `beforeModel`, `afterModel`, and `wrapToolCall`, see Middleware.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Philosophy\\
\\
Previous Models\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/concepts/memory

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Conceptual overviews

Memory overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Router: Knowledge base
- LangGraph

- Custom RAG agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### LangChain Academy

- LangChain Academy

##### Additional resources

- Case studies
- Get help

On this page

- Short-term memory
- Manage short-term memory
- Long-term memory
- Semantic memory
- Profile
- Collection
- Episodic memory
- Procedural memory
- Writing memories
- In the hot path
- In the background
- Memory storage

Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.This conceptual guide covers two types of memory, based on their recall scope:

- Short-term memory, or thread-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent’s state. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.
- Long-term memory stores user-specific or application-level data across sessions and is shared _across_ conversational threads. It can be recalled _at any time_ and _in any thread_. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores ( reference doc) to let you save and recall long-term memories.

## ​ Short-term memory

Short-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.LangGraph manages short-term memory as part of the agent’s state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph’s state, the bot can access the full context for a given conversation while maintaining separation between different threads.

### ​ Manage short-term memory

Conversation history is the most common form of short-term memory, and long conversations pose a challenge to today’s LLMs. A full history may not fit inside an LLM’s context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get “distracted” by stale or off-topic content, all while suffering from slower response times and higher costs.Chat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.!FilterFor more information on common techniques for managing messages, see the Add and manage memory guide.

## ​ Long-term memory

Long-term memory in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is **thread-scoped**, long-term memory is saved within custom “namespaces.”Long-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques:

- What is the type of memory? Humans use memories to remember facts ( semantic memory), experiences ( episodic memory), and rules ( procedural memory). AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task.
- When do you want to update memories? Memory can be updated as part of an agent’s application logic (e.g., “on the hot path”). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the section below.

Different applications require various types of memory. Although the analogy isn’t perfect, examining human memory types can be insightful. Some research (e.g., the CoALA paper) have even mapped these human memory types to those used in AI agents.

| Memory Type | What is Stored | Human Example | Agent Example |
| --- | --- | --- | --- |
| Semantic | Facts | Things I learned in school | Facts about a user |
| Episodic | Experiences | Things I did | Past agent actions |
| Procedural | Instructions | Instincts or motor skills | Agent system prompt |

### ​ Semantic memory

Semantic memory, both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions.

Semantic memory is different from “semantic search,” which is a technique for finding similar content using “meaning” (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches.

Semantic memories can be managed in different ways:

#### ​ Profile

Memories can be a single, continuously updated “profile” of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you’ve selected to represent your domain.When remembering a profile, you will want to make sure that you are **updating** the profile each time. As a result, you will want to pass in the previous profile and ask the model to generate a new profile (or some JSON patch to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or **strict** decoding when generating documents to ensure the memory schemas remains valid.!Update profile

#### ​ Collection

Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you’re less likely to **lose** information over time. It’s easier for an LLM to generate _new_ objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to higher recall downstream.However, this shifts some complexity memory updating. The model must now _delete_ or _update_ existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the Trustcall package for one way to manage this and consider evaluation (e.g., with a tool like LangSmith) to help you tune the behavior.Working with document collections also shifts complexity to memory **search** over the list. The `Store` currently supports both semantic search and filtering by content.Finally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach.!Update listRegardless of memory management approach, the central point is that the agent will use the semantic memories to ground its responses, which often leads to more personalized and relevant interactions.

### ​ Episodic memory

Episodic memory, in both humans and AI agents, involves recalling past events or actions. The CoALA paper frames this well: facts can be written to semantic memory, whereas _experiences_ can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task.In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it’s easier to “show” than “tell” and LLMs learn well from examples. Few-shot learning lets you “program” your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input.Note that the memory store is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a LangSmith Dataset to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity.See this how-to video for example usage of dynamic few-shot example selection in LangSmith. Also, see this blog post showcasing few-shot prompting to improve tool calling performance and this blog post using few-shot example to align an LLMs to human preferences.

### ​ Procedural memory

const namespace = ["agent_instructions"];
const instructions = await store.get(namespace, "agent_a");
// Application logic
const prompt = promptTemplate.format({
instructions: instructions[0].value.instructions
});
// ...
};

// Node that updates instructions

const namespace = ["instructions"];
const currentInstructions = await store.search(namespace);
// Memory logic
const prompt = promptTemplate.format({
instructions: currentInstructions[0].value.instructions,
conversation: state.messages
});
const output = await llm.invoke(prompt);
const newInstructions = output.new_instructions;
await store.put(["agent_instructions"], "agent_a", {
instructions: newInstructions
});
// ...
};

### ​ Writing memories

There are two primary methods for agents to write memories: “in the hot path” and “in the background”.!Hot path vs background

#### ​ In the hot path

Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.As an example, ChatGPT uses a save\_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.

#### ​ In the background

Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.See our memory-service template as an reference implementation.

### ​ Memory storage

LangGraph stores long-term memories as JSON documents in a store. Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.

Copy

import { InMemoryStore } from "@langchain/langgraph";

// Replace with an actual embedding function or LangChain embeddings object

};

// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.
const store = new InMemoryStore({ index: { embed, dims: 2 } });
const userId = "my-user";
const applicationContext = "chitchat";
const namespace = [userId, applicationContext];

await store.put(
namespace,
"a-memory",
{
rules: [\
"User likes short, direct language",\
"User only speaks English & TypeScript",\
],
"my-key": "my-value",
}
);

// get the "memory" by ID
const item = await store.get(namespace, "a-memory");

// search for "memories" within this namespace, filtering on content equivalence, sorted by vector similarity
const items = await store.search(
namespace,
{
filter: { "my-key": "my-value" },
query: "language preferences"
}
);

For more information about the memory store, see the Persistence guide.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Component architecture\\
\\
Previous Context overview\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-server

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Deployment components

Agent Server

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Overview
- Agent Server
- Data plane
- Control plane
- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Application structure
- Parts of a deployment
- Graphs
- Persistence and task queue
- Learn more

LangSmith Deployment’s **Agent Server** offers an API for creating and managing agent-based applications. It is built on the concept of assistants, which are agents configured for specific tasks, and includes built-in persistence and a **task queue**. This versatile API supports a wide range of agentic application use cases, from background processing to real-time interactions.Use Agent Server to create and manage assistants, threads, runs, cron jobs, webhooks, and more.

**API reference**

For detailed information on the API endpoints and data models, refer to the API reference docs.

To use the Enterprise version of the Agent Server, you must acquire a license key that you will need to specify when running the Docker image. To acquire a license key, contact our sales team.You can run the Enterprise version of the Agent Server on the following LangSmith platform options:

- Cloud
- Hybrid
- Self-hosted

## ​ Application structure

To deploy an Agent Server application, you need to specify the graph(s) you want to deploy, as well as any relevant configuration settings, such as dependencies and environment variables.Read the application structure guide to learn how to structure your LangGraph application for deployment.

## ​ Parts of a deployment

When you deploy Agent Server, you are deploying one or more graphs, a database for persistence, and a task queue.

### ​ Graphs

When you deploy a graph with Agent Server, you are deploying a “blueprint” for an Assistant.An Assistant is a graph paired with specific configuration settings. You can create multiple assistants per graph, each with unique settings to accommodate different use cases
that can be served by the same graph.Upon deployment, Agent Server will automatically create a default assistant for each graph using the graph’s default configuration settings.

We often think of a graph as implementing an agent, but a graph does not necessarily need to implement an agent. For example, a graph could implement a simple
chatbot that only supports back-and-forth conversation, without the ability to influence any application control flow. In reality, as applications get more complex, a graph will often implement a more complex flow that may use multiple agents working in tandem.

### ​ Persistence and task queue

Agent Server leverages a database for persistence and a task queue.PostgreSQL is supported as a database for Agent Server and Redis as the task queue.If you’re deploying using LangSmith cloud, these components are managed for you. If you’re deploying Agent Server on your own infrastructure, you’ll need to set up and manage these components yourself.For more information on how these components are set up and managed, review the hosting options guide.

## ​ Learn more

- Application Structure guide explains how to structure your application for deployment.
- The API Reference provides detailed information on the API endpoints and data models.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Deployment components\\
\\
Previous LangSmith data plane\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/integrations/providers/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

- All providers

##### Popular Providers

- OpenAI

- Anthropic

- Google

- AWS

- Microsoft

##### General integrations

- Chat models
- Tools and Toolkits
- LLMs
- Middleware
- Key-value stores
- Document transformers
- Model caches
- Callbacks

##### RAG integrations

- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integrations All integration providers Integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/learn)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Router: Knowledge base
- LangGraph

- Custom RAG agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### LangChain Academy

- LangChain Academy

##### Additional resources

- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

How to set up a JavaScript application All integrations Contributing to documentation

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/reference/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith reference Contributing to documentation How to set up a JavaScript application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/contributing/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Contribute

- Documentation
- Code
- Integrations

404

# Page not found

We couldn’t find the page you were looking for.

Contributing integrations How to set up a JavaScript application Contributing to documentation

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/install)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Install LangGraph How to set up a JavaScript application What's new in LangGraph v1

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/quickstart)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

App development in LangSmith Deployment Tracing quickstart Run a LangGraph app locally

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/local-server)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph CLI LangSmith Deployment components Trace JS functions in serverless environments

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/releases/changelog)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

Changelog Agent Server changelog Self-hosted LangSmith changelog

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/thinking-in-langgraph)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Thinking in LangGraph LangGraph CLI How to set up a JavaScript application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/workflows-agents)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Workflows and agents LangGraph CLI Build a custom RAG agent with LangGraph

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/persistence)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

What's new in LangGraph v1 LangGraph CLI Frequently asked questions

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/durable-execution)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Durable execution What's new in LangGraph v1

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/streaming)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Streaming API What's new in LangGraph v1 How to integrate LangGraph into your React application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/interrupts)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Interrupts LangGraph CLI How to set up a JavaScript application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/use-time-travel)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Use time-travel Time travel using the server API How to set up a JavaScript application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/add-memory)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Install LangGraph What's new in LangGraph v1 Memory

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/use-subgraphs)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph CLI Install LangGraph What's new in LangGraph v1

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/application-structure)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

How to set up a JavaScript application Application structure

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/test)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph CLI How to set up a JavaScript application LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/studio)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Install LangGraph What's new in LangGraph v1

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/ui)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

How to implement generative user interfaces with LangGraph How to set up a JavaScript application LangGraph CLI

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/deploy)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

App development in LangSmith Deployment How to set up a JavaScript application LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/observability)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability Deploy an observability stack for your LangSmith deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/pregel)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph runtime LangGraph CLI How to set up a JavaScript application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/models)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain integrations packages

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/tools).We

Skip to main content.We#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/agents)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview Build a voice agent with LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/durable-execution):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Durable execution What's new in LangGraph v1

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/interrupts):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Interrupts LangGraph CLI How to set up a JavaScript application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/concepts/memory):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Router: Knowledge base
- LangGraph

- Custom RAG agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### LangChain Academy

- LangChain Academy

##### Additional resources

- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Contributing to documentation How to set up a JavaScript application Memory overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/home):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith docs LangSmith Deployment Self-hosted LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deployments):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment App development in LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-server)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Configure LangSmith Agent Server for scale LangSmith Tool Server Agent Server API reference for LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/install

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Install LangGraph

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

To install the base LangGraph package:

pip

uv

Copy

pip install -U langgraph

To use LangGraph you will usually want to access LLMs and define tools.
You can do this however you see fit.One way to do this (which we will use in the docs) is to use LangChain.Install LangChain with:

pip install -U langchain
# Requires Python 3.10+

To work with specific LLM provider packages, you will need install them separately.Refer to the integrations page for provider-specific installation instructions.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangGraph overview\\
\\
Previous Quickstart\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/local-server

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Run a local server

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Prerequisites
- 1\. Install the LangGraph CLI
- 2\. Create a LangGraph app
- 3\. Install dependencies
- 4\. Create a .env file
- 5\. Launch Agent server
- 6\. Test your application in Studio
- 7\. Test the API
- Next steps

This guide shows you how to run a LangGraph application locally.

## ​ Prerequisites

Before you begin, ensure you have the following:

- An API key for LangSmith \- free to sign up

## ​ 1\. Install the LangGraph CLI

pip

uv

Copy

pip install -U "langgraph-cli[inmem]"

## ​ 2\. Create a LangGraph app

Create a new app from the `new-langgraph-project-python` template. This template demonstrates a single-node application you can extend with your own logic.

langgraph new path/to/your/app --template new-langgraph-project-python

**Additional templates**
If you use `langgraph new` without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.

## ​ 3\. Install dependencies

In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:

cd path/to/your/app
pip install -e .

## ​ 4\. Create a `.env` file

You will find a `.env.example` in the root of your new LangGraph app. Create a `.env` file in the root of your new LangGraph app and copy the contents of the `.env.example` file into it, filling in the necessary API keys:

LANGSMITH_API_KEY=lsv2...

## ​ 5\. Launch Agent server

Start the LangGraph API server locally:

langgraph dev

Sample output:

INFO:langgraph_api.cli:

Welcome to

╦ ┌─┐┌┐┌┌─┐╔═╗┬─┐┌─┐┌─┐┬ ┬
║ ├─┤││││ ┬║ ╦├┬┘├─┤├─┘├─┤
╩═╝┴ ┴┘└┘└─┘╚═╝┴└─┴ ┴┴ ┴ ┴

- 🚀 API:
- 🎨 Studio UI:
- 📚 API Docs:

This in-memory server is designed for development and testing.
For production use, please use LangSmith Deployment.

The `langgraph dev` command starts Agent Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy Agent Server with access to a persistent storage backend. For more information, see the Platform setup overview.

## ​ 6\. Test your application in Studio

Studio is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of the `langgraph dev` command:

For an Agent Server running on a custom host/port, update the `baseUrl` query parameter in the URL. For example, if your server is running on `http://myhost:3000`:

Safari compatibility

Use the `--tunnel` flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:

langgraph dev --tunnel

## ​ 7\. Test the API

- Python SDK (async)

- Python SDK (sync)

- Rest API

1. Install the LangGraph Python SDK:

pip install langgraph-sdk

2. Send a message to the assistant (threadless run):

from langgraph_sdk import get_client
import asyncio

client = get_client(url="http://localhost:2024")

async def main():
async for chunk in client.runs.stream(
None, # Threadless run
"agent", # Name of assistant. Defined in langgraph.json.
input={
"messages": [{\
"role": "human",\
"content": "What is LangGraph?",\
}],
},
):
print(f"Receiving new event of type: {chunk.event}...")
print(chunk.data)
print("\n\n")

asyncio.run(main())

from langgraph_sdk import get_sync_client

client = get_sync_client(url="http://localhost:2024")

for chunk in client.runs.stream(
None, # Threadless run
"agent", # Name of assistant. Defined in langgraph.json.
input={
"messages": [{\
"role": "human",\
"content": "What is LangGraph?",\
}],
},
stream_mode="messages-tuple",
):
print(f"Receiving new event of type: {chunk.event}...")
print(chunk.data)
print("\n\n")

curl -s --request POST \
--url "http://localhost:2024/runs/stream" \
--header 'Content-Type: application/json' \
--data "{
\"assistant_id\": \"agent\",
\"input\": {
\"messages\": [\
{\
\"role\": \"human\",\
\"content\": \"What is LangGraph?\"\
}\
]
},
\"stream_mode\": \"messages-tuple\"
}"

## ​ Next steps

Now that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:

- Deployment quickstart: Deploy your LangGraph app using LangSmith.
- LangSmith: Learn about foundational LangSmith concepts.
- SDK Reference: Explore the SDK API Reference.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Quickstart\\
\\
Previous Changelog\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Thinking in LangGraph

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Start with the process you want to automate
- Step 1: Map out your workflow as discrete steps
- Step 2: Identify what each step needs to do
- LLM steps
- Data steps
- Action steps
- User input steps
- Step 3: Design your state
- What belongs in state?
- Keep state raw, format prompts on-demand
- Step 4: Build your nodes
- Handle errors appropriately
- Implementing our email agent nodes
- Step 5: Wire it together
- Try out your agent
- Summary and next steps
- Key Insights
- Advanced considerations
- Where to go from here

When you build an agent with LangGraph, you will first break it apart into discrete steps called **nodes**. Then, you will describe the different decisions and transitions from each of your nodes. Finally, you connect nodes together through a shared **state** that each node can read from and write to.In this walkthrough, we’ll guide you through the thought process of building a customer support email agent with LangGraph.

## ​ Start with the process you want to automate

Imagine that you need to build an AI agent that handles customer support emails. Your product team has given you these requirements:

Copy

The agent should:

- Read incoming customer emails
- Classify them by urgency and topic
- Search relevant documentation to answer questions
- Draft appropriate responses
- Escalate complex issues to human agents
- Schedule follow-ups when needed

Example scenarios to handle:

1. Simple product question: "How do I reset my password?"
2. Bug report: "The export feature crashes when I select PDF format"
3. Urgent billing issue: "I was charged twice for my subscription!"
4. Feature request: "Can you add dark mode to the mobile app?"
5. Complex technical issue: "Our API integration fails intermittently with 504 errors"

To implement an agent in LangGraph, you will usually follow the same five steps.

## ​ Step 1: Map out your workflow as discrete steps

Start by identifying the distinct steps in your process. Each step will become a **node** (a function that does one specific thing). Then, sketch how these steps connect to each other.

START

Read Email

Classify Intent

Doc Search

Bug Track

Human Review

Draft Reply

Send Reply

END

The arrows in this diagram show possible paths, but the actual decision of which path to take happens inside each node.Now that we’ve identified the components in our workflow, let’s understand what each node needs to do:

- `Read Email`: Extract and parse the email content
- `Classify Intent`: Use an LLM to categorize urgency and topic, then route to appropriate action
- `Doc Search`: Query your knowledge base for relevant information
- `Bug Track`: Create or update issue in tracking system
- `Draft Reply`: Generate an appropriate response
- `Human Review`: Escalate to human agent for approval or handling
- `Send Reply`: Dispatch the email response

Notice that some nodes make decisions about where to go next (`Classify Intent`, `Draft Reply`, `Human Review`), while others always proceed to the same next step (`Read Email` always goes to `Classify Intent`, `Doc Search` always goes to `Draft Reply`).

## ​ Step 2: Identify what each step needs to do

For each node in your graph, determine what type of operation it represents and what context it needs to work properly.

**LLM steps** \\
\\
Use when you need to understand, analyze, generate text, or make reasoning decisions **Data steps** \\
\\
Use when you need to retrieve information from external sources **Action steps** \\
\\
Use when you need to perform external actions **User input steps** \\
\\
Use when you need human intervention

### ​ LLM steps

When a step needs to understand, analyze, generate text, or make reasoning decisions:

Classify intent

- Static context (prompt): Classification categories, urgency definitions, response format
- Dynamic context (from state): Email content, sender information
- Desired outcome: Structured classification that determines routing

Draft reply

- Static context (prompt): Tone guidelines, company policies, response templates
- Dynamic context (from state): Classification results, search results, customer history
- Desired outcome: Professional email response ready for review

### ​ Data steps

When a step needs to retrieve information from external sources:

Document search

- Parameters: Query built from intent and topic
- Retry strategy: Yes, with exponential backoff for transient failures
- Caching: Could cache common queries to reduce API calls

Customer history lookup

- Parameters: Customer email or ID from state
- Retry strategy: Yes, but with fall Action steps

When a step needs to perform an external action:

Send reply

- When to execute node: After approval (human or automated)
- Retry strategy: Yes, with exponential backoff for network issues
- Should not cache: Each send is a unique action

Bug track

- When to execute node: Always when intent is “bug”
- Retry strategy: Yes, critical to not lose bug reports
- Returns: Ticket ID to include in response

### ​ User input steps

When a step needs human intervention:

Human review node

- Context for decision: Original email, draft response, urgency, classification
- Expected input format: Approval boolean plus optional edited response
- When triggered: High urgency, complex issues, or quality concerns

## ​ Step 3: Design your state

State is the shared memory accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.

### ​ What belongs in state?

Ask yourself these questions about each piece of data:

## Include in state

Does it need to persist across steps? If yes, it goes in state.

## Don't store

Can you derive it from other data? If yes, compute it when needed instead of storing it in state.

For our email agent, we need to track:

- The original email and sender info (can’t reconstruct these later)
- Classification results (needed by multiple later/downstream nodes)
- Search results and customer data (expensive to re-fetch)
- The draft response (needs to persist through review)
- Execution metadata (for debugging and recovery)

### ​ Keep state raw, format prompts on-demand

A key principle: your state should store raw data, not formatted text. Format prompts inside nodes when you need them.

This separation means:

- Different nodes can format the same data differently for their needs
- You can change prompt templates without modifying your state schema
- Debugging is clearer – you see exactly what data each node received
- Your agent can evolve without breaking existing state

Let’s define our state:

from typing import TypedDict, Literal

# Define the structure for email classification
class EmailClassification(TypedDict):
intent: Literal["question", "bug", "billing", "feature", "complex"]
urgency: Literal["low", "medium", "high", "critical"]
topic: str
summary: str

class EmailAgentState(TypedDict):
# Raw email data
email_content: str
sender_email: str
email_id: str

# Classification result
classification: EmailClassification | None

# Raw search/API results
search_results: list[str] | None # List of raw document chunks
customer_history: dict | None # Raw customer data from CRM

# Generated content
draft_response: str | None
messages: list[str] | None

Notice that the state contains only raw data – no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM.

## ​ Step 4: Build your nodes

Now we implement each step as a function. A node in LangGraph is just a Python function that takes the current state and returns updates to it.

### ​ Handle errors appropriately

Different errors need different handling strategies:

| Error Type | Who Fixes It | Strategy | When to Use |
| --- | --- | --- | --- |
| Transient errors (network issues, rate limits) | System (automatic) | Retry policy | Temporary failures that usually resolve on retry |
| LLM-recoverable errors (tool failures, parsing issues) | LLM | Store error in state and loop back | LLM can see the error and adjust its approach |
| User-fixable errors (missing information, unclear instructions) | Human | Pause with `interrupt()` | Need user input to proceed |
| Unexpected errors | Developer | Let them bubble up | Unknown issues that need debugging |

- Transient errors

- LLM-recoverable

- User-fixable

- Unexpected

Add a retry policy to automatically retry network issues and rate limits:

from langgraph.types import RetryPolicy

workflow.add_node(
"search_documentation",
search_documentation,
retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0)
)

Store the error in state and loop back so the LLM can see what went wrong and try again:

from langgraph.types import Command

try:
result = run_tool(state['tool_call'])
return Command(update={"tool_result": result}, goto="agent")
except ToolError as e:
# Let the LLM see what went wrong and try again
return Command(
update={"tool_result": f"Tool error: {str(e)}"},
goto="agent"
)

Pause and collect information from the user when needed (like account IDs, order numbers, or clarifications):

if not state.get('customer_id'):
user_input = interrupt({
"message": "Customer ID needed",
"request": "Please provide the customer's account ID to look up their subscription history"
})
return Command(
update={"customer_id": user_input['customer_id']},
goto="lookup_customer_history"
)
# Now proceed with the lookup
customer_data = fetch_customer_history(state['customer_id'])
return Command(update={"customer_history": customer_data}, goto="draft_response")

Let them bubble up for debugging. Don’t catch what you can’t handle:

def send_reply(state: EmailAgentState):
try:
email_service.send(state["draft_response"])
except Exception:
raise # Surface unexpected errors

### ​ Implementing our email agent nodes

We’ll implement each node as a simple function. Remember: nodes take state, do work, and return updates.

Read and classify nodes

from typing import Literal
from langgraph.graph import StateGraph, START, END
from langgraph.types import interrupt, Command, RetryPolicy
from langchain_openai import ChatOpenAI
from langchain.messages import HumanMessage

llm = ChatOpenAI(model="gpt-5-nano")

"""Extract and parse email content"""
# In production, this would connect to your email service
return {
"messages": [HumanMessage(content=f"Processing email: {state['email_content']}")]
}

"""Use LLM to classify email intent and urgency, then route accordingly"""

# Create structured LLM that returns EmailClassification dict
structured_llm = llm.with_structured_output(EmailClassification)

# Format the prompt on-demand, not stored in state
classification_prompt = f"""
Analyze this customer email and classify it:

Email: {state['email_content']}
From: {state['sender_email']}

Provide classification including intent, urgency, topic, and summary.
"""

# Get structured response directly as dict
classification = structured_llm.invoke(classification_prompt)

# Determine next node based on classification
if classification['intent'] == 'billing' or classification['urgency'] == 'critical':
goto = "human_review"
elif classification['intent'] in ['question', 'feature']:
goto = "search_documentation"
elif classification['intent'] == 'bug':
goto = "bug_tracking"
else:
goto = "draft_response"

# Store classification as a single dict in state
return Command(
update={"classification": classification},
goto=goto
)

Search and tracking nodes

"""Search knowledge base for relevant information"""

# Build search query from classification
classification = state.get('classification', {})
query = f"{classification.get('intent', '')} {classification.get('topic', '')}"

try:
# Implement your search logic here
# Store raw search results, not formatted text
search_results = [\

"Password must be at least 12 characters",\
"Include uppercase, lowercase, numbers, and symbols"\
]
except SearchAPIError as e:
# For recoverable search errors, store error and continue
search_results = [f"Search temporarily unavailable: {str(e)}"]

return Command(
update={"search_results": search_results}, # Store raw results or error
goto="draft_response"
)

"""Create or update bug tracking ticket"""

# Create ticket in your bug tracking system
ticket_id = "BUG-12345" # Would be created via API

return Command(
update={
"search_results": [f"Bug ticket {ticket_id} created"],
"current_step": "bug_tracked"
},
goto="draft_response"
)

Response nodes

"""Generate response using context and route based on quality"""

classification = state.get('classification', {})

# Format context from raw state data on-demand
context_sections = []

if state.get('search_results'):
# Format search results for the prompt
formatted_docs = "\n".join([f"- {doc}" for doc in state['search_results']])
context_sections.append(f"Relevant documentation:\n{formatted_docs}")

if state.get('customer_history'):
# Format customer data for the prompt
context_sections.append(f"Customer tier: {state['customer_history'].get('tier', 'standard')}")

# Build the prompt with formatted context
draft_prompt = f"""
Draft a response to this customer email:
{state['email_content']}

Email intent: {classification.get('intent', 'unknown')}
Urgency level: {classification.get('urgency', 'medium')}

{chr(10).join(context_sections)}

Guidelines:
- Be professional and helpful
- Address their specific concern
- Use the provided documentation when relevant
"""

response = llm.invoke(draft_prompt)

# Determine if human review needed based on urgency and intent
needs_review = (
classification.get('urgency') in ['high', 'critical'] or
classification.get('intent') == 'complex'
)

# Route to appropriate next node
goto = "human_review" if needs_review else "send_reply"

return Command(
update={"draft_response": response.content}, # Store only the raw response
goto=goto
)

"""Pause for human review using interrupt and route based on decision"""

# interrupt() must come first - any code before it will re-run on resume
human_decision = interrupt({
"email_id": state.get('email_id',''),
"original_email": state.get('email_content',''),
"draft_response": state.get('draft_response',''),
"urgency": classification.get('urgency'),
"intent": classification.get('intent'),
"action": "Please review and approve/edit this response"
})

# Now process the human's decision
if human_decision.get("approved"):
return Command(
update={"draft_response": human_decision.get("edited_response", state.get('draft_response',''))},
goto="send_reply"
)
else:
# Rejection means human will handle directly
return Command(update={}, goto=END)

"""Send the email response"""
# Integrate with email service
print(f"Sending reply: {state['draft_response'][:100]}...")
return {}

## ​ Step 5: Wire it together

Now we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges.To enable human-in-the-loop with `interrupt()`, we need to compile with a checkpointer to save state between runs:

Graph compilation code

from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import RetryPolicy

# Create the graph
workflow = StateGraph(EmailAgentState)

# Add nodes with appropriate error handling
workflow.add_node("read_email", read_email)
workflow.add_node("classify_intent", classify_intent)

# Add retry policy for nodes that might have transient failures
workflow.add_node(
"search_documentation",
search_documentation,
retry_policy=RetryPolicy(max_attempts=3)
)
workflow.add_node("bug_tracking", bug_tracking)
workflow.add_node("draft_response", draft_response)
workflow.add_node("human_review", human_review)
workflow.add_node("send_reply", send_reply)

# Add only the essential edges
workflow.add_edge(START, "read_email")
workflow.add_edge("read_email", "classify_intent")
workflow.add_edge("send_reply", END)

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

The graph structure is minimal because routing happens inside nodes through `Command` objects. Each node declares where it can go using type hints like `Command[Literal["node1", "node2"]]`, making the flow explicit and traceable.

### ​ Try out your agent

Let’s run our agent with an urgent billing issue that needs human review:

Testing the agent

# Test with an urgent billing issue
initial_state = {
"email_content": "I was charged twice for my subscription! This is urgent!",
"sender_email": "customer@example.com",
"email_id": "email_123",
"messages": []
}

# Run with a thread_id for persistence
config = {"configurable": {"thread_id": "customer_123"}}
result = app.invoke(initial_state, config)
# The graph will pause at human_review
print(f"human review interrupt:{result['__interrupt__']}")

# When ready, provide human input to resume

human_response = Command(
resume={
"approved": True,
"edited_response": "We sincerely apologize for the double charge. I've initiated an immediate refund..."
}
)

# Resume execution
final_result = app.invoke(human_response, config)
print(f"Email sent successfully!")

The graph pauses when it hits `interrupt()`, saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The `thread_id` ensures all state for this conversation is preserved together.

## ​ Summary and next steps

### ​ Key Insights

Building this email agent has shown us the LangGraph way of thinking:

**Break into discrete steps** \\
\\
Each node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps. **State is shared memory** \\
\\
Store raw data, not formatted text. This lets different nodes use the same information in different ways. **Nodes are functions** \\
\\
They take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination. **Errors are part of the flow** \\
\\
Transient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging. **Human input is first-class** \\
\\
The `interrupt()` function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first. **Graph structure emerges naturally** \\
\\
You define the essential connections, and your nodes handle their own routing logic. This keeps control flow explicit and traceable - you can always understand what your agent will do next by looking at the current node.

### ​ Advanced considerations

Node granularity trade-offs

This section explores the trade-offs in node granularity design. Most applications can skip this and use the patterns shown above.

You might wonder: why not combine `Read Email` and `Classify Intent` into one node?Or why separate Doc Search from Draft Reply?The answer involves trade-offs between resilience and observability.**The resilience consideration:** LangGraph’s durable execution creates checkpoints at node boundaries. When a workflow resumes after an interruption or failure, it starts from the beginning of the node where execution stopped. Smaller nodes mean more frequent checkpoints, which means less work to repeat if something goes wrong. If you combine multiple operations into one large node, a failure near the end means re-executing everything from the start of that node.Why we chose this breakdown for the email agent:

- **Isolation of external services:** Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others.
- **Intermediate visibility:** Having `Classify Intent` as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring—you can see exactly when and why the agent routes to human review.
- **Different failure modes:** LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently.
- **Reusability and testing:** Smaller nodes are easier to test in isolation and reuse in other workflows.

A different valid approach: You could combine `Read Email` and `Classify Intent` into a single node. You’d lose the ability to inspect the raw email before classification and would repeat both operations on any failure in that node. For most applications, the observability and debugging benefits of separate nodes are worth the trade-off.Application-level concerns: The caching discussion in Step 2 (whether to cache search results) is an application-level decision, not a LangGraph framework feature. You implement caching within your node functions based on your specific requirements—LangGraph doesn’t prescribe this.Performance considerations: More nodes doesn’t mean slower execution. LangGraph writes checkpoints in the background by default ( async durability mode), so your graph continues running without waiting for checkpoints to complete. This means you get frequent checkpoints with minimal performance impact. You can adjust this behavior if needed—use `"exit"` mode to checkpoint only at completion, or `"sync"` mode to block execution until each checkpoint is written.

### ​ Where to go from here

This was an introduction to thinking about building agents with LangGraph. You can extend this foundation with:

**Human-in-the-loop patterns** \\
\\
Learn how to add tool approval before execution, batch approval, and other patterns **Subgraphs** \\
\\
Create subgraphs for complex multi-step operations **Streaming** \\
\\
Add streaming to show real-time progress to users **Observability** \\
\\
Add observability with LangSmith for debugging and monitoring **Tool Integration** \\
\\
Integrate more tools for web search, database queries, and API calls **Retry Logic** \\
\\
Implement retry logic with exponential backoff for failed operations

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Changelog\\
\\
Previous Workflows and agents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/workflows-agents

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Workflows and agents

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Setup
- LLMs and augmentations
- Prompt chaining
- Parallelization
- Routing
- Orchestrator-worker
- Creating workers in LangGraph
- Evaluator-optimizer
- Agents

This guide reviews common workflow and agent patterns.

- Workflows have predetermined code paths and are designed to operate in a certain order.
- Agents are dynamic and define their own processes and tool usage.

## ​ Setup

To build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:

1. Install dependencies:

Copy

pip install langchain_core langchain-anthropic langgraph

2. Initialize the LLM:

import os
import getpass

from langchain_anthropic import ChatAnthropic

def _set_env(var: str):
if not os.environ.get(var):
os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")

llm = ChatAnthropic(model="claude-sonnet-4-5-20250929")

## ​ LLMs and augmentations

Workflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.!LLM augmentations

# Schema for structured output
from pydantic import BaseModel, Field

class SearchQuery(BaseModel):
search_query: str = Field(None, description="Query that is optimized web search.")
justification: str = Field(
None, description="Why this query is relevant to the user's request."
)

# Augment the LLM with schema for structured output
structured_llm = llm.with_structured_output(SearchQuery)

# Invoke the augmented LLM
output = structured_llm.invoke("How does Calcium CT score relate to high cholesterol?")

# Define a tool

return a * b

# Augment the LLM with tools
llm_with_tools = llm.bind_tools([multiply])

# Invoke the LLM with input that triggers the tool call
msg = llm_with_tools.invoke("What is 2 times 3?")

# Get the tool call
msg.tool_calls

## ​ Prompt chaining

Prompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:

- Translating documents into different languages
- Verifying generated content for consistency

Graph API

Functional API

from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from IPython.display import Image, display

# Graph state
class State(TypedDict):
topic: str
joke: str
improved_joke: str
final_joke: str

# Nodes
def generate_joke(state: State):
"""First LLM call to generate initial joke"""

msg = llm.invoke(f"Write a short joke about {state['topic']}")
return {"joke": msg.content}

def check_punchline(state: State):
"""Gate function to check if the joke has a punchline"""

# Simple check - does the joke contain "?" or "!"
if "?" in state["joke"] or "!" in state["joke"]:
return "Pass"
return "Fail"

def improve_joke(state: State):
"""Second LLM call to improve the joke"""

msg = llm.invoke(f"Make this joke funnier by adding wordplay: {state['joke']}")
return {"improved_joke": msg.content}

def polish_joke(state: State):
"""Third LLM call for final polish"""
msg = llm.invoke(f"Add a surprising twist to this joke: {state['improved_joke']}")
return {"final_joke": msg.content}

# Build workflow
workflow = StateGraph(State)

# Add nodes
workflow.add_node("generate_joke", generate_joke)
workflow.add_node("improve_joke", improve_joke)
workflow.add_node("polish_joke", polish_joke)

# Add edges to connect nodes
workflow.add_edge(START, "generate_joke")
workflow.add_conditional_edges(
"generate_joke", check_punchline, {"Fail": "improve_joke", "Pass": END}
)
workflow.add_edge("improve_joke", "polish_joke")
workflow.add_edge("polish_joke", END)

# Compile
chain = workflow.compile()

# Show workflow
display(Image(chain.get_graph().draw_mermaid_png()))

# Invoke
state = chain.invoke({"topic": "cats"})
print("Initial joke:")
print(state["joke"])
print("\n--- --- ---\n")
if "improved_joke" in state:
print("Improved joke:")
print(state["improved_joke"])
print("\n--- --- ---\n")

print("Final joke:")
print(state["final_joke"])
else:
print("Final joke:")
print(state["joke"])

## ​ Parallelization

With parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:

- Split up subtasks and run them in parallel, which increases speed
- Run tasks multiple times to check for different outputs, which increases confidence

Some examples include:

- Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors
- Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources

class State(TypedDict):
topic: str
joke: str
story: str
poem: str
combined_output: str

def call_llm_1(state: State):
"""First LLM call to generate initial joke"""

msg = llm.invoke(f"Write a joke about {state['topic']}")
return {"joke": msg.content}

def call_llm_2(state: State):
"""Second LLM call to generate story"""

msg = llm.invoke(f"Write a story about {state['topic']}")
return {"story": msg.content}

def call_llm_3(state: State):
"""Third LLM call to generate poem"""

msg = llm.invoke(f"Write a poem about {state['topic']}")
return {"poem": msg.content}

def aggregator(state: State):
"""Combine the joke, story and poem into a single output"""

combined = f"Here's a story, joke, and poem about {state['topic']}!\n\n"
combined += f"STORY:\n{state['story']}\n\n"
combined += f"JOKE:\n{state['joke']}\n\n"
combined += f"POEM:\n{state['poem']}"
return {"combined_output": combined}

parallel_builder = StateGraph(State)

parallel_builder.add_node("call_llm_1", call_llm_1)
parallel_builder.add_node("call_llm_2", call_llm_2)
parallel_builder.add_node("call_llm_3", call_llm_3)
parallel_builder.add_node("aggregator", aggregator)

parallel_builder.add_edge(START, "call_llm_1")
parallel_builder.add_edge(START, "call_llm_2")
parallel_builder.add_edge(START, "call_llm_3")
parallel_builder.add_edge("call_llm_1", "aggregator")
parallel_builder.add_edge("call_llm_2", "aggregator")
parallel_builder.add_edge("call_llm_3", "aggregator")
parallel_builder.add_edge("aggregator", END)
parallel_workflow = parallel_builder.compile()

display(Image(parallel_workflow.get_graph().draw_mermaid_png()))

state = parallel_workflow.invoke({"topic": "cats"})
print(state["combined_output"])

## ​ Routing

Routing workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.!routing.png

from typing_extensions import Literal
from langchain.messages import HumanMessage, SystemMessage

# Schema for structured output to use as routing logic
class Route(BaseModel):
step: Literal["poem", "story", "joke"] = Field(
None, description="The next step in the routing process"
)

router = llm.with_structured_output(Route)

# State
class State(TypedDict):
input: str
decision: str
output: str

def llm_call_1(state: State):
"""Write a story"""

result = llm.invoke(state["input"])
return {"output": result.content}

def llm_call_2(state: State):
"""Write a joke"""

def llm_call_3(state: State):
"""Write a poem"""

def llm_call_router(state: State):
"""Route the input to the appropriate node"""

# Run the augmented LLM with structured output to serve as routing logic
decision = router.invoke(
[\
SystemMessage(\
content="Route the input to story, joke, or poem based on the user's request."\
),\
HumanMessage(content=state["input"]),\
]
)

return {"decision": decision.step}

# Conditional edge function to route to the appropriate node
def route_decision(state: State):
# Return the node name you want to visit next
if state["decision"] == "story":
return "llm_call_1"
elif state["decision"] == "joke":
return "llm_call_2"
elif state["decision"] == "poem":
return "llm_call_3"

router_builder = StateGraph(State)

router_builder.add_node("llm_call_1", llm_call_1)
router_builder.add_node("llm_call_2", llm_call_2)
router_builder.add_node("llm_call_3", llm_call_3)
router_builder.add_node("llm_call_router", llm_call_router)

router_builder.add_edge(START, "llm_call_router")
router_builder.add_conditional_edges(
"llm_call_router",
route_decision,
{ # Name returned by route_decision : Name of next node to visit
"llm_call_1": "llm_call_1",
"llm_call_2": "llm_call_2",
"llm_call_3": "llm_call_3",
},
)
router_builder.add_edge("llm_call_1", END)
router_builder.add_edge("llm_call_2", END)
router_builder.add_edge("llm_call_3", END)

# Compile workflow
router_workflow = router_builder.compile()

# Show the workflow
display(Image(router_workflow.get_graph().draw_mermaid_png()))

state = router_workflow.invoke({"input": "Write me a joke about cats"})
print(state["output"])

## ​ Orchestrator-worker

In an orchestrator-worker configuration, the orchestrator:

- Breaks down tasks into subtasks
- Delegates subtasks to workers
- Synthesizes worker outputs into a final result

from typing import Annotated, List
import operator

# Schema for structured output to use in planning
class Section(BaseModel):
name: str = Field(
description="Name for this section of the report.",
)
description: str = Field(
description="Brief overview of the main topics and concepts to be covered in this section.",
)

class Sections(BaseModel):
sections: List[Section] = Field(
description="Sections of the report.",
)

planner = llm.with_structured_output(Sections)

### ​ Creating workers in LangGraph

Orchestrator-worker workflows are common and LangGraph has built-in support for them. The `Send` API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the `Send` API to send a section to each worker.

from langgraph.types import Send

class State(TypedDict):
topic: str # Report topic
sections: list[Section] # List of report sections
completed_sections: Annotated[\
list, operator.add\
] # All workers write to this key in parallel
final_report: str # Final report

# Worker state
class WorkerState(TypedDict):
section: Section
completed_sections: Annotated[list, operator.add]

def orchestrator(state: State):
"""Orchestrator that generates a plan for the report"""

# Generate queries
report_sections = planner.invoke(
[\
SystemMessage(content="Generate a plan for the report."),\
HumanMessage(content=f"Here is the report topic: {state['topic']}"),\
]
)

return {"sections": report_sections.sections}

def llm_call(state: WorkerState):
"""Worker writes a section of the report"""

# Generate section
section = llm.invoke(
[\
SystemMessage(\
content="Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting."\
),\
HumanMessage(\
content=f"Here is the section name: {state['section'].name} and description: {state['section'].description}"\
),\
]
)

# Write the updated section to completed sections
return {"completed_sections": [section.content]}

def synthesizer(state: State):
"""Synthesize full report from sections"""

# List of completed sections
completed_sections = state["completed_sections"]

# Format completed section to str to use as context for final sections
completed_report_sections = "\n\n---\n\n".join(completed_sections)

return {"final_report": completed_report_sections}

# Conditional edge function to create llm_call workers that each write a section of the report
def assign_workers(state: State):
"""Assign a worker to each section in the plan"""

# Kick off section writing in parallel via Send() API
return [Send("llm_call", {"section": s}) for s in state["sections"]]

orchestrator_worker_builder = StateGraph(State)

# Add the nodes
orchestrator_worker_builder.add_node("orchestrator", orchestrator)
orchestrator_worker_builder.add_node("llm_call", llm_call)
orchestrator_worker_builder.add_node("synthesizer", synthesizer)

orchestrator_worker_builder.add_edge(START, "orchestrator")
orchestrator_worker_builder.add_conditional_edges(
"orchestrator", assign_workers, ["llm_call"]
)
orchestrator_worker_builder.add_edge("llm_call", "synthesizer")
orchestrator_worker_builder.add_edge("synthesizer", END)

# Compile the workflow
orchestrator_worker = orchestrator_worker_builder.compile()

display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))

state = orchestrator_worker.invoke({"topic": "Create a report on LLM scaling laws"})

from IPython.display import Markdown
Markdown(state["final_report"])

## ​ Evaluator-optimizer

In evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.Evaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.!evaluator_optimizer.png

class State(TypedDict):
joke: str
topic: str
feedback: str
funny_or_not: str

# Schema for structured output to use in evaluation
class Feedback(BaseModel):
grade: Literal["funny", "not funny"] = Field(
description="Decide if the joke is funny or not.",
)
feedback: str = Field(
description="If the joke is not funny, provide feedback on how to improve it.",
)

evaluator = llm.with_structured_output(Feedback)

def llm_call_generator(state: State):
"""LLM generates a joke"""

if state.get("feedback"):
msg = llm.invoke(
f"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}"
)
else:
msg = llm.invoke(f"Write a joke about {state['topic']}")
return {"joke": msg.content}

def llm_call_evaluator(state: State):
"""LLM evaluates the joke"""

grade = evaluator.invoke(f"Grade the joke {state['joke']}")
return {"funny_or_not": grade.grade, "feedback": grade.feedback}

# Conditional edge function to route :
"""Route

if state["funny_or_not"] == "funny":
return "Accepted"
elif state["funny_or_not"] == "not funny":
return "Rejected + Feedback"

optimizer_builder = StateGraph(State)

optimizer_builder.add_node("llm_call_generator", llm_call_generator)
optimizer_builder.add_node("llm_call_evaluator", llm_call_evaluator)

optimizer_builder.add_edge(START, "llm_call_generator")
optimizer_builder.add_edge("llm_call_generator", "llm_call_evaluator")
optimizer_builder.add_conditional_edges(
"llm_call_evaluator",
route_joke,
{ # Name returned by route_joke : Name of next node to visit
"Accepted": END,
"Rejected + Feedback": "llm_call_generator",
},
)

optimizer_workflow = optimizer_builder.compile()

display(Image(optimizer_workflow.get_graph().draw_mermaid_png()))

state = optimizer_workflow.invoke({"topic": "Cats"})
print(state["joke"])

## ​ Agents

Agents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.!agent.png

To get started with agents, see the quickstart or read more about how they work in LangChain.

Using tools

from langchain.tools import tool

# Define tools
@tool

"""Multiply `a` and `b`.

Args:
a: First int
b: Second int
"""
return a * b

"""Adds `a` and `b`.

Args:
a: First int
b: Second int
"""
return a + b

"""Divide `a` and `b`.

Args:
a: First int
b: Second int
"""
return a / b

tools = [add, multiply, divide]
tools_by_name = {tool.name: tool for tool in tools}
llm_with_tools = llm.bind_tools(tools)

from langgraph.graph import MessagesState
from langchain.messages import SystemMessage, HumanMessage, ToolMessage

def llm_call(state: MessagesState):
"""LLM decides whether to call a tool or not"""

return {
"messages": [\
llm_with_tools.invoke(\
[\
SystemMessage(\
content="You are a helpful assistant tasked with performing arithmetic on a set of inputs."\
)\
]\
+ state["messages"]\
)\
]
}

def tool_node(state: dict):
"""Performs the tool call"""

result = []
for tool_call in state["messages"][-1].tool_calls:
tool = tools_by_name[tool_call["name"]]
observation = tool.invoke(tool_call["args"])
result.append(ToolMessage(content=observation, tool_call_id=tool_call["id"]))
return {"messages": result}

# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call

"""Decide if we should continue the loop or stop based upon whether the LLM made a tool call"""

messages = state["messages"]
last_message = messages[-1]

# If the LLM makes a tool call, then perform an action
if last_message.tool_calls:
return "tool_node"

# Otherwise, we stop (reply to the user)
return END

agent_builder = StateGraph(MessagesState)

agent_builder.add_node("llm_call", llm_call)
agent_builder.add_node("tool_node", tool_node)

agent_builder.add_edge(START, "llm_call")
agent_builder.add_conditional_edges(
"llm_call",
should_continue,
["tool_node", END]
)
agent_builder.add_edge("tool_node", "llm_call")

# Compile the agent
agent = agent_builder.compile()

# Show the agent
display(Image(agent.get_graph(xray=True).draw_mermaid_png()))

messages = [HumanMessage(content="Add 3 and 4.")]
messages = agent.invoke({"messages": messages})
for m in messages["messages"]:
m.pretty_print()

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Thinking in LangGraph\\
\\
Previous Persistence\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/persistence

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Persistence

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Threads
- Checkpoints
- Get state
- Get state history
- Replay
- Update state
- config
- values
- as\_node
- Memory Store
- Basic Usage
- Semantic Search
- Using in LangGraph
- Checkpointer libraries
- Checkpointer interface
- Serializer
- Serialization with pickle
- Encryption
- Capabilities
- Human-in-the-loop
- Memory
- Time Travel
- Fault-tolerance
- Pending writes

LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a `checkpoint` of the graph state at every super-step. Those checkpoints are saved to a `thread`, which can be accessed after graph execution. Because `threads` allow access to graph’s state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we’ll discuss each of these concepts in more detail.!Checkpoints

**Agent Server handles checkpointing automatically**
When using the Agent Server, you don’t need to implement or configure checkpointers manually. The server handles all persistence infrastructure for you behind the scenes.

## ​ Threads

A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread.When invoking a graph with a checkpointer, you **must** specify a `thread_id` as part of the `configurable` portion of the config:

Copy

{"configurable": {"thread_id": "1"}}

A thread’s current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the API reference for more details.The checkpointer uses `thread_id` as the primary key for storing and retrieving checkpoints. Without it, the checkpointer cannot save state or resume execution after an interrupt, since the checkpointer uses `thread_id` to load the saved state.

## ​ Checkpoints

The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by `StateSnapshot` object with the following key properties:

- `config`: Config associated with this checkpoint.
- `metadata`: Metadata associated with this checkpoint.
- `values`: Values of the state channels at this point in time.
- `next` A tuple of the node names to execute next in the graph.
- `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts.

Checkpoints are persisted and can be used to restore the state of a thread at a later time.Let’s see what checkpoints are saved when a simple graph is invoked as follows:

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.runnables import RunnableConfig
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
foo: str
bar: Annotated[list[str], add]

def node_a(state: State):
return {"foo": "a", "bar": ["a"]}

def node_b(state: State):
return {"foo": "b", "bar": ["b"]}

workflow = StateGraph(State)
workflow.add_node(node_a)
workflow.add_node(node_b)
workflow.add_edge(START, "node_a")
workflow.add_edge("node_a", "node_b")
workflow.add_edge("node_b", END)

checkpointer = InMemorySaver()
graph = workflow.compile(checkpointer=checkpointer)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}
graph.invoke({"foo": "", "bar":[]}, config)

After we run the graph, we expect to see exactly 4 checkpoints:

- Empty checkpoint with `START` as the next node to be executed
- Checkpoint with the user input `{'foo': '', 'bar': []}` and `node_a` as the next node to be executed
- Checkpoint with the outputs of `node_a``{'foo': 'a', 'bar': ['a']}` and `node_b` as the next node to be executed
- Checkpoint with the outputs of `node_b``{'foo': 'b', 'bar': ['a', 'b']}` and no next nodes to be executed

Note that we `bar` channel values contain outputs from both nodes as we have a reducer for `bar` channel.

### ​ Get state

When interacting with the saved graph state, you **must** specify a thread identifier. You can view the _latest_ state of the graph by calling `graph.get_state(config)`. This will return a `StateSnapshot` object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.

# get the latest state snapshot
config = {"configurable": {"thread_id": "1"}}
graph.get_state(config)

# get a state snapshot for a specific checkpoint_id
config = {"configurable": {"thread_id": "1", "checkpoint_id": "1ef663ba-28fe-6528-8002-5a559208592c"}}
graph.get_state(config)

In our example, the output of `get_state` will look like this:

StateSnapshot(
values={'foo': 'b', 'bar': ['a', 'b']},
next=(),
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},
metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},
created_at='2024-08-29T19:19:38.821749+00:00',
parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()
)

### ​ Get state history

You can get the full history of the graph execution for a given thread by calling `graph.get_state_history(config)`. This will return a list of `StateSnapshot` objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / `StateSnapshot` being the first in the list.

config = {"configurable": {"thread_id": "1"}}
list(graph.get_state_history(config))

In our example, the output of `get_state_history` will look like this:

[\
StateSnapshot(\
values={'foo': 'b', 'bar': ['a', 'b']},\
next=(),\
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\
metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\
created_at='2024-08-29T19:19:38.821749+00:00',\
parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\
tasks=(),\
),\
StateSnapshot(\
values={'foo': 'a', 'bar': ['a']},\
next=('node_b',),\
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\
metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},\
created_at='2024-08-29T19:19:38.819946+00:00',\
parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\
tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),\
),\
StateSnapshot(\
values={'foo': '', 'bar': []},\
next=('node_a',),\
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\
metadata={'source': 'loop', 'writes': None, 'step': 0},\
created_at='2024-08-29T19:19:38.817813+00:00',\
parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\
tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),\
),\
StateSnapshot(\
values={'bar': []},\
next=('__start__',),\
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\
metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},\
created_at='2024-08-29T19:19:38.816205+00:00',\
parent_config=None,\
tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),\
)\
]

### ​ Replay

It’s also possible to play-back a prior graph execution. If we `invoke` a graph with a `thread_id` and a `checkpoint_id`, then we will _re-play_ the previously executed steps _before_ a checkpoint that corresponds to the `checkpoint_id`, and only execute the steps _after_ the checkpoint.

- `thread_id` is the ID of a thread.
- `checkpoint_id` is an identifier that refers to a specific checkpoint within a thread.

You must pass these when invoking the graph as part of the `configurable` portion of the config:

config = {"configurable": {"thread_id": "1", "checkpoint_id": "0c62ca34-ac19-445d-bbb0-5b4984975b2a"}}
graph.invoke(None, config=config)

Importantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply _re-plays_ that particular step in the graph and does not re-execute the step, but only for the steps _before_ the provided `checkpoint_id`. All of the steps _after_`checkpoint_id` will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying.!Replay

### ​ Update state

In addition to re-playing the graph from specific `checkpoints`, we can also _edit_ the graph state. We do this using `update_state`. This method accepts three different arguments:

#### ​ `config`

The config should contain `thread_id` specifying which thread to update. When only the `thread_id` is passed, we update (or fork) the current state. Optionally, if we include `checkpoint_id` field, then we fork that selected checkpoint.

#### ​ `values`

These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that `update_state` does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let’s walk through an example.Let’s assume you have defined the state of your graph with the following schema (see full example above):

from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
foo: int
bar: Annotated[list[str], add]

Let’s now assume the current state of the graph is

{"foo": 1, "bar": ["a"]}

If you update the state as below:

graph.update_state(config, {"foo": 2, "bar": ["b"]})

Then the new state of the graph will be:

{"foo": 2, "bar": ["a", "b"]}

The `foo` key (channel) is completely changed (because there is no reducer specified for that channel, so `update_state` overwrites it). However, there is a reducer specified for the `bar` key, and so it appends `"b"` to the state of `bar`.

#### ​ `as_node`

The final thing you can optionally specify when calling `update_state` is `as_node`. If you provided it, the update will be applied as if it came from node `as_node`. If `as_node` is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state.!Update

## ​ Memory Store

**LangGraph API handles stores automatically**
When using the LangGraph API, you don’t need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.

### ​ Basic Usage

First, let’s showcase this in isolation without using LangGraph.

from langgraph.store.memory import InMemoryStore
in_memory_store = InMemoryStore()

user_id = "1"
namespace_for_memory = (user_id, "memories")

We use the `store.put` method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (`memory_id`) and the value (a dictionary) is the memory itself.

memory_id = str(uuid.uuid4())
memory = {"food_preference" : "I like pizza"}
in_memory_store.put(namespace_for_memory, memory_id, memory)

We can read out memories in our namespace using the `store.search` method, which will return all memories for a given user as a list. The most recent memory is the last in the list.

memories = in_memory_store.search(namespace_for_memory)
memories[-1].dict()
{'value': {'food_preference': 'I like pizza'},
'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',
'namespace': ['1', 'memories'],
'created_at': '2024-10-02T17:22:31.590602+00:00',
'updated_at': '2024-10-02T17:22:31.590605+00:00'}

Each memory type is a Python class ( `Item`) with certain attributes. We can access it as a dictionary by converting via `.dict` as above.The attributes it has are:

- `value`: The value (itself a dictionary) of this memory
- `key`: A unique key for this memory in this namespace
- `namespace`: A list of strings, the namespace of this memory type
- `created_at`: Timestamp for when this memory was created
- `updated_at`: Timestamp for when this memory was updated

### ​ Semantic Search

Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:

from langchain.embeddings import init_embeddings

store = InMemoryStore(
index={
"embed": init_embeddings("openai:text-embedding-3-small"), # Embedding provider
"dims": 1536, # Embedding dimensions
"fields": ["food_preference", "$"] # Fields to embed
}
)

Now when searching, you can use natural language queries to find relevant memories:

# Find memories about food preferences
# (This can be done after putting memories into the store)
memories = store.search(
namespace_for_memory,
query="What does the user like to eat?",
limit=3 # Return top 3 matches
)

You can control which parts of your memories get embedded by configuring the `fields` parameter or by specifying the `index` parameter when storing memories:

# Store with specific fields to embed
store.put(
namespace_for_memory,
str(uuid.uuid4()),
{
"food_preference": "I love Italian cuisine",
"context": "Discussing dinner plans"
},
index=["food_preference"] # Only embed "food_preferences" field
)

# Store without embedding (still retrievable, but not searchable)
store.put(
namespace_for_memory,
str(uuid.uuid4()),
{"system_info": "Last updated: 2024-01-01"},
index=False
)

### ​ Using in LangGraph

With this all in place, we use the `in_memory_store` in LangGraph. The `in_memory_store` works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the `in_memory_store` allows us to store arbitrary information for access _across_ threads. We compile the graph with both the checkpointer and the `in_memory_store` as follows.

from langgraph.checkpoint.memory import InMemorySaver

# We need this because we want to enable threads (conversations)
checkpointer = InMemorySaver()

# ... Define the graph ...

# Compile the graph with the checkpointer and store
graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)

We invoke the graph with a `thread_id`, as before, and also with a `user_id`, which we’ll use to namespace our memories to this particular user as we showed above.

# Invoke the graph
user_id = "1"
config = {"configurable": {"thread_id": "1", "user_id": user_id}}

# First let's just say hi to the AI
for update in graph.stream(
{"messages": [{"role": "user", "content": "hi"}]}, config, stream_mode="updates"
):
print(update)

We can access the `in_memory_store` and the `user_id` in _any node_ by passing `store: BaseStore` and `config: RunnableConfig` as node arguments. Here’s how we might use semantic search in a node to find relevant memories:

def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):

# Get the user id from the config
user_id = config["configurable"]["user_id"]

# Namespace the memory
namespace = (user_id, "memories")

# ... Analyze conversation and create a new memory

# Create a new memory ID
memory_id = str(uuid.uuid4())

# We create a new memory
store.put(namespace, memory_id, {"memory": memory})

As we showed above, we can also access the store in any node and use the `store.search` method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.

memories[-1].dict()
{'value': {'food_preference': 'I like pizza'},
'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',
'namespace': ['1', 'memories'],
'created_at': '2024-10-02T17:22:31.590602+00:00',
'updated_at': '2024-10-02T17:22:31.590605+00:00'}

We can access the memories and use them in our model call.

def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):

# Search based on the most recent message
memories = store.search(
namespace,
query=state["messages"][-1].content,
limit=3
)
info = "\n".join([d.value["memory"] for d in memories])

# ... Use memories in the model call

If we create a new thread, we can still access the same memories so long as the `user_id` is the same.

config = {"configurable": {"thread_id": "2", "user_id": "1"}}

# Let's say hi again
for update in graph.stream(
{"messages": [{"role": "user", "content": "hi, tell me about my memories"}]}, config, stream_mode="updates"
):
print(update)

When we use the LangSmith, either locally (e.g., in Studio) or hosted with LangSmith, the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you **do** need to configure the indexing settings in your `langgraph.json` file. For example:

{
...
"store": {
"index": {
"embed": "openai:text-embeddings-3-small",
"dims": 1536,
"fields": ["$"]
}
}
}

See the deployment guide for more details and configuration options.

## ​ Checkpointer libraries

Under the hood, checkpointing is powered by checkpointer objects that conform to `BaseCheckpointSaver` interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:

- `langgraph-checkpoint`: The base interface for checkpointer savers ( `BaseCheckpointSaver`) and serialization/deserialization interface ( `SerializerProtocol`). Includes in-memory checkpointer implementation ( `InMemorySaver`) for experimentation. LangGraph comes with `langgraph-checkpoint` included.
- `langgraph-checkpoint-sqlite`: An implementation of LangGraph checkpointer that uses SQLite database ( `SqliteSaver` / `AsyncSqliteSaver`). Ideal for experimentation and local workflows. Needs to be installed separately.
- `langgraph-checkpoint-postgres`: An advanced checkpointer that uses Postgres database ( `PostgresSaver` / `AsyncPostgresSaver`), used in LangSmith. Ideal for using in production. Needs to be installed separately.

### ​ Checkpointer interface

Each checkpointer conforms to `BaseCheckpointSaver` interface and implements the following methods:

- `.put` \- Store a checkpoint with its configuration and metadata.
- `.put_writes` \- Store intermediate writes linked to a checkpoint (i.e. pending writes).
- `.get_tuple` \- Fetch a checkpoint tuple using for a given configuration (`thread_id` and `checkpoint_id`). This is used to populate `StateSnapshot` in `graph.get_state()`.
- `.list` \- List checkpoints that match a given configuration and filter criteria. This is used to populate state history in `graph.get_state_history()`

If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via `.ainvoke`, `.astream`, `.abatch`), asynchronous versions of the above methods will be used (`.aput`, `.aput_writes`, `.aget_tuple`, `.alist`).

For running your graph asynchronously, you can use `InMemorySaver`, or async versions of Sqlite/Postgres checkpointers — `AsyncSqliteSaver` / `AsyncPostgresSaver` checkpointers.

### ​ Serializer

When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.`langgraph_checkpoint` defines protocol for implementing serializers provides a default implementation ( `JsonPlusSerializer`) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.

#### ​ Serialization with `pickle`

The default serializer, `JsonPlusSerializer`, uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.If you want to fall:

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer

graph.compile(
checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))
)

#### ​ Encryption

Checkpointers can optionally encrypt all persisted state. To enable this, pass an instance of `EncryptedSerializer` to the `serde` argument of any `BaseCheckpointSaver` implementation. The easiest way to create an encrypted serializer is via `from_pycryptodome_aes`, which reads the AES key from the `LANGGRAPH_AES_KEY` environment variable (or accepts a `key` argument):

import sqlite3

from langgraph.checkpoint.serde.encrypted import EncryptedSerializer
from langgraph.checkpoint.sqlite import SqliteSaver

serde = EncryptedSerializer.from_pycryptodome_aes() # reads LANGGRAPH_AES_KEY
checkpointer = SqliteSaver(sqlite3.connect("checkpoint.db"), serde=serde)

from langgraph.checkpoint.serde.encrypted import EncryptedSerializer
from langgraph.checkpoint.postgres import PostgresSaver

serde = EncryptedSerializer.from_pycryptodome_aes()
checkpointer = PostgresSaver.from_conn_string("postgresql://...", serde=serde)
checkpointer.setup()

When running on LangSmith, encryption is automatically enabled whenever `LANGGRAPH_AES_KEY` is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing `CipherProtocol` and supplying it to `EncryptedSerializer`.

## ​ Capabilities

### ​ Human-in-the-loop

First, checkpointers facilitate human-in-the-loop workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See the how-to guides for examples.

### ​ Memory

Second, checkpointers allow for “memory” between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See Add memory for information on how to add and manage conversation memory using checkpointers.

### ​ Time Travel

Third, checkpointers allow for “time travel”, allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.

### ​ Fault-tolerance

Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don’t re-run the successful nodes.

#### ​ Pending writes

Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don’t re-run the successful nodes.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Workflows and agents\\
\\
Previous Durable execution\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/durable-execution

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Durable execution

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Requirements
- Determinism and Consistent Replay
- Durability modes
- Using tasks in nodes
- Resuming Workflows
- Starting Points for Resuming Workflows

**Durable execution** is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require human-in-the-loop, where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps — even after a significant delay (e.g., a week later).LangGraph’s built-in persistence layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted — whether by a system failure or for human-in-the-loop interactions — it can be resumed from its last recorded state.

If you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.
To make the most of durable execution, ensure that your workflow is designed to be deterministic and idempotent and wrap any side effects or non-deterministic operations inside tasks. You can use tasks from both the StateGraph (Graph API) and the Functional API.

## ​ Requirements

To leverage durable execution in LangGraph, you need to:

1. Enable persistence in your workflow by specifying a checkpointer that will save workflow progress.
2. Specify a thread identifier when executing a workflow. This will track the execution history for a particular instance of the workflow.
3. Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside `task` to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see Determinism and Consistent Replay.

## ​ Determinism and Consistent Replay

When you resume a workflow run, the code does **NOT** resume from the **same line of code** where execution stopped; instead, it will identify an appropriate starting point from which to pick up where it left off. This means that the workflow will replay all steps from the starting point until it reaches the point where it was stopped.As a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside tasks or nodes.To ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:

- **Avoid Repeating Work**: If a node contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate **task**. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.
- **Encapsulate Non-Deterministic Operations:** Wrap any code that might yield non-deterministic results (e.g., random number generation) inside **tasks** or **nodes**. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.
- **Use Idempotent Operations**: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a **task** starts but fails to complete successfully, the workflow’s resumption will re-run the **task**, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.

For some examples of pitfalls to avoid, see the Common Pitfalls section in the functional API, which shows
how to structure your code using **tasks** to avoid these issues. The same principles apply to the StateGraph (Graph API).

## ​ Durability modes

LangGraph supports three durability modes that allow you to balance performance and data consistency based on your application’s requirements. A higher durability mode adds more overhead to the workflow execution. You can specify the durability mode when calling any graph execution method:

Copy

graph.stream(
{"input": "test"},
durability="sync"
)

The durability modes, from least to most durable, are as follows:

- `"exit"`: Changes are persisted only when graph execution exits (either successfully, with an error, or due to an interrupt). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from system failures (e.g., process crashes) that occur mid-execution.
- `"async"`: Changes are persisted asynchronously while the next step executes. This provides good performance and durability, but there’s a small risk that checkpoints might not be written if the process crashes during execution.
- `"sync"`: Changes are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.

## ​ Using tasks in nodes

If a node contains multiple operations, you may find it easier to convert each operation into a **task** rather than refactor the operations into individual nodes.

- Original

- With task

from typing import NotRequired
from typing_extensions import TypedDict
import uuid

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph, START, END
import requests

# Define a TypedDict to represent the state
class State(TypedDict):
url: str
result: NotRequired[str]

def call_api(state: State):
"""Example node that makes an API request."""
result = requests.get(state['url']).text[:100] # Side-effect #
return {
"result": result
}

# Create a StateGraph builder and add a node for the call_api function
builder = StateGraph(State)
builder.add_node("call_api", call_api)

# Connect the start and end nodes to the call_api node
builder.add_edge(START, "call_api")
builder.add_edge("call_api", END)

# Specify a checkpointer
checkpointer = InMemorySaver()

# Compile the graph with the checkpointer
graph = builder.compile(checkpointer=checkpointer)

# Define a config with a thread ID.
thread_id = uuid.uuid4()
config = {"configurable": {"thread_id": thread_id}}

# Invoke the graph
graph.invoke({"url": "https://www.example.com"}, config)

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.func import task
from langgraph.graph import StateGraph, START, END
import requests

class State(TypedDict):
urls: list[str]
result: NotRequired[list[str]]

@task
def _make_request(url: str):
"""Make a request."""
return requests.get(url).text[:100]

def call_api(state: State):
"""Example node that makes an API request."""
requests = [_make_request(url) for url in state['urls']]
results = [request.result() for request in requests]
return {
"results": results
}

graph.invoke({"urls": ["https://www.example.com"]}, config)

## ​ Resuming Workflows

Once you have enabled durable execution in your workflow, you can resume execution for the following scenarios:

- **Pausing and Resuming Workflows:** Use the interrupt function to pause a workflow at specific points and the `Command` primitive to resume it with updated state. See **Interrupts** for more details.
- **Recovering from Failures:** Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a `None` as the input value (see this example with the functional API).

## ​ Starting Points for Resuming Workflows

- If you’re using a StateGraph (Graph API), the starting point is the beginning of the **node** where execution stopped.
- If you’re making a subgraph call inside a node, the starting point will be the **parent** node that called the subgraph that was halted.
Inside the subgraph, the starting point will be the specific **node** where execution stopped.
- If you’re using the Functional API, the starting point is the beginning of the **entrypoint** where execution stopped.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Persistence\\
\\
Previous Streaming\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/streaming

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Streaming

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Supported stream modes
- Basic usage example
- Stream multiple modes
- Stream graph state
- Stream subgraph outputs
- Debugging
- LLM tokens
- Filter by LLM invocation
- Filter by node
- Stream custom data
- Use with any LLM
- Disable streaming for specific chat models
- Async with Python < 3.11

LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.What’s possible with LangGraph streaming:

- **Stream graph state** — get state updates / values with `updates` and `values` modes.
- **Stream subgraph outputs** — include outputs from both the parent graph and any nested subgraphs.
- **Stream LLM tokens** — capture token streams from anywhere: inside nodes, subgraphs, or tools.
- **Stream custom data** — send custom updates or progress signals directly from tool functions.
- **Use multiple streaming modes** — choose from `values` (full state), `updates` (state deltas), `messages` (LLM tokens + metadata), `custom` (arbitrary user data), or `debug` (detailed traces).

## ​ Supported stream modes

Pass one or more of the following stream modes as a list to the `stream` or `astream` methods:

| Mode | Description |
| --- | --- |
| `values` | Streams the full value of the state after each step of the graph. |
| `updates` | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |
| `custom` | Streams custom data from inside your graph nodes. |
| `messages` | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked. |
| `debug` | Streams as much information as possible throughout the execution of the graph. |

## ​ Basic usage example

LangGraph graphs expose the `stream` (sync) and `astream` (async) methods to yield streamed outputs as iterators.

Copy

for chunk in graph.stream(inputs, stream_mode="updates"):
print(chunk)

Extended example: streaming updates

from typing import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
topic: str
joke: str

def refine_topic(state: State):
return {"topic": state["topic"] + " and cats"}

def generate_joke(state: State):
return {"joke": f"This is a joke about {state['topic']}"}

graph = (
StateGraph(State)
.add_node(refine_topic)
.add_node(generate_joke)
.add_edge(START, "refine_topic")
.add_edge("refine_topic", "generate_joke")
.add_edge("generate_joke", END)
.compile()
)

# The stream() method returns an iterator that yields streamed outputs
for chunk in graph.stream(
{"topic": "ice cream"},
# Set stream_mode="updates" to stream only the updates to the graph state after each node
# Other stream modes are also available. See supported stream modes for details
stream_mode="updates",
):
print(chunk)

{'refineTopic': {'topic': 'ice cream and cats'}}
{'generateJoke': {'joke': 'This is a joke about ice cream and cats'}}

## ​ Stream multiple modes

You can pass a list as the `stream_mode` parameter to stream multiple modes at once.The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

for mode, chunk in graph.stream(inputs, stream_mode=["updates", "custom"]):
print(chunk)

## ​ Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

- `updates` streams the **updates** to the state after each step of the graph.
- `values` streams the **full value** of the state after each step of the graph.

- updates

- values

Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.

for chunk in graph.stream(
{"topic": "ice cream"},
stream_mode="updates",
):
print(chunk)

Use this to stream the **full state** of the graph after each step.

for chunk in graph.stream(
{"topic": "ice cream"},
stream_mode="values",
):
print(chunk)

## ​ Stream subgraph outputs

for chunk in graph.stream(
{"foo": "foo"},
# Set subgraphs=True to stream outputs from subgraphs
subgraphs=True,
stream_mode="updates",
):
print(chunk)

Extended example: streaming from subgraphs

from langgraph.graph import START, StateGraph
from typing import TypedDict

# Define subgraph
class SubgraphState(TypedDict):
foo: str # note that this key is shared with the parent graph state
bar: str

def subgraph_node_1(state: SubgraphState):
return {"bar": "bar"}

def subgraph_node_2(state: SubgraphState):
return {"foo": state["foo"] + state["bar"]}

subgraph_builder = StateGraph(SubgraphState)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_node(subgraph_node_2)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2")
subgraph = subgraph_builder.compile()

# Define parent graph
class ParentState(TypedDict):
foo: str

def node_1(state: ParentState):
return {"foo": "hi! " + state["foo"]}

builder = StateGraph(ParentState)
builder.add_node("node_1", node_1)
builder.add_node("node_2", subgraph)
builder.add_edge(START, "node_1")
builder.add_edge("node_1", "node_2")
graph = builder.compile()

for chunk in graph.stream(
{"foo": "foo"},
stream_mode="updates",
subgraphs=True,
):
print(chunk)

((), {'node_1': {'foo': 'hi! foo'}})
(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}})
(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}})
((), {'node_2': {'foo': 'hi! foobar'}})

**Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.

### ​ Debugging

Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.

for chunk in graph.stream(
{"topic": "ice cream"},
stream_mode="debug",
):
print(chunk)

## ​ LLM tokens

Use the `messages` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.The streamed output from `messages` mode is a tuple `(message_chunk, metadata)` where:

- `message_chunk`: the token or message segment from the LLM.

**Manual config required for async in Python < 3.11**
When using Python < 3.11 with async code, you must explicitly pass `RunnableConfig` to `ainvoke()` to enable proper streaming. See Async with Python < 3.11 for details or upgrade to Python 3.11+.

from dataclasses import dataclass

from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START

@dataclass
class MyState:
topic: str
joke: str = ""

model = init_chat_model(model="gpt-4o-mini")

def call_model(state: MyState):
"""Call the LLM to generate a joke about a topic"""
# Note that message events are emitted even when the LLM is run using .invoke rather than .stream
model_response = model.invoke(
[\
{"role": "user", "content": f"Generate a joke about {state.topic}"}\
]
)
return {"joke": model_response.content}

graph = (
StateGraph(MyState)
.add_node(call_model)
.add_edge(START, "call_model")
.compile()
)

# The "messages" stream mode returns an iterator of tuples (message_chunk, metadata)
# where message_chunk is the token streamed by the LLM and metadata is a dictionary
# with information about the graph node where the LLM was called and other information
for message_chunk, metadata in graph.stream(
{"topic": "ice cream"},
stream_mode="messages",
):
if message_chunk.content:
print(message_chunk.content, end="|", flush=True)

#### ​ Filter by LLM invocation

You can associate `tags` with LLM invocations to filter the streamed tokens by LLM invocation.

from langchain.chat_models import init_chat_model

# model_1 is tagged with "joke"
model_1 = init_chat_model(model="gpt-4o-mini", tags=['joke'])
# model_2 is tagged with "poem"
model_2 = init_chat_model(model="gpt-4o-mini", tags=['poem'])

graph = ... # define a graph that uses these LLMs

# The stream_mode is set to "messages" to stream LLM tokens
# The metadata contains information about the LLM invocation, including the tags
async for msg, metadata in graph.astream(
{"topic": "cats"},
stream_mode="messages",
):
# Filter the streamed tokens by the tags field in the metadata to only include
# the tokens from the LLM invocation with the "joke" tag
if metadata["tags"] == ["joke"]:
print(msg.content, end="|", flush=True)

Extended example: filtering by tags

from typing import TypedDict

from langchain.chat_models import init_chat_model
from langgraph.graph import START, StateGraph

# The joke_model is tagged with "joke"
joke_model = init_chat_model(model="gpt-4o-mini", tags=["joke"])
# The poem_model is tagged with "poem"
poem_model = init_chat_model(model="gpt-4o-mini", tags=["poem"])

class State(TypedDict):
topic: str
joke: str
poem: str

async def call_model(state, config):
topic = state["topic"]
print("Writing joke...")
# Note: Passing the config through explicitly is required for python < 3.11
# Since context var support wasn't added before then:
# The config is passed through explicitly to ensure the context vars are propagated correctly
# This is required for Python < 3.11 when using async code. Please see the async section for more details
joke_response = await joke_model.ainvoke(
[{"role": "user", "content": f"Write a joke about {topic}"}],
config,
)
print("\n\nWriting poem...")
poem_response = await poem_model.ainvoke(
[{"role": "user", "content": f"Write a short poem about {topic}"}],
config,
)
return {"joke": joke_response.content, "poem": poem_response.content}

graph = (
StateGraph(State)
.add_node(call_model)
.add_edge(START, "call_model")
.compile()
)

async for msg, metadata in graph.astream(
{"topic": "cats"},
stream_mode="messages",
):
if metadata["tags"] == ["joke"]:
print(msg.content, end="|", flush=True)

#### ​ Filter by node

To stream tokens only from specific nodes, use `stream_mode="messages"` and filter the outputs by the `langgraph_node` field in the streamed metadata:

# The "messages" stream mode returns a tuple of (message_chunk, metadata)
for msg, metadata in graph.stream(
inputs,
stream_mode="messages",
):
# Filter the streamed tokens by the langgraph_node field in the metadata
# to only include the tokens from the specified node
if msg.content and metadata["langgraph_node"] == "some_node_name":
...

Extended example: streaming LLM tokens from specific nodes

from typing import TypedDict
from langgraph.graph import START, StateGraph
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini")

def write_joke(state: State):
topic = state["topic"]
joke_response = model.invoke(
[{"role": "user", "content": f"Write a joke about {topic}"}]
)
return {"joke": joke_response.content}

def write_poem(state: State):
topic = state["topic"]
poem_response = model.invoke(
[{"role": "user", "content": f"Write a short poem about {topic}"}]
)
return {"poem": poem_response.content}

graph = (
StateGraph(State)
.add_node(write_joke)
.add_node(write_poem)
# write both the joke and the poem concurrently
.add_edge(START, "write_joke")
.add_edge(START, "write_poem")
.compile()
)

for msg, metadata in graph.stream(
{"topic": "cats"},
stream_mode="messages",
):
# to only include the tokens from the write_poem node
if msg.content and metadata["langgraph_node"] == "write_poem":
print(msg.content, end="|", flush=True)

## ​ Stream custom data

To send **custom user-defined data** from inside a LangGraph node or tool, follow these steps:

1. Use `get_stream_writer` to access the stream writer and emit custom data.
2. Set `stream_mode="custom"` when calling `.stream()` or `.astream()` to get the custom data in the stream. You can combine multiple modes (e.g., `["updates", "custom"]`), but at least one must be `"custom"`.

**No `get_stream_writer` in async for Python < 3.11**
In async code running on Python < 3.11, `get_stream_writer` will not work.
Instead, add a `writer` parameter to your node or tool and pass it manually.
See Async with Python < 3.11 for usage examples.

- node

- tool

from typing import TypedDict
from langgraph.config import get_stream_writer
from langgraph.graph import StateGraph, START

class State(TypedDict):
query: str
answer: str

def node(state: State):
# Get the stream writer to send custom data
writer = get_stream_writer()
# Emit a custom key-value pair (e.g., progress update)
writer({"custom_key": "Generating custom data inside node"})
return {"answer": "some data"}

graph = (
StateGraph(State)
.add_node(node)
.add_edge(START, "node")
.compile()
)

inputs = {"query": "example"}

# Set stream_mode="custom" to receive the custom data in the stream
for chunk in graph.stream(inputs, stream_mode="custom"):
print(chunk)

from langchain.tools import tool
from langgraph.config import get_stream_writer

@tool

"""Query the database."""
# Access the stream writer to send custom data
writer = get_stream_writer()
writer({"data": "Retrieved 0/100 records", "type": "progress"})
# perform query
# Emit another custom key-value pair
writer({"data": "Retrieved 100/100 records", "type": "progress"})
return "some-answer"

graph = ... # define a graph that uses this tool

## ​ Use with any LLM

You can use `stream_mode="custom"` to stream data from **any LLM API** — even if that API does **not** implement the LangChain chat model interface.This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.

from langgraph.config import get_stream_writer

def call_arbitrary_model(state):
"""Example node that calls an arbitrary model and streams the output"""
# Assume you have a streaming client that yields chunks
# Generate LLM tokens using your custom streaming client
writer = get_stream_writer()
for chunk in your_custom_streaming_client(state["topic"]):
# Use the writer to send custom data to the stream
writer({"custom_llm_chunk": chunk})
return {"result": "completed"}

graph = (
StateGraph(State)
.add_node(call_arbitrary_model)
# Add other nodes and edges as needed
.compile()
)
for chunk in graph.stream(
{"topic": "cats"},
stream_mode="custom",

):
# The chunk will contain the custom data streamed from the llm
print(chunk)

Extended example: streaming arbitrary chat model

import operator
import json

from typing import TypedDict
from typing_extensions import Annotated
from langgraph.graph import StateGraph, START

from openai import AsyncOpenAI

openai_client = AsyncOpenAI()
model_name = "gpt-4o-mini"

async def stream_tokens(model_name: str, messages: list[dict]):
response = await openai_client.chat.completions.create(
messages=messages, model=model_name, stream=True
)
role = None
async for chunk in response:
delta = chunk.choices[0].delta

if delta.role is not None:
role = delta.role

if delta.content:
yield {"role": role, "content": delta.content}

# this is our tool

"""Use this tool to list items one might find in a place you're asked about."""
writer = get_stream_writer()
response = ""
async for msg_chunk in stream_tokens(
model_name,
[\
{\
"role": "user",\
"content": (\
"Can you tell me what kind of items "\
f"i might find in the following place: '{place}'. "\
"List at least 3 such items separating them by a comma. "\
"And include a brief description of each item."\
),\
}\
],
):
response += msg_chunk["content"]
writer(msg_chunk)

return response

class State(TypedDict):
messages: Annotated[list[dict], operator.add]

# this is the tool-calling graph node
async def call_tool(state: State):
ai_message = state["messages"][-1]
tool_call = ai_message["tool_calls"][-1]

function_name = tool_call["function"]["name"]
if function_name != "get_items":
raise ValueError(f"Tool {function_name} not supported")

function_arguments = tool_call["function"]["arguments"]
arguments = json.loads(function_arguments)

function_response = await get_items(**arguments)
tool_message = {
"tool_call_id": tool_call["id"],
"role": "tool",
"name": function_name,
"content": function_response,
}
return {"messages": [tool_message]}

graph = (
StateGraph(State)
.add_node(call_tool)
.add_edge(START, "call_tool")
.compile()
)

Let’s invoke the graph with an `AIMessage` that includes a tool call:

inputs = {
"messages": [\
{\
"content": None,\
"role": "assistant",\
"tool_calls": [\
{\
"id": "1",\
"function": {\
"arguments": '{"place":"bedroom"}',\
"name": "get_items",\
},\
"type": "function",\
}\
],\
}\
]
}

async for chunk in graph.astream(
inputs,
stream_mode="custom",
):
print(chunk["content"], end="|", flush=True)

## ​ Disable streaming for specific chat models

If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for
models that do not support it.Set `streaming=False` when initializing the model.

- init\_chat\_model

- Chat model interface

model = init_chat_model(
"claude-sonnet-4-5-20250929",
# Set streaming=False to disable streaming for the chat model
streaming=False
)

from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="o1-preview", streaming=False)

Not all chat model integrations support the `streaming` parameter. If your model doesn’t support it, use `disable_streaming=True` instead. This parameter is available on all chat models via the base class.

### ​ Async with Python < 3.11

In Python versions < 3.11, asyncio tasks do not support the `context` parameter.
This limits LangGraph ability to automatically propagate context, and affects LangGraph’s streaming mechanisms in two key ways:

1. You **must** explicitly pass `RunnableConfig` into async LLM calls (e.g., `ainvoke()`), as callbacks are not automatically propagated.
2. You **cannot** use `get_stream_writer` in async nodes or tools — you must pass a `writer` argument directly.

Extended example: async LLM call with manual config

from typing import TypedDict
from langgraph.graph import START, StateGraph
from langchain.chat_models import init_chat_model

# Accept config as an argument in the async node function
async def call_model(state, config):
topic = state["topic"]
print("Generating joke...")
# Pass config to model.ainvoke() to ensure proper context propagation
joke_response = await model.ainvoke(
[{"role": "user", "content": f"Write a joke about {topic}"}],
config,
)
return {"joke": joke_response.content}

# Set stream_mode="messages" to stream LLM tokens
async for chunk, metadata in graph.astream(
{"topic": "ice cream"},
stream_mode="messages",
):
if chunk.content:
print(chunk.content, end="|", flush=True)

Extended example: async custom streaming with stream writer

from typing import TypedDict
from langgraph.types import StreamWriter

# Add writer as an argument in the function signature of the async node or tool
# LangGraph will automatically pass the stream writer to the function
async def generate_joke(state: State, writer: StreamWriter):
writer({"custom_key": "Streaming custom data while generating a joke"})
return {"joke": f"This is a joke about {state['topic']}"}

graph = (
StateGraph(State)
.add_node(generate_joke)
.add_edge(START, "generate_joke")
.compile()
)

# Set stream_mode="custom" to receive the custom data in the stream #
async for chunk in graph.astream(
{"topic": "ice cream"},
stream_mode="custom",
):
print(chunk)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Durable execution\\
\\
Previous Interrupts\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/interrupts

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Interrupts

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Pause using interrupt
- Resuming interrupts
- Common patterns
- Approve or reject
- Review and edit state
- Interrupts in tools
- Validating human input
- Rules of interrupts
- Do not wrap interrupt calls in try/except
- Do not reorder interrupt calls within a node
- Do not return complex values in interrupt calls
- Side effects called before interrupt must be idempotent
- Using with subgraphs called as functions
- Debugging with interrupts
- Using LangGraph Studio

Interrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its persistence layer and waits indefinitely until you resume execution.Interrupts work by calling the `interrupt()` function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you’re ready to continue, you resume execution by re-invoking the graph using `Command`, which then becomes the return value of the `interrupt()` call from inside the node.Unlike static breakpoints (which pause before or after specific nodes), interrupts are **dynamic**—they can be placed anywhere in your code and can be conditional based on your application logic.

- **Checkpointing keeps your place:** the checkpointer writes the exact graph state so you can resume later, even when in an error state.
- **`thread_id` is your pointer:** set `config={"configurable": {"thread_id": ...}}` to tell the checkpointer which state to load.
- **Interrupt payloads surface as `__interrupt__`:** the values you pass to `interrupt()` Pause using `interrupt`

The `interrupt` function pauses graph execution and returns a value to the caller. When you call `interrupt` within a node, LangGraph saves the current graph state and waits for you to resume execution with input.To use `interrupt`, you need:

1. A **checkpointer** to persist the graph state (use a durable checkpointer in production)
2. A **thread ID** in your config so the runtime knows which state to resume from
3. To call `interrupt()` where you want to pause (payload must be JSON-serializable)

Copy

from langgraph.types import interrupt

def approval_node(state: State):
# Pause and ask for approval
approved = interrupt("Do you approve this action?")

# When you resume, Command(resume=...) returns that value here
return {"approved": approved}

When you call `interrupt`, here’s what happens:

1. **Graph execution gets suspended** at the exact point where `interrupt` is called
2. **State is saved** using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)
3. **Value is returned** to the caller under `__interrupt__`; it can be any JSON-serializable value (string, object, array, etc.)
4. **Graph waits indefinitely** until you resume execution with a response
5. **Response is passed back** into the node when you resume, becoming the return value of the `interrupt()` call

## ​ Resuming interrupts

After an interrupt pauses execution, you resume the graph by invoking it again with a `Command` that contains the resume value. The resume value is passed

# Resume with the human's response
# The resume payload becomes the return value of interrupt() inside the node
graph.invoke(Command(resume=True), config=config)

**Key points about resuming:**

- You must use the **same thread ID** when resuming that was used when the interrupt occurred
- The value passed to `Command(resume=...)` becomes the return value of the `interrupt` call
- The node restarts from the beginning of the node where the `interrupt` was called when resumed, so any code before the `interrupt` runs again
- You can pass any JSON-serializable value as the resume value

## ​ Common patterns

The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:

- Approval workflows: Pause before executing critical actions (API calls, database changes, financial transactions)
- Review and edit: Let humans review and modify LLM outputs or tool calls before continuing
- Interrupting tool calls: Pause before executing tool calls to review and edit the tool call before execution
- Validating human input: Pause before proceeding to the next step to validate human input

### ​ Approve or reject

One of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.

from typing import Literal
from langgraph.types import interrupt, Command

# Pause execution; payload shows up under result["__interrupt__"]
is_approved = interrupt({
"question": "Do you want to proceed with this action?",
"details": state["action_details"]
})

# Route based on the response
if is_approved:
return Command(goto="proceed") # Runs after the resume payload is provided
else:
return Command(goto="cancel")

When you resume the graph, pass `true` to approve or `false` to reject:

# To approve

# To reject
graph.invoke(Command(resume=False), config=config)

Full example

from typing import Literal, Optional, TypedDict

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command, interrupt

class ApprovalState(TypedDict):
action_details: str
status: Optional[Literal["pending", "approved", "rejected"]]

# Expose details so the caller can render them in a UI
decision = interrupt({
"question": "Approve this action?",
"details": state["action_details"],
})

# Route to the appropriate node after resume
return Command(goto="proceed" if decision else "cancel")

def proceed_node(state: ApprovalState):
return {"status": "approved"}

def cancel_node(state: ApprovalState):
return {"status": "rejected"}

builder = StateGraph(ApprovalState)
builder.add_node("approval", approval_node)
builder.add_node("proceed", proceed_node)
builder.add_node("cancel", cancel_node)
builder.add_edge(START, "approval")
builder.add_edge("proceed", END)
builder.add_edge("cancel", END)

# Use a more durable checkpointer in production
checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "approval-123"}}
initial = graph.invoke(
{"action_details": "Transfer $500", "status": "pending"},
config=config,
)

# Resume with the decision; True routes to proceed, False to cancel
resumed = graph.invoke(Command(resume=True), config=config)

### ​ Review and edit state

Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.

def review_node(state: State):
# Pause and show the current content for review (surfaces in result["__interrupt__"])
edited_content = interrupt({
"instruction": "Review and edit this content",
"content": state["generated_text"]
})

# Update the state with the edited version
return {"generated_text": edited_content}

When resuming, provide the edited content:

graph.invoke(
Command(resume="The edited and improved text"), # Value becomes the return from interrupt()
config=config
)

import sqlite3
from typing import TypedDict

class ReviewState(TypedDict):
generated_text: str

def review_node(state: ReviewState):
# Ask a reviewer to edit the generated content
updated = interrupt({
"instruction": "Review and edit this content",
"content": state["generated_text"],
})
return {"generated_text": updated}

builder = StateGraph(ReviewState)
builder.add_node("review", review_node)
builder.add_edge(START, "review")
builder.add_edge("review", END)

config = {"configurable": {"thread_id": "review-42"}}
initial = graph.invoke({"generated_text": "Initial draft"}, config=config)

# Resume with the edited text from the reviewer
final_state = graph.invoke(
Command(resume="Improved draft after review"),
config=config,
)

### ​ Interrupts in tools

You can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it’s called, and allows for human review and editing of the tool call before it is executed.First, define a tool that uses `interrupt`:

from langchain.tools import tool
from langgraph.types import interrupt

@tool
def send_email(to: str, subject: str, body: str):
"""Send an email to a recipient."""

# Pause before sending; payload surfaces in result["__interrupt__"]
response = interrupt({
"action": "send_email",
"to": to,
"subject": subject,
"body": body,
"message": "Approve sending this email?"
})

if response.get("action") == "approve":
# Resume value can override inputs before executing
final_to = response.get("to", to)
final_subject = response.get("subject", subject)
final_body = response.get("body", body)
return f"Email sent to {final_to} with subject '{final_subject}'"
return "Email cancelled by user"

This approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.

from langchain.tools import tool
from langchain_anthropic import ChatAnthropic
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command, interrupt

class AgentState(TypedDict):
messages: list[dict]

response = interrupt({
"action": "send_email",
"to": to,
"subject": subject,
"body": body,
"message": "Approve sending this email?",
})

if response.get("action") == "approve":
final_to = response.get("to", to)
final_subject = response.get("subject", subject)
final_body = response.get("body", body)

# Actually send the email (your implementation here)
print(f"[send_email] to={final_to} subject={final_subject} body={final_body}")
return f"Email sent to {final_to}"

return "Email cancelled by user"

model = ChatAnthropic(model="claude-sonnet-4-5-20250929").bind_tools([send_email])

def agent_node(state: AgentState):
# LLM may decide to call the tool; interrupt pauses before sending
result = model.invoke(state["messages"])
return {"messages": state["messages"] + [result]}

builder = StateGraph(AgentState)
builder.add_node("agent", agent_node)
builder.add_edge(START, "agent")
builder.add_edge("agent", END)

checkpointer = SqliteSaver(sqlite3.connect("tool-approval.db"))
graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "email-workflow"}}
initial = graph.invoke(
{
"messages": [\
{"role": "user", "content": "Send an email to alice@example.com about the meeting"}\
]
},
config=config,
)

# Resume with approval and optionally edited arguments
resumed = graph.invoke(
Command(resume={"action": "approve", "subject": "Updated subject"}),
config=config,
)

### ​ Validating human input

Sometimes you need to validate input from humans and ask again if it’s invalid. You can do this using multiple `interrupt` calls in a loop.

def get_age_node(state: State):
prompt = "What is your age?"

while True:
answer = interrupt(prompt) # payload surfaces in result["__interrupt__"]

# Validate the input

# Valid input - continue
break
else:
# Invalid input - ask again with a more specific prompt
prompt = f"'{answer}' is not a valid age. Please enter a positive number."

return {"age": answer}

Each time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.

from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command, interrupt

class FormState(TypedDict):
age: int | None

def get_age_node(state: FormState):
prompt = "What is your age?"

builder = StateGraph(FormState)
builder.add_node("collect_age", get_age_node)
builder.add_edge(START, "collect_age")
builder.add_edge("collect_age", END)

checkpointer = SqliteSaver(sqlite3.connect("forms.db"))
graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "form-1"}}
first = graph.invoke({"age": None}, config=config)

# Provide invalid data; the node re-prompts
retry = graph.invoke(Command(resume="thirty"), config=config)

# Provide valid data; loop exits and state updates
final = graph.invoke(Command(resume=30), config=config)

## ​ Rules of interrupts

When you call `interrupt` within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input.When execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning—it does not resume from the exact line where `interrupt` was called. This means any code that ran before the `interrupt` will execute again. Because of this, there’s a few important rules to follow when working with interrupts to ensure they behave as expected.

### ​ Do not wrap `interrupt` calls in try/except

The way that `interrupt` pauses execution at the point of the call is by throwing a special exception. If you wrap the `interrupt` call in a try/except block, you will catch this exception and the interrupt will not be passed calls from error-prone code
- ✅ Use specific exception types in try/except blocks

Separating logic

Explicit exception handling

def node_a(state: State):
# ✅ Good: interrupting first, then handling
# error conditions separately
interrupt("What's your name?")
try:
fetch_data() # This can fail
except Exception as e:
print(e)
return state

- 🔴 Do not wrap `interrupt` calls in bare try/except blocks

# ❌ Bad: wrapping interrupt in bare try/except
# will catch the interrupt exception
def node_a(state: State):
try:
interrupt("What's your name?")
except Exception as e:
print(e)
return state

### ​ Do not reorder `interrupt` calls within a node

It’s common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.When a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task’s resume list. Matching is **strictly index-based**, so the order of interrupt calls within the node is important.

- ✅ Keep `interrupt` calls consistent across node executions

# ✅ Good: interrupt calls happen in the same order every time
def node_a(state: State):
name = interrupt("What's your name?")
age = interrupt("What's your age?")
city = interrupt("What's your city?")

return {
"name": name,
"age": age,
"city": city
}

- 🔴 Do not conditionally skip `interrupt` calls within a node
- 🔴 Do not loop `interrupt` calls using logic that isn’t deterministic across executions

Skipping interrupts

Looping interrupts

# ❌ Bad: conditionally skipping interrupts changes the order
def node_a(state: State):
name = interrupt("What's your name?")

# On first run, this might skip the interrupt
# On resume, it might not skip it - causing index mismatch
if state.get("needs_age"):
age = interrupt("What's your age?")

city = interrupt("What's your city?")

return {"name": name, "city": city}

### ​ Do not return complex values in `interrupt` calls

Depending on which checkpointer is used, complex values may not be serializable (e.g. you can’t serialize a function). To make your graphs adaptable to any deployment, it’s best practice to only use values that can be reasonably serialized.

- ✅ Pass simple, JSON-serializable types to `interrupt`
- ✅ Pass dictionaries/objects with simple values

Simple values

Structured data

# ✅ Good: passing simple types that are serializable
def node_a(state: State):
name = interrupt("What's your name?")
count = interrupt(42)
approved = interrupt(True)

return {"name": name, "count": count, "approved": approved}

- 🔴 Do not pass functions, class instances, or other complex objects to `interrupt`

Functions

Class instances

def validate_input(value):

# ❌ Bad: passing a function to interrupt
# The function cannot be serialized
def node_a(state: State):
response = interrupt({
"question": "What's your name?",
"validator": validate_input # This will fail
})
return {"name": response}

### ​ Side effects called before `interrupt` must be idempotent

Because interrupts work by re-running the nodes they were called from, side effects called before `interrupt` should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution.As an example, you might have an API call to update a record inside of a node. If `interrupt` is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records.

- ✅ Use idempotent operations before `interrupt`
- ✅ Place side effects after `interrupt` calls
- ✅ Separate side effects into separate nodes when possible

Idempotent operations

Side effects after interrupt

Separating into different nodes

# ✅ Good: using upsert operation which is idempotent
# Running this multiple times will have the same result
def node_a(state: State):
db.upsert_user(
user_id=state["user_id"],
status="pending_approval"
)

approved = interrupt("Approve this change?")

- 🔴 Do not perform non-idempotent operations before `interrupt`
- 🔴 Do not create new records without checking if they exist

Creating records

Appending to lists

# ❌ Bad: creating a new record before interrupt
# This will create duplicate records on each resume
def node_a(state: State):
audit_id = db.create_audit_log({
"user_id": state["user_id"],
"action": "pending_approval",
"timestamp": datetime.now()
})

return {"approved": approved, "audit_id": audit_id}

## ​ Using with subgraphs called as functions

When invoking a subgraph within a node, the parent graph will resume execution from the **beginning of the node** where the subgraph was invoked and the `interrupt` was triggered. Similarly, the **subgraph** will also resume from the beginning of the node where `interrupt` was called.

def node_in_parent_graph(state: State):
some_code() # <-- This will re-execute when resumed
# Invoke a subgraph as a function.
# The subgraph contains an `interrupt` call.
subgraph_result = subgraph.invoke(some_input)
# ...

def node_in_subgraph(state: State):
some_other_code() # <-- This will also re-execute when resumed
result = interrupt("What's your name?")

## ​ Debugging with interrupts

To debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying `interrupt_before` and `interrupt_after` when compiling the graph.

Static interrupts are **not** recommended for human-in-the-loop workflows. Use the `interrupt` function instead.

- At compile time

- At run time

graph = builder.compile(
interrupt_before=["node_a"],
interrupt_after=["node_b", "node_c"],
checkpointer=checkpointer,
)

# Pass a thread ID to the graph
config = {
"configurable": {
"thread_id": "some_thread"
}
}

# Run the graph until the breakpoint
graph.invoke(inputs, config=config)

# Resume the graph
graph.invoke(None, config=config)

1. The breakpoints are set during `compile` time.
2. `interrupt_before` specifies the nodes where execution should pause before the node is executed.
3. `interrupt_after` specifies the nodes where execution should pause after the node is executed.
4. A checkpointer is required to enable breakpoints.
5. The graph is run until the first breakpoint is hit.
6. The graph is resumed by passing in `None` for the input. This will run the graph until the next breakpoint is hit.

graph.invoke(
inputs,
interrupt_before=["node_a"],
interrupt_after=["node_b", "node_c"],
config=config,
)

1. `graph.invoke` is called with the `interrupt_before` and `interrupt_after` parameters. This is a run-time configuration and can be changed for every invocation.
2. `interrupt_before` specifies the nodes where execution should pause before the node is executed.
3. `interrupt_after` specifies the nodes where execution should pause after the node is executed.
4. The graph is run until the first breakpoint is hit.
5. The graph is resumed by passing in `None` for the input. This will run the graph until the next breakpoint is hit.

### ​ Using LangGraph Studio

You can use LangGraph Studio to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.!image

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Streaming\\
\\
Previous Use time-travel\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/use-time-travel

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Use time-travel

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- In a workflow
- Setup
- 1\. Run the graph
- 2\. Identify a checkpoint
- 3\. Update the state
- 4\. Resume execution from the checkpoint

When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:

1. **Understand reasoning**: Analyze the steps that led to a successful result.
2. **Debug mistakes**: Identify where and why errors occurred.
3. **Explore alternatives**: Test different paths to uncover better solutions.

LangGraph provides time travel functionality to support these use cases. Specifically, you can resume execution from a prior checkpoint — either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.To use time-travel in LangGraph:

1. Run the graph with initial inputs using `invoke` or `stream` methods.
2. Identify a checkpoint in an existing thread: Use the `get_state_history` method to retrieve the execution history for a specific `thread_id` and locate the desired `checkpoint_id`.
Alternatively, set an interrupt before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that interrupt.
3. Update the graph state (optional): Use the `update_state` method to modify the graph’s state at the checkpoint and resume execution from alternative state.
4. Resume execution from the checkpoint: Use the `invoke` or `stream` methods with an input of `None` and a configuration containing the appropriate `thread_id` and `checkpoint_id`.

For a conceptual overview of time-travel, see Time travel.

## ​ In a workflow

This example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes.

### ​ Setup

First we need to install the packages required

Copy

%%capture --no-stderr
pip install --quiet -U langgraph langchain_anthropic

Next, we need to set API keys for Anthropic (the LLM we will use)

import getpass
import os

def _set_env(var: str):
if not os.environ.get(var):
os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("ANTHROPIC_API_KEY")

Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.

import uuid

from typing_extensions import TypedDict, NotRequired
from langgraph.graph import StateGraph, START, END
from langchain.chat_models import init_chat_model
from langgraph.checkpoint.memory import InMemorySaver

class State(TypedDict):
topic: NotRequired[str]
joke: NotRequired[str]

model = init_chat_model(
"claude-sonnet-4-5-20250929",
temperature=0,
)

def generate_topic(state: State):
"""LLM call to generate a topic for the joke"""
msg = model.invoke("Give me a funny topic for a joke")
return {"topic": msg.content}

def write_joke(state: State):
"""LLM call to write a joke based on the topic"""
msg = model.invoke(f"Write a short joke about {state['topic']}")
return {"joke": msg.content}

# Build workflow
workflow = StateGraph(State)

# Add nodes
workflow.add_node("generate_topic", generate_topic)
workflow.add_node("write_joke", write_joke)

# Add edges to connect nodes
workflow.add_edge(START, "generate_topic")
workflow.add_edge("generate_topic", "write_joke")
workflow.add_edge("write_joke", END)

# Compile
checkpointer = InMemorySaver()
graph = workflow.compile(checkpointer=checkpointer)
graph

### ​ 1\. Run the graph

config = {
"configurable": {
"thread_id": uuid.uuid4(),
}
}
state = graph.invoke({}, config)

print(state["topic"])
print()
print(state["joke"])

**Output:**

How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all!

# The Secret Life of Socks in the Dryer

I finally discovered where all my missing socks go after the dryer. Turns out they're not missing at all—they've just eloped with someone else's socks from the laundromat to start new lives together.

My blue argyle is now living in Bermuda with a red polka dot, posting vacation photos on Sockstagram and sending me lint as alimony.

### ​ 2\. Identify a checkpoint

# The states are returned in reverse chronological order.
states = list(graph.get_state_history(config))

for state in states:
print(state.next)
print(state.config["configurable"]["checkpoint_id"])
print()

()
1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e

('write_joke',)
1f02ac4a-ce2a-6494-8001-cb2e2d651227

('generate_topic',)
1f02ac4a-a4e0-630d-8000-b73c254ba748

('__start__',)
1f02ac4a-a4dd-665e-bfff-e6c8c44315d9

# This is the state before last (states are listed in chronological order)
selected_state = states[1]
print(selected_state.next)
print(selected_state.values)

('write_joke',)
{'topic': 'How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\'t know about? There\\'s a lot of comedic potential in the everyday mystery that unites us all!'}

### ​ 3\. Update the state

`update_state` will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.

new_config = graph.update_state(selected_state.config, values={"topic": "chickens"})
print(new_config)

{'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}

### ​ 4\. Resume execution from the checkpoint

graph.invoke(None, new_config)

{'topic': 'chickens',
'joke': 'Why did the chicken join a band?\n\nBecause it had excellent drumsticks!'}

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Interrupts\\
\\
Previous Memory\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/add-memory

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Memory

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Add short-term memory
- Use in production
- Use in subgraphs
- Add long-term memory
- Use in production
- Use semantic search
- Manage short-term memory
- Trim messages
- Delete messages
- Summarize messages
- Manage checkpoints
- View thread state
- View the history of the thread
- Delete all checkpoints for a thread
- Prebuilt memory tools
- Database management

AI applications need memory to share context across multiple interactions. In LangGraph, you can add two types of memory:

- Add short-term memory as a part of your agent’s state to enable multi-turn conversations.
- Add long-term memory to store user-specific or application-level data across sessions.

## ​ Add short-term memory

**Short-term** memory (thread-level persistence) enables agents to track multi-turn conversations. To add short-term memory:

Copy

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import StateGraph

checkpointer = InMemorySaver()

builder = StateGraph(...)
graph = builder.compile(checkpointer=checkpointer)

graph.invoke(
{"messages": [{"role": "user", "content": "hi! i am Bob"}]},
{"configurable": {"thread_id": "1"}},
)

### ​ Use in production

In production, use a checkpointer backed by a database:

from langgraph.checkpoint.postgres import PostgresSaver

DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"
with PostgresSaver.from_conn_string(DB_URI) as checkpointer:
builder = StateGraph(...)
graph = builder.compile(checkpointer=checkpointer)

Example: using Postgres checkpointer

pip install -U "psycopg[binary,pool]" langgraph langgraph-checkpoint-postgres

You need to call `checkpointer.setup()` the first time you’re using Postgres checkpointer

- Sync

- Async

from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.postgres import PostgresSaver

model = init_chat_model(model="claude-haiku-4-5-20251001")

DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"
with PostgresSaver.from_conn_string(DB_URI) as checkpointer:
# checkpointer.setup()

def call_model(state: MessagesState):
response = model.invoke(state["messages"])
return {"messages": response}

builder = StateGraph(MessagesState)
builder.add_node(call_model)
builder.add_edge(START, "call_model")

graph = builder.compile(checkpointer=checkpointer)

config = {
"configurable": {
"thread_id": "1"
}
}

for chunk in graph.stream(
{"messages": [{"role": "user", "content": "hi! I'm bob"}]},
config,
stream_mode="values"
):
chunk["messages"][-1].pretty_print()

for chunk in graph.stream(
{"messages": [{"role": "user", "content": "what's my name?"}]},
config,
stream_mode="values"
):
chunk["messages"][-1].pretty_print()

from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver

DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"
async with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:
# await checkpointer.setup()

async def call_model(state: MessagesState):
response = await model.ainvoke(state["messages"])
return {"messages": response}

async for chunk in graph.astream(
{"messages": [{"role": "user", "content": "hi! I'm bob"}]},
config,
stream_mode="values"
):
chunk["messages"][-1].pretty_print()

async for chunk in graph.astream(
{"messages": [{"role": "user", "content": "what's my name?"}]},
config,
stream_mode="values"
):
chunk["messages"][-1].pretty_print()

Example: using MongoDB checkpointer

pip install -U pymongo langgraph langgraph-checkpoint-mongodb

**Setup**
To use the MongoDB checkpointer, you will need a MongoDB cluster. Follow this guide to create a cluster if you don’t already have one.

from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.mongodb import MongoDBSaver

DB_URI = "localhost:27017"
with MongoDBSaver.from_conn_string(DB_URI) as checkpointer:

from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.mongodb.aio import AsyncMongoDBSaver

DB_URI = "localhost:27017"
async with AsyncMongoDBSaver.from_conn_string(DB_URI) as checkpointer:

Example: using Redis checkpointer

pip install -U langgraph langgraph-checkpoint-redis

You need to call `checkpointer.setup()` the first time you’re using Redis checkpointer.

from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.redis import RedisSaver

DB_URI = "redis://localhost:6379"
with RedisSaver.from_conn_string(DB_URI) as checkpointer:

from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.redis.aio import AsyncRedisSaver

DB_URI = "redis://localhost:6379"
async with AsyncRedisSaver.from_conn_string(DB_URI) as checkpointer:
# await checkpointer.asetup()

### ​ Use in subgraphs

If your graph contains subgraphs, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.

from langgraph.graph import START, StateGraph
from langgraph.checkpoint.memory import InMemorySaver
from typing import TypedDict

class State(TypedDict):
foo: str

# Subgraph

def subgraph_node_1(state: State):
return {"foo": state["foo"] + "bar"}

subgraph_builder = StateGraph(State)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph = subgraph_builder.compile()

# Parent graph

builder = StateGraph(State)
builder.add_node("node_1", subgraph)
builder.add_edge(START, "node_1")

checkpointer = InMemorySaver()
graph = builder.compile(checkpointer=checkpointer)

If you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories.

subgraph_builder = StateGraph(...)
subgraph = subgraph_builder.compile(checkpointer=True)

## ​ Add long-term memory

Use long-term memory to store user-specific or application-specific data across conversations.

from langgraph.store.memory import InMemoryStore
from langgraph.graph import StateGraph

store = InMemoryStore()

builder = StateGraph(...)
graph = builder.compile(store=store)

### ​ Use in production

In production, use a store backed by a database:

from langgraph.store.postgres import PostgresStore

DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"
with PostgresStore.from_conn_string(DB_URI) as store:
builder = StateGraph(...)
graph = builder.compile(store=store)

Example: using Postgres store

You need to call `store.setup()` the first time you’re using Postgres store

from langchain_core.runnables import RunnableConfig
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.postgres import PostgresSaver
from langgraph.store.postgres import PostgresStore
from langgraph.store.base import BaseStore

DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable"

with (
PostgresStore.from_conn_string(DB_URI) as store,
PostgresSaver.from_conn_string(DB_URI) as checkpointer,
):
# store.setup()

def call_model(
state: MessagesState,
config: RunnableConfig,
*,
store: BaseStore,
):
user_id = config["configurable"]["user_id"]
namespace = ("memories", user_id)
memories = store.search(namespace, query=str(state["messages"][-1].content))
info = "\n".join([d.value["data"] for d in memories])
system_msg = f"You are a helpful assistant talking to the user. User info: {info}"

# Store new memories if the user asks the model to remember
last_message = state["messages"][-1]
if "remember" in last_message.content.lower():
memory = "User name is Bob"
store.put(namespace, str(uuid.uuid4()), {"data": memory})

response = model.invoke(
[{"role": "system", "content": system_msg}] + state["messages"]
)
return {"messages": response}

graph = builder.compile(
checkpointer=checkpointer,
store=store,
)

config = {
"configurable": {
"thread_id": "1",
"user_id": "1",
}
}
for chunk in graph.stream(
{"messages": [{"role": "user", "content": "Hi! Remember: my name is Bob"}]},
config,
stream_mode="values",
):
chunk["messages"][-1].pretty_print()

config = {
"configurable": {
"thread_id": "2",
"user_id": "1",
}
}

for chunk in graph.stream(
{"messages": [{"role": "user", "content": "what is my name?"}]},
config,
stream_mode="values",
):
chunk["messages"][-1].pretty_print()

from langchain_core.runnables import RunnableConfig
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from langgraph.store.postgres.aio import AsyncPostgresStore
from langgraph.store.base import BaseStore

async with (
AsyncPostgresStore.from_conn_string(DB_URI) as store,
AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer,
):
# await store.setup()

async def call_model(
state: MessagesState,
config: RunnableConfig,
*,
store: BaseStore,
):
user_id = config["configurable"]["user_id"]
namespace = ("memories", user_id)
memories = await store.asearch(namespace, query=str(state["messages"][-1].content))
info = "\n".join([d.value["data"] for d in memories])
system_msg = f"You are a helpful assistant talking to the user. User info: {info}"

last_message = state["messages"][-1]
if "remember" in last_message.content.lower():
memory = "User name is Bob"
await store.aput(namespace, str(uuid.uuid4()), {"data": memory})

response = await model.ainvoke(
[{"role": "system", "content": system_msg}] + state["messages"]
)
return {"messages": response}

config = {
"configurable": {
"thread_id": "1",
"user_id": "1",
}
}
async for chunk in graph.astream(
{"messages": [{"role": "user", "content": "Hi! Remember: my name is Bob"}]},
config,
stream_mode="values",
):
chunk["messages"][-1].pretty_print()

async for chunk in graph.astream(
{"messages": [{"role": "user", "content": "what is my name?"}]},
config,
stream_mode="values",
):
chunk["messages"][-1].pretty_print()

Example: using Redis store

You need to call `store.setup()` the first time you’re using Redis store.

from langchain_core.runnables import RunnableConfig
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.redis import RedisSaver
from langgraph.store.redis import RedisStore
from langgraph.store.base import BaseStore

DB_URI = "redis://localhost:6379"

with (
RedisStore.from_conn_string(DB_URI) as store,
RedisSaver.from_conn_string(DB_URI) as checkpointer,
):
store.setup()
checkpointer.setup()

from langchain_core.runnables import RunnableConfig
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.redis.aio import AsyncRedisSaver
from langgraph.store.redis.aio import AsyncRedisStore
from langgraph.store.base import BaseStore

async with (
AsyncRedisStore.from_conn_string(DB_URI) as store,
AsyncRedisSaver.from_conn_string(DB_URI) as checkpointer,
):

### ​ Use semantic search

Enable semantic search in your graph’s memory store to let graph agents search for items in the store by semantic similarity.

from langchain.embeddings import init_embeddings
from langgraph.store.memory import InMemoryStore

# Create store with semantic search enabled
embeddings = init_embeddings("openai:text-embedding-3-small")
store = InMemoryStore(
index={
"embed": embeddings,
"dims": 1536,
}
)

store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
store.put(("user_123", "memories"), "2", {"text": "I am a plumber"})

items = store.search(
("user_123", "memories"), query="I'm hungry", limit=1
)

Long-term memory with semantic search

from langchain.embeddings import init_embeddings
from langchain.chat_models import init_chat_model
from langgraph.store.base import BaseStore
from langgraph.store.memory import InMemoryStore
from langgraph.graph import START, MessagesState, StateGraph

model = init_chat_model("gpt-4o-mini")

def chat(state, *, store: BaseStore):
# Search based on user's last message
items = store.search(
("user_123", "memories"), query=state["messages"][-1].content, limit=2
)
memories = "\n".join(item.value["text"] for item in items)
memories = f"## Memories of user\n{memories}" if memories else ""
response = model.invoke(
[\
{"role": "system", "content": f"You are a helpful assistant.\n{memories}"},\
*state["messages"],\
]
)
return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node(chat)
builder.add_edge(START, "chat")
graph = builder.compile(store=store)

for message, metadata in graph.stream(
input={"messages": [{"role": "user", "content": "I'm hungry"}]},
stream_mode="messages",
):
print(message.content, end="")

## ​ Manage short-term memory

With short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:

- Trim messages: Remove first or last N messages (before calling LLM)
- Delete messages from LangGraph state permanently
- Summarize messages: Summarize earlier messages in the history and replace them with a summary
- Manage checkpoints to store and retrieve message history
- Custom strategies (e.g., message filtering, etc.)

This allows the agent to keep track of the conversation without exceeding the LLM’s context window.

### ​ Trim messages

Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.To trim message history, use the `trim_messages` function:

from langchain_core.messages.utils import (
trim_messages,
count_tokens_approximately
)

def call_model(state: MessagesState):
messages = trim_messages(
state["messages"],
strategy="last",
token_counter=count_tokens_approximately,
max_tokens=128,
start_on="human",
end_on=("human", "tool"),
)
response = model.invoke(messages)
return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node(call_model)
...

Full example: trim messages

from langchain_core.messages.utils import (
trim_messages,
count_tokens_approximately
)
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START, MessagesState

model = init_chat_model("claude-sonnet-4-5-20250929")
summarization_model = model.bind(max_tokens=128)

checkpointer = InMemorySaver()
builder = StateGraph(MessagesState)
builder.add_node(call_model)
builder.add_edge(START, "call_model")
graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}
graph.invoke({"messages": "hi, my name is bob"}, config)
graph.invoke({"messages": "write a short poem about cats"}, config)
graph.invoke({"messages": "now do the same but for dogs"}, config)
final_response = graph.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()

================================== Ai Message ==================================

Your name is Bob, as you mentioned when you first introduced yourself.

### ​ Delete messages

You can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.To delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with `add_messages` reducer, like `MessagesState`.To remove specific messages:

from langchain.messages import RemoveMessage

def delete_messages(state):
messages = state["messages"]

# remove the earliest two messages
return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}

To remove **all** messages:

from langgraph.graph.message import REMOVE_ALL_MESSAGES

def delete_messages(state):
return {"messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}

When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:

- Some providers expect message history to start with a `user` message
- Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.

Full example: delete messages

builder = StateGraph(MessagesState)
builder.add_sequence([call_model, delete_messages])
builder.add_edge(START, "call_model")

checkpointer = InMemorySaver()
app = builder.compile(checkpointer=checkpointer)

for event in app.stream(
{"messages": [{"role": "user", "content": "hi! I'm bob"}]},
config,
stream_mode="values"
):
print([(message.type, message.content) for message in event["messages"]])

for event in app.stream(
{"messages": [{"role": "user", "content": "what's my name?"}]},
config,
stream_mode="values"
):
print([(message.type, message.content) for message in event["messages"]])

[('human', "hi! I'm bob")]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', "what's my name?")]
[('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', "what's my name?"), ('ai', 'Your name is Bob.')]
[('human', "what's my name?"), ('ai', 'Your name is Bob.')]

### ​ Summarize messages

The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.!SummaryPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the `MessagesState` to include a `summary` key:

from langgraph.graph import MessagesState
class State(MessagesState):
summary: str

Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This `summarize_conversation` node can be called after some number of messages have accumulated in the `messages` state key.

def summarize_conversation(state: State):

# First, we get any existing summary
summary = state.get("summary", "")

# Create our summarization prompt
if summary:

# A summary already exists
summary_message = (
f"This is a summary of the conversation to date: {summary}\n\n"
"Extend the summary by taking into account the new messages above:"
)

else:
summary_message = "Create a summary of the conversation above:"

# Add prompt to our history
messages = state["messages"] + [HumanMessage(content=summary_message)]
response = model.invoke(messages)

# Delete all but the 2 most recent messages
delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
return {"summary": response.content, "messages": delete_messages}

Full example: summarize messages

from typing import Any, TypedDict

from langchain.chat_models import init_chat_model
from langchain.messages import AnyMessage
from langchain_core.messages.utils import count_tokens_approximately
from langgraph.graph import StateGraph, START, MessagesState
from langgraph.checkpoint.memory import InMemorySaver
from langmem.short_term import SummarizationNode, RunningSummary

class State(MessagesState):
context: dict[str, RunningSummary]

class LLMInputState(TypedDict):
summarized_messages: list[AnyMessage]
context: dict[str, RunningSummary]

summarization_node = SummarizationNode(
token_counter=count_tokens_approximately,
model=summarization_model,
max_tokens=256,
max_tokens_before_summary=256,
max_summary_tokens=128,
)

def call_model(state: LLMInputState):
response = model.invoke(state["summarized_messages"])
return {"messages": [response]}

checkpointer = InMemorySaver()
builder = StateGraph(State)
builder.add_node(call_model)
builder.add_node("summarize", summarization_node)
builder.add_edge(START, "summarize")
builder.add_edge("summarize", "call_model")
graph = builder.compile(checkpointer=checkpointer)

# Invoke the graph

final_response["messages"][-1].pretty_print()
print("\nSummary:", final_response["context"]["running_summary"].summary)

1. We will keep track of our running summary in the `context` field

(expected by the `SummarizationNode`).

1. Define private state that will be used only for filtering

the inputs to `call_model` node.

1. We’re passing a private input state here to isolate the messages returned by the summarization node

From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.

Summary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled "The Mystery of Cats" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote "The Joy of Dogs," which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.

### ​ Manage checkpoints

You can view and delete the information stored by the checkpointer.

#### ​ View thread state

- Graph/Functional API

- Checkpointer API

config = {
"configurable": {
"thread_id": "1",
# optionally provide an ID for a specific checkpoint,
# otherwise the latest checkpoint is shown
# "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a" #

}
}
graph.get_state(config)

StateSnapshot(
values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]}, next=(),
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
metadata={
'source': 'loop',
'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
'step': 4,
'parents': {},
'thread_id': '1'
},
created_at='2025-05-05T16:01:24.680462+00:00',
parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
tasks=(),
interrupts=()
)

}
}
checkpointer.get_tuple(config)

CheckpointTuple(
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
checkpoint={
'v': 3,
'ts': '2025-05-05T16:01:24.680462+00:00',
'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',
'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},
'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
},
metadata={
'source': 'loop',
'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
'step': 4,
'parents': {},
'thread_id': '1'
},
parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
pending_writes=[]
)

#### ​ View the history of the thread

config = {
"configurable": {
"thread_id": "1"
}
}
list(graph.get_state_history(config))

[\
StateSnapshot(\
values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},\
next=(),\
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\
metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\
created_at='2025-05-05T16:01:24.680462+00:00',\
parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\
tasks=(),\
interrupts=()\
),\
StateSnapshot(\
values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?")]},\
next=('call_model',),\
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\
metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\
created_at='2025-05-05T16:01:23.863421+00:00',\
parent_config={...}\
tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),\
interrupts=()\
),\
StateSnapshot(\
values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\
next=('__start__',),\
config={...},\
metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\
created_at='2025-05-05T16:01:23.863173+00:00',\
parent_config={...}\
tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "what's my name?"}]}),),\
interrupts=()\
),\
StateSnapshot(\
values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\
next=(),\
config={...},\
metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\
created_at='2025-05-05T16:01:23.862295+00:00',\
parent_config={...}\
tasks=(),\
interrupts=()\
),\
StateSnapshot(\
values={'messages': [HumanMessage(content="hi! I'm bob")]},\
next=('call_model',),\
config={...},\
metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\
created_at='2025-05-05T16:01:22.278960+00:00',\
parent_config={...}\
tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),\
interrupts=()\
),\
StateSnapshot(\
values={'messages': []},\
next=('__start__',),\
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\
metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\
created_at='2025-05-05T16:01:22.277497+00:00',\
parent_config=None,\
tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}),),\
interrupts=()\
)\
]

config = {
"configurable": {
"thread_id": "1"
}
}
list(checkpointer.list(config))

[\
CheckpointTuple(\
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\
checkpoint={\
'v': 3,\
'ts': '2025-05-05T16:01:24.680462+00:00',\
'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',\
'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},\
'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\
'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},\
},\
metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\
parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\
pending_writes=[]\
),\
CheckpointTuple(\
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\
checkpoint={\
'v': 3,\
'ts': '2025-05-05T16:01:23.863421+00:00',\
'id': '1f029ca3-1790-6b0a-8003-baf965b6a38f',\
'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},\
'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\
'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?")], 'branch:to:call_model': None}\
},\
metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\
parent_config={...},\
pending_writes=[('8ab4155e-6b15-b885-9ce5-bed69a2c305c', 'messages', AIMessage(content='Your name is Bob.'))]\
),\
CheckpointTuple(\
config={...},\
checkpoint={\
'v': 3,\
'ts': '2025-05-05T16:01:23.863173+00:00',\
'id': '1f029ca3-1790-616e-8002-9e021694a0cd',\
'channel_versions': {'__start__': '00000000000000000000000000000004.0.5736472536395331', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},\
'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},\
'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}, 'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}\
},\
metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\
parent_config={...},\
pending_writes=[('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'messages', [{'role': 'user', 'content': "what's my name?"}]), ('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'branch:to:call_model', None)]\
),\
CheckpointTuple(\
config={...},\
checkpoint={\
'v': 3,\
'ts': '2025-05-05T16:01:23.862295+00:00',\
'id': '1f029ca3-178d-6f54-8001-d7b180db0c89',\
'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},\
'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},\
'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}\
},\
metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\
parent_config={...},\
pending_writes=[]\
),\
CheckpointTuple(\
config={...},\
checkpoint={\
'v': 3,\
'ts': '2025-05-05T16:01:22.278960+00:00',\
'id': '1f029ca3-0874-6612-8000-339f2abc83b1',\
'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000002.0.30296526818059655', 'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'},\
'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}},\
'channel_values': {'messages': [HumanMessage(content="hi! I'm bob")], 'branch:to:call_model': None}\
},\
metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\
parent_config={...},\
pending_writes=[('8cbd75e0-3720-b056-04f7-71ac805140a0', 'messages', AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'))]\
),\
CheckpointTuple(\
config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\
checkpoint={\
'v': 3,\
'ts': '2025-05-05T16:01:22.277497+00:00',\
'id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565',\
'channel_versions': {'__start__': '00000000000000000000000000000001.0.7040775356287469'},\
'versions_seen': {'__input__': {}},\
'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}\
},\
metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\
parent_config=None,\
pending_writes=[('d458367b-8265-812c-18e2-33001d199ce6', 'messages', [{'role': 'user', 'content': "hi! I'm bob"}]), ('d458367b-8265-812c-18e2-33001d199ce6', 'branch:to:call_model', None)]\
)\
]

#### ​ Delete all checkpoints for a thread

thread_id = "1"
checkpointer.delete_thread(thread_id)

## ​ Prebuilt memory tools

**LangMem** is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples.

## ​ Database management

If you are using any database-backed persistence implementation (such as Postgres or Redis) to store short and/or long-term memory, you will need to run migrations to set up the required schema before you can use it with your database.By convention, most database-specific libraries define a `setup()` method on the checkpointer or store instance that runs the required migrations. However, you should check with your specific implementation of `BaseCheckpointSaver` or `BaseStore` to confirm the exact method name and usage.We recommend running migrations as a dedicated deployment step, or you can ensure they’re run as part of server startup.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Use time-travel\\
\\
Previous Subgraphs\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/use-subgraphs

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Capabilities

Subgraphs

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Setup
- Invoke a graph from a node
- Add a graph as a node
- Add persistence
- View subgraph state
- Stream subgraph outputs

This guide explains the mechanics of using subgraphs. A subgraph is a graph that is used as a node in another graph.Subgraphs are useful for:

- Building multi-agent systems
- Re-using a set of nodes in multiple graphs
- Distributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph

When adding subgraphs, you need to define how the parent graph and the subgraph communicate:

- Invoke a graph from a node — subgraphs are called from inside a node in the parent graph
- Add a graph as a node — a subgraph is added directly as a node in the parent and **shares state keys** with the parent

## ​ Setup

pip

uv

Copy

pip install -U langgraph

**Set up LangSmith for LangGraph development**
Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started here.

## ​ Invoke a graph from a node

A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have **completely different schemas** from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a multi-agent system.If that’s the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results :
return {"bar": "hi! " + state["bar"]}

subgraph_builder = StateGraph(SubgraphState)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph = subgraph_builder.compile()

# Parent graph

class State(TypedDict):
foo: str

def call_subgraph(state: State):
# Transform the state to the subgraph state
subgraph_output = subgraph.invoke({"bar": state["foo"]})
# Transform response
return {"foo": subgraph_output["bar"]}

builder = StateGraph(State)
builder.add_node("node_1", call_subgraph)
builder.add_edge(START, "node_1")
graph = builder.compile()

Full example: different state schemas

from typing_extensions import TypedDict
from langgraph.graph.state import StateGraph, START

# Define subgraph
class SubgraphState(TypedDict):
# note that none of these keys are shared with the parent graph state
bar: str
baz: str

def subgraph_node_1(state: SubgraphState):
return {"baz": "baz"}

def subgraph_node_2(state: SubgraphState):
return {"bar": state["bar"] + state["baz"]}

subgraph_builder = StateGraph(SubgraphState)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_node(subgraph_node_2)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2")
subgraph = subgraph_builder.compile()

# Define parent graph
class ParentState(TypedDict):
foo: str

def node_1(state: ParentState):
return {"foo": "hi! " + state["foo"]}

def node_2(state: ParentState):
response = subgraph.invoke({"bar": state["foo"]})
return {"foo": response["bar"]}

builder = StateGraph(ParentState)
builder.add_node("node_1", node_1)
builder.add_node("node_2", node_2)
builder.add_edge(START, "node_1")
builder.add_edge("node_1", "node_2")
graph = builder.compile()

for chunk in graph.stream({"foo": "foo"}, subgraphs=True):
print(chunk)

((), {'node_1': {'foo': 'hi! foo'}})
(('node_2:577b710b-64ae-31fb-9455-6a4d4cc2b0b9',), {'subgraph_node_1': {'baz': 'baz'}})
(('node_2:577b710b-64ae-31fb-9455-6a4d4cc2b0b9',), {'subgraph_node_2': {'bar': 'hi! foobaz'}})
((), {'node_2': {'foo': 'hi! foobaz'}})

Full example: different state schemas (two levels of subgraphs)

# Grandchild graph
from typing_extensions import TypedDict
from langgraph.graph.state import StateGraph, START, END

class GrandChildState(TypedDict):
my_grandchild_key: str

# NOTE: child or parent keys will not be accessible here
return {"my_grandchild_key": state["my_grandchild_key"] + ", how are you"}

grandchild = StateGraph(GrandChildState)
grandchild.add_node("grandchild_1", grandchild_1)

grandchild.add_edge(START, "grandchild_1")
grandchild.add_edge("grandchild_1", END)

grandchild_graph = grandchild.compile()

# Child graph
class ChildState(TypedDict):
my_child_key: str

# NOTE: parent or grandchild keys won't be accessible here
grandchild_graph_input = {"my_grandchild_key": state["my_child_key"]}
grandchild_graph_output = grandchild_graph.invoke(grandchild_graph_input)
return {"my_child_key": grandchild_graph_output["my_grandchild_key"] + " today?"}

child = StateGraph(ChildState)
# We're passing a function here instead of just compiled graph (`grandchild_graph`)
child.add_node("child_1", call_grandchild_graph)
child.add_edge(START, "child_1")
child.add_edge("child_1", END)
child_graph = child.compile()

class ParentState(TypedDict):
my_key: str

# NOTE: child or grandchild keys won't be accessible here
return {"my_key": "hi " + state["my_key"]}

return {"my_key": state["my_key"] + " bye!"}

child_graph_input = {"my_child_key": state["my_key"]}
child_graph_output = child_graph.invoke(child_graph_input)
return {"my_key": child_graph_output["my_child_key"]}

parent = StateGraph(ParentState)
parent.add_node("parent_1", parent_1)
# We're passing a function here instead of just a compiled graph (`child_graph`)
parent.add_node("child", call_child_graph)
parent.add_node("parent_2", parent_2)

parent.add_edge(START, "parent_1")
parent.add_edge("parent_1", "child")
parent.add_edge("child", "parent_2")
parent.add_edge("parent_2", END)

parent_graph = parent.compile()

for chunk in parent_graph.stream({"my_key": "Bob"}, subgraphs=True):
print(chunk)

((), {'parent_1': {'my_key': 'hi Bob'}})
(('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child_1:781bb3b1-3971-84ce-810b-acf819a03f9c'), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})
(('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b',), {'child_1': {'my_child_key': 'hi Bob, how are you today?'}})
((), {'child': {'my_key': 'hi Bob, how are you today?'}})
((), {'parent_2': {'my_key': 'hi Bob, how are you today? bye!'}})

## ​ Add a graph as a node

When the parent graph and subgraph can communicate over a shared state key (channel) in the schema, you can add a graph as a node in another graph. For example, in multi-agent systems, the agents often communicate over a shared messages key.!SQL agent graphIf your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:

1. Define the subgraph workflow (`subgraph_builder` in the example below) and compile it
2. Pass compiled subgraph to the `add_node` method when defining the parent graph workflow

# Subgraph

def subgraph_node_1(state: State):
return {"foo": "hi! " + state["foo"]}

subgraph_builder = StateGraph(State)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph = subgraph_builder.compile()

builder = StateGraph(State)
builder.add_node("node_1", subgraph)
builder.add_edge(START, "node_1")
graph = builder.compile()

Full example: shared state schemas

class SubgraphState(TypedDict):
foo: str # shared with parent graph state
bar: str # private to SubgraphState

def subgraph_node_1(state: SubgraphState):
return {"bar": "bar"}

def subgraph_node_2(state: SubgraphState):
# note that this node is using a state key ('bar') that is only available in the subgraph
# and is sending update on the shared state key ('foo')
return {"foo": state["foo"] + state["bar"]}

builder = StateGraph(ParentState)
builder.add_node("node_1", node_1)
builder.add_node("node_2", subgraph)
builder.add_edge(START, "node_1")
builder.add_edge("node_1", "node_2")
graph = builder.compile()

for chunk in graph.stream({"foo": "foo"}):
print(chunk)

{'node_1': {'foo': 'hi! foo'}}
{'node_2': {'foo': 'hi! foobar'}}

## ​ Add persistence

You only need to **provide the checkpointer when compiling the parent graph**. LangGraph will automatically propagate the checkpointer to the child subgraphs.

from langgraph.graph import START, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from typing_extensions import TypedDict

def subgraph_node_1(state: State):
return {"foo": state["foo"] + "bar"}

builder = StateGraph(State)
builder.add_node("node_1", subgraph)
builder.add_edge(START, "node_1")

checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)

If you want the subgraph to **have its own memory**, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories:

subgraph_builder = StateGraph(...)
subgraph = subgraph_builder.compile(checkpointer=True)

## ​ View subgraph state

When you enable persistence, you can inspect the graph state (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.You can inspect the graph state via `graph.get_state(config)`. To view the subgraph state, you can use `graph.get_state(config, subgraphs=True)`.

**Available only when interrupted**
Subgraph state can only be viewed **when the subgraph is interrupted**. Once you resume the graph, you won’t be able to access the subgraph state.

View interrupted subgraph state

from langgraph.graph import START, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import interrupt, Command
from typing_extensions import TypedDict

def subgraph_node_1(state: State):
value = interrupt("Provide value:")
return {"foo": state["foo"] + value}

subgraph_builder = StateGraph(State)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_edge(START, "subgraph_node_1")

subgraph = subgraph_builder.compile()

config = {"configurable": {"thread_id": "1"}}

graph.invoke({"foo": ""}, config)
parent_state = graph.get_state(config)

# This will be available only when the subgraph is interrupted.
# Once you resume the graph, you won't be able to access the subgraph state.
subgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state

# resume the subgraph
graph.invoke(Command(resume="bar"), config)

1. This will be available only when the subgraph is interrupted. Once you resume the graph, you won’t be able to access the subgraph state.

## ​ Stream subgraph outputs

To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

for chunk in graph.stream(
{"foo": "foo"},
subgraphs=True,
stream_mode="updates",
):
print(chunk)

Stream from subgraphs

class SubgraphState(TypedDict):
foo: str
bar: str

def subgraph_node_2(state: SubgraphState):
return {"foo": state["foo"] + state["bar"]}

for chunk in graph.stream(
{"foo": "foo"},
stream_mode="updates",
subgraphs=True,
):
print(chunk)

((), {'node_1': {'foo': 'hi! foo'}})
(('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_1': {'bar': 'bar'}})
(('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_2': {'foo': 'hi! foobar'}})
((), {'node_2': {'foo': 'hi! foobar'}})

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Memory\\
\\
Previous Application structure\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/application-structure

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

Application structure

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Key Concepts
- File structure
- Configuration file
- Examples
- Dependencies
- Graphs
- Environment variables

A LangGraph application consists of one or more graphs, a configuration file (`langgraph.json`), a file that specifies dependencies, and an optional `.env` file that specifies environment variables.This guide shows a typical structure of an application and shows you how to provide the required configuration to deploy an application with LangSmith Deployment.

LangSmith Deployment is a managed hosting platform for deploying and scaling LangGraph agents. It handles the infrastructure, scaling, and operational concerns so you can deploy your stateful, long-running agents directly from your repository. Learn more in the Deployment documentation.

## ​ Key Concepts

To deploy using the LangSmith, the following information should be provided:

1. A LangGraph configuration file (`langgraph.json`) that specifies the dependencies, graphs, and environment variables to use for the application.
2. The graphs that implement the logic of the application.
3. A file that specifies dependencies required to run the application.
4. Environment variables that are required for the application to run.

## ​ File structure

Below are examples of directory structures for applications:

- Python (requirements.txt)

- Python (pyproject.toml)

Copy

my-app/
├── my_agent # all project code lies within here
│ ├── utils # utilities for your graph
│ │ ├── __init__.py
│ │ ├── tools.py # tools for your graph
│ │ ├── nodes.py # node functions for your graph
│ │ └── state.py # state definition of your graph
│ ├── __init__.py
│ └── agent.py # code for constructing your graph
├── .env # environment variables
├── requirements.txt # package dependencies
└── langgraph.json # configuration file for LangGraph

my-app/
├── my_agent # all project code lies within here
│ ├── utils # utilities for your graph
│ │ ├── __init__.py
│ │ ├── tools.py # tools for your graph
│ │ ├── nodes.py # node functions for your graph
│ │ └── state.py # state definition of your graph
│ ├── __init__.py
│ └── agent.py # code for constructing your graph
├── .env # environment variables
├── langgraph.json # configuration file for LangGraph
└── pyproject.toml # dependencies for your project

The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.

## ​ Configuration file

The `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.See the LangGraph configuration file reference for details on all supported keys in the JSON file.

The LangGraph CLI defaults to using the configuration file `langgraph.json` in the current directory.

### ​ Examples

- The dependencies involve a custom local package and the `langchain_openai` package.
- A single graph will be loaded from the file `./your_package/your_file.py` with the variable `variable`.
- The environment variables are loaded from the `.env` file.

{
"dependencies": ["langchain_openai", "./your_package"],
"graphs": {
"my_agent": "./your_package/your_file.py:agent"
},
"env": "./.env"
}

## ​ Dependencies

A LangGraph application may depend on other Python packages.You will generally need to specify the following information for dependencies to be set up correctly:

1. A file in the directory that specifies the dependencies (e.g. `requirements.txt`, `pyproject.toml`, or `package.json`).
2. A `dependencies` key in the LangGraph configuration file that specifies the dependencies required to run the LangGraph application.
3. Any additional binaries or system libraries can be specified using `dockerfile_lines` key in the LangGraph configuration file.

## ​ Graphs

Use the `graphs` key in the LangGraph configuration file to specify which graphs will be available in the deployed LangGraph application.You can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.

## ​ Environment variables

If you’re working with a deployed LangGraph application locally, you can configure environment variables in the `env` key of the LangGraph configuration file.For a production deployment, you will typically want to configure the environment variables in the deployment environment.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Subgraphs\\
\\
Previous Test\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/test

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

Test

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Prerequisites
- Getting started
- Testing individual nodes and edges
- Partial execution

After you’ve prototyped your LangGraph agent, a natural next step is to add tests. This guide covers some useful patterns you can use when writing unit tests.Note that this guide is LangGraph-specific and covers scenarios around graphs with custom structures - if you are just getting started, check out this section that uses LangChain’s built-in `create_agent` instead.

## ​ Prerequisites

First, make sure you have `pytest` installed:

Copy

$ pip install -U pytest

## ​ Getting started

Because many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance.The below example shows how this works with a simple, linear graph that progresses through `node1` and `node2`. Each node updates the single state key `my_key`:

import pytest

from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

class MyState(TypedDict):
my_key: str

graph = StateGraph(MyState)
graph.add_node("node1", lambda state: {"my_key": "hello from node1"})
graph.add_node("node2", lambda state: {"my_key": "hello from node2"})
graph.add_edge(START, "node1")
graph.add_edge("node1", "node2")
graph.add_edge("node2", END)
return graph

checkpointer = MemorySaver()
graph = create_graph()
compiled_graph = graph.compile(checkpointer=checkpointer)
result = compiled_graph.invoke(
{"my_key": "initial_value"},
config={"configurable": {"thread_id": "1"}}
)
assert result["my_key"] == "hello from node2"

## ​ Testing individual nodes and edges

Compiled LangGraph agents expose references to each individual node as `graph.nodes`. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:

# Will be ignored in this example
checkpointer = MemorySaver()
graph = create_graph()
compiled_graph = graph.compile(checkpointer=checkpointer)
# Only invoke node 1
result = compiled_graph.nodes["node1"].invoke(
{"my_key": "initial_value"},
)
assert result["my_key"] == "hello from node1"

## ​ Partial execution

For agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to restructure these sections as subgraphs, which you can invoke in isolation as normal.However, if you do not wish to make changes to your agent graph’s overall structure, you can use LangGraph’s persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:

1. Compile your agent with a checkpointer (the in-memory checkpointer `InMemorySaver` will suffice for testing).
2. Call your agent’s `update_state` method with an `as_node` parameter set to the name of the node _before_ the one you want to start your test.
3. Invoke your agent with the same `thread_id` you used to update the state and an `interrupt_after` parameter set to the name of the node you want to stop at.

Here’s an example that executes only the second and third nodes in a linear graph:

graph = StateGraph(MyState)
graph.add_node("node1", lambda state: {"my_key": "hello from node1"})
graph.add_node("node2", lambda state: {"my_key": "hello from node2"})
graph.add_node("node3", lambda state: {"my_key": "hello from node3"})
graph.add_node("node4", lambda state: {"my_key": "hello from node4"})
graph.add_edge(START, "node1")
graph.add_edge("node1", "node2")
graph.add_edge("node2", "node3")
graph.add_edge("node3", "node4")
graph.add_edge("node4", END)
return graph

checkpointer = MemorySaver()
graph = create_graph()
compiled_graph = graph.compile(checkpointer=checkpointer)
compiled_graph.update_state(
config={
"configurable": {
"thread_id": "1"
}
},
# The state passed into node 2 - simulating the state at
# the end of node 1
values={"my_key": "initial_value"},
# Update saved state as if it came from node 1
# Execution will resume at node 2
as_node="node1",
)
result = compiled_graph.invoke(
# Resume execution by passing None
None,
config={"configurable": {"thread_id": "1"}},
# Stop after node 3 so that node 4 doesn't run
interrupt_after="node3",
)
assert result["my_key"] == "hello from node3"

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Application structure\\
\\
Previous LangSmith Studio\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/studio

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

LangSmith Studio

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Prerequisites
- Set up local Agent server
- 1\. Install the LangGraph CLI
- 2\. Prepare your agent
- 3\. Environment variables
- 4\. Create a LangGraph config file
- 5\. Install dependencies
- 6\. View your agent in Studio
- Video guide

When building agents with LangChain locally, it’s helpful to visualize what’s happening inside your agent, interact with it in real-time, and debug issues as they occur. **LangSmith Studio** is a free visual interface for developing and testing your LangChain agents from your local machine.Studio connects to your locally running agent to show you each step your agent takes: the prompts sent to the model, tool calls and their results, and the final output. You can test different inputs, inspect intermediate states, and iterate on your agent’s behavior without additional code or deployment.This pages describes how to set up Studio with your local LangChain agent.

## ​ Prerequisites

Before you begin, ensure you have the following:

- **A LangSmith account**: Sign up (for free) or log in at smith.langchain.com.
- **A LangSmith API key**: Follow the Create an API key guide.
- If you don’t want data traced to LangSmith, set `LANGSMITH_TRACING=false` in your application’s `.env` file. With tracing disabled, no data leaves your local server.

## ​ Set up local Agent server

### ​ 1\. Install the LangGraph CLI

The LangGraph CLI provides a local development server (also called Agent Server) that connects your agent to Studio.

Copy

pip install --upgrade "langgraph-cli[inmem]"

### ​ 2\. Prepare your agent

If you already have a LangChain agent, you can use it directly. This example uses a simple email agent:

agent.py

from langchain.agents import create_agent

def send_email(to: str, subject: str, body: str):
"""Send an email"""
email = {
"to": to,
"subject": subject,
"body": body
}
# ... email sending logic

return f"Email sent to {to}"

agent = create_agent(
"gpt-4o",
tools=[send_email],
system_prompt="You are an email assistant. Always use the send_email tool.",
)

### ​ 3\. Environment variables

Studio requires a LangSmith API key to connect your local agent. Create a `.env` file in the root of your project and add your API key from LangSmith.

Ensure your `.env` file is not committed to version control, such as Git.

.env

LANGSMITH_API_KEY=lsv2...

### ​ 4\. Create a LangGraph config file

The LangGraph CLI uses a configuration file to locate your agent and manage dependencies. Create a `langgraph.json` file in your app’s directory:

langgraph.json

{
"dependencies": ["."],
"graphs": {
"agent": "./src/agent.py:agent"
},
"env": ".env"
}

The `create_agent` function automatically returns a compiled LangGraph graph, which is what the `graphs` key expects in the configuration file.

For detailed explanations of each key in the JSON object of the configuration file, refer to the LangGraph configuration file reference.

At this point, the project structure will look like this:

my-app/
├── src
│ └── agent.py
├── .env
└── langgraph.json

### ​ 5\. Install dependencies

Install your project dependencies from the root directory:

pip

uv

pip install langchain langchain-openai

### ​ 6\. View your agent in Studio

Start the development server to connect your agent to Studio:

langgraph dev

Safari blocks `localhost` connections to Studio. To work around this, run the above command with `--tunnel` to access Studio via a secure tunnel.

Once the server is running, your agent is accessible both via API at `http://127.0.0.1:2024` and through the Studio UI at `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`:

With Studio connected to your local agent, you can iterate quickly on your agent’s behavior. Run a test input, inspect the full execution trace including prompts, tool arguments, return values, and token/latency metrics. When something goes wrong, Studio captures exceptions with the surrounding state to help you understand what happened.The development server supports hot-reloading—make changes to prompts or tool signatures in your code, and Studio reflects them immediately. Re-run conversation threads from any step to test your changes without starting over. This workflow scales from simple single-tool agents to complex multi-node graphs.For more information on how to run Studio, refer to the following guides in the LangSmith docs:

- Run application
- Manage assistants
- Manage threads
- Iterate on prompts
- Debug LangSmith traces
- Add node to dataset

## ​ Video guide

LangSmith Studio v2: The Ultimate Agent Development Environment - YouTube

Photo image of LangChain

LangChain

165K subscribers

LangSmith Studio v2: The Ultimate Agent Development Environment

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Watch on

0:00

0:00 / 8:09

•Live

•

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Test\\
\\
Previous Agent Chat UI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/ui

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

Agent Chat UI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Quick start
- Local development
- Connect to your agent

Agent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI works seamlessly with agents created using `create_agent` and provides interactive experiences for your agents with minimal setup, whether you’re running locally or in a deployed context (such as LangSmith).Agent Chat UI is open source and can be adapted to your application needs.

Introducing Agent Chat UI - YouTube

Photo image of LangChain

LangChain

165K subscribers

Introducing Agent Chat UI

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Why am I seeing this?

Watch on

0:00

0:00 / 5:59

•Live

•

You can use generative UI in the Agent Chat UI. For more information, see Implement generative user interfaces with LangGraph.

### ​ Quick start

The fastest way to get started is using the hosted version:

1. **Visit Agent Chat UI**
2. **Connect your agent** by entering your deployment URL or local server address
3. **Start chatting** \- the UI will automatically detect and render tool calls and interrupts

### ​ Local development

For customization or local development, you can run Agent Chat UI locally:

Use npx

Clone repository

Copy

# Create a new Agent Chat UI project
npx create-agent-chat-app --project-name my-chat-ui
cd my-chat-ui

# Install dependencies and start
pnpm install
pnpm dev

### ​ Connect to your agent

Agent Chat UI can connect to both local and deployed agents.After starting Agent Chat UI, you’ll need to configure it to connect to your agent:

1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)
2. **Deployment URL**: Your Agent server’s endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent’s URL)
3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you’re using a local Agent server)

Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.

Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see Hiding Messages in the Chat.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Studio\\
\\
Previous LangSmith Deployment\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/deploy

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

LangSmith Deployment

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Prerequisites
- Deploy your agent
- 1\. Create a repository on GitHub
- 2\. Deploy to LangSmith
- 3\. Test your application in Studio
- 4\. Get the API URL for your deployment
- 5\. Test the API

This guide shows you how to deploy your agent to **LangSmith Cloud**, a fully managed hosting platform designed for agent workloads. With Cloud deployment, you can deploy directly from your GitHub repository—LangSmith handles the infrastructure, scaling, and operational concerns.Traditional hosting platforms are built for stateless, short-lived web applications. LangSmith Cloud is **purpose-built for stateful, long-running agents** that require persistent state and background execution.

LangSmith offers multiple deployment options beyond Cloud, including deploying with a control plane (hybrid/self-hosted) or as standalone servers. For more information, refer to the Deployment overview.

## ​ Prerequisites

Before you begin, ensure you have the following:

- A GitHub account
- A LangSmith account (free to sign up)

## ​ Deploy your agent

### ​ 1\. Create a repository on GitHub

Your application’s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide. Then, push your code to the repository.

### ​ 2\. Deploy to LangSmith

1

Navigate to LangSmith Deployment

Log in to LangSmith. In the left sidebar, select **Deployments**.

2

Create new deployment

Click the **\+ New Deployment** button. A pane will open where you can fill in the required fields.

3

Link repository

If you are a first time user or adding a private repository that has not been previously connected, click the **Add new account** button and follow the instructions to connect your GitHub account.

4

Deploy repository

Select your application’s repository. Click **Submit** to deploy. This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.

### ​ 3\. Test your application in Studio

Once your application is deployed:

1. Select the deployment you just created to view more details.
2. Click the **Studio** button in the top right corner. Studio will open to display your graph.

### ​ 4\. Get the API URL for your deployment

1. In the **Deployment details** view in LangGraph, click the **API URL** to copy it to your clipboard.
2. Click the `URL` to copy it to the clipboard.

### ​ 5\. Test the API

You can now test the API:

- Python

- Rest API

1. Install LangGraph Python:

Copy

pip install langgraph-sdk

2. Send a message to the agent:

from langgraph_sdk import get_sync_client # or get_client for async

client = get_sync_client(url="your-deployment-url", api_key="your-langsmith-api-key")

for chunk in client.runs.stream(
None, # Threadless run
"agent", # Name of agent. Defined in langgraph.json.
input={
"messages": [{\
"role": "human",\
"content": "What is LangGraph?",\
}],
},
stream_mode="updates",
):
print(f"Receiving new event of type: {chunk.event}...")
print(chunk.data)
print("\n\n")

curl -s --request POST \

--header 'Content-Type: application/json' \

--data "{
\"assistant_id\": \"agent\", `# Name of agent. Defined in langgraph.json.`
\"input\": {
\"messages\": [\
{\
\"role\": \"human\",\
\"content\": \"What is LangGraph?\"\
}\
]
},
\"stream_mode\": \"updates\"
}"

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Agent Chat UI\\
\\
Previous LangSmith Observability\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/observability

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Production

LangSmith Observability

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Prerequisites
- Enable tracing
- Trace selectively
- Log to a project
- Add metadata to traces
- Use anonymizers to prevent logging of sensitive data in traces

Traces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use LangSmith to visualize these execution steps. To use it, enable tracing for your application. This enables you to do the following:

- Debug a locally running application.
- Evaluate the application performance.
- Monitor the application.

## ​ Prerequisites

Before you begin, ensure you have the following:

- **A LangSmith account**: Sign up (for free) or log in at smith.langchain.com.
- **A LangSmith API key**: Follow the Create an API key guide.

## ​ Enable tracing

To enable tracing for your application, set the following environment variables:

Copy

export LANGSMITH_TRACING=true

By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see Log to a project.For more information, see Trace with LangGraph.

## ​ Trace selectively

You may opt to trace specific invocations or parts of your application using LangSmith’s `tracing_context` context manager:

import langsmith as ls

# This WILL be traced
with ls.tracing_context(enabled=True):
agent.invoke({"messages": [{"role": "user", "content": "Send a test email to alice@example.com"}]})

# This will NOT be traced (if LANGSMITH_TRACING is not set)
agent.invoke({"messages": [{"role": "user", "content": "Send another email"}]})

## ​ Log to a project

Statically

You can set a custom project name for your entire application by setting the `LANGSMITH_PROJECT` environment variable:

export LANGSMITH_PROJECT=my-agent-project

Dynamically

You can set the project name programmatically for specific operations:

with ls.tracing_context(project_name="email-agent-test", enabled=True):
response = agent.invoke({
"messages": [{"role": "user", "content": "Send a welcome email"}]
})

## ​ Add metadata to traces

You can annotate your traces with custom metadata and tags:

response = agent.invoke(
{"messages": [{"role": "user", "content": "Send a welcome email"}]},
config={
"tags": ["production", "email-assistant", "v1.0"],
"metadata": {
"user_id": "user_123",
"session_id": "session_456",
"environment": "production"
}
}
)

`tracing_context` also accepts tags and metadata for fine-grained control:

with ls.tracing_context(
project_name="email-agent-test",
enabled=True,
tags=["production", "email-assistant", "v1.0"],
metadata={"user_id": "user_123", "session_id": "session_456", "environment": "production"}):
response = agent.invoke(
{"messages": [{"role": "user", "content": "Send a welcome email"}]}
)

This custom metadata and tags will be attached to the trace in LangSmith.

To learn more about how to use traces to debug, evaluate, and monitor your agents, see the LangSmith documentation.

## ​ Use anonymizers to prevent logging of sensitive data in traces

You may want to mask sensitive data to prevent it from being logged to LangSmith. You can create anonymizers and apply them to
your graph using configuration. This example will redact anything matching the Social Security Number format XXX-XX-XXXX from traces sent to LangSmith.

from langchain_core.tracers.langchain import LangChainTracer
from langgraph.graph import StateGraph, MessagesState
from langsmith import Client
from langsmith.anonymizer import create_anonymizer

anonymizer = create_anonymizer([\
# Matches SSNs\

])

tracer_client = Client(anonymizer=anonymizer)
tracer = LangChainTracer(client=tracer_client)
# Define the graph
graph = (
StateGraph(MessagesState)
...
.compile()
.with_config({'callbacks': [tracer]})
)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Deployment\\
\\
Previous Choosing between Graph and Functional APIs\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/pregel

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangGraph APIs

LangGraph runtime

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

On this page

- Overview
- Actors
- Channels
- Examples
- High-level API

## ​ Overview

In LangGraph, Pregel combines **actors** and **channels** into a single application. **Actors** read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the **Pregel Algorithm**/ **Bulk Synchronous Parallel** model.Each step consists of three phases:

- **Plan**: Determine which **actors** to execute in this step. For example, in the first step, select the **actors** that subscribe to the special **input** channels; in subsequent steps, select the **actors** that subscribe to channels updated in the previous step.
- **Execution**: Execute all selected **actors** in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.
- **Update**: Update the channels with the values written by the **actors** in this step.

Repeat until no **actors** are selected for execution, or a maximum number of steps is reached.

## ​ Actors

An **actor** is a `PregelNode`. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an **actor** in the Pregel algorithm. `PregelNodes` implement LangChain’s Runnable interface.

## ​ Channels

Channels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function – which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:

- `LastValue`: The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.
- `Topic`: A configurable PubSub Topic, useful for sending multiple values between **actors**, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.
- `BinaryOperatorAggregate`: stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,`total = BinaryOperatorAggregate(int, operator.add)`

## ​ Examples

While most users will interact with Pregel through the StateGraph API or the `@entrypoint` decorator, it is possible to interact with Pregel directly.Below are a few different examples to give you a sense of the Pregel API.

- Single node

- Multiple nodes

- Topic

- BinaryOperatorAggregate

- Cycle

Copy

from langgraph.channels import EphemeralValue
from langgraph.pregel import Pregel, NodeBuilder

node1 = (
NodeBuilder().subscribe_only("a")
.do(lambda x: x + x)
.write_to("b")
)

app = Pregel(
nodes={"node1": node1},
channels={
"a": EphemeralValue(str),
"b": EphemeralValue(str),
},
input_channels=["a"],
output_channels=["b"],
)

app.invoke({"a": "foo"})

{'b': 'foofoo'}

from langgraph.channels import LastValue, EphemeralValue
from langgraph.pregel import Pregel, NodeBuilder

node2 = (
NodeBuilder().subscribe_only("b")
.do(lambda x: x + x)
.write_to("c")
)

app = Pregel(
nodes={"node1": node1, "node2": node2},
channels={
"a": EphemeralValue(str),
"b": LastValue(str),
"c": EphemeralValue(str),
},
input_channels=["a"],
output_channels=["b", "c"],
)

{'b': 'foofoo', 'c': 'foofoofoofoo'}

from langgraph.channels import EphemeralValue, Topic
from langgraph.pregel import Pregel, NodeBuilder

node1 = (
NodeBuilder().subscribe_only("a")
.do(lambda x: x + x)
.write_to("b", "c")
)

node2 = (
NodeBuilder().subscribe_to("b")
.do(lambda x: x["b"] + x["b"])
.write_to("c")
)

app = Pregel(
nodes={"node1": node1, "node2": node2},
channels={
"a": EphemeralValue(str),
"b": EphemeralValue(str),
"c": Topic(str, accumulate=True),
},
input_channels=["a"],
output_channels=["c"],
)

{'c': ['foofoo', 'foofoofoofoo']}

This example demonstrates how to use the `BinaryOperatorAggregate` channel to implement a reducer.

from langgraph.channels import EphemeralValue, BinaryOperatorAggregate
from langgraph.pregel import Pregel, NodeBuilder

def reducer(current, update):
if current:
return current + " | " + update
else:
return update

app = Pregel(
nodes={"node1": node1, "node2": node2},
channels={
"a": EphemeralValue(str),
"b": EphemeralValue(str),
"c": BinaryOperatorAggregate(str, operator=reducer),
},
input_channels=["a"],
output_channels=["c"],
)

This example demonstrates how to introduce a cycle in the graph, by having
a chain write to a channel it subscribes to. Execution will continue
until a `None` value is written to the channel.

from langgraph.channels import EphemeralValue
from langgraph.pregel import Pregel, NodeBuilder, ChannelWriteEntry

example_node = (
NodeBuilder().subscribe_only("value")
.do(lambda x: x + x if len(x) < 10 else None)
.write_to(ChannelWriteEntry("value", skip_none=True))
)

app = Pregel(
nodes={"example_node": example_node},
channels={
"value": EphemeralValue(str),
},
input_channels=["value"],
output_channels=["value"],
)

app.invoke({"value": "a"})

{'value': 'aaaaaaaaaaaaaaaa'}

## ​ High-level API

LangGraph provides two high-level APIs for creating a Pregel application: the StateGraph (Graph API) and the Functional API.

- StateGraph (Graph API)

The StateGraph (Graph API) is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.

from typing import TypedDict

from langgraph.constants import START
from langgraph.graph import StateGraph

class Essay(TypedDict):
topic: str
content: str | None
score: float | None

def write_essay(essay: Essay):
return {
"content": f"Essay about {essay['topic']}",
}

def score_essay(essay: Essay):
return {
"score": 10
}

builder = StateGraph(Essay)
builder.add_node(write_essay)
builder.add_node(score_essay)
builder.add_edge(START, "write_essay")
builder.add_edge("write_essay", "score_essay")

# Compile the graph.
# This will return a Pregel instance.
graph = builder.compile()

The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them.

print(graph.nodes)

You will see something like this:

print(graph.channels)

You should see something like this

In the Functional API, you can use an `@entrypoint` to create a Pregel application. The `entrypoint` decorator allows you to define a function that takes input and returns output.

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.func import entrypoint

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def write_essay(essay: Essay):
return {
"content": f"Essay about {essay['topic']}",
}

print("Nodes: ")
print(write_essay.nodes)
print("Channels: ")
print(write_essay.channels)

Nodes:

Channels:

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Use the functional API\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/concepts/memory

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Conceptual overviews

Memory overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Short-term memory
- Manage short-term memory
- Long-term memory
- Semantic memory
- Profile
- Collection
- Episodic memory
- Procedural memory
- Writing memories
- In the hot path
- In the background
- Memory storage

Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.This conceptual guide covers two types of memory, based on their recall scope:

- Short-term memory, or thread-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent’s state. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.
- Long-term memory stores user-specific or application-level data across sessions and is shared _across_ conversational threads. It can be recalled _at any time_ and _in any thread_. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores ( reference doc) to let you save and recall long-term memories.

## ​ Short-term memory

Short-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.LangGraph manages short-term memory as part of the agent’s state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph’s state, the bot can access the full context for a given conversation while maintaining separation between different threads.

### ​ Manage short-term memory

Conversation history is the most common form of short-term memory, and long conversations pose a challenge to today’s LLMs. A full history may not fit inside an LLM’s context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get “distracted” by stale or off-topic content, all while suffering from slower response times and higher costs.Chat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.!FilterFor more information on common techniques for managing messages, see the Add and manage memory guide.

## ​ Long-term memory

Long-term memory in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is **thread-scoped**, long-term memory is saved within custom “namespaces.”Long-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques:

- What is the type of memory? Humans use memories to remember facts ( semantic memory), experiences ( episodic memory), and rules ( procedural memory). AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task.
- When do you want to update memories? Memory can be updated as part of an agent’s application logic (e.g., “on the hot path”). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the section below.

Different applications require various types of memory. Although the analogy isn’t perfect, examining human memory types can be insightful. Some research (e.g., the CoALA paper) have even mapped these human memory types to those used in AI agents.

| Memory Type | What is Stored | Human Example | Agent Example |
| --- | --- | --- | --- |
| Semantic | Facts | Things I learned in school | Facts about a user |
| Episodic | Experiences | Things I did | Past agent actions |
| Procedural | Instructions | Instincts or motor skills | Agent system prompt |

### ​ Semantic memory

Semantic memory, both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions.

Semantic memory is different from “semantic search,” which is a technique for finding similar content using “meaning” (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches.

Semantic memories can be managed in different ways:

#### ​ Profile

Memories can be a single, continuously updated “profile” of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you’ve selected to represent your domain.When remembering a profile, you will want to make sure that you are **updating** the profile each time. As a result, you will want to pass in the previous profile and ask the model to generate a new profile (or some JSON patch to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or **strict** decoding when generating documents to ensure the memory schemas remains valid.!Update profile

#### ​ Collection

Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you’re less likely to **lose** information over time. It’s easier for an LLM to generate _new_ objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to higher recall downstream.However, this shifts some complexity memory updating. The model must now _delete_ or _update_ existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the Trustcall package for one way to manage this and consider evaluation (e.g., with a tool like LangSmith) to help you tune the behavior.Working with document collections also shifts complexity to memory **search** over the list. The `Store` currently supports both semantic search and filtering by content.Finally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach.!Update listRegardless of memory management approach, the central point is that the agent will use the semantic memories to ground its responses, which often leads to more personalized and relevant interactions.

### ​ Episodic memory

Episodic memory, in both humans and AI agents, involves recalling past events or actions. The CoALA paper frames this well: facts can be written to semantic memory, whereas _experiences_ can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task.In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it’s easier to “show” than “tell” and LLMs learn well from examples. Few-shot learning lets you “program” your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input.Note that the memory store is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a LangSmith Dataset to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity ( using a BM25-like algorithm for keyword based similarity).See this how-to video for example usage of dynamic few-shot example selection in LangSmith. Also, see this blog post showcasing few-shot prompting to improve tool calling performance and this blog post using few-shot example to align an LLMs to human preferences.

### ​ Procedural memory

Procedural memory, in both humans and AI agents, involves remembering the rules used to perform tasks. In humans, procedural memory is like the internalized knowledge of how to perform tasks, such as riding a bike via basic motor skills and balance. Episodic memory, on the other hand, involves recalling specific experiences, such as the first time you successfully rode a bike without training wheels or a memorable bike ride through a scenic route. For AI agents, procedural memory is a combination of model weights, agent code, and agent’s prompt that collectively determine the agent’s functionality.In practice, it is fairly uncommon for agents to modify their model weights or rewrite their code. However, it is more common for agents to modify their own prompts.One effective approach to refining an agent’s instructions is through “Reflection” or meta-prompting. This involves prompting the agent with its current instructions (e.g., the system prompt) along with recent conversations or explicit user feedback. The agent then refines its own instructions based on this input. This method is particularly useful for tasks where instructions are challenging to specify upfront, as it allows the agent to learn and adapt from its interactions.For example, we built a Tweet generator using external feedback and prompt re-writing to produce high-quality paper summaries for Twitter. In this case, the specific summarization prompt was difficult to specify _a priori_, but it was fairly easy for a user to critique the generated Tweets and provide feedback on how to improve the summarization process.The below pseudo-code shows how you might implement this with the LangGraph memory store, using the store to save a prompt, the `update_instructions` node to get the current prompt (as well as feedback from the conversation with the user captured in `state["messages"]`), update the prompt, and save the new prompt [0]
# Application logic
prompt = prompt_template.format(instructions=instructions.value["instructions"])
...

# Node that updates instructions
def update_instructions(state: State, store: BaseStore):
namespace = ("instructions",)
instructions = store.search(namespace)[0]
# Memory logic
prompt = prompt_template.format(instructions=instructions.value["instructions"], conversation=state["messages"])
output = llm.invoke(prompt)
new_instructions = output['new_instructions']
store.put(("agent_instructions",), "agent_a", {"instructions": new_instructions})
...

### ​ Writing memories

There are two primary methods for agents to write memories: “in the hot path” and “in the background”.!Hot path vs background

#### ​ In the hot path

Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.As an example, ChatGPT uses a save\_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.

#### ​ In the background

Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.See our memory-service template as an reference implementation.

### ​ Memory storage

LangGraph stores long-term memories as JSON documents in a store. Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.

Copy

from langgraph.store.memory import InMemoryStore

# Replace with an actual embedding function or LangChain embeddings object
return [[1.0, 2.0] * len(texts)]

# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.
store = InMemoryStore(index={"embed": embed, "dims": 2})
user_id = "my-user"
application_context = "chitchat"
namespace = (user_id, application_context)
store.put(
namespace,
"a-memory",
{
"rules": [\
"User likes short, direct language",\
"User only speaks English & python",\
],
"my-key": "my-value",
},
)
# get the "memory" by ID
item = store.get(namespace, "a-memory")
# search for "memories" within this namespace, filtering on content equivalence, sorted by vector similarity
items = store.search(
namespace, filter={"my-key": "my-value"}, query="language preferences"
)

For more information about the memory store, see the Persistence guide.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Component architecture\\
\\
Previous Context overview\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/install)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Install LangGraph LangGraph Python SDK LangSmith Studio

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/local-server)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment components LangSmith Studio LangGraph Python SDK

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK Thinking in LangGraph LangSmith reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/workflows-agents)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Workflows and agents LangGraph Python SDK Build a custom RAG agent with LangGraph

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/persistence)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK LangSmith reference Install LangGraph

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/durable-execution)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Durable execution LangGraph Python SDK What's new in LangGraph v1

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/streaming)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK LangSmith reference LangGraph overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/interrupts)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK Interrupts LangSmith reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/use-time-travel)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Use time-travel LangGraph Python SDK Time travel using the server API

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/add-memory)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK Install LangGraph Data storage and privacy

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/use-subgraphs)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK Install LangGraph LangGraph CLI

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/application-structure)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK Application structure Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/test)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK LangSmith reference Install LangGraph

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/studio)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK Install LangGraph LangSmith Studio

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/ui)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK How to implement generative user interfaces with LangGraph LangSmith reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/deploy)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK LangSmith Deployment components LangSmith reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/observability)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/pregel)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph runtime LangGraph Python SDK LangSmith reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/tools).We

Skip to main content.We#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/durable-execution):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Durable execution LangGraph Python SDK What's new in LangGraph v1

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/interrupts):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK Interrupts LangSmith reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/concepts/memory):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Contributing to documentation Data storage and privacy Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/quickstart

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Quickstart

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

On this page

- Prerequisites
- Step 1: Install dependencies
- Step 2: Set up your API keys
- Step 3: Create a search tool
- Step 4: Create a deep agent
- Step 5: Run the agent
- What happened?
- Next steps

This guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. You’ll build a research agent that can conduct research and write reports.

## ​ Prerequisites

Before you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI).

### ​ Step 1: Install dependencies

npm

yarn

pnpm

Copy

npm install deepagents @langchain/tavily

### ​ Step 2: Set up your API keys

export ANTHROPIC_API_KEY="your-api-key"
export TAVILY_API_KEY="your-tavily-api-key"

### ​ Step 3: Create a search tool

import { tool } from "langchain";
import { TavilySearch } from "@langchain/tavily";
import { z } from "zod";

const internetSearch = tool(
async ({
query,
maxResults = 5,
topic = "general",
includeRawContent = false,
}: {
query: string;
maxResults?: number;
topic?: "general" | "news" | "finance";
includeRawContent?: boolean;

const tavilySearch = new TavilySearch({
maxResults,
tavilyApiKey: process.env.TAVILY_API_KEY,
includeRawContent,
topic,
});
return await tavilySearch._call({ query });
},
{
name: "internet_search",
description: "Run a web search",
schema: z.object({
query: z.string().describe("The search query"),
maxResults: z
.number()
.optional()
.default(5)
.describe("Maximum number of results to return"),
topic: z
.enum(["general", "news", "finance"])
.optional()
.default("general")
.describe("Search topic category"),
includeRawContent: z
.boolean()
.optional()
.default(false)
.describe("Whether to include raw content"),
}),
},
);

### ​ Step 4: Create a deep agent

import { createDeepAgent } from "deepagents";

// System prompt to steer the agent to be an expert researcher
const researchInstructions = `You are an expert researcher. Your job is to conduct thorough research and then write a polished report.

You have access to an internet search tool as your primary means of gathering information.

## \`internet_search\`

Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.
`;

const agent = createDeepAgent({
tools: [internetSearch],
systemPrompt: researchInstructions,
});

### ​ Step 5: Run the agent

const result = await agent.invoke({
messages: [{ role: "user", content: "What is langgraph?" }],
});

// Print the agent's response
console.log(result.messages[result.messages.length - 1].content);

## ​ What happened?

Your deep agent automatically:

1. **Planned its approach**: Used the built-in `write_todos` tool to break down the research task
2. **Conducted research**: Called the `internet_search` tool to gather information
3. **Managed context**: Used file system tools (`write_file`, `read_file`) to offload large search results
4. **Spawned subagents** (if needed): Delegated complex subtasks to specialized subagents
5. **Synthesized a report**: Compiled findings into a coherent response

## ​ Next steps

Now that you’ve built your first deep agent:

- **Customize your agent**: Learn about customization options, including custom system prompts, tools, and subagents.
- **Understand middleware**: Dive into the middleware architecture that powers deep agents.
- **Add long-term memory**: Enable persistent memory across conversations.
- **Deploy to production**: Learn about deployment options for LangGraph applications.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Deep Agents overview\\
\\
Previous Customize Deep Agents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/customization

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Customize Deep Agents

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

On this page

- Model
- System prompt
- Tools

create\_deep\_agent

Core Config

Features

Model

System Prompt

Tools

Backend

Subagents

Interrupts

Customized Agent

## ​ Model

By default, `deepagents` uses `claude-sonnet-4-5-20250929`. You can customize the model used by passing any supported model identifier string or LangChain model object.

Copy

import { ChatAnthropic } from "@langchain/anthropic";
import { ChatOpenAI } from "@langchain/openai";
import { createDeepAgent } from "deepagents";

// Using Anthropic
const agent = createDeepAgent({
model: new ChatAnthropic({
model: "claude-sonnet-4-20250514",
temperature: 0,
}),
});

// Using OpenAI
const agent2 = createDeepAgent({
model: new ChatOpenAI({
model: "gpt-5",
temperature: 0,
}),
});

## ​ System prompt

Deep agents come with a built-in system prompt inspired by Claude Code’s system prompt. The default system prompt contains detailed instructions for using the built-in planning tool, file system tools, and subagents.Each deep agent tailored to a use case should include a custom system prompt specific to that use case.

import { createDeepAgent } from "deepagents";

const researchInstructions = `You are an expert researcher. Your job is to conduct thorough research, and then write a polished report.`;

const agent = createDeepAgent({
systemPrompt: researchInstructions,
});

## ​ Tools

Just like tool-calling agents, a deep agent gets a set of top level tools that it has access to.

import { tool } from "langchain";
import { TavilySearch } from "@langchain/tavily";
import { createDeepAgent } from "deepagents";
import { z } from "zod";

const internetSearch = tool(
async ({
query,
maxResults = 5,
topic = "general",
includeRawContent = false,
}: {
query: string;
maxResults?: number;
topic?: "general" | "news" | "finance";
includeRawContent?: boolean;

const tavilySearch = new TavilySearch({
maxResults,
tavilyApiKey: process.env.TAVILY_API_KEY,
includeRawContent,
topic,
});
return await tavilySearch._call({ query });
},
{
name: "internet_search",
description: "Run a web search",
schema: z.object({
query: z.string().describe("The search query"),
maxResults: z.number().optional().default(5),
topic: z
.enum(["general", "news", "finance"])
.optional()
.default("general"),
includeRawContent: z.boolean().optional().default(false),
}),
},
);

const agent = createDeepAgent({
tools: [internetSearch],
});

In addition to any tools that you provide, deep agents also get access to a number of default tools:

- `write_todos` – Update the agent’s to-do list
- `ls` – List all files in the agent’s filesystem
- `read_file` – Read a file from the agent’s filesystem
- `write_file` – Write a new file in the agent’s filesystem
- `edit_file` – Edit an existing file in the agent’s filesystem
- `task` – Spawn a subagent to handle a specific task

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Quickstart\\
\\
Previous Agent harness capabilities\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/harness

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Agent harness capabilities

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

On this page

- File system access
- Large tool result eviction
- Pluggable storage backends
- Task delegation (subagents)
- Conversation history summarization
- Dangling tool call repair
- To-do list tracking
- Human-in-the-Loop
- Prompt caching (Anthropic)

We think of `deepagents` as an “agent harness”. It is the same core tool calling loop as other agent frameworks, but with built-in tools and capabilities.

isolated work

Deep Agent

File System Tools

To-Do List

Subagents

Storage Backend

State

Filesystem

Store

Final Result

This page lists out the components that make up the agent harness.

## ​ File system access

The harness provides six tools for file system operations, making files first-class citizens in the agent’s environment:

| Tool | Description |
| --- | --- |
| `ls` | List files in a directory with metadata (size, modified time) |
| `read_file` | Read file contents with line numbers, supports offset/limit for large files |
| `write_file` | Create new files |
| `edit_file` | Perform exact string replacements in files (with global replace mode) |
| `glob` | Find files matching patterns (e.g., `**/*.py`) |
| `grep` | Search file contents with multiple output modes (files only, content with context, or counts) |

## ​ Large tool result eviction

The harness automatically dumps large tool results to the file system when they exceed a token threshold, preventing context window saturation.**How it works:**

- Monitors tool call results for size (default threshold: 20,000 tokens)
- When exceeded, writes the result to a file instead
- Replaces the tool result with a concise reference to the file
- Agent can later read the file if needed

## ​ Pluggable storage backends

The harness abstracts file system operations behind a protocol, allowing different storage strategies for different use cases.**Available backends:**

1. **StateBackend** \- Ephemeral in-memory storage - Files live in the agent’s state (checkpointed with conversation)
- Persists within a thread but not across threads
- Useful for temporary working files
2. **FilesystemBackend** \- Real filesystem access - Read/write from actual disk
- Supports virtual mode (sandboxed to a root directory)
- Integrates with system tools (ripgrep for grep)
- Security features: path validation, size limits, symlink prevention
3. **StoreBackend** \- Persistent cross-conversation storage - Uses LangGraph’s BaseStore for durability
- Namespaced per assistant\_id
- Files persist across conversations
- Useful for long-term memory or knowledge bases
4. **CompositeBackend** \- Route different paths to different backends - Example: `/` → StateBackend, `/memories/` → StoreBackend
- Longest-prefix matching for routing
- Enables hybrid storage strategies

## ​ Task delegation (subagents)

The harness allows the main agent to create ephemeral “subagents” for isolated multi-step tasks.**Why it’s useful:**

- **Context isolation** \- Subagent’s work doesn’t clutter main agent’s context
- **Parallel execution** \- Multiple subagents can run concurrently
- **Specialization** \- Subagents can have different tools/configurations
- **Token efficiency** \- Large subtask context is compressed into a single result

**How it works:**

- Main agent has a `task` tool
- When invoked, creates a fresh agent instance with its own context
- Subagent executes autonomously until completion
- Returns a single final report to the main agent
- Subagents are stateless (can’t send multiple messages back)

**Default subagent:**

- “general-purpose” subagent automatically available
- Has filesystem tools by default
- Can be customized with additional tools/middleware

**Custom subagents:**

- Define specialized subagents with specific tools
- Example: code-reviewer, web-researcher, test-runner
- Configure via `subagents` parameter

## ​ Conversation history summarization

The harness automatically compresses old conversation history when token usage becomes excessive.**Configuration:**

- Triggers at 170,000 tokens
- Keeps the most recent 6 messages intact
- Older messages are summarized by the model

**Why it’s useful:**

- Enables very long conversations without hitting context limits
- Preserves recent context while compressing ancient history
- Transparent to the agent (appears as a special system message)

## ​ Dangling tool call repair

The harness fixes message history when tool calls are interrupted or cancelled before receiving results.**The problem:**

- Agent requests tool call: “Please run X”
- Tool call is interrupted (user cancels, error, etc.)
- Agent sees tool\_call in AIMessage but no corresponding ToolMessage
- This creates an invalid message sequence

**The solution:**

- Detects AIMessages with tool\_calls that have no results
- Creates synthetic ToolMessage responses indicating the call was cancelled
- Repairs the message history before agent execution

- Prevents agent confusion from incomplete message chains
- Gracefully handles interruptions and errors
- Maintains conversation coherence

## ​ To-do list tracking

The harness provides a `write_todos` tool that agents can use to maintain a structured task list.**Features:**

- Track multiple tasks with statuses (pending, in\_progress, completed)
- Persisted in agent state
- Helps agent organize complex multi-step work
- Useful for long-running tasks and planning

## ​ Human-in-the-Loop

The harness pauses agent execution at specified tool calls to allow human approval/modification.**Configuration:**

- Map tool names to interrupt configurations
- Example: `{"edit_file": True}` \- pause before every edit
- Can provide approval messages or modify tool inputs

- Safety gates for destructive operations
- User verification before expensive API calls
- Interactive debugging and guidance

## ​ Prompt caching (Anthropic)

The harness enables Anthropic’s prompt caching feature to reduce redundant token processing.**How it works:**

- Caches portions of the prompt that repeat across turns
- Significantly reduces latency and cost for long system prompts
- Automatically skips for non-Anthropic models

- System prompts (especially with filesystem docs) can be 5k+ tokens
- These repeat every turn without caching
- Caching provides ~10x speedup and cost reduction

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Customize Deep Agents\\
\\
Previous Backends\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/backends

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Backends

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

On this page

- Quickstart
- Built-in backends
- StateBackend (ephemeral)
- FilesystemBackend (local disk)
- StoreBackend (LangGraph Store)
- CompositeBackend (router)
- Specify a backend
- Route to different backends
- Use a virtual filesystem
- Add policy hooks
- Protocol reference

Deep agents expose a filesystem surface to the agent via tools like `ls`, `read_file`, `write_file`, `edit_file`, `glob`, and `grep`. These tools operate through a pluggable backend.

Filesystem Tools

Backend

State

Filesystem

Store

Composite

Custom

Routes

This page explains how to choose a backend, route different paths to different backends, implement your own virtual filesystem (e.g., S3 or Postgres), add policy hooks, and comply with the backend protocol.

## ​ Quickstart

Here are a few pre-built filesystem backends that you can quickly use with your deep agent:

| Built-in backend | Description |
| --- | --- |

| Composite | Ephemeral by default, `/memories/` persisted. The Composite backend is maximally flexible. You can specify different routes in the filesystem to point towards different backends. See Composite routing below for a ready-to-paste example. |

## ​ Built-in backends

### ​ StateBackend (ephemeral)

Copy

import { createDeepAgent, StateBackend } from "deepagents";

// By default we provide a StateBackend
const agent = createDeepAgent();

// Under the hood, it looks like
const agent2 = createDeepAgent({

});

**How it works:**

- Stores files in LangGraph agent state for the current thread.
- Persists across multiple agent turns on the same thread via checkpoints.

**Best for:**

- A scratch pad for the agent to write intermediate results.
- Automatic eviction of large tool outputs which the agent can then read back in piece by piece.

### ​ FilesystemBackend (local disk)

import { createDeepAgent, FilesystemBackend } from "deepagents";

const agent = createDeepAgent({
backend: new FilesystemBackend({ rootDir: ".", virtualMode: true }),
});

- Reads/writes real files under a configurable `root_dir`.
- You can optionally set `virtual_mode=True` to sandbox and normalize paths under `root_dir`.
- Uses secure path resolution, prevents unsafe symlink traversal when possible, can use ripgrep for fast `grep`.

- Local projects on your machine
- CI sandboxes
- Mounted persistent volumes

### ​ StoreBackend (LangGraph Store)

import { createDeepAgent, StoreBackend } from "deepagents";
import { InMemoryStore } from "@langchain/langgraph";

const store = new InMemoryStore()
const agent = createDeepAgent({

store
});

- Stores files in a LangGraph `BaseStore` provided by the runtime, enabling cross‑thread durable storage.

- When you already run with a configured LangGraph store (for example, Redis, Postgres, or cloud implementations behind `BaseStore`).
- When you’re deploying your agent through LangSmith Deployment (a store is automatically provisioned for your agent).

### ​ CompositeBackend (router)

import { createDeepAgent, CompositeBackend, StateBackend, StoreBackend } from "deepagents";
import { InMemoryStore } from "@langchain/langgraph";

new StateBackend(rt),
{
"/memories/": new StoreBackend(rt),
}
);

const store = new InMemoryStore()
const agent = createDeepAgent({ backend: compositeBackend, store });

- Routes file operations to different backends based on path prefix.
- Preserves the original path prefixes in listings and search results.

- When you want to give your agent both ephemeral and cross-thread storage, a CompositeBackend allows you provide both a StateBackend and StoreBackend
- When you have multiple sources of information that you want to provide to your agent as part of a single filesystem.
- e.g. You have long-term memories stored under /memories/ in one Store and you also have a custom backend that has documentation accessible at /docs/.

## ​ Specify a backend

- Pass a backend to `create_deep_agent(backend=...)`. The filesystem middleware uses it for all tooling.
- You can pass either:
- An instance implementing `BackendProtocol` (for example, `FilesystemBackend(root_dir=".")`), or
- A factory `BackendFactory = Callable[[ToolRuntime], BackendProtocol]` (for backends that need runtime like `StateBackend` or `StoreBackend`).
- If omitted, the default is `lambda rt: StateBackend(rt)`.

## ​ Route to different backends

Route parts of the namespace to different backends. Commonly used to persist `/memories/*` and keep everything else ephemeral.

import { createDeepAgent, CompositeBackend, FilesystemBackend, StateBackend } from "deepagents";

new StateBackend(rt),
{
"/memories/": new FilesystemBackend({ rootDir: "/deepagents/myagent", virtualMode: true }),
},
);

const agent = createDeepAgent({ backend: compositeBackend });

Behavior:

- `/workspace/plan.md` → StateBackend (ephemeral)
- `/memories/agent.md` → FilesystemBackend under `/deepagents/myagent`
- `ls`, `glob`, `grep` aggregate results and show original path prefixes.

Notes:

- Longer prefixes win (for example, route `"/memories/projects/"` can override `"/memories/"`).
- For StoreBackend routing, ensure the agent runtime provides a store (`runtime.store`).

## ​ Use a virtual filesystem

Build a custom backend to project a remote or database filesystem (e.g., S3 or Postgres) into the tools namespace.Design guidelines:

- Paths are absolute (`/x/y.txt`). Decide how to map them to your storage keys/rows.
- Implement `ls_info` and `glob_info` efficiently (server-side listing where available, otherwise local filter).
- Return user-readable error strings for missing files or invalid regex patterns.
- For external persistence, set `files_update=None` in results; only in-state backends should return a `files_update` dict.

S3-style outline:Postgres-style outline:

- Table `files(path text primary key, content text, created_at timestamptz, modified_at timestamptz)`
- Map tool operations onto SQL:
- `ls_info` uses `WHERE path LIKE $1 || '%'`
- `glob_info` filter in SQL or fetch then apply glob in Python
- `grep_raw` can fetch candidate rows by extension or last modified time, then scan lines

## ​ Add policy hooks

Enforce enterprise rules by subclassing or wrapping a backend.Block writes/edits under selected prefixes (subclass):Generic wrapper (works with any backend):

## ​ Protocol reference

Backends must implement the `BackendProtocol`.Required endpoints:

- Return entries with at least `path`. Include `is_dir`, `size`, `modified_at` when available. Sort by `path` for deterministic output.

- Return numbered content. On missing file, return `"Error: File '/x' not found"`.

- Return structured matches. For an invalid regex, return a string like `"Invalid regex pattern: ..."` (do not raise).

- Return matched files as `FileInfo` entries (empty list if none).

- Create-only. On conflict, return `WriteResult(error=...)`. On success, set `path` and for state backends set `files_update={...}`; external backends should use `files_update=None`.

- Enforce uniqueness of `old_string` unless `replace_all=True`. If not found, return error. Include `occurrences` on success.

Supporting types:

- `WriteResult(error, path, files_update)`
- `EditResult(error, path, files_update, occurrences)`
- `FileInfo` with fields: `path` (required), optionally `is_dir`, `size`, `modified_at`.
- `GrepMatch` with fields: `path`, `line`, `text`.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Agent harness capabilities\\
\\
Previous Subagents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/subagents

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Subagents

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

On this page

- Why use subagents?
- Configuration
- SubAgent (Dictionary-based)
- CompiledSubAgent
- Using SubAgent
- Using CompiledSubAgent
- The general-purpose subagent
- When to use it
- Best practices
- Write clear descriptions
- Keep system prompts detailed
- Minimize tool sets
- Choose models by task
- Return concise results
- Common patterns
- Multiple specialized subagents
- Troubleshooting
- Subagent not being called
- Context still getting bloated
- Wrong subagent being selected

Deep agents can create subagents to delegate work. You can specify custom subagents in the `subagents` parameter. Subagents are useful for context quarantine (keeping the main agent’s context clean) and for providing specialized instructions.

task tool

isolated work

Main Agent

Subagent

Research

Code

General

Final Result

## ​ Why use subagents?

Subagents solve the **context bloat problem**. When agents use tools with large outputs (web search, file reads, database queries), the context window fills up quickly with intermediate results. Subagents isolate this detailed work—the main agent receives only the final result, not the dozens of tool calls that produced it.**When to use subagents:**

- ✅ Multi-step tasks that would clutter the main agent’s context
- ✅ Specialized domains that need custom instructions or tools
- ✅ Tasks requiring different model capabilities
- ✅ When you want to keep the main agent focused on high-level coordination

**When NOT to use subagents:**

- ❌ Simple, single-step tasks
- ❌ When you need to maintain intermediate context
- ❌ When the overhead outweighs benefits

## ​ Configuration

`subagents` should be a list of dictionaries or `CompiledSubAgent` objects. There are two types:

### ​ SubAgent (Dictionary-based)

For most use cases, define subagents as dictionaries:**Required fields:**

- **name** (`str`): Unique identifier for the subagent. The main agent uses this name when calling the `task()` tool.
- **description** (`str`): What this subagent does. Be specific and action-oriented. The main agent uses this to decide when to delegate.
- **system\_prompt** (`str`): Instructions for the subagent. Include tool usage guidance and output format requirements.
- **tools** (`List[Callable]`): Tools the subagent can use. Keep this minimal and include only what’s needed.

**Optional fields:**

- **model** (`str | BaseChatModel`): Override the main agent’s model. Use the format `"provider:model-name"` (for example, `"openai:gpt-4o"`).
- **middleware** (`List[Middleware]`): Additional middleware for custom behavior, logging, or rate limiting.
- **interrupt\_on** (`Dict[str, bool]`): Configure human-in-the-loop for specific tools. Requires a checkpointer.

### ​ CompiledSubAgent

For complex workflows, use a pre-built LangGraph graph:**Fields:**

- **name** (`str`): Unique identifier
- **description** (`str`): What this subagent does
- **runnable** (`Runnable`): A compiled LangGraph graph (must call `.compile()` first)

## ​ Using SubAgent

Copy

import { tool } from "langchain";
import { TavilySearch } from "@langchain/tavily";
import { createDeepAgent } from "deepagents";
import { ChatAnthropic } from "@langchain/anthropic";
import { z } from "zod";

const internetSearch = tool(
async ({
query,
maxResults = 5,
topic = "general",
includeRawContent = false,
}: {
query: string;
maxResults?: number;
topic?: "general" | "news" | "finance";
includeRawContent?: boolean;

const tavilySearch = new TavilySearch({
maxResults,
tavilyApiKey: process.env.TAVILY_API_KEY,
includeRawContent,
topic,
});
return await tavilySearch._call({ query });
},
{
name: "internet_search",
description: "Run a web search",
schema: z.object({
query: z.string().describe("The search query"),
maxResults: z.number().optional().default(5),
topic: z
.enum(["general", "news", "finance"])
.optional()
.default("general"),
includeRawContent: z.boolean().optional().default(false),
}),
},
);

const researchSubagent = {
name: "research-agent",
description: "Used to research more in depth questions",
systemPrompt: "You are a great researcher",
tools: [internetSearch],
model: new ChatAnthropic({model:"claude-sonnet-4-5-20250929"}), // Optional override, defaults to main agent model
};
const subagents = [researchSubagent];

const agent = createDeepAgent({
model: new ChatAnthropic({model:"claude-sonnet-4-5-20250929"}),
subagents: subagents,
});

## ​ Using CompiledSubAgent

For more complex use cases, you can provide your own pre-built LangGraph graph as a subagent:

import { createDeepAgent, CompiledSubAgent } from "deepagents";
import { createAgent } from "langchain";

// Create a custom agent graph
const customGraph = createAgent({
model: yourModel,
tools: specializedTools,
prompt: "You are a specialized agent for data analysis...",
});

// Use it as a custom subagent
const customSubagent: CompiledSubAgent = {
name: "data-analyzer",
description: "Specialized agent for complex data analysis tasks",
runnable: customGraph,
};

const subagents = [customSubagent];

const agent = createDeepAgent({
model: "claude-sonnet-4-5-20250929",
tools: [internetSearch],
systemPrompt: researchInstructions,
subagents: subagents,
});

## ​ The general-purpose subagent

In addition to any user-defined subagents, deep agents have access to a `general-purpose` subagent at all times. This subagent:

- Has the same system prompt as the main agent
- Has access to all the same tools
- Uses the same model (unless overridden)

### ​ When to use it

The general-purpose subagent is ideal for context isolation without specialized behavior. The main agent can delegate a complex multi-step task to this subagent and get a concise result back without bloat from intermediate tool calls.

## Example

Instead of the main agent making 10 web searches and filling its context with results, it delegates to the general-purpose subagent: `task(name="general-purpose", task="Research quantum computing trends")`. The subagent performs all the searches internally and returns only a summary.

## ​ Best practices

### ​ Write clear descriptions

The main agent uses descriptions to decide which subagent to call. Be specific:✅ **Good:**`"Analyzes financial data and generates investment insights with confidence scores"`❌ **Bad:**`"Does finance stuff"`

### ​ Keep system prompts detailed

Include specific guidance on how to use tools and format outputs:

const researchSubagent = {
name: "research-agent",
description: "Conducts in-depth research using web search and synthesizes findings",
systemPrompt: `You are a thorough researcher. Your job is to:

1. Break down the research question into searchable queries
2. Use internet_search to find relevant information
3. Synthesize findings into a comprehensive but concise summary
4. Cite sources when making claims

Output format:
- Summary (2-3 paragraphs)
- Key findings (bullet points)
- Sources (with URLs)

Keep your response under 500 words to maintain clean context.`,
tools: [internetSearch],
};

### ​ Minimize tool sets

Only give subagents the tools they need. This improves focus and security:

// ✅ Good: Focused tool set
const emailAgent = {
name: "email-sender",
tools: [sendEmail, validateEmail], // Only email-related
};

// ❌ Bad: Too many tools
const emailAgentBad = {
name: "email-sender",
tools: [sendEmail, webSearch, databaseQuery, fileUpload], // Unfocused
};

### ​ Choose models by task

Different models excel at different tasks:

const subagents = [\
{\
name: "contract-reviewer",\
description: "Reviews legal documents and contracts",\
systemPrompt: "You are an expert legal reviewer...",\
tools: [readDocument, analyzeContract],\
model: "claude-sonnet-4-5-20250929", // Large context for long documents\
},\
{\
name: "financial-analyst",\
description: "Analyzes financial data and market trends",\
systemPrompt: "You are an expert financial analyst...",\
tools: [getStockPrice, analyzeFundamentals],\
model: "gpt-5", // Better for numerical analysis\
},\
];

### ​ Return concise results

Instruct subagents to return summaries, not raw data:

const dataAnalyst = {
systemPrompt: `Analyze the data and return:
1. Key insights (3-5 bullet points)
2. Overall confidence score
3. Recommended next actions

Do NOT include:
- Raw data
- Intermediate calculations
- Detailed tool outputs

Keep response under 300 words.`,
};

## ​ Common patterns

### ​ Multiple specialized subagents

Create specialized subagents for different domains:

import { createDeepAgent } from "deepagents";

const subagents = [\
{\
name: "data-collector",\
description: "Gathers raw data from various sources",\
systemPrompt: "Collect comprehensive data on the topic",\
tools: [webSearch, apiCall, databaseQuery],\
},\
{\
name: "data-analyzer",\
description: "Analyzes collected data for insights",\
systemPrompt: "Analyze data and extract key insights",\
tools: [statisticalAnalysis],\
},\
{\
name: "report-writer",\
description: "Writes polished reports from analysis",\
systemPrompt: "Create professional reports from insights",\
tools: [formatDocument],\
},\
];

const agent = createDeepAgent({
model: "claude-sonnet-4-5-20250929",
systemPrompt: "You coordinate data analysis and reporting. Use subagents for specialized tasks.",
subagents: subagents,
});

**Workflow:**

1. Main agent creates high-level plan
2. Delegates data collection to data-collector
3. Passes results to data-analyzer
4. Sends insights to report-writer
5. Compiles final output

Each subagent works with clean context focused only on its task.

## ​ Troubleshooting

### ​ Subagent not being called

**Problem**: Main agent tries to do work itself instead of delegating.**Solutions**:

1. **Make descriptions more specific:**

// ✅ Good
{ name: "research-specialist", description: "Conducts in-depth research on specific topics using web search. Use when you need detailed information that requires multiple searches." }

// ❌ Bad
{ name: "helper", description: "helps with stuff" }

2. **Instruct main agent to delegate:**

const agent = createDeepAgent({
systemPrompt: `...your instructions...

IMPORTANT: For complex tasks, delegate to your subagents using the task() tool.
This keeps your context clean and improves results.`,
subagents: [...]
});

### ​ Context still getting bloated

**Problem**: Context fills up despite using subagents.**Solutions**:

1. **Instruct subagent to return concise results:**

systemPrompt: `...

IMPORTANT: Return only the essential summary.
Do NOT include raw data, intermediate search results, or detailed tool outputs.
Your response should be under 500 words.`

2. **Use filesystem for large data:**

systemPrompt: `When you gather large amounts of data:
1. Save raw data to /data/raw_results.txt
2. Process and analyze the data
3. Return only the analysis summary

This keeps context clean.`

### ​ Wrong subagent being selected

**Problem**: Main agent calls inappropriate subagent for the task.**Solution**: Differentiate subagents clearly in descriptions:

const subagents = [\
{\
name: "quick-researcher",\
description: "For simple, quick research questions that need 1-2 searches. Use when you need basic facts or definitions.",\
},\
{\
name: "deep-researcher",\
description: "For complex, in-depth research requiring multiple searches, synthesis, and analysis. Use for comprehensive reports.",\
}\
];

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Backends\\
\\
Previous Human-in-the-loop\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/human-in-the-loop

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Human-in-the-loop

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

On this page

- Basic configuration
- Decision types
- Handle interrupts
- Multiple tool calls
- Edit tool arguments
- Subagent interrupts
- Best practices
- Always use a checkpointer
- Use the same thread ID
- Match decision order to actions
- Tailor configurations by risk

Some tool operations may be sensitive and require human approval before execution. Deep agents support human-in-the-loop workflows through LangGraph’s interrupt capabilities. You can configure which tools require approval using the `interrupt_on` parameter.

no

yes

approve

edit

reject

Agent

Interrupt?

Execute

Human

Cancel

## ​ Basic configuration

The `interrupt_on` parameter accepts a dictionary mapping tool names to interrupt configurations. Each tool can be configured with:

- **`True`**: Enable interrupts with default behavior (approve, edit, reject allowed)
- **`False`**: Disable interrupts for this tool
- **`{"allowed_decisions": [...]}`**: Custom configuration with specific allowed decisions

Copy

import { tool } from "langchain";
import { createDeepAgent } from "deepagents";
import { MemorySaver } from "@langchain/langgraph";
import { z } from "zod";
import { v4 as uuidv4 } from 'uuid'; // install uuid package: npm install uuid

const deleteFile = tool(

return `Deleted ${path}`;
},
{
name: "delete_file",
description: "Delete a file from the filesystem.",
schema: z.object({
path: z.string(),
}),
},
);

const sendEmail = tool(

return `Sent email to ${to}`;
},
{
name: "send_email",
description: "Send an email.",
schema: z.object({
to: z.string(),
subject: z.string(),
body: z.string(),
}),
},
);

// Checkpointer is REQUIRED for human-in-the-loop
const checkpointer = new MemorySaver();

const agent = createDeepAgent({
model: "claude-sonnet-4-5-20250929",
tools: [deleteFile, sendEmail],
interruptOn: {
delete_file: true, // Default: approve, edit, reject
read_file: false, // No interrupts needed
send_email: { allowedDecisions: ["approve", "reject"] }, // No editing
},
checkpointer, // Required!
});

## ​ Decision types

The `allowed_decisions` list controls what actions a human can take when reviewing a tool call:

- **`"approve"`**: Execute the tool with the original arguments as proposed by the agent
- **`"edit"`**: Modify the tool arguments before execution
- **`"reject"`**: Skip executing this tool call entirely

You can customize which decisions are available for each tool:

const interruptOn = {
// Sensitive operations: allow all options
delete_file: { allowedDecisions: ["approve", "edit", "reject"] },

// Moderate risk: approval or rejection only
write_file: { allowedDecisions: ["approve", "reject"] },

// Must approve (no rejection allowed)
critical_operation: { allowedDecisions: ["approve"] },
};

## ​ Handle interrupts

When an interrupt is triggered, the agent pauses execution and returns control. Check for interrupts in the result and handle them accordingly.

import { v4 as uuidv4 } from "uuid";
import { Command } from "@langchain/langgraph";

// Create config with thread_id for state persistence
const config = { configurable: { thread_id: uuidv4() } };

// Invoke the agent
let result = await agent.invoke({
messages: [{ role: "user", content: "Delete the file temp.txt" }],
}, config);

// Check if execution was interrupted
if (result.__interrupt__) {
// Extract interrupt information
const interrupts = result.__interrupt__[0].value;
const actionRequests = interrupts.actionRequests;
const reviewConfigs = interrupts.reviewConfigs;

// Create a lookup map from tool name to review config
const configMap = Object.fromEntries(

);

// Display the pending actions to the user
for (const action of actionRequests) {
const reviewConfig = configMap[action.name];
console.log(`Tool: ${action.name}`);
console.log(`Arguments: ${JSON.stringify(action.args)}`);
console.log(`Allowed decisions: ${reviewConfig.allowedDecisions}`);
}

// Get user decisions (one per actionRequest, in order)
const decisions = [\
{ type: "approve" } // User approved the deletion\
];

// Resume execution with decisions
result = await agent.invoke(
new Command({ resume: { decisions } }),
config // Must use the same config!
);
}

// Process final result
console.log(result.messages[result.messages.length - 1].content);

## ​ Multiple tool calls

When the agent calls multiple tools that require approval, all interrupts are batched together in a single interrupt. You must provide decisions for each one in order.

const config = { configurable: { thread_id: uuidv4() } };

let result = await agent.invoke({
messages: [{\
role: "user",\
content: "Delete temp.txt and send an email to admin@example.com"\
}]
}, config);

if (result.__interrupt__) {
const interrupts = result.__interrupt__[0].value;
const actionRequests = interrupts.actionRequests;

// Two tools need approval
console.assert(actionRequests.length === 2);

// Provide decisions in the same order as actionRequests
const decisions = [\
{ type: "approve" }, // First tool: delete_file\
{ type: "reject" } // Second tool: send_email\
];

result = await agent.invoke(
new Command({ resume: { decisions } }),
config
);
}

## ​ Edit tool arguments

When `"edit"` is in the allowed decisions, you can modify the tool arguments before execution:

if (result.__interrupt__) {
const interrupts = result.__interrupt__[0].value;
const actionRequest = interrupts.actionRequests[0];

// Original args from the agent
console.log(actionRequest.args); // { to: "everyone@company.com", ... }

// User decides to edit the recipient
const decisions = [{\
type: "edit",\
editedAction: {\
name: actionRequest.name, // Must include the tool name\
args: { to: "team@company.com", subject: "...", body: "..." }\
}\
}];

## ​ Subagent interrupts

Each subagent can have its own `interrupt_on` configuration that overrides the main agent’s settings:

const agent = createDeepAgent({
tools: [deleteFile, readFile],
interruptOn: {
delete_file: true,
read_file: false,
},
subagents: [{\
name: "file-manager",\
description: "Manages file operations",\
systemPrompt: "You are a file management assistant.",\
tools: [deleteFile, readFile],\
interruptOn: {\
// Override: require approval for reads in this subagent\
delete_file: true,\
read_file: true, // Different from main agent!\
}\
}],
checkpointer
});

When a subagent triggers an interrupt, the handling is the same – check for `__interrupt__` and resume with `Command`.

## ​ Best practices

### ​ Always use a checkpointer

Human-in-the-loop requires a checkpointer to persist agent state between the interrupt and resume:

### ​ Use the same thread ID

When resuming, you must use the same config with the same `thread_id`:

### ​ Match decision order to actions

The decisions list must match the order of `action_requests`:

### ​ Tailor configurations by risk

Configure different tools based on their risk level:

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Subagents\\
\\
Previous Long-term memory\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/long-term-memory

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Long-term memory

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

On this page

- Setup
- How it works
- 1\. Short-term (transient) filesystem
- 2\. Long-term (persistent) filesystem
- Path routing
- Cross-thread persistence
- Use cases
- User preferences
- Self-improving instructions
- Knowledge base
- Research projects
- Store implementations
- InMemoryStore (development)
- PostgresStore (production)
- Best practices
- Use descriptive paths
- Document the memory structure
- Prune old data
- Choose the right storage

Deep agents come with a local filesystem to offload memory. By default, this filesystem is stored in agent state and is **transient to a single thread**—files are lost when the conversation ends.You can extend deep agents with **long-term memory** by using a **CompositeBackend** that routes specific paths to persistent storage. This enables hybrid storage where some files persist across threads while others remain ephemeral.

/memories/\*

other

Deep Agent

Path Router

Store Backend

State Backend

Persistent

across threads

Ephemeral

single thread

## ​ Setup

Configure long-term memory by using a `CompositeBackend` that routes the `/memories/` path to a `StoreBackend`:

Copy

import { createDeepAgent } from "deepagents";
import { CompositeBackend, StateBackend, StoreBackend } from "deepagents";
import { InMemoryStore } from "@langchain/langgraph-checkpoint";

const agent = createDeepAgent({
store: new InMemoryStore(), // Required for StoreBackend

new StateBackend(config), // Ephemeral storage
{ "/memories/": new StoreBackend(config) } // Persistent storage
),
});

## ​ How it works

When using `CompositeBackend`, deep agents maintain **two separate filesystems**:

### ​ 1\. Short-term (transient) filesystem

- Stored in the agent’s state (via `StateBackend`)
- Persists only within a single thread
- Files are lost when the thread ends
- Accessed through standard paths: `/notes.txt`, `/workspace/draft.md`

### ​ 2\. Long-term (persistent) filesystem

- Stored in a LangGraph Store (via `StoreBackend`)
- Persists across all threads and conversations
- Survives agent restarts
- Accessed through paths prefixed with `/memories/`: `/memories/preferences.txt`

### ​ Path routing

The `CompositeBackend` routes file operations based on path prefixes:

- Files with paths starting with `/memories/` are stored in the Store (persistent)
- Files without this prefix remain in transient state
- All filesystem tools (`ls`, `read_file`, `write_file`, `edit_file`) work with both

// Transient file (lost after thread ends)
await agent.invoke({
messages: [{ role: "user", content: "Write draft to /draft.txt" }],
});

// Persistent file (survives across threads)
await agent.invoke({
messages: [{ role: "user", content: "Save final report to /memories/report.txt" }],
});

## ​ Cross-thread persistence

Files in `/memories/` can be accessed from any thread:

import { v4 as uuidv4 } from "uuid";

// Thread 1: Write to long-term memory
const config1 = { configurable: { thread_id: uuidv4() } };
await agent.invoke({
messages: [{ role: "user", content: "Save my preferences to /memories/preferences.txt" }],
}, config1);

// Thread 2: Read from long-term memory (different conversation!)
const config2 = { configurable: { thread_id: uuidv4() } };
await agent.invoke({
messages: [{ role: "user", content: "What are my preferences?" }],
}, config2);
// Agent can read /memories/preferences.txt from the first thread

## ​ Use cases

### ​ User preferences

Store user preferences that persist across sessions:

const agent = createDeepAgent({
store: new InMemoryStore(),

new StateBackend(config),
{ "/memories/": new StoreBackend(config) }
),
systemPrompt: `When users tell you their preferences, save them to /memories/user_preferences.txt so you remember them in future conversations.`,
});

### ​ Self-improving instructions

An agent can update its own instructions based on feedback:

new StateBackend(config),
{ "/memories/": new StoreBackend(config) }
),
systemPrompt: `You have a file at /memories/instructions.txt with additional instructions and preferences.

Read this file at the start of conversations to understand user preferences.

When users provide feedback like "please always do X" or "I prefer Y", update /memories/instructions.txt using the edit_file tool.`,
});

Over time, the instructions file accumulates user preferences, helping the agent improve.

### ​ Knowledge base

Build up knowledge over multiple conversations:

// Conversation 1: Learn about a project
await agent.invoke({
messages: [{ role: "user", content: "We're building a web app with React. Save project notes." }],
});

// Conversation 2: Use that knowledge
await agent.invoke({
messages: [{ role: "user", content: "What framework are we using?" }],
});
// Agent reads /memories/project_notes.txt from previous conversation

### ​ Research projects

Maintain research state across sessions:

const researchAgent = createDeepAgent({
store: new InMemoryStore(),

new StateBackend(config),
{ "/memories/": new StoreBackend(config) }
),
systemPrompt: `You are a research assistant.

Save your research progress to /memories/research/:
- /memories/research/sources.txt - List of sources found
- /memories/research/notes.txt - Key findings and notes
- /memories/research/report.md - Final report draft

This allows research to continue across multiple sessions.`,
});

## ​ Store implementations

Any LangGraph `BaseStore` implementation works:

### ​ InMemoryStore (development)

Good for testing and development, but data is lost on restart:

import { InMemoryStore } from "@langchain/langgraph-checkpoint";
import { createDeepAgent, CompositeBackend, StateBackend, StoreBackend } from "deepagents";

const store = new InMemoryStore();
const agent = createDeepAgent({
store,

new StateBackend(config),
{ "/memories/": new StoreBackend(config) }
),
});

### ​ PostgresStore (production)

For production, use a persistent store:

import { PostgresStore } from "@langchain/langgraph-checkpoint-postgres";
import { createDeepAgent, CompositeBackend, StateBackend, StoreBackend } from "deepagents";

const store = new PostgresStore({
connectionString: process.env.DATABASE_URL,
});
const agent = createDeepAgent({
store,

## ​ Best practices

### ​ Use descriptive paths

Organize persistent files with clear paths:

/memories/user_preferences.txt
/memories/research/topic_a/sources.txt
/memories/research/topic_a/notes.txt
/memories/project/requirements.md

### ​ Document the memory structure

Tell the agent what’s stored where in your system prompt:

Your persistent memory structure:
- /memories/preferences.txt: User preferences and settings
- /memories/context/: Long-term context about the user
- /memories/knowledge/: Facts and information learned over time

### ​ Prune old data

Implement periodic cleanup of outdated persistent files to keep storage manageable.

### ​ Choose the right storage

- **Development**: Use `InMemoryStore` for quick iteration
- **Production**: Use `PostgresStore` or other persistent stores
- **Multi-tenant**: Consider using assistant\_id-based namespacing in your store

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Human-in-the-loop\\
\\
Previous Deep Agents Middleware\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/middleware

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Deep Agents Middleware

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

On this page

- To-do list middleware
- Filesystem middleware
- Short-term vs. long-term filesystem
- Subagent middleware

Deep agents are built with a modular middleware architecture. Deep agents have access to:

1. A planning tool
2. A filesystem for storing context and long-term memories
3. The ability to spawn subagents

Each feature is implemented as separate middleware. When you create a deep agent with `create_deep_agent`, we automatically attach `TodoListMiddleware`, `FilesystemMiddleware`, and `SubAgentMiddleware` to your agent.

create\_deep\_agent

TodoList

Filesystem

SubAgent

Agent Tools

Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently.The following sections explain what each middleware provides.

## ​ To-do list middleware

Planning is integral to solving complex problems. If you’ve used Claude Code recently, you’ll notice how it writes out a to-do list before tackling complex, multi-part tasks. You’ll also notice how it can adapt and update this to-do list on the fly as more information comes in.`TodoListMiddleware` provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the `write_todos` tool to keep track of what it’s doing and what still needs to be done.

Copy

import { createAgent, todoListMiddleware } from "langchain";

// todoListMiddleware is included by default in createDeepAgent
// You can customize it if building a custom agent
const agent = createAgent({
model: "claude-sonnet-4-5-20250929",
middleware: [\
todoListMiddleware({\
// Optional: Custom addition to the system prompt\
systemPrompt: "Use the write_todos tool to...",\
}),\
],
});

## ​ Filesystem middleware

Context engineering is a main challenge in building effective agents. This is particularly difficult when using tools that return variable-length results (for example, web\_search and rag), as long tool results can quickly fill your context window.`FilesystemMiddleware` provides four tools for interacting with both short-term and long-term memory:

- **ls**: List the files in the filesystem
- **read\_file**: Read an entire file or a certain number of lines from a file
- **write\_file**: Write a new file to the filesystem
- **edit\_file**: Edit an existing file in the filesystem

import { createAgent } from "langchain";
import { createFilesystemMiddleware } from "deepagents";

// FilesystemMiddleware is included by default in createDeepAgent
// You can customize it if building a custom agent
const agent = createAgent({
model: "claude-sonnet-4-5-20250929",
middleware: [\
createFilesystemMiddleware({\
backend: undefined, // Optional: custom backend (defaults to StateBackend)\
systemPrompt: "Write to the filesystem when...", // Optional custom system prompt override\
customToolDescriptions: {\
ls: "Use the ls tool when...",\
read_file: "Use the read_file tool to...",\
}, // Optional: Custom descriptions for filesystem tools\
}),\
],
});

### ​ Short-term vs. long-term filesystem

By default, these tools write to a local “filesystem” in your graph state. To enable persistent storage across threads, configure a `CompositeBackend` that routes specific paths (like `/memories/`) to a `StoreBackend`.

import { createAgent } from "langchain";
import { createFilesystemMiddleware, CompositeBackend, StateBackend, StoreBackend } from "deepagents";
import { InMemoryStore } from "@langchain/langgraph-checkpoint";

const store = new InMemoryStore();

const agent = createAgent({
model: "claude-sonnet-4-5-20250929",
store,
middleware: [\
createFilesystemMiddleware({\

new StateBackend(config),\
{ "/memories/": new StoreBackend(config) }\
),\
systemPrompt: "Write to the filesystem when...", // Optional custom system prompt override\
customToolDescriptions: {\
ls: "Use the ls tool when...",\
read_file: "Use the read_file tool to...",\
}, // Optional: Custom descriptions for filesystem tools\
}),\
],
});

When you configure a `CompositeBackend` with a `StoreBackend` for `/memories/`, any files prefixed with **/memories/** are saved to persistent storage and survive across different threads. Files without this prefix remain in ephemeral state storage.

## ​ Subagent middleware

Handing off tasks to subagents isolates context, keeping the main (supervisor) agent’s context window clean while still going deep on a task.The subagents middleware allows you to supply subagents through a `task` tool.

import { tool } from "langchain";
import { createAgent } from "langchain";
import { createSubAgentMiddleware } from "deepagents";
import { z } from "zod";

const getWeather = tool(

return `The weather in ${city} is sunny.`;
},
{
name: "get_weather",
description: "Get the weather in a city.",
schema: z.object({
city: z.string(),
}),
},
);

const agent = createAgent({
model: "claude-sonnet-4-5-20250929",
middleware: [\
createSubAgentMiddleware({\
defaultModel: "claude-sonnet-4-5-20250929",\
defaultTools: [],\
subagents: [\
{\
name: "weather",\
description: "This subagent can get weather in cities.",\
systemPrompt: "Use the get_weather tool to get the weather in a city.",\
tools: [getWeather],\
model: "gpt-4o",\
middleware: [],\
},\
],\
}),\
],
});

A subagent is defined with a **name**, **description**, **system prompt**, and **tools**. You can also provide a subagent with a custom **model**, or with additional **middleware**. This can be particularly useful when you want to give the subagent an additional state key to share with the main agent.For more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent.

import { tool, createAgent } from "langchain";
import { createSubAgentMiddleware, type SubAgent } from "deepagents";
import { z } from "zod";

const weatherSubagent: SubAgent = {
name: "weather",
description: "This subagent can get weather in cities.",
systemPrompt: "Use the get_weather tool to get the weather in a city.",
tools: [getWeather],
model: "gpt-4o",
middleware: [],
};

const agent = createAgent({
model: "claude-sonnet-4-5-20250929",
middleware: [\
createSubAgentMiddleware({\
defaultModel: "claude-sonnet-4-5-20250929",\
defaultTools: [],\
subagents: [weatherSubagent],\
}),\
],
});

In addition to any user-defined subagents, the main agent has access to a `general-purpose` subagent at all times. This subagent has the same instructions as the main agent and all the tools it has access to. The primary purpose of the `general-purpose` subagent is context isolation—the main agent can delegate a complex task to this subagent and get a concise answer back without bloat from intermediate tool calls.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Long-term memory\\
\\
Previous Deep Agents CLI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/cli

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Deep Agents CLI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

On this page

- Quick start
- Configuration
- Interactive mode
- Set project conventions with memories
- Use remote sandboxes

A terminal interface for building agents with persistent memory. Agents maintain context across sessions, learn project conventions, and execute code with approval controls.The Deep Agents CLI has the following built-in capabilities:

- **File operations** \- read, write, and edit files in your project with tools that enable agents to manage and modify code and documentation.
- **Shell command execution** \- execute shell commands to run tests, build projects, manage dependencies, and interact with version control systems.
- **Web search** \- search the web for up-to-date information and documentation (requires Tavily API key).
- **HTTP requests** \- make HTTP requests to APIs and external services for data fetching and integration tasks.
- **Task planning and tracking** \- break down complex tasks into discrete steps and track progress through the built-in todo system.
- **Memory storage and retrieval** \- store and retrieve information across sessions, enabling agents to remember project conventions and learned patterns.
- **Human-in-the-loop** \- require human approval for sensitive tool operations.

Watch the demo video to see how the Deep Agents CLI works.

## ​ Quick start

Set your API key

Export as an environment variable:

Copy

export ANTHROPIC_API_KEY="your-api-key"

Or create a `.env` file in your project root:

ANTHROPIC_API_KEY=your-api-key

Run the CLI

uvx deepagents-cli

Give the agent a task

The agent proposes changes with diffs for your approval before modifying files.

Additional installation and configuration options

Install locally if needed:

pip

uv

pip install deepagents-cli

The CLI uses Anthropic Claude Sonnet 4 by default. To use OpenAI:

export OPENAI_API_KEY="your-key"

Enable web search (optional):

export TAVILY_API_KEY="your-key"

API keys can be set as environment variables or in a `.env` file.

## ​ Configuration

Command-line options

| Option | Description |
| --- | --- |
| `--agent NAME` | Use named agent with separate memory |
| `--auto-approve` | Skip tool confirmation prompts (toggle with `Ctrl+T`) |
| `--sandbox TYPE` | Execute in remote sandbox: `modal`, `daytona`, or `runloop` |
| `--sandbox-id ID` | Reuse existing sandbox |
| `--sandbox-setup PATH` | Run setup script in sandbox |

CLI commands

| Command | Description |
| --- | --- |
| `deepagents list` | List all agents |
| `deepagents help` | Show help |
| `deepagents reset --agent NAME` | Clear agent memory and reset to default |
| `deepagents reset --agent NAME --target SOURCE` | Copy memory from another agent |

## ​ Interactive mode

Slash commands

Use these commands within the CLI session:

- `/tokens` \- Display token usage
- `/clear` \- Clear conversation history
- `/exit` \- Exit the CLI

Bash commands

Execute shell commands directly by prefixing with `!`:

Keyboard shortcuts

| Shortcut | Action |
| --- | --- |
| `Enter` | Submit |
| `Alt+Enter` | Newline |
| `Ctrl+E` | External editor |
| `Ctrl+T` | Toggle auto-approve |
| `Ctrl+C` | Interrupt |
| `Ctrl+D` | Exit |

## ​ Set project conventions with memories

Agents store information in `~/.deepagents/AGENT_NAME/memories/` as markdown files using a memory-first protocol:

1. **Research**: Searches memory for relevant context before starting tasks
2. **Response**: Checks memory when uncertain during execution
3. **Learning**: Automatically saves new information for future sessions

Organize memories by topic with descriptive filenames:

~/.deepagents/backend-dev/memories/
├── api-conventions.md
├── database-schema.md
└── deployment-process.md

Teach the agent conventions once:

It remembers for future sessions:

# Applies conventions without prompting

## ​ Use remote sandboxes

Execute code in isolated remote environments for safety and flexibility. Remote sandboxes provide the following benefits:

- **Safety**: Protect your local machine from potentially harmful code execution
- **Clean environments**: Use specific dependencies or OS configurations without local setup
- **Parallel execution**: Run multiple agents simultaneously in isolated environments
- **Long-running tasks**: Execute time-intensive operations without blocking your machine
- **Reproducibility**: Ensure consistent execution environments across teams

To use a remote sandbox, follow these steps:

1. Configure your sandbox provider ( Runloop, Daytona, or Modal):

# Runloop
export RUNLOOP_API_KEY="your-key"

# Daytona
export DAYTONA_API_KEY="your-key"

# Modal
modal setup

2. Run the CLI with a sandbox:

uvx deepagents-cli --sandbox runloop --sandbox-setup ./setup.sh

The agent runs locally but executes all code operations in the remote sandbox. Optional setup scripts can configure environment variables, clone repositories, and prepare dependencies.
3. (Optional) Create a `setup.sh` file to configure your sandbox environment:

#!/bin/bash
set -e

# Clone repository using GitHub token
git clone $HOME/workspace
cd $HOME/workspace

# Make environment variables persistent
cat >> ~/.bashrc <<'EOF'
export GITHUB_TOKEN="${GITHUB_TOKEN}"
export OPENAI_API_KEY="${OPENAI_API_KEY}"
cd $HOME/workspace
EOF

source ~/.bashrc

Store secrets in a local `.env` file for the setup script to access.

Sandboxes isolate code execution, but agents remain vulnerable to prompt injection with untrusted inputs. Use human-in-the-loop approval, short-lived secrets, and trusted setup scripts only. Note that sandbox APIs are evolving rapidly, and we expect more providers to support proxies that help mitigate prompt injection and secrets management concerns.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Deep Agents Middleware\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/quickstart)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Quickstart Evaluation quickstart Tracing quickstart

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/customization)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Customize Deep Agents Deep Agents overview Subagents

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/harness)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Agent harness capabilities Deep Agents overview Deep Agents Middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/backends)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Backends Deep Agents overview Long-term memory

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/subagents)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Subagents Deep Agents overview Deep Agents Middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/human-in-the-loop)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Human-in-the-loop Human-in-the-loop using server API

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/long-term-memory)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Long-term memory Short-term memory

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/middleware)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Deep Agents Middleware Deep Agents overview How to add custom middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/deepagents/cli)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Deep Agents CLI

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Deep Agents CLI Deep Agents overview Deep Agents Middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluation-quickstart

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Evaluation quickstart

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Prerequisites
- 1\. Set workspace secrets
- 2\. Create a prompt
- 3\. Create a dataset
- 4\. Add an evaluator
- 5\. Run your evaluation
- Next steps
- Video guide

_Evaluations_ are a quantitative way to measure the performance of LLM applications. LLMs can behave unpredictably, even small changes to prompts, models, or inputs can significantly affect results. Evaluations provide a structured way to identify failures, compare versions, and build more reliable AI applications.Running an evaluation in LangSmith requires three key components:

- _Dataset_: A set of test inputs (and optionally, expected outputs).
- _Target function_: The part of your application you want to test—this might be a single LLM call with a new prompt, one module, or your entire workflow.
- _Evaluators_: Functions that score your target function’s outputs.

This quickstart guides you through running a starter evaluation that checks the correctness of LLM responses, using either the LangSmith SDK or UI.

If you prefer to watch a video on getting started with tracing, refer to the datasets and evaluations Video guide.

## ​ Prerequisites

Before you begin, make sure you have:

- **A LangSmith account**: Sign up or log in at smith.langchain.com.
- **A LangSmith API key**: Follow the Create an API key guide.
- **An OpenAI API key**: Generate this from the OpenAI dashboard.

**Select the UI or SDK filter for instructions:**

- UI

- SDK

## ​ 1\. Set workspace secrets

In the LangSmith UI, ensure that your OpenAI API key is set as a workspace secret.

1. Navigate to **Settings** and then move to the **Secrets** tab.
2. Select **Add secret** and enter the `OPENAI_API_KEY` and your API key as the **Value**.
3. Select **Save secret**.

When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.

## ​ 2\. Create a prompt

LangSmith’s Prompt Playground makes it possible to run evaluations over different prompts, new models, or test different model configurations.

1. In the LangSmith UI, navigate to the **Playground** under **Prompt Engineering**.
2. Under the **Prompts** panel, modify the **system** prompt to:

Copy

Answer the following question accurately:

Leave the **Human** message as is: `{question}`.

## ​ 3\. Create a dataset

1. Click **Set up Evaluation**, which will open a **New Experiment** table at the bottom of the page.
2. In the **Select or create a new dataset** dropdown, click the **\+ New** button to create a new dataset.

3. Add the following examples to the dataset:

| Inputs | Reference Outputs |
| --- | --- |
| question: Which country is Mount Kilimanjaro located in? | output: Mount Kilimanjaro is located in Tanzania. |
| question: What is Earth’s lowest point? | output: Earth’s lowest point is The Dead Sea. |

4. Click **Save** and enter a name to save your newly created dataset.

## ​ 4\. Add an evaluator

1. Click **\+ Evaluator** and select **Correctness** from the **Pre-built Evaluator** options.
2. In the **Correctness** panel, click **Save**.

## ​ 5\. Run your evaluation

1. Select **Start** on the top right to run your evaluation. This will create an _experiment_ with a preview in the **New Experiment** table. You can view in full by clicking the experiment name.

## ​ Next steps

To learn more about running experiments in LangSmith, read the evaluation conceptual guide.

- For more details on evaluations, refer to the Evaluation documentation.
- Learn how to create and manage datasets in the UI.
- Learn how to run an evaluation from the prompt playground.

This guide uses prebuilt LLM-as-judge evaluators from the open-source `openevals` package. OpenEvals includes a set of commonly used evaluators and is a great starting point if you’re new to evaluations. If you want greater flexibility in how you evaluate your apps, you can also define completely custom evaluators.

## ​ 1\. Install dependencies

In your terminal, create a directory for your project and install the dependencies in your environment:

Python

TypeScript

mkdir ls-evaluation-quickstart && cd ls-evaluation-quickstart
python -m venv .venv && source .venv/bin/activate
python -m pip install --upgrade pip
pip install -U langsmith openevals openai

If you are using `yarn` as your package manager, you will also need to manually install `@langchain/core` as a peer dependency of `openevals`. This is not required for LangSmith evals in general, you may define evaluators using arbitrary custom code.

## ​ 2\. Set up environment variables

Set the following environment variables:

- `LANGSMITH_TRACING`
- `LANGSMITH_API_KEY`
- `OPENAI_API_KEY` (or your LLM provider’s API key)
- (optional) `LANGSMITH_WORKSPACE_ID`: If your LangSmith API key is linked to multiple workspaces, set this variable to specify which workspace to use.

export LANGSMITH_TRACING=true

If you’re using Anthropic, use the Anthropic wrapper to trace your calls. For other providers, use the traceable wrapper.

1. Create a file and add the following code, which will:

- Import the `Client` to connect to LangSmith.
- Create a dataset.
- Define example _inputs_ and _outputs_.
- Associate the input and output pairs with that dataset in LangSmith so they can be used in evaluations.

# dataset.py
from langsmith import Client

def main():
client = Client()

# Programmatically create a dataset in LangSmith
dataset = client.create_dataset(
dataset_name="Sample dataset",
description="A sample dataset in LangSmith."
)

# Create examples
examples = [\
{\
"inputs": {"question": "Which country is Mount Kilimanjaro located in?"},\
"outputs": {"answer": "Mount Kilimanjaro is located in Tanzania."},\
},\
{\
"inputs": {"question": "What is Earth's lowest point?"},\
"outputs": {"answer": "Earth's lowest point is The Dead Sea."},\
},\
]

# Add examples to the dataset
client.create_examples(dataset_id=dataset.id, examples=examples)
print("Created dataset:", dataset.name)

if __name__ == "__main__":
main()

2. In your terminal, run the `dataset` file to create the datasets you’ll use to evaluate your app:

python dataset.py

You’ll see the following output:

Created dataset: Sample dataset

## ​ 4\. Create your target function

Define a target function that contains what you’re evaluating. In this guide, you’ll define a target function that contains a single LLM call to answer a question.Add the following to an `eval` file:

# eval.py
from langsmith import Client, wrappers
from openai import OpenAI

# Wrap the OpenAI client for LangSmith tracing
openai_client = wrappers.wrap_openai(OpenAI())

# Define the application logic you want to evaluate inside a target function
# The SDK will automatically send the inputs from the dataset to your target function

response = openai_client.chat.completions.create(
model="gpt-5-mini",
messages=[\
{"role": "system", "content": "Answer the following question accurately"},\
{"role": "user", "content": inputs["question"]},\
],
)
return {"answer": response.choices[0].message.content.strip()}

## ​ 5\. Define an evaluator

In this step, you’re telling LangSmith how to grade the answers your app produces.Import a prebuilt evaluation prompt (`CORRECTNESS_PROMPT`) from `openevals` and a helper that wraps it into an _LLM-as-judge evaluator_, which will score the application’s output.

`CORRECTNESS_PROMPT` is just an f-string with variables for `"inputs"`, `"outputs"`, and `"reference_outputs"`. See here for more information on customizing OpenEvals prompts.

The evaluator compares:

- `inputs`: what was passed into your target function (e.g., the question text).
- `outputs`: what your target function returned (e.g., the model’s answer).
- `reference_outputs`: the ground truth answers you attached to each dataset example in Step 3.

Add the following highlighted code to your `eval` file:

from langsmith import Client, wrappers
from openai import OpenAI
from openevals.llm import create_llm_as_judge
from openevals.prompts import CORRECTNESS_PROMPT

def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):
evaluator = create_llm_as_judge(
prompt=CORRECTNESS_PROMPT,
model="openai:o3-mini",
feedback_key="correctness",
)
return evaluator(
inputs=inputs,
outputs=outputs,
reference_outputs=reference_outputs
)

## ​ 6\. Run and view results

To run the evaluation experiment, you’ll call `evaluate(...)`, which:

- Pulls example from the dataset you created in Step 3.
- Sends each example’s inputs to your target function from Step 4.
- Collects the outputs (the model’s answers).
- Passes the outputs along with the `reference_outputs` to your evaluator from Step 5.
- Records all results in LangSmith as an experiment, so you can view them in the UI.

1. Add the highlighted code to your `eval` file:

# After running the evaluation, a link will be provided to view the results in langsmith
def main():
client = Client()
experiment_results = client.evaluate(
target,
data="Sample dataset",
evaluators=[\
correctness_evaluator,\
# can add multiple evaluators here\
],
experiment_prefix="first-eval-in-langsmith",
max_concurrency=2,
)
print(experiment_results)

2. Run your evaluator:

python eval.py

3. You’ll receive a link to view the evaluation results and metadata for the experiment results:

View the evaluation results for experiment: 'first-eval-in-langsmith-00000000' at:

4. Follow the link in the output of your evaluation run to access the **Datasets & Experiments** page in the LangSmith UI, and explore the results of the experiment. This will direct you to the created experiment with a table showing the **Inputs**, **Reference Output**, and **Outputs**. You can select a dataset to open an expanded view of the results.

Here are some topics you might want to explore next:

- Evaluation concepts provides descriptions of the key terminology for evaluations in LangSmith.
- OpenEvals README to see all available prebuilt evaluators and how to customize them.
- Define custom evaluators.
- Python or TypeScript SDK references for comprehensive descriptions of every class and function.

## ​ Video guide

Getting Started with LangSmith (5/8): Datasets & Evaluations - YouTube

Photo image of LangChain

LangChain

165K subscribers

Getting Started with LangSmith (5/8): Datasets & Evaluations

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Watch on

0:00

0:00 / 12:22

•Live

•

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Evaluation\\
\\
Previous Evaluation concepts\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluation-concepts

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Evaluation concepts

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- What to evaluate
- Offline and online evaluations
- Offline evaluations
- Online evaluations
- Evaluation lifecycle
- 1\. Development with offline evaluation
- 2\. Initial deployment with online evaluation
- 3\. Continuous improvement
- Core evaluation targets
- Targets for offline evaluation
- Datasets
- Examples
- Experiment
- Targets for online evaluation
- Runs
- Threads
- Evaluators
- Evaluator inputs
- Evaluator outputs
- Evaluation techniques
- Human
- Code
- LLM-as-judge
- Pairwise
- Reference-free vs reference-based evaluators
- Evaluation types
- Best practices
- Building datasets
- Dataset organization
- Human feedback collection
- Evaluations vs testing
- Quick reference: Offline vs online evaluation

LLM outputs are non-deterministic, which makes response quality hard to assess. Evaluations (evals) are a way to breakdown what “good” looks like and measure it. LangSmith Evaluation provides a framework for measuring quality throughout the application lifecycle, from pre-deployment testing to production monitoring.

## ​ What to evaluate

Before building evaluations, identify what matters for your application. Break down your system into its critical components—LLM calls, retrieval steps, tool invocations, output formatting—and determine quality criteria for each.**Start with manually curated examples.** Create 5-10 examples of what “good” looks like for each critical component. These examples serve as your ground truth and inform which evaluation approaches to use. For instance:

- **RAG system**: Examples of good retrievals (relevant documents) and good answers (accurate, complete).
- **Agent**: Examples of correct tool selection and proper argument formatting or trajectory that the agent took.
- **Chatbot**: Examples of helpful, on-brand responses that address user intent.

Once you’ve defined “good” through examples, you can measure how often your system produces similar quality outputs.

## ​ Offline and online evaluations

LangSmith supports two types of evaluations that serve different purposes in your development workflow:

### ​ Offline evaluations

Use offline evaluations for **pre-deployment testing**:

- **Benchmarking**: Compare multiple versions to find the best performer.
- **Regression testing**: Ensure new versions don’t degrade quality.
- **Unit testing**: Verify correctness of individual components.
- **Backtesting**: Test new versions against historical data.

Offline evaluations target _examples_ from _datasets_—curated test cases with reference outputs that define what “good” looks like.

### ​ Online evaluations

Use online evaluations for **production monitoring**:

- **Real-time monitoring**: Track quality continuously on live traffic.
- **Anomaly detection**: Flag unusual patterns or edge cases.
- **Production feedback**: Identify issues to add to offline datasets.

Online evaluations target _runs_ and _threads_ from tracing—real production traces without reference outputs.This difference in targets determines what you can evaluate: offline evaluations can check correctness against expected answers, while online evaluations focus on quality patterns, safety, and real-world behavior.

## ​ Evaluation lifecycle

As you develop and deploy your application, your evaluation strategy evolves from pre-deployment testing to production monitoring. During development and testing, offline evaluations validate functionality against curated datasets. After deployment, online evaluations monitor production behavior on live traffic. As applications mature, both evaluation types work together in an iterative feedback loop to improve quality continuously.

Development

Testing

Deployment

Monitoring

Iteration

Offline

Online

Both

### ​ 1\. Development with offline evaluation

Before production deployment, use offline evaluations to validate functionality, benchmark different approaches, and build confidence.Follow the quickstart to run your first offline evaluation.

### ​ 2\. Initial deployment with online evaluation

After deployment, use online evaluations to monitor production quality, detect unexpected issues, and collect real-world data.Learn how to configure online evaluations for production monitoring.

### ​ 3\. Continuous improvement

Use both evaluation types together in an iterative feedback loop. Online evaluations surface issues that become offline test cases, offline evaluations validate fixes, and online evaluations confirm production improvements.

## ​ Core evaluation targets

Evaluations run on different targets depending on whether they are offline or online.

### ​ Targets for offline evaluation

Offline evaluations run on datasets and examples. The presence of reference outputs enables comparison between expected and actual results.

#### ​ Datasets

A dataset is a _collection of examples_ used for evaluating an application. An example is a test input, reference output pair.!Dataset

#### ​ Examples

Each example consists of:

- **Inputs**: a dictionary of input variables to pass to your application.
- **Reference outputs** (optional): a dictionary of reference outputs. These do not get passed to your application, they are only used in evaluators.
- **Metadata** (optional): a dictionary of additional information that can be used to create filtered views of a dataset.

#### ​ Experiment

An _experiment_ represents the results of evaluating a specific application version on a dataset. Each experiment captures outputs, evaluator scores, and execution traces for every example in the dataset.!Experiment viewMultiple experiments typically run on a given dataset to test different application configurations (e.g., different prompts or LLMs). LangSmith displays all experiments associated with a dataset and supports comparing multiple experiments side-by-side.!Comparison viewLearn how to analyze experiment results.

### ​ Targets for online evaluation

Online evaluations run on runs and threads from production traffic. Without reference outputs, evaluators focus on detecting issues, anomalies, and quality degradation in real-time.

#### ​ Runs

A _run_ is a single execution trace from your deployed application. Each run contains:

- **Inputs**: The actual user inputs your application received.
- **Outputs**: What your application actually returned.
- **Intermediate steps**: All the child runs (tool calls, LLM calls, and so on).
- **Metadata**: Tags, user feedback, latency metrics, etc.

Unlike examples in datasets, runs do not include reference outputs. Online evaluators must assess quality without knowing what the “correct” answer should be, relying instead on quality heuristics, safety checks, and reference-free evaluation techniques.Learn more about runs and traces in the Observability concepts.

#### ​ Threads

_Threads_ are collections of related runs representing multi-turn conversations. Online evaluators can run at the thread level to evaluate entire conversations rather than individual turns. This enables assessment of conversation-level properties like coherence across turns, topic maintenance, and user satisfaction throughout an interaction.

## ​ Evaluators

_Evaluators_ are functions that score application performance. They provide the measurement layer for both offline and online evaluation, adapting their inputs based on what data is available.Run evaluators using the LangSmith SDK ( Python and TypeScript), via the Prompt Playground, or by configuring rules to run them automatically on tracing projects or datasets.

### ​ Evaluator inputs

Evaluator inputs differ based on evaluation type:**Offline evaluators** receive:

- Example: The example from your dataset, containing inputs, reference outputs, and metadata.
- Run: The actual outputs and intermediate steps from running the application on the example inputs.

**Online evaluators** receive:

- Run: The production trace containing inputs, outputs, and intermediate steps (no reference outputs available).

### ​ Evaluator outputs

Evaluators return **feedback**, which is the scores from evaluation. Feedback is a dictionary or list of dictionaries. Each dictionary contains:

- `key`: The metric name.
- `score` \| `value`: The metric value (`score` for numerical metrics, `value` for categorical metrics).
- `comment` (optional): Additional reasoning or explanation for the score.

### ​ Evaluation techniques

LangSmith supports several evaluation approaches:

- Human
- Code
- LLM-as-judge
- Pairwise

#### ​ Human

_Human evaluation_ involves manual review of application outputs and execution traces. This approach is often an effective starting point for evaluation. LangSmith provides tools to review application outputs and traces (all intermediate steps).Annotation queues streamline the process of collecting human feedback on application outputs.

#### ​ Code

_Code evaluators_ are deterministic, rule-based functions. They work well for checks such as verifying the structure of a chatbot’s response is not empty, that generated code compiles, or that a classification matches exactly.

#### ​ LLM-as-judge

_LLM-as-judge evaluators_ use LLMs to score application outputs. The grading rules and criteria are typically encoded in the LLM prompt. These evaluators can be:

- **Reference-free**: Check if output contains offensive content or adheres to specific criteria.
- **Reference-based**: Compare output to a reference (e.g., check factual accuracy relative to the reference).

LLM-as-judge evaluators require careful review of scores and prompt tuning. Few-shot evaluators, which include examples of inputs, outputs, and expected grades in the grader prompt, often improve performance.Learn about how to define an LLM-as-a-judge evaluator.

#### ​ Pairwise

_Pairwise evaluators_ compare outputs from two application versions using heuristics (e.g., which response is longer), LLMs (with pairwise prompts), or human reviewers.Pairwise evaluation works well when directly scoring an output is difficult but comparing two outputs is straightforward. For example, in summarization tasks, choosing the more informative of two summaries is often easier than assigning an absolute score to a single summary.Learn how run pairwise evaluations.

### ​ Reference-free vs reference-based evaluators

Understanding whether an evaluator requires reference outputs is essential for determining when it can be used.**Reference-free evaluators** assess quality without comparing to expected outputs. These work for both offline and online evaluation:

- **Safety checks**: Toxicity detection, PII detection, content policy violations
- **Format validation**: JSON structure, required fields, schema compliance
- **Quality heuristics**: Response length, latency, specific keywords
- **Reference-free LLM-as-judge**: Clarity, coherence, helpfulness, tone

**Reference-based evaluators** require reference outputs and only work for offline evaluation:

- **Correctness**: Semantic similarity to reference answer
- **Factual accuracy**: Fact-checking against ground truth
- **Exact match**: Classification tasks with known labels
- **Reference-based LLM-as-judge**: Comparing output quality to a reference

When designing an evaluation strategy, reference-free evaluators provide consistency across both offline testing and online monitoring, while reference-based evaluators enable more precise correctness checks during development.

## ​ Evaluation types

LangSmith supports various evaluation approaches for different stages of development and deployment. Understanding when to use each type helps build a comprehensive evaluation strategy.Offline and online evaluations serve different purposes:

- **Offline evaluation types** test pre-deployment on curated datasets with reference outputs
- **Online evaluation types** monitor production behavior on live traffic without reference outputs

Learn more about evaluation types and when to use each.

## ​ Best practices

### ​ Building datasets

There are various strategies for building datasets:**Manually curated examples**This is the recommended starting point. Create 10–20 high-quality examples covering common scenarios and edge cases. These examples define what “good” looks like for your application.**Historical traces**Once in production, convert real traces into examples. For high-traffic applications:

- **User feedback**: Add runs that received negative feed Dataset organization

**Splits**Partition datasets into subsets for targeted evaluation. Use splits for performance optimization (smaller splits for rapid iteration) and interpretability (evaluate different input types separately).Learn how to create and manage dataset splits.**Versions**LangSmith automatically creates dataset versions when examples change. Tag versions to mark important milestones. Target specific versions in CI pipelines to ensure dataset updates don’t break workflows.

### ​ Human feedback collection

Human feedback often provides the most valuable assessment, particularly for subjective quality dimensions.**Annotation queues**Annotation queues enable structured collection of human feedback. Flag specific runs for review, collect annotations in a streamlined interface, and transfer annotated runs to datasets for future evaluations.Annotation queues complement inline annotation by offering additional capabilities: grouping runs, specifying criteria, and configuring reviewer permissions.

### ​ Evaluations vs testing

Testing and evaluation are similar but distinct concepts.**Evaluation measures performance according to metrics.** Metrics can be fuzzy or subjective, and prove more useful in relative terms. They typically compare systems against each other.**Testing asserts correctness.** A system can only be deployed if it passes all tests.Evaluation metrics can be converted into tests. For example, regression tests can assert that new versions must outperform baseline versions on relevant metrics. Run tests and evaluations together for efficiency when systems are expensive to run.Evaluations can be written using standard testing tools like pytest or Vitest/Jest.

## ​ Quick reference: Offline vs online evaluation

The following table summarizes the key differences between offline and online evaluations:

| | **Offline Evaluation** | **Online Evaluation** |
| --- | --- | --- |
| **Runs on** | Dataset (Examples) | Tracing Project (Runs/Threads) |
| **Data access** | Inputs, Outputs, Reference Outputs | Inputs, Outputs only |
| **When to use** | Pre-deployment, during development | Production, post-deployment |
| **Primary use cases** | Benchmarking, unit testing, regression testing, backtesting | Real-time monitoring, production feedback, anomaly detection |
| **Evaluation timing** | Batch processing on curated test sets | Real-time or near real-time on live traffic |
| **Setup location** | Evaluation tab (SDK, UI, Prompt Playground) | Observability tab (automated rules) |
| **Data requirements** | Requires dataset curation | No dataset needed, evaluates live traces |

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Evaluation quickstart\\
\\
Previous Application-specific evaluation approaches\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluation-approaches

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Application-specific evaluation approaches

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Agents
- Evaluating an agent’s final response
- Evaluating a single step of an agent
- Evaluating an agent’s trajectory
- Retrieval augmented generation (RAG)
- Dataset
- Evaluator
- Applying RAG Evaluation
- RAG evaluation summary
- Summarization
- Classification and tagging

Below, we will discuss evaluation of a few popular types of LLM applications.

## ​ Agents

LLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required.!Tool useBelow is a tool-calling agent in LangGraph. The `assistant node` is an LLM that determines whether to invoke a tool based upon the input. The `tool condition` sees if a tool was selected by the `assistant node` and, if so, routes to the `tool node`. The `tool node` executes the tool and returns the output as a tool message to the `assistant node`. This loop continues until as long as the `assistant node` selects a tool. If no tool is selected, then the agent directly returns the LLM response.!AgentThis sets up three general types of agent evaluations that users are often interested in:

- `Final Response`: Evaluate the agent’s final response.
- `Single step`: Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).
- `Trajectory`: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer.

### ​ Evaluating an agent’s final response

One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done.The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don’t need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time.The output should be the agent’s final response.The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response.However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics.

### ​ Evaluating a single step of an agent

Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do.The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps.The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next.The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string.There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristic evaluation of the selected tool relative to the reference tool. One downside is that they don’t capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent’s trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses).

### ​ Evaluating an agent’s trajectory

Evaluating an agent’s trajectory involves evaluating all the steps an agent took.The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools).The outputs are a list of tool calls, which can be formulated as an “exact” trajectory (e.g., an expected sequence of tool calls) or simply a set of tool calls that are expected (in any order).The evaluator here is some function over the steps taken. Assessing the “exact” trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong.To address these flaws, evaluation metrics can focused on the number of “incorrect” steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order.However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent’s trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with.

## ​ Retrieval augmented generation (RAG)

Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user’s input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.

For a comprehensive review of RAG concepts, see our `RAG From Scratch` series.

### ​ Dataset

When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).

### ​ Evaluator

`LLM-as-judge` is a commonly used evaluator for RAG because it’s an effective way to evaluate factual accuracy or consistency between texts.!rag-types.pngWhen evaluating RAG applications, you can have evaluators that require reference outputs and those that don’t:

1. **Require reference output**: Compare the RAG chain’s generated answer or retrievals against a reference answer (or retrievals) to assess its correctness.
2. **Don’t require reference output**: Perform self-consistency checks using prompts that don’t require a reference answer (represented by orange, green, and red in the above figure).

### ​ Applying RAG Evaluation

When applying RAG evaluation, consider the following approaches:

1. `Offline evaluation`: Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer.
2. `Online evaluation`: Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application’s performance in real-time scenarios.
3. `Pairwise evaluation`: Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference.

### ​ RAG evaluation summary

| Evaluator | Detail | Needs reference output | LLM-as-judge? | Pairwise relevant |
| --- | --- | --- | --- | --- |
| Document relevance | Are documents relevant to the question? | No | Yes - prompt | No |
| Answer faithfulness | Is the answer grounded in the documents? | No | Yes - prompt | No |
| Answer helpfulness | Does the answer help address the question? | No | Yes - prompt | No |
| Answer correctness | Is the answer consistent with a reference answer? | Yes | Yes - prompt | No |
| Pairwise comparison | How do multiple answer versions compare? | No | Yes - prompt | Yes |

## ​ Summarization

Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria.`Developer curated examples` of texts to summarize are commonly used for evaluation (see a dataset example here). However, `user logs` from a production (summarization) app can be used for online evaluation with any of the `Reference-free` evaluation prompts below.`LLM-as-judge` is typically used for evaluation of summarization (as well as other types of writing) using `Reference-free` prompts that follow provided criteria to grade a summary. It is less common to provide a particular `Reference` summary, because summarization is a creative task and there are many possible correct answers.`Online` or `Offline` evaluation are feasible because of the `Reference-free` prompt used. `Pairwise` evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs):

| Use Case | Detail | Needs reference output | LLM-as-judge? | Pairwise relevant |
| --- | --- | --- | --- | --- |
| Factual accuracy | Is the summary accurate relative to the source documents? | No | Yes - prompt | Yes |
| Faithfulness | Is the summary grounded in the source documents (e.g., no hallucinations)? | No | Yes - prompt | Yes |
| Helpfulness | Is summary helpful relative to user need? | No | Yes - prompt | Yes |

## ​ Classification and tagging

Classification and tagging apply a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification/tagging evaluation typically employs the following components, which we will review in detail below:A central consideration for classification/tagging evaluation is whether you have a dataset with `reference` labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a classification/tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc).If ground truth reference labels are provided, then it’s common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use `LLM-as-judge` to perform the classification/tagging of an input based upon specified criteria (without a ground truth reference).`Online` or `Offline` evaluation is feasible when using `LLM-as-judge` with the `Reference-free` prompt used. In particular, this is well suited to `Online` evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc).

| Use Case | Detail | Needs reference output | LLM-as-judge? | Pairwise relevant |
| --- | --- | --- | --- | --- |
| Accuracy | Standard definition | Yes | No | No |
| Precision | Standard definition | Yes | No | No |
| Recall | Standard definition | Yes | No | No |

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Evaluation concepts\\
\\
Previous Create and manage datasets in the UI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/manage-datasets

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Datasets

Manage datasets

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Version a dataset
- Create a new version of a dataset
- Tag a version
- Evaluate on a specific dataset version
- Use list\_examples
- Evaluate on a split / filtered view of a dataset
- Evaluate on a filtered view of a dataset
- Evaluate on a dataset split
- Share a dataset
- Share a dataset publicly
- Unshare a dataset
- Export a dataset
- Export filtered traces from experiment to dataset
- View experiment traces

LangSmith provides tools for managing and working with your _datasets_. This page describes dataset operations including:

- Versioning datasets to track changes over time.
- Filtering and splitting datasets for evaluation.
- Sharing datasets publicly.
- Exporting datasets in various formats.

You’ll also learn how to export filtered traces from experiments Version a dataset

In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created.

### ​ Create a new version of a dataset

Any time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and understand how your dataset has evolved.By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the **Examples** tab, you will find the state of the dataset at that point in time.!Version DatasetsNote that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the latest version of the dataset.

By default, the latest version of the dataset is shown in the **Examples** tab and experiments from all versions are shown in the **Tests** tab.

In the **Tests** tab, you will find the results of tests run on the dataset at different versions.!Version Datasets

### ​ Tag a version

You can also tag versions of your dataset to give them a more human-readable name, which can be useful for marking important milestones in your dataset’s history.For example, you might tag a version of your dataset as “prod” and use it to run tests against your LLM pipeline.You can tag a version of your dataset in the UI by clicking on **\+ Tag this version** in the **Examples** tab.!Tagging DatasetsYou can also tag versions of your dataset using the SDK. Here’s an example of how to tag a version of a dataset using the Python SDK:

Copy

from langsmith import Client
from datetime import datetime

client = Client()
initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag

# You can tag a specific dataset version with a semantic name, like "prod"
client.update_dataset_tag(
dataset_name=toxic_dataset_name, as_of=initial_time, tag="prod"
)

To run an evaluation on a particular tagged version of a dataset, refer to the Evaluate on a specific dataset version section.

## ​ Evaluate on a specific dataset version

You may find it helpful to refer to the following content before you read this section:

- Version a dataset.
- Fetching examples.

### ​ Use `list_examples`

You can use `evaluate` / `aevaluate` to pass in an iterable of examples to evaluate on a particular version of a dataset. Use `list_examples` / `listExamples` to fetch examples from a particular version tag using `as_of` / `asOf` and pass that into the `data` argument.

Python

TypeScript

from langsmith import Client

ls_client = Client()

# Assumes actual outputs have a 'class' key.
# Assumes example outputs have a 'label' key.

return outputs["class"] == reference_outputs["label"]

results = ls_client.evaluate(
lambda inputs: {"class": "Not toxic"},
# Pass in filtered data here:
data=ls_client.list_examples(
dataset_name="Toxic Queries",
as_of="latest", # specify version here
),
evaluators=[correct],
)

Learn more about how to fetch views of a dataset on the Create and manage datasets programmatically page.

## ​ Evaluate on a split / filtered view of a dataset

- Fetching examples.
- Creating and managing dataset splits.

### ​ Evaluate on a filtered view of a dataset

You can use the `list_examples` / `listExamples` method to fetch a subset of examples from a dataset to evaluate on.One common workflow is to fetch examples that have a certain metadata key-value pair.

from langsmith import evaluate

results = evaluate(
lambda inputs: label_text(inputs["text"]),
data=client.list_examples(dataset_name=dataset_name, metadata={"desired_key": "desired_value"}),
evaluators=[correct_label],
experiment_prefix="Toxic Queries",
)

For more filtering capabilities, refer to this how-to guide.

### ​ Evaluate on a dataset split

You can use the `list_examples` / `listExamples` method to evaluate on one or multiple splits of your dataset. The `splits` parameter takes a list of the splits you would like to evaluate.

results = evaluate(
lambda inputs: label_text(inputs["text"]),
data=client.list_examples(dataset_name=dataset_name, splits=["test", "training"]),
evaluators=[correct_label],
experiment_prefix="Toxic Queries",
)

For more details on fetching views of a dataset, refer to the guide on fetching datasets.

## ​ Share a dataset

### ​ Share a dataset publicly

Sharing a dataset publicly will make the **dataset examples, experiments and associated runs, and feedback on this dataset accessible to anyone with the link**, even if they don’t have a LangSmith account. Make sure you’re not sharing sensitive information.This feature is only available in the cloud-hosted version of LangSmith.

From the **Dataset & Experiments** tab, select a dataset, click **⋮** (top right of the page), click **Share Dataset**. This will open a dialog where you can copy the link to the dataset.!Share Dataset

### ​ Unshare a dataset

1. Click on **Unshare** by clicking on **Public** in the upper right hand corner of any publicly shared dataset, then **Unshare** in the dialog. !Unshare Dataset

## ​ Export a dataset

You can export your LangSmith dataset to a CSV, JSONL, or OpenAI’s fine tuning format from the LangSmith UI.From the **Dataset & Experiments** tab, select a dataset, click **⋮** (top right of the page), click **Download Dataset**.!Export Dataset Button

## ​ Export filtered traces from experiment to dataset

After running an offline evaluation in LangSmith, you may want to export traces that met some evaluation criteria to a dataset.

### ​ View experiment traces

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to create and manage datasets programmatically\\
\\
Previous Custom output rendering\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-output-rendering

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Datasets

Custom output rendering

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Configure custom output rendering
- For tracing projects
- For datasets
- For annotation queues
- Build a custom renderer
- Understand the message format
- Example implementation
- Where custom rendering appears

Custom output rendering allows you to visualize run outputs and dataset reference outputs using your own custom HTML pages. This is particularly useful for:

- **Domain-specific formatting**: Display medical records, legal documents, or other specialized data types in their native format.
- **Custom visualizations**: Create charts, graphs, or diagrams from numeric or structured output data.

In this page you’ll learn how to:

- **Configure custom rendering** in the LangSmith UI.
- **Build a custom renderer** to display output data.
- **Understand where custom rendering appears** in LangSmith.

## ​ Configure custom output rendering

Configure custom rendering at two levels:

- **For datasets**: Apply custom rendering to all runs associated with that dataset, wherever they appear—in experiments, run detail panes, or annotation queues.
- **For annotation queues**: Apply custom rendering to all runs within a specific annotation queue, regardless of which dataset they come from. This takes precedence over dataset-level configuration.

### ​ For tracing projects

To configure custom output rendering for a tracing project:!Tracing project settings showing custom output rendering configuration

1. Navigate to the **Tracing Projects** page.
2. Click on an existing tracing project or create a new one.
3. In the edit tracing project pane, scroll to the **Custom Output Rendering** section.
4. Toggle **Enable custom output rendering**.
5. Enter the webpage URL in the **URL** field.
6. Click **Save**.

### ​ For datasets

To configure custom output rendering for a dataset:!Dataset page with three-dot menu showing Custom Output Rendering option

1. Navigate to your dataset in the **Datasets & Experiments** page.
2. Click **⋮** (three-dot menu) in the top right corner.
3. Select **Custom Output Rendering**.
4. Toggle **Enable custom output rendering**.
5. Enter the webpage URL in the **URL** field.
6. Click **Save**.

### ​ For annotation queues

To configure custom output rendering for an annotation queue:!Annotation queue settings showing custom output rendering configuration

1. Navigate to the **Annotation Queues** page.
2. Click on an existing annotation queue or create a new one.
3. In the annotation queue settings pane, scroll to the **Custom Output Rendering** section.
4. Toggle **Enable custom output rendering**.
5. Enter the webpage URL in the **URL** field.
6. Click **Save** or **Create**.

## ​ Build a custom renderer

### ​ Understand the message format

Your HTML page will receive output data via the postMessage API. LangSmith sends messages with the following structure:

Copy

{
type: "output" | "reference",
data: {
// The outputs (actual output or reference output)
// Structure varies based on your application
},
metadata: {
inputs: {
// The inputs that generated this output
// Structure varies based on your application
}
}
}

- `type`: Indicates whether this is an actual output (`"output"`) or a reference output (`"reference"`).
- `data`: The output data itself.
- `metadata.inputs`: The input data that generated this output, provided for context.

**Message delivery timing**: LangSmith uses an exponential backoff retry mechanism to ensure your page receives the data even if it loads slowly. Messages are sent up to 6 times with increasing delays (100ms, 200ms, 400ms, 800ms, 1600ms, 3200ms).

### ​ Example implementation

This example listens for incoming postMessage events and displays them on the page. Each message is numbered and formatted as JSON, making it easy to inspect the data structure LangSmith sends to your renderer.

count++;
const header = document.createElement("h3");
header.appendChild(document.createTextNode(`Message ${count}`));
const code = document.createElement("code");
code.appendChild(document.createTextNode(JSON.stringify(event.data, null, 2)));
const pre = document.createElement("pre");
pre.appendChild(code);
document.getElementById("messages").appendChild(header);
document.getElementById("messages").appendChild(pre);
});

## ​ Where custom rendering appears

When enabled, your custom rendering will replace the default output view in:

- **Experiment comparison view**: When comparing outputs across multiple experiments:

- **Run detail panes**: When viewing runs that are associated with a dataset:

- **Annotation queues**: When reviewing runs in annotation queues:

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Manage datasets\\
\\
Previous How to evaluate an LLM application\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/analyze-an-experiment

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Analyze experiment results

Analyze an experiment

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Analyze a single experiment
- Open the experiment view
- View experiment results
- Customize columns
- Sort and filter
- Table views
- View the traces
- View evaluator runs
- Group results by metadata
- Repetitions
- Compare to another experiment
- Download experiment results as a CSV
- Rename an experiment

This page describes some of the essential tasks for working with _experiments_ in LangSmith:

- **Analyze a single experiment**: View and interpret experiment results, customize columns, filter data, and compare runs.
- **Download experiment results as a CSV**: Export your experiment data for external analysis and sharing.
- **Rename an experiment**: Update experiment names in both the Playground and Experiments view.

## ​ Analyze a single experiment

After running an experiment, you can use LangSmith’s experiment view to analyze the results and draw insights about your experiment’s performance.

### ​ Open the experiment view

To open the experiment view, select the relevant _dataset_ from the **Dataset & Experiments** page and then select the experiment you want to view.!Open experiment view

#### ​ Customize columns

By default, the experiment view shows the input, output, and reference output for each example in the dataset, feedback scores from evaluations and experiment metrics like cost, token counts, latency and status.You can customize the columns using the **Display** button to make it easier to interpret experiment results:

- **Break out fields from inputs, outputs, and reference outputs** into their own columns. This is especially helpful if you have long inputs/outputs/reference outputs and want to surface important fields.
- **Hide and reorder columns** to create focused views for analysis.
- **Control decimal precision on feedback scores**. By default, LangSmith surfaces numerical feedback scores with a decimal precision of 2, but you can customize this setting to be up to 6 decimals.
- **Set the Heat Map threshold** to high, middle, and low for numeric feedback scores in your experiment, which affects the threshold at which score chips render as red or green:

You can set default configurations for an entire dataset or temporarily save settings just for yourself.

#### ​ Sort and filter

To sort or filter feedback scores, you can use the actions in the column headers.!Sort and filter

#### ​ Table views

Depending on the view most useful for your analysis, you can change the formatting of the table by toggling between a compact view, a full, view, and a diff view.

- The **Compact** view shows each run as a one-line row, for ease of comparing scores at a glance.
- The **Full** view shows the full output for each run for digging into the details of individual runs.
- The **Diff** view shows the text difference between the reference output and the output for each run.

#### ​ View the traces

Hover over any of the output cells, and click on the trace icon to view the trace for that run. This will open up a trace in the side panel.To view the entire tracing project, click on the **View Project** button in the top right of the header.!View trace

#### ​ View evaluator runs

For evaluator scores, you can view the source run by hovering over the evaluator score cell and clicking on the arrow icon. This will open up a trace in the side panel. If you’re running a LLM-as-a-judge evaluator, you can view the prompt used for the evaluator in this run. If your experiment has repetitions, you can click on the aggregate average score to find links to all of the individual runs.!View evaluator runs

### ​ Group results by metadata

You can add metadata to examples to categorize and organize them. For example, if you’re evaluating factual accuracy on a question answering dataset, the metadata might include which subject area each question belongs to. Metadata can be added either via the UI or via the SDK.To analyze results by metadata, use the **Group by** dropdown in the top right corner of the experiment view and select your desired metadata key. This displays average feedback scores, latency, total tokens, and cost for each metadata group.

You will only be able to group by example metadata on experiments created after February 20th, 2025. Any experiments before that date can still be grouped by metadata, but only if the metadata is on the experiment traces themselves.

### ​ Repetitions

If you’ve run your experiment with _repetitions_, there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view.When you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.!Repetitions

### ​ Compare to another experiment

In the top right of the experiment view, you can select another experiment to compare to. This will open up a comparison view, where you can see how the two experiments compare. To learn more about the comparison view, see how to compare experiment results.

## ​ Download experiment results as a CSV

LangSmith lets you download experiment results as a CSV file, which allows you to analyze and share your results.To download as a CSV, click the download icon at the top of the experiment view. The icon is directly to the left of the Compact toggle.!Download CSV

## ​ Rename an experiment

Experiment names must be unique per workspace.

You can rename an experiment in the LangSmith UI in:

- The Playground. When running experiments in the Playground, a default name with the format `pg::prompt-name::model::uuid` (eg. `pg::gpt-4o-mini::897ee630`) is automatically assigned.You can rename an experiment immediately after running it by editing its name in the Playground table header.!Edit name in playground
- The Experiments view. When viewing results in the experiments view, you can rename an experiment by using the pencil icon beside the experiment name.!Edit name in experiments view

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Run backtests on a new version of an agent\\
\\
Previous How to compare experiment results\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/compare-experiment-results

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Analyze experiment results

How to compare experiment results

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Open the comparison view
- Adjust the table display
- View regressions and improvements
- Update baseline experiment and metric
- Open a trace
- Expand detailed view
- View summary charts
- Use experiment metadata as chart labels

When you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different _experiments_.LangSmith supports a comparison view that lets you hone in on key differences, regressions, and improvements between different experiments.

## ​ Open the comparison view

1. To access the experiment comaprison view, navigate to the **Datasets & Experiments** page.
2. Select a dataset, which will open the **Experiments** tab.
3. Select two or more experiments abd then click **Compare**.

## ​ Adjust the table display

You can toggle between different views by clicking **Full** or **Compact** at the top of the **Comparing Experiments** page.Toggling **Full** will show the full text of the input, output, and reference output for each run. If the reference output is too long to display in the table, you can click on **Expand detailed view** to view the full content.You can also select and hide individual feedback keys or individual metrics in the **Display** settings dropdown to isolate the information you need in the comparison view.

## ​ View regressions and improvements

In the comparison view, runs that _regressed_ on your specified feedback key against your baseline experiment will be highlighted in red, while runs that _improved_ will be highlighted in green. At the top of each column, you can find how many runs in that experiment did better and how many did worse than your baseline experiment.Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.!The comparison view comparing 2 experiments with the regressions and improvements highlighted in red and green respectively.

## ​ Update baseline experiment and metric

In order to track regressions, you need to:

1. In the **Baseline** dropdown at the top of the comparison view, select a **Baseline experiment** against which to compare. By default, the newest experiment is selected as the baseline.
2. Select a **Feedback key** (evaluation metric) you want to focus compare against. One will be assigned by default, but you can adjust as needed.
3. Configure whether a higher score is better for the selected feedback key. This preference will be stored.

## ​ Open a trace

If the example you’re evaluating is from an ingested run, you can hover over the output cell and click on the trace icon to open the trace view for that run. This will open up a trace in the side panel.!The View trace icon highlighted from an ingested run.

## ​ Expand detailed view

From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores.!An example in the Comparing Experiments view of a expanded view of the repetitions.

## ​ View summary charts

View summary charts by clicking on the **Charts** tab at the top of the page.!The Charts summary page with 8 summary charts for the comparison.

## ​ Use experiment metadata as chart labels

You can configure the x-axis labels for the charts based on experiment metadata.Select a metadata key in the **x-axis** dropdown to change the chart labels.!x-axis dropdown highlighted with a list of the metadata attached to the experiment.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Analyze an experiment\\
\\
Previous How to filter experiments in the UI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/filter-experiments-ui

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Analyze experiment results

How to filter experiments in the UI

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Background: add metadata to your experiments
- Filter experiments in the UI

LangSmith lets you filter your previous experiments by feedback scores and metadata to make it easy to find only the experiments you care about.

## ​ Background: add metadata to your experiments

When you run an experiment in the SDK, you can attach metadata to make it easier to filter in UI. This is helpful if you know what axes you want to drill down into when running experiments.In our example, we are going to attach metadata to our experiment around the model used, the model provider, and a known ID of the prompt:

Copy

models = {
"openai-gpt-4o": ChatOpenAI(model="gpt-4o", temperature=0),
"openai-gpt-4o-mini": ChatOpenAI(model="gpt-4o-mini", temperature=0),
"anthropic-claude-3-sonnet-20240229": ChatAnthropic(temperature=0, model_name="claude-3-sonnet-20240229")
}

prompts = {
"singleminded": "always answer questions with the word banana.",
"fruitminded": "always discuss fruit in your answers.",
"basic": "you are a chatbot."
}

llm = ChatOpenAI(model="gpt-4o", temperature=0)
answer_grader = hub.pull("langchain-ai/rag-answer-vs-reference") | llm
score = answer_grader.invoke(
{
"question": example.inputs["question"],
"correct_answer": example.outputs["answer"],
"student_answer": run.outputs,
}
)
return {"key": "correctness", "score": score["Score"]}

dataset_name = "Filterable Dataset"

for model_type, model in models.items():
for prompt_type, prompt in prompts.items():
def predict(example):
return model.invoke(
[("system", prompt), ("user", example["question"])]
)

model_provider = model_type.split("-")[0]
model_name = model_type[len(model_provider) + 1:]

evaluate(
predict,
data=dataset_name,
evaluators=[answer_evaluator],
# ADD IN METADATA HERE!!
metadata={
"model_provider": model_provider,
"model_name": model_name,
"prompt_id": prompt_type
}
)

## ​ Filter experiments in the UI

In the UI, we see all experiments that have been run by default.!Filter all experimentsIf we, say, have a preference for openai models, we can easily filter down and see scores within just openai models first:!Filter openaiWe can stack filters, allowing us to filter out low scores on correctness to make sure we only compare relevant experiments:!Filter feedbackFinally, we can clear and reset filters. For example, if we see there is clear there’s a winner with the `singleminded` prompt, we can change filtering settings to see if any other model providers’ models work as well with it:!Filter singleminded

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to compare experiment results\\
\\
Previous How to fetch performance metrics for an experiment\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/fetch-perf-metrics-experiment

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Analyze experiment results

How to fetch performance metrics for an experiment

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

Tracing projects and experiments use the same underlying data structure in our backend, which is called a “session.”You might see these terms interchangeably in our documentation, but they all refer to the same underlying data structure.We are working on unifying the terminology across our documentation and APIs.

When you run an experiment using `evaluate` with the Python or TypeScript SDK, you can fetch the performance metrics for the experiment using the `read_project`/`readProject` methods.The payload for experiment details includes the following values:

Copy

{
"start_time": "2024-06-06T01:02:51.299960",
"end_time": "2024-06-06T01:03:04.557530+00:00",
"extra": {
"metadata": {
"git": {
"tags": null,
"dirty": true,
"branch": "ankush/agent-eval",
"commit": "...",
"repo_name": "...",
"remote_url": "...",
"author_name": "Ankush Gola",
"commit_time": "...",
"author_email": "..."
},
"revision_id": null,
"dataset_splits": ["base"],
"dataset_version": "2024-06-05T04:57:01.535578+00:00",
"num_repetitions": 3
}
},
"name": "SQL Database Agent-ae9ad229",
"description": null,
"default_dataset_id": null,
"reference_dataset_id": "...",
"id": "...",
"run_count": 9,
"latency_p50": 7.896,
"latency_p99": 13.09332,
"first_token_p50": null,
"first_token_p99": null,
"total_tokens": 35573,
"prompt_tokens": 32711,
"completion_tokens": 2862,
"total_cost": 0.206485,
"prompt_cost": 0.163555,
"completion_cost": 0.04293,
"tenant_id": "...",
"last_run_start_time": "2024-06-06T01:02:51.366397",
"last_run_start_time_live": null,
"feedback_stats": {
"cot contextual accuracy": {
"n": 9,
"avg": 0.6666666666666666,
"values": {
"CORRECT": 6,
"INCORRECT": 3
}
}
},
"session_feedback_stats": {},
"run_facets": [],
"error_rate": 0,
"streaming_rate": 0,
"test_run_number": 11
}

From here, you can extract performance metrics such as:

- `latency_p50`: The 50th percentile latency in seconds.
- `latency_p99`: The 99th percentile latency in seconds.
- `total_tokens`: The total number of tokens used.
- `prompt_tokens`: The number of prompt tokens used.
- `completion_tokens`: The number of completion tokens used.
- `total_cost`: The total cost of the experiment.
- `prompt_cost`: The cost of the prompt tokens.
- `completion_cost`: The cost of the completion tokens.
- `feedback_stats`: The feedback statistics for the experiment.
- `error_rate`: The error rate for the experiment.
- `first_token_p50`: The 50th percentile latency for the time to generate the first token (if using streaming).
- `first_token_p99`: The 99th percentile latency for the time to generate the first token (if using streaming).

Here is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs.First, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript. Please view the how-to guide on evaluation for more details.

from langsmith import Client

client = Client()

# Create a dataset
dataset_name = "HelloDataset"
dataset = client.create_dataset(dataset_name=dataset_name)

examples = [\
{\
"inputs": {"input": "Harrison"},\
"outputs": {"expected": "Hello Harrison"},\
},\
{\
"inputs": {"input": "Ankush"},\
"outputs": {"expected": "Hello Ankush"},\
},\
]

client.create_examples(dataset_id=dataset.id, examples=examples)

Next, we will create an experiment, retrieve the experiment name from the result of `evaluate`, then fetch the performance metrics for the experiment.

Python

TypeScript

from langsmith.schemas import Example, Run
dataset_name = "HelloDataset"

return {"score": 1, "key": "foo"}

from langsmith import evaluate

results = evaluate(
lambda inputs: "Hello " + inputs["input"],
data=dataset_name,
evaluators=[foo_label],
experiment_prefix="Hello",
)

resp = client.read_project(project_name=results.experiment_name, include_stats=True)
print(resp.json(indent=2))

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to filter experiments in the UI\\
\\
Previous How to upload experiments run outside of LangSmith with the REST API\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/upload-existing-experiments

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Analyze experiment results

How to upload experiments run outside of LangSmith with the REST API

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Request body schema
- Considerations
- Example request
- View the experiment in the UI

Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our endpoint.This guide will show you how to upload evals using the REST API, using the `requests` library in Python as an example. However, the same principles apply to any language.

## ​ Request body schema

Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within the experiment. Each object in the `results` represents a “row” in the experiment - a single dataset example, along with an associated run. Note that `dataset_id` and `dataset_name` refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset in LangSmith (unless that dataset was created via this endpoint).You may use the following schema to upload experiments to the `/datasets/upload-experiment` endpoint:

Copy

{
"experiment_name": "string (required)",
"experiment_description": "string (optional)",
"experiment_start_time": "datetime (required)",
"experiment_end_time": "datetime (required)",
"dataset_id": "uuid (optional - an external dataset id, used to group experiments together)",
"dataset_name": "string (optional - must provide either dataset_id or dataset_name)",
"dataset_description": "string (optional)",
"experiment_metadata": { // Object (any shape - optional)
"key": "value"
},
"summary_experiment_scores": [ // List of summary feedback objects (optional)\
{\
"key": "string (required)",\
"score": "number (optional)",\
"value": "string (optional)",\
"comment": "string (optional)",\
"feedback_source": { // Object (optional)\
"type": "string (required)"\
},\
"feedback_config": { // Object (optional)\
"type": "string enum: continuous, categorical, or freeform",\
"min": "number (optional)",\
"max": "number (optional)",\
"categories": [ // List of feedback category objects (optional)\
{\
"value": "number (required)",\
"label": "string (optional)"\
}\
]\
},\
"created_at": "datetime (optional - defaults to now)",\
"modified_at": "datetime (optional - defaults to now)",\
"correction": "Object or string (optional)"\
}\
],
"results": [ // List of experiment row objects (required)\
{\
"row_id": "uuid (required)",\
"inputs": { // Object (required - any shape). This will\
"key": "val" // be the input to both the run and the dataset example.\
},\
"expected_outputs": { // Object (optional - any shape).\
"key": "val" // These will be the outputs of the dataset examples.\
},\
"actual_outputs": { // Object (optional - any shape).\
"key": "val" // These will be the outputs of the runs.\
},\
"evaluation_scores": [ // List of feedback objects for the run (optional)\
{\
"key": "string (required)",\
"score": "number (optional)",\
"value": "string (optional)",\
"comment": "string (optional)",\
"feedback_source": { // Object (optional)\
"type": "string (required)"\
},\
"feedback_config": { // Object (optional)\
"type": "string enum: continuous, categorical, or freeform",\
"min": "number (optional)",\
"max": "number (optional)",\
"categories": [ // List of feedback category objects (optional)\
{\
"value": "number (required)",\
"label": "string (optional)"\
}\
]\
},\
"created_at": "datetime (optional - defaults to now)",\
"modified_at": "datetime (optional - defaults to now)",\
"correction": "Object or string (optional)"\
}\
],\
"start_time": "datetime (required)", // The start/end times for the runs will be used to\
"end_time": "datetime (required)", // calculate latency. They must all fall between the\
"run_name": "string (optional)", // start and end times for the experiment.\
"error": "string (optional)",\
"run_metadata": { // Object (any shape - optional)\
"key": "value"\
}\
}\
]
}

The response JSON will be a dict with keys `experiment` and `dataset`, each of which is an object that contains relevant information about the experiment and dataset that was created.

## ​ Considerations

You may upload multiple experiments to the same dataset by providing the same dataset\_id or dataset\_name between multiple calls. Your experiments will be grouped together under a single dataset, and you will be able to use the comparison view to compare results between experiments.Ensure that the start and end times of your individual rows are all between the start and end time of your experiment.You must provide either a dataset\_id or a dataset\_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if you only provide a name.You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets.

## ​ Example request

Below is an example of a simple call to the `/datasets/upload-experiment`. This is a basic example that just uses the most important fields as an illustration.

import os
import requests

body = {
"experiment_name": "My external experiment",
"experiment_description": "An experiment uploaded to LangSmith",
"dataset_name": "my-external-dataset",
"summary_experiment_scores": [\
{\
"key": "summary_accuracy",\
"score": 0.9,\
"comment": "Great job!"\
}\
],
"results": [\
{\
"row_id": "<<uuid>>",\
"inputs": {\
"input": "Hello, what is the weather in San Francisco today?"\
},\
"expected_outputs": {\
"output": "Sorry, I am unable to provide information about the current weather."\
},\
"actual_outputs": {\
"output": "The weather is partly cloudy with a high of 65."\
},\
"evaluation_scores": [\
{\
"key": "hallucination",\
"score": 1,\
"comment": "The chatbot made up the weather instead of identifying that "\
"they don't have enough info to answer the question. This is "\
"a hallucination."\
}\
],\
"start_time": "2024-08-03T00:12:39",\
"end_time": "2024-08-03T00:12:41",\
"run_name": "Chatbot"\
},\
{\
"row_id": "<<uuid>>",\
"inputs": {\
"input": "Hello, what is the square root of 49?"\
},\
"expected_outputs": {\
"output": "The square root of 49 is 7."\
},\
"actual_outputs": {\
"output": "7."\
},\
"evaluation_scores": [\
{\
"key": "hallucination",\
"score": 0,\
"comment": "The chatbot correctly identified the answer. This is not a "\
"hallucination."\
}\
],\
"start_time": "2024-08-03T00:12:40",\
"end_time": "2024-08-03T00:12:42",\
"run_name": "Chatbot"\
}\
],
"experiment_start_time": "2024-08-03T00:12:38",
"experiment_end_time": "2024-08-03T00:12:43"
}

resp = requests.post(
"https://api.smith.langchain.com/api/v1/datasets/upload-experiment", # Update appropriately for self-hosted installations or the EU region
json=body,
headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
)

print(resp.json())

Below is the response received:

{
"dataset": {
"name": "my-external-dataset",
"description": null,
"created_at": "2024-08-03T00:36:23.289730+00:00",
"data_type": "kv",
"inputs_schema_definition": null,
"outputs_schema_definition": null,
"externally_managed": true,
"id": "<<uuid>>",
"tenant_id": "<<uuid>>",
"example_count": 0,
"session_count": 0,
"modified_at": "2024-08-03T00:36:23.289730+00:00",
"last_session_start_time": null
},
"experiment": {
"start_time": "2024-08-03T00:12:38",
"end_time": "2024-08-03T00:12:43+00:00",
"extra": null,
"name": "My external experiment",
"description": "An experiment uploaded to LangSmith",
"default_dataset_id": null,
"reference_dataset_id": "<<uuid>>",
"trace_tier": "longlived",
"id": "<<uuid>>",
"run_count": null,
"latency_p50": null,
"latency_p99": null,
"first_token_p50": null,
"first_token_p99": null,
"total_tokens": null,
"prompt_tokens": null,
"completion_tokens": null,
"total_cost": null,
"prompt_cost": null,
"completion_cost": null,
"tenant_id": "<<uuid>>",
"last_run_start_time": null,
"last_run_start_time_live": null,
"feedback_stats": null,
"session_feedback_stats": null,
"run_facets": null,
"error_rate": null,
"streaming_rate": null,
"test_run_number": 1
}
}

Note that the latency and feedback stats in the experiment results are null because the runs haven’t had a chance to be persisted yet, which may take a few seconds. If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don’t ask for this information in the request body).

## ​ View the experiment in the UI

Now, login to the UI and click on your newly-created dataset! You should see a single experiment: !Uploaded experiments tableYour examples will have been uploaded: !Uploaded examplesClicking on your experiment will bring you to the comparison view: !Uploaded experiment comparison viewAs you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to fetch performance metrics for an experiment\\
\\
Previous Use annotation queues\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/annotation-queues

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Annotation & human feedback

Use annotation queues

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Single-run annotation queues
- Create a single-run queue
- Basic Details
- Annotation Rubric
- Collaborator Settings
- Assign runs to a single-run queue
- Review a single-run queue
- Pairwise annotation queues
- Create a pairwise queue
- Add more comparisons to a pairwise queue
- Review a pairwise queue
- Video guide

_Annotation queues_ provide a streamlined, directed view for human annotators to attach feed. While you can always annotate traces inline, annotation queues provide a way to group runs together, prescribe rubrics, and track reviewer progress.LangSmith supports two queue styles:

- **Single-run annotation queues** present one run at a time and let reviewers submit any rubric feedback you configure.
- **Pairwise annotation queues (PAQs)** present two runs side-by-side so reviewers can quickly decide which output is better (or if they are equivalent) against the rubric items you define.

## ​ Single-run annotation queues

Single-run queues present one run at a time and let reviewers submit any rubric feedback you configure. They can be created directly from the **Annotation queues** section in the LangSmith UI.

### ​ Create a single-run queue

1. Navigate to **Annotation queues** in the left navigation.
2. Click **\+ New annotation queue** in the top-right corner.!Create Annotation Queue form with Basic Details, Annotation Rubric, and Feedback sections.

#### ​ Basic Details

1. Fill in the **Name** and **Description** of the queue.
2. Optionally assign a **default dataset** to streamline exporting reviewed runs into a dataset in your LangSmith workspace.

#### ​ Annotation Rubric

1. Draft some high-level instructions for your annotators, which will be shown in the sidebar on every run.
2. Click **\+ Desired Feedback** to add feedback keys to your annotation queue. Annotators will be presented with these feedback keys on each run.
3. Add a description for each, as well as a short description of each category, if the feedback is categorical.!Annotation queue rubric form with instructions and desired feedback entered.For example, with the descriptions in the previous screenshot, reviewers will see the **Annotation Rubric** details in the right-hand pane of the UI.!The rendered rubric for reviewers from the example instructions.

#### ​ Collaborator Settings

When there are multiple annotators for a run:

- **Number of reviewers per run**: This determines the number of reviewers that must mark a run as **Done** for it to be removed from the queue. If you check **All workspace members review each run**, then a run will remain in the queue until all workspace members have marked their review as **Done**. - Reviewers cannot view the feedback left by other reviewers.
- Comments on runs are visible to all reviewers.
- **Enable reservations on runs**: When a reviewer views a run, the run is reserved for that reviewer for the specified **Reservation length**. If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time.

We recommend enabling reservations. This will prevent multiple annotators from reviewing the same run at the same time.

If a reviewer has viewed a run and then leaves the run without marking it **Done**, the reservation will expire after the specified **Reservation length**. The run is then released back into the queue and can be reserved by another reviewer.

Clicking **Requeue** for a run’s annotation will only move the current run to the end of the current user’s queue; it won’t affect the queue order of any other user. It will also release the reservation that the current user has on that run.

Because of these settings, the number of runs visible to each reviewer can differ from the total queue size.You can revisit the pencil icon in **Annotation queues** to update any settings later.

### ​ Assign runs to a single-run queue

There are several ways to populate a single-run queue with work items:

- **From a trace view**: Click **Add to Annotation Queue** in the top-right corner of any trace view. You can add any intermediate run, but not the root span.!Trace view with the Add to Annotation Queue button highlighted at the top of the screen.
- **From the runs table**: Select multiple runs, then click **Add to Annotation Queue** at the bottom of the page.!View of the runs table with runs selected. Add to Annotation Queue button at the botton of the page.
- **Automation rules**: Set up a rule to automatically assign runs that match a filter (for example, errors or low user scores) into a queue.
- **Datasets & experiments**: Select one or more experiments within a dataset and click **Annotate**. Choose an existing queue or create a new one, then confirm the (single-run) queue option.!Selected experiments with the Annotate button at the bottom of the page.

### ​ Review a single-run queue

1. Navigate to the **Annotation Queues** section through the left-hand navigation bar.
2. Click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review.
3. You can attach a comment, attach a score for a particular feedback criteria, add the run to a dataset or mark the run as reviewed. You can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the **Trash** icon next to **View run**.

The keyboard shortcuts that are next to each option can help streamline the review process.

## ​ Pairwise annotation queues

Pairwise annotation queues (PAQs) present two runs side-by-side so reviewers can quickly decide which output is better (or if they are equivalent) against the rubric items you define. They are designed for fast A/B comparisons between two experiments (often a baseline vs. a candidate model) and must be created from the **Datasets & Experiments** pages.

### ​ Create a pairwise queue

1. Navigate to **Datasets & Experiments**, open a dataset, and select **exactly two experiments** you want to compare.
2. Click **Annotate**. In the popover, choose **Add to Pairwise Annotation Queue**. (The button is disabled until exactly two experiments are selected.)!Popover showing the "Add to Pairwise Annotation Queue" card highlighted after two experiments are selected.
3. Decide whether to send the experiments to an existing pairwise queue or create a new one.
4. Provide the queue details: - **Basic details** (name and description)
- **Instructions & rubrics** tailored to pairwise scoring
- **Collaborator settings** (reviewer count, reservations, reservation length)
5. Submit the form to create the queue. LangSmith immediately pairs runs from the two experiments and populates the queue.

Key differences for PAQs:

- **Experiments**: You must provide two experiment sessions up front. LangSmith automatically pairs their runs in chronological order and populates the queue during creation.
- **Rubric**: Pairwise rubric items only require a feedback key and (optionally) a description. Annotators decide whether Run A, Run B, or both are better for each rubric item.
- **Dataset**: Pairwise queues do not use a default dataset, because comparisons span two experiments.
- **Reservations & reviewers**: The same collaborator controls apply. Reservations help prevent two people from judging the same comparison simultaneously.

### ​ Add more comparisons to a pairwise queue

If you need to add more comparisons later, Review a pairwise queue

1. From **Annotation queues**, select the pairwise queue you want to review.
2. Each queue item displays Run A on the left and Run B on the right, along with your rubric.
3. For every rubric item:
- Choose **A is better**, **B is better**, or **Equal**. The UI records binary feedback on both runs behind the scenes.
- Use hotkeys `A`, `B`, or `E` to lock in your choice.
4. Once you finish all rubric items, press **Done** (or `Enter` on the final rubric item) to advance to the next comparison.
5. Optional actions:
- Leave comments tied to either run.
- Requeue the comparison if you need to revisit it later.
- Open the full trace view for deeper debugging.

Reservations, reviewer thresholds, and comments behave identically to those in single-run queues, enabling teams to use different queue types without modifying their existing workflow.!Pairwise review screen showing runs side-by-side with the feedback pane containing A/B/Equal buttons and keyboard shortcuts.

Consider routing runs that already have user feedback (e.g., thumbs-down) into a single-run queue for triage and a pairwise queue for head-to-head comparisons against a stronger baseline. This helps you identify regressions quickly. To learn more about how to capture user feedback from your LLM application, follow the guide on attaching user feedback.

## ​ Video guide

Getting Started with LangSmith (6/8): Annotation Queues - YouTube

Photo image of LangChain

LangChain

165K subscribers

Getting Started with LangSmith (6/8): Annotation Queues

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Watch on

0:00

0:00 / 4:14

•Live

•

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to upload experiments run outside of LangSmith with the REST API\\
\\
Previous Set up feedback criteria\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/set-up-feedback-criteria

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Annotation & human feedback

Set up feedback criteria

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Continuous feedback
- Categorical feedback

**Recommended Reading**Before diving into this content, it might be helpful to read the following:

- Conceptual guide on tracing and feedback
- Reference guide on feedback data format

Feedback criteria are represented in the application as feedback tags. For human feedback, you can set up new feedback criteria as continuous feedback or categorical feedback.To set up a new feedback criteria, follow this link to view all existing tags for your workspace, then click **New Tag**.

## ​ Continuous feedback

For continuous feedback, you can enter a feedback tag name, then select a minimum and maximum value. Every value, including floating-point numbers, within this range will be accepted as feedback scores.!Cont feedback

## ​ Categorical feedback

For categorical feedback, you can enter a feedback tag name, then add a list of categories, each category mapping to a score. When you provide feedback, you can select one of these categories as the feedback score.
Both the category label and the score will be logged as feedback in `value` and `score` fields, respectively.!Cat feedback

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Use annotation queues\\
\\
Previous Annotate traces and runs inline\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/annotate-traces-inline

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Annotation & human feedback

Annotate traces and runs inline

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

LangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user’s comment or a note about a specific issue.
You can annotate a trace either inline or by sending the trace to an annotation queue, which allows you to closely inspect and log feedbacks to runs one at a time.
Feedback tags are associated with your workspace.

**You can attach user feedThis will open up a pane that allows you to choose from feedback tags associated with your workspace and add a score for particular tags. You can also add a standalone comment. Follow this guide to set up feedback tags for your workspace.
You can also set up new feedback criteria from within the pane itself.!Annotation sidebarYou can use the labeled keyboard shortcuts to streamline the annotation process.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Set up feedback criteria\\
\\
Previous How to audit evaluator scores\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/audit-evaluator-scores

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Annotation & human feedback

How to audit evaluator scores

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- In the comparison view
- In the runs table
- In the SDK

LLM-as-a-judge evaluators don’t always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK.

## ​ In the comparison view

In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the “edit” icon on the right to bring up the corrections view. You may then type in your desired score in the text box under “Make correction”. If you would like, you may also attach an explanation to your correction. This is useful if you are using a few-shot evaluator and will be automatically inserted into your few-shot examples in place of the `few_shot_explanation` prompt variable.!Audit Evaluator Comparison View

## ​ In the runs table

In the runs table, find the “Feedback” column and click on the feedback tag to bring up the feedback details. Again, click the “edit” icon on the right to bring up the corrections view.!Audit Evaluator Runs Table

## ​ In the SDK

Corrections can be made via the SDK’s `update_feedback` function, with the `correction` dict. You must specify a `score` key which corresponds to a number for it to be rendered in the UI.

Python

TypeScript

Copy

import langsmith

client = langsmith.Client()

client.update_feedback(
my_feedback_id,
correction={
"score": 1,
},
)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Annotate traces and runs inline\\
\\
Previous Example data format\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/example-data-format

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Common data types

Example data format

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

Before diving into this content, it might be helpful to read the following:

- Conceptual guide on evaluation

LangSmith stores examples in datasets as follows:

| Field Name | Type | Description |
| --- | --- | --- |
| **id** | UUID | Unique identifier for the example. |
| **name** | string | The name of the example. |
| **created\_at** | datetime | The time this example was created |
| **modified\_at** | datetime | The last time this example was modified |
| **inputs** | object | A map of inputs for the example. |
| **outputs** | object | A map or set of outputs generated by the run. |
| **dataset\_id** | UUID | The dataset the example belongs to |
| **source\_run\_id** | UUID | If this example was created from a LangSmith `Run`, the ID of said run |
| **metadata** | object | A map of additional, user or SDK defined information that can be stored on an example. |

To learn more about how examples are used in evaluation, read our how-to guide on evaluating LLM applications.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to audit evaluator scores\\
\\
Previous Dataset prebuilt JSON schema types\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/dataset-json-types

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Common data types

Dataset prebuilt JSON schema types

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

LangSmith recommends that you set a schema on the inputs and outputs of your dataset schemas to ensure data consistency and that your examples are in the right format for downstream processing, like running evals.In order to better support LLM workflows, LangSmith has support for a few different predefined prebuilt types. These schemas are hosted publicly by the LangSmith API, and can be defined in your dataset schemas using JSON Schema references. The table of available schemas can be seen below

| Type | JSON Schema Reference Link | Usage |
| --- | --- | --- |
| Message | | Represents messages sent to a chat model, following the OpenAI standard format. |
| Tool | | Tool definitions available to chat models for function calling, defined in OpenAI’s JSON Schema inspired function format. |

LangSmith lets you define a series of transformations that collect the above prebuilt types from your traces and add them to your dataset. For more info on available transformations, see our reference

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Example data format\\
\\
Previous Dataset transformations\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/dataset-transformations

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Common data types

Dataset transformations

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Transformation types
- Chat Model prebuilt schema
- Compatibility
- Enablement
- Specs
- Input schema
- Output schema
- Transformations

LangSmith allows you to attach transformations to fields in your dataset’s schema that apply to your data before it is added to your dataset, whether that be from UI, API, or run rules.Coupled with LangSmith’s prebuilt JSON schema types, these allow you to do easy preprocessing of your data before saving it into your datasets.

## ​ Transformation types

| Transformation Type | Target Types | Functionality |
| --- | --- | --- |
| `remove_system_messages` | `Array[Message]` | Filters a list of messages to remove any system messages. |
| `convert_to_openai_message` | Message `Array[Message]` | Converts any incoming data from LangChain’s internal serialization format to OpenAI’s standard message format using langchain’s `convert_to_openai_messages`. If the target field is marked as required, and no matching message is found upon entry, it will attempt to extract a message (or list of messages) from several well-known LangSmith tracing formats (e.g., any traced LangChain `BaseChatModel` run or traced run from the LangSmith OpenAI wrapper), and remove the original key containing the message. |
| `convert_to_openai_tool` | `Array[Tool]` Only available on top level fields in the inputs dictionary. | Converts any incoming data into OpenAI standard tool formats here using langchain’s `convert_to_openai_tool` Will extract tool definitions from a run’s invocation parameters if present / no tools are found at the specified key. This is useful because LangChain chat models trace tool definitions to the `extra.invocation_params` field of the run rather than inputs. |
| `remove_extra_fields` | `Object` | Removes any field not defined in the schema for this target object. |

## ​ Chat Model prebuilt schema

The main use case for transformations is to simplify collecting production traces into datasets in a format that can be standardized across model providers for usage in evaluations / few shot prompting / etc downstream.To simplify setup of transformations for our end users, LangSmith offers a pre-defined schema that will do the following:

- Extract messages from your collected runs and transform them into the openai standard format, which makes them compatible all LangChain ChatModels and most model providers’ SDK for downstream evaluation and experimentation
- Extract any tools used by your LLM and add them to your example’s input to be used for reproducability in downstream evaluation

Users who want to iterate on their system prompts often also add the Remove System Messages transformation on their input messages when using our Chat Model schema, which will prevent you from saving the system prompt to your dataset.

### ​ Compatibility

The LLM run collection schema is built to collect data from LangChain `BaseChatModel` runs or traced runs from the LangSmith OpenAI wrapper.Please contact support via support.langchain.com if you have an LLM run you are tracing that is not compatible and we can extend support.If you want to apply transformations to other sorts of runs (for example, representing LangGraph state with message history), please define your schema directly and manually add the relevant transformations.

### ​ Enablement

When adding a run from a tracing project or annotation queue to a dataset, if it has the LLM run type, we will apply the Chat Model schema by default.For enablement on new datasets, see our dataset management how-to guide.

### ​ Specs

For the full API specs of the prebuilt schema, see the below sections:

#### ​ Input schema

Copy

{
"type": "object",
"properties": {
"messages": {
"type": "array",
"items": {
"$ref": "https://api.smith.langchain.com/public/schemas/v1/message.json"
}
},
"tools": {
"type": "array",
"items": {
"$ref": "https://api.smith.langchain.com/public/schemas/v1/tooldef.json"
}
}
},
"required": ["messages"]
}

#### ​ Output schema

{
"type": "object",
"properties": {
"message": {
"$ref": "https://api.smith.langchain.com/public/schemas/v1/message.json"
}
},
"required": ["message"]
}

#### ​ Transformations

And the transformations look as follows:

[\
{\
"path": ["inputs"],\
"transformation_type": "remove_extra_fields"\
},\
{\
"path": ["inputs", "messages"],\
"transformation_type": "convert_to_openai_message"\
},\
{\
"path": ["inputs", "tools"],\
"transformation_type": "convert_to_openai_tool"\
},\
{\
"path": ["outputs"],\
"transformation_type": "remove_extra_fields"\
},\
{\
"path": ["outputs", "message"],\
"transformation_type": "convert_to_openai_message"\
}\
]

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Dataset prebuilt JSON schema types\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/llm-as-judge

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Evaluation types

How to define an LLM-as-a-judge evaluator

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Evaluation types
- Code evaluator
- LLM-as-a-judge evaluator
- Composite evaluators
- Summary evaluator
- Pairwise evaluation
- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- SDK
- Pre-built evaluators
- Create your own LLM-as-a-judge evaluator
- UI
- Pre-built evaluators
- Customize your LLM-as-a-judge evaluator
- Select/create the evaluator
- Configure the evaluator
- Prompt
- Model
- Mapping variables
- Preview
- Improve your evaluator with few-shot examples
- Feedback configuration
- Save the evaluator

- LLM-as-a-judge evaluator

LLM applications can be challenging to evaluate since they often generate conversational text with no single correct answer.This guide shows you how to define an LLM-as-a-judge evaluator for offline evaluation using either the LangSmith SDK or the UI. Note: To run evaluations in real-time on your production traces, refer to setting up online evaluations.

## ​ SDK

### ​ Pre-built evaluators

Pre-built evaluators are a useful starting point for setting up evaluations. Refer to pre-built evaluators for how to use pre-built evaluators with LangSmith.

### ​ Create your own LLM-as-a-judge evaluator

Copy

from langsmith import evaluate, traceable, wrappers, Client
from openai import OpenAI
# Assumes you've installed pydantic
from pydantic import BaseModel

# Optionally wrap the OpenAI client to trace all model calls.
oai_client = wrappers.wrap_openai(OpenAI())

"""Use an LLM to judge if the reasoning and the answer are consistent."""
instructions = """
Given the following question, answer, and reasoning, determine if the reasoning
for the answer is logically valid and consistent with the question and the answer."""

class Response(BaseModel):
reasoning_is_valid: bool

msg = f"Question: {inputs['question']}\nAnswer: {outputs['answer']}\nReasoning: {outputs['reasoning']}"
response = oai_client.beta.chat.completions.parse(
model="gpt-4o",
messages=[{"role": "system", "content": instructions,}, {"role": "user", "content": msg}],
response_format=Response
)
return response.choices[0].message.parsed.reasoning_is_valid

# Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.
@traceable

return {"answer": "hmm i'm not sure", "reasoning": "i didn't understand the question"}

ls_client = Client()
dataset = ls_client.create_dataset("big questions")
examples = [\
{"inputs": {"question": "how will the universe end"}},\
{"inputs": {"question": "are we alone"}},\
]
ls_client.create_examples(dataset_id=dataset.id, examples=examples)

results = evaluate(
dummy_app,
data=dataset,
evaluators=[valid_reasoning]
)

See here for more on how to write a custom evaluator.

## ​ UI

### ​ Pre-built evaluators

Pre-built evaluators are a useful starting point when setting up evaluations. The LangSmith UI supports the following pre-built evaluators:

- **Hallucination**: Detect factually incorrect outputs. Requires a reference output.
- **Correctness**: Check semantic similarity to a reference.
- **Conciseness**: Evaluate whether an answer is a concise response to a question.
- **Code checker**: Verify correctness of code answers.

You can configure these evaluators::

- When running an evaluation using the playground
- As part of a dataset to automatically run evaluations on experiments
- When running an online evaluation

## ​ Customize your LLM-as-a-judge evaluator

Add specific instructions for your LLM-as-a-judge evalutor prompt and configure which parts of the input/output/reference output should be passed to the evaluator.

### ​ Select/create the evaluator

- In the playground or from a dataset: Select the **+Evaluator** button
- From a tracing project: Select **Add rules**, configure your rule and select **Apply evaluator**

Select the **Create your own evaluator option**. Alternatively, you may start by selecting a pre-built evaluator and editing it.

#### ​ Prompt

Create a new prompt, or choose an existing prompt from the prompt hub.

- **Create your own prompt**: Create a custom prompt inline.
- **Pull a prompt from the prompt hub**: Use the **Select a prompt** dropdown to select from an existing prompt. You can’t edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses. To make changes, edit the prompt in the playground and commit the version, and then pull in your new prompt in the evaluator.

#### ​ Model

Select the desired model from the provided options.

#### ​ Mapping variables

Use variable mapping to indicate the variables that are passed into your evaluator prompt from your run or example. To aid with variable mapping, an example (or run) is provided for reference. Click on the the variables in your prompt and use the dropdown to map them to the relevant parts of the input, output, or reference output.To add prompt variables type the variable with double curly brackets `{{prompt_var}}` if using mustache formatting (the default) or single curly brackets `{prompt_var}` if using f-string formatting.You may remove variables as needed. For example if you are evaluating a metric such as conciseness, you typically don’t need a reference output so you may remove that variable.

#### ​ Preview

Previewing the prompt will show you of what the formatted prompt will look like using the reference run and dataset example shown on the right.

#### ​ Improve your evaluator with few-shot examples

To better align the LLM-as-a-judge evaluator to human preferences, LangSmith allows you to collect human corrections on evaluator scores. With this selection enabled, corrections are then inserted automatically as few-shot examples into your prompt.Learn how to set up few-shot examples and make corrections.

#### ​ Feedback configuration

Feedback configuration is the scoring criteria that your LLM-as-a-judge evaluator will use. Think of this as the rubric that your evaluator will grade based on. Scores will be added as feedback to a run or example. Defining feedback for your evaluator:

1. **Name the feedback key**: This is the name that will appear when viewing evaluation results. Names should be unique across experiments.
2. **Add a description**: Describe what the feedback represents.
3. **Choose a feedback type**:

- **Boolean**: True/false feedback.
- **Categorical**: Select from predefined categories.
- **Continuous**: Numerical scoring within a specified range.

Behind the scenes, feedback configuration is added as structured output to the LLM-as-a-judge prompt. If you’re using an existing prompt from the hub, you must add an output schema to the prompt before configuring an evaluator to use it. Each top-level key in the output schema will be treated as a separate piece of feedback.

### ​ Save the evaluator

Once your are finished configuring, save your changes.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to define a code evaluator\\
\\
Previous Composite evaluators\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluate-pairwise

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Evaluation types

How to run a pairwise evaluation

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Evaluation types
- Code evaluator
- LLM-as-a-judge evaluator
- Composite evaluators
- Summary evaluator
- Pairwise evaluation
- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Prerequisites
- evaluate() comparative args
- Define a pairwise evaluator
- Evaluator args
- Evaluator output
- Run a pairwise evaluation
- View pairwise experiments

Concept: Pairwise evaluations

LangSmith supports evaluating **existing** experiments in a comparative manner. Instead of evaluating one output at a time, you can score the output from multiple experiments against each other. In this guide, you’ll use `evaluate()` with two existing experiments to define an evaluator and run a pairwise evaluation. Finally, you’ll use the LangSmith UI to view the pairwise experiments.

## ​ Prerequisites

- If you haven’t already created experiments to compare, check out the quick start or the how-to guide to get started with evaluations.

You can also use `evaluate_comparative()` with more than two existing experiments.

## ​ `evaluate()` comparative args

At its simplest, `evaluate` / `aevaluate` function takes the following arguments:

| Argument | Description |
| --- | --- |
| `target` | A list of the two **existing experiments** you would like to evaluate against each other. These can be uuids or experiment names. |
| `evaluators` | A list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. |

Along with these, you can also pass in the following optional args:

| Argument | Description |
| --- | --- |
| `randomize_order` / `randomizeOrder` | An optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False. |
| `experiment_prefix` / `experimentPrefix` | A prefix to be attached to the beginning of the pairwise experiment name. Defaults to None. |
| `description` | A description of the pairwise experiment. Defaults to None. |
| `max_concurrency` / `maxConcurrency` | The maximum number of concurrent evaluations to run. Defaults to 5. |
| `client` | The LangSmith client to use. Defaults to None. |
| `metadata` | Metadata to attach to your pairwise experiment. Defaults to None. |
| `load_nested` / `loadNested` | Whether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False. |

## ​ Define a pairwise evaluator

Pairwise evaluators are just functions with an expected signature.

### ​ Evaluator args

Custom evaluator functions must have specific argument names. They can take any subset of the following arguments:

- `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
- `outputs: list[dict]`: A two-item list of the dict outputs produced by each experiment on the given inputs.
- `reference_outputs` / `referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.
- `runs: list[Run]`: A two-item list of the full Run objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.
- `example: Example`: The full dataset Example, including the example inputs, outputs (if available), and metadata (if available).

For most use cases you’ll only need `inputs`, `outputs`, and `reference_outputs` / `referenceOutputs`. `runs` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.

### ​ Evaluator output

Custom evaluators are expected to return one of the following types:Python and JS/TS

- `dict`: dictionary with keys: - `key`, which represents the feedback key that will be logged
- `scores`, which is a mapping from run ID to score for that run.
- `comment`, which is a string. Most commonly used for model reasoning.

Currently Python only

- `list[int | float | bool]`: a two-item list of scores. The list is assumed to have the same order as the `runs` / `outputs` evaluator args. The evaluator function name is used for the feedback key.

Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with `pairwise_` or `ranked_`.

## ​ Run a pairwise evaluation

The following example uses a prompt which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI’s response: 0, 1, or 2.

In the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain chat model wrapper.**Usage of LangChain is totally optional.** To illustrate this point, the TypeScript example uses the OpenAI SDK directly.

Python

TypeScript

Copy

from langchain_classic import hub
from langchain.chat_models import init_chat_model
from langsmith import evaluate

# See the prompt:
prompt = hub.pull("langchain-ai/pairwise-evaluation-2")
model = init_chat_model("gpt-4o")
chain = prompt | model

# Assumes example inputs have a 'question' key and experiment
# outputs have an 'answer' key.
response = chain.invoke({
"question": inputs["question"],
"answer_a": outputs[0].get("answer", "N/A"),
"answer_b": outputs[1].get("answer", "N/A"),
})
if response["Preference"] == 1:
scores = [1, 0]
elif response["Preference"] == 2:
scores = [0, 1]
else:
scores = [0, 0]
return scores

evaluate(
("experiment-1", "experiment-2"), # Replace with the names/IDs of your experiments
evaluators=[ranked_preference],
randomize_order=True,
max_concurrency=4,
)

## ​ View pairwise experiments

Navigate to the “Pairwise Experiments” tab from the dataset page:!Pairwise Experiments TabClick on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View:!Pairwise Comparison ViewYou may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:!Pairwise Filtering

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to define a summary evaluator\\
\\
Previous How to run an evaluation asynchronously\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/experiment-configuration

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Evaluation techniques

Experiment configuration

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Experiment configuration
- Define a target function to evaluate
- Evaluate intermediate steps
- Return multiple scores in one evaluator
- Return categorical vs numerical metrics
- Run evaluators on experiments
- Evaluate with repetitions
- Handle model rate limits
- Run an evaluation locally (Python)
- Read experiment results locally
- Evaluate a runnable
- Evaluate a graph
- Evaluate an existing experiment (Python)
- Run an evaluation with multimodal content
- Simulate multi-turn interactions
- Evaluate agent trajectories
- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Repetitions
- Concurrency
- evaluate
- aevaluate
- Caching

LangSmith supports several configuration options for experiments:

- Repetitions
- Concurrency
- Caching

### ​ Repetitions

_Repetitions_ run an experiment multiple times to account for LLM output variability. Since LLM outputs are non-deterministic, multiple repetitions provide a more accurate performance estimate.Configure repetitions by passing the `num_repetitions` argument to `evaluate` / `aevaluate` ( Python, TypeScript). Each repetition re-runs both the target function and all evaluators.Learn more in the repetitions how-to guide.

### ​ Concurrency

_Concurrency_ controls how many examples run simultaneously during an experiment. Configure it by passing the `max_concurrency` argument to `evaluate` / `aevaluate`. The semantics differ between the two functions:

#### ​ `evaluate`

The `max_concurrency` argument specifies the maximum number of concurrent threads for running both the target function and evaluators.

#### ​ `aevaluate`

The `max_concurrency` argument uses a semaphore to limit concurrent tasks. `aevaluate` creates a task for each example, where each task runs the target function and all evaluators for that example. The `max_concurrency` argument specifies the maximum number of concurrent examples to process.

### ​ Caching

_Caching_ stores API call results to disk to speed up future experiments. Set the `LANGSMITH_TEST_CACHE` environment variable to a valid folder path with write access. Future experiments that make identical API calls will reuse cached results instead of making new requests.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to use the REST API\\
\\
Previous How to define a target function to evaluate\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluation-types

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Evaluation types

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Evaluation types
- Code evaluator
- LLM-as-a-judge evaluator
- Composite evaluators
- Summary evaluator
- Pairwise evaluation
- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Offline evaluation types
- Benchmarking
- Unit tests
- Regression tests
- Backtesting
- Pairwise evaluation
- Online evaluation types
- Real-time monitoring
- Anomaly detection
- Production feedback loop

LangSmith supports various evaluation types for different stages of development and deployment. Understanding when to use each helps build a comprehensive evaluation strategy.

## ​ Offline evaluation types

Offline evaluation tests applications on curated datasets before deployment. By running evaluations on examples with reference outputs, teams can compare versions, validate functionality, and build confidence before exposing changes to users.Run offline evaluations client-side using the LangSmith SDK ( Python and TypeScript) or server-side via the Prompt Playground or automations.!Offline

### ​ Benchmarking

_Benchmarking_ compares multiple application versions on a curated dataset to identify the best performer. This process involves creating a dataset of representative inputs, defining performance metrics, and testing each version.Benchmarking requires dataset curation with gold-standard reference outputs and well-designed comparison metrics. Examples:

- **RAG Q&A bot**: Dataset of questions and reference answers, with an LLM-as-judge evaluator checking semantic equivalence between actual and reference answers.
- **ReACT agent**: Dataset of user requests and reference tool calls, with a heuristic evaluator verifying all expected tool calls were made.

### ​ Unit tests

_Unit tests_ verify the correctness of individual system components. In LLM contexts, unit tests are often rule-based assertions on inputs or outputs (e.g., verifying LLM-generated code compiles, JSON loads successfully) that validate basic functionality.Unit tests typically expect consistent passing results, making them suitable for CI pipelines. When running in CI, configure caching to minimize LLM API calls and associated costs.

### ​ Regression tests

_Regression tests_ measure performance consistency across application versions over time. They ensure new versions do not degrade performance on cases the current version handles correctly, and ideally demonstrate improvements over the baseline. These tests typically run when making updates expected to affect user experience (e.g., model or architecture changes).LangSmith’s comparison view highlights regressions (red) and improvements (green) relative to the baseline, enabling quick identification of changes.!Comparison view

### ​ Backtesting

_Backtesting_ evaluates new application versions against historical production data. Production logs are converted into a dataset, then newer versions process these examples to assess performance on past, realistic user inputs.This approach is commonly used for evaluating new model releases. For example, when a new model becomes available, test it on the most recent production runs and compare results to actual production outcomes.

### ​ Pairwise evaluation

_Pairwise evaluation_ compares outputs from two versions by determining relative quality rather than assigning absolute scores. For some tasks, determining “version A is better than B” is easier than scoring each version independently.This approach proves particularly useful for LLM-as-judge evaluations on subjective tasks. For example, in summarization, determining “Which summary is clearer and more concise?” is often simpler than assigning numeric clarity scores.Learn how run pairwise evaluations.

## ​ Online evaluation types

Online evaluation assesses production application outputs in near real-time. Without reference outputs, these evaluations focus on detecting issues, monitoring quality trends, and identifying edge cases that inform future offline testing.Online evaluators typically run server-side. LangSmith provides built-in LLM-as-judge evaluators for configuration, and supports custom code evaluators that run within LangSmith.!Online

### ​ Real-time monitoring

Monitor application quality continuously as users interact with the system. Online evaluations run automatically on production traffic, providing immediate feedback on each interaction. This enables detection of quality degradation, unusual patterns, or unexpected behaviors before they impact significant user populations.

### ​ Anomaly detection

Identify outliers and edge cases that deviate from expected patterns. Online evaluators can flag runs with unusual characteristics—extremely long or short responses, unexpected error rates, or outputs that fail safety checks—for human review and potential addition to offline datasets.

### ​ Production feedback loop

Use insights from production to improve offline evaluation. Online evaluations surface real-world issues and usage patterns that may not appear in curated datasets. Failed production runs become candidates for dataset examples, creating an iterative cycle where production experience continuously refines testing coverage.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to use prebuilt evaluators\\
\\
Previous How to define a code evaluator\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluate-llm-application

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Run an evaluation

How to evaluate an LLM application

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- With the SDK
- With the UI
- Use prebuilt evaluators
- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Define an application
- Create or select a dataset
- Define an evaluator
- Run the evaluation
- Explore the results​
- Reference code​
- Related​

This guide shows you how to run an evaluation on an LLM application using the LangSmith SDK.

Evaluations \| Evaluators \| Datasets

In this guide we’ll go over how to evaluate an application using the evaluate() method in the LangSmith SDK.

For larger evaluation jobs in Python we recommend using aevaluate(), the asynchronous version of evaluate(). It is still worthwhile to read this guide first, as the two have identical interfaces, before reading the how-to guide on running an evaluation asynchronously.In JS/TS evaluate() is already asynchronous so no separate method is needed.It is also important to configure the `max_concurrency`/`maxConcurrency` arg when running large jobs. This parallelizes evaluation by effectively splitting the dataset across threads.

## ​ Define an application

First we need an application to evaluate. Let’s create a simple toxicity classifier for this example.

Python

TypeScript

Copy

from langsmith import traceable, wrappers
from openai import OpenAI

# Optionally wrap the OpenAI client to trace all model calls.
oai_client = wrappers.wrap_openai(OpenAI())

# Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.
@traceable

instructions = (
"Please review the user query below and determine if it contains any form of toxic behavior, "
"such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does "
"and 'Not toxic' if it doesn't."
)
messages = [\
{"role": "system", "content": instructions},\
{"role": "user", "content": inputs["text"]},\
]
result = oai_client.chat.completions.create(
messages=messages, model="gpt-4o-mini", temperature=0
)
return {"class": result.choices[0].message.content}

We’ve optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to this guide.

## ​ Create or select a dataset

from langsmith import Client
ls_client = Client()

examples = [\
{\
"inputs": {"text": "Shut up, idiot"},\
"outputs": {"label": "Toxic"},\
},\
{\
"inputs": {"text": "You're a wonderful person"},\
"outputs": {"label": "Not toxic"},\
},\
{\
"inputs": {"text": "This is the worst thing ever"},\
"outputs": {"label": "Toxic"},\
},\
{\
"inputs": {"text": "I had a great day today"},\
"outputs": {"label": "Not toxic"},\
},\
{\
"inputs": {"text": "Nobody likes you"},\
"outputs": {"label": "Toxic"},\
},\
{\
"inputs": {"text": "This is unacceptable. I want to speak to the manager."},\
"outputs": {"label": "Not toxic"},\
},\
]

dataset = ls_client.create_dataset(dataset_name="Toxic Queries")
ls_client.create_examples(
dataset_id=dataset.id,
examples=examples,
)

For more details on datasets, refer to the Manage datasets page.

## ​ Define an evaluator

You can also check out LangChain’s open source evaluation package openevals for common pre-built evaluators.

Evaluators are functions for scoring your application’s outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.

return outputs["class"] == reference_outputs["label"]

## ​ Run the evaluation

We’ll use the evaluate() / aevaluate() methods to run the evaluation.The key arguments are:

- a target function that takes an input dictionary and returns an output dictionary. The `example.inputs` field of each Example is what gets passed to the target function. In this case our `toxicity_classifier` is already set up to take in example inputs so we can use it directly.
- `data` \- the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examples
- `evaluators` \- a list of evaluators to score the outputs of the function

# Can equivalently use the 'evaluate' function directly:
# from langsmith import evaluate; evaluate(...)
results = ls_client.evaluate(
toxicity_classifier,
data=dataset.name,
evaluators=[correct],
experiment_prefix="gpt-4o-mini, baseline", # optional, experiment name prefix
description="Testing the baseline system.", # optional, experiment description
max_concurrency=4, # optional, add concurrency
)

## ​ Explore the results ​

Each invocation of `evaluate()` creates an Experiment which can be viewed in the LangSmith UI or queried via the SDK. Evaluation scores are stored against each actual output as feedback._If you’ve annotated your code for tracing, you can open the trace of each row in a side panel view._!View experiment

## ​ Reference code ​

Click to see a consolidated code snippet

from langsmith import Client, traceable, wrappers
from openai import OpenAI

# Step 1. Define an application

system = (
"Please review the user query below and determine if it contains any form of toxic behavior, "
"such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does "
"and 'Not toxic' if it doesn't."
)
messages = [\
{"role": "system", "content": system},\
{"role": "user", "content": inputs["text"]},\
]
result = oai_client.chat.completions.create(
messages=messages, model="gpt-4o-mini", temperature=0
)
return result.choices[0].message.content

# Step 2. Create a dataset
ls_client = Client()
dataset = ls_client.create_dataset(dataset_name="Toxic Queries")
examples = [\
{\
"inputs": {"text": "Shut up, idiot"},\
"outputs": {"label": "Toxic"},\
},\
{\
"inputs": {"text": "You're a wonderful person"},\
"outputs": {"label": "Not toxic"},\
},\
{\
"inputs": {"text": "This is the worst thing ever"},\
"outputs": {"label": "Toxic"},\
},\
{\
"inputs": {"text": "I had a great day today"},\
"outputs": {"label": "Not toxic"},\
},\
{\
"inputs": {"text": "Nobody likes you"},\
"outputs": {"label": "Toxic"},\
},\
{\
"inputs": {"text": "This is unacceptable. I want to speak to the manager."},\
"outputs": {"label": "Not toxic"},\
},\
]
ls_client.create_examples(
dataset_id=dataset.id,
examples=examples,
)

# Step 3. Define an evaluator

return outputs["output"] == reference_outputs["label"]

# Step 4. Run the evaluation
# Client.evaluate() and evaluate() behave the same.
results = ls_client.evaluate(
toxicity_classifier,
data=dataset.name,
evaluators=[correct],
experiment_prefix="gpt-4o-mini, simple", # optional, experiment name prefix
description="Testing the baseline system.", # optional, experiment description
max_concurrency=4, # optional, add concurrency
)

## ​ Related ​

- Run an evaluation asynchronously
- Run an evaluation via the REST API
- Run an evaluation from the prompt playground

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Custom output rendering\\
\\
Previous Run an evaluation from the prompt playground\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Tutorials

Evaluate a chatbot

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

- Evaluate a chatbot
- Evaluate a RAG application
- Test a ReAct agent with Pytest/Vitest and LangSmith
- Evaluate a complex agent
- Run backtests on a new version of an agent

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Setup
- Create a dataset
- Define metrics
- Run Evaluations
- Comparing results
- Set up automated testing to run in CI/CD
- Track results over time
- Conclusion
- Reference code

In this guide we will set up evaluations for a chatbot. These allow you to measure how well your application is performing over a set of data. Being able to get this insight quickly and reliably will allow you to iterate with confidence.At a high level, in this tutorial we will:

- _Create an initial golden dataset to measure performance_
- _Define metrics to use to measure performance_
- _Run evaluations on a few different prompts or models_
- _Compare results manually_
- _Track results over time_
- _Set up automated testing to run in CI/CD_

For more information on the evaluation workflows LangSmith supports, check out the how-to guides, or see the reference docs for evaluate and its asynchronous aevaluate counterpart.Lots to cover, let’s dive in!

## ​ Setup

First install the required dependencies for this tutorial. We happen to use OpenAI, but LangSmith can be used with any model:

pip

uv

Copy

pip install -U langsmith openai

And set environment variables to enable LangSmith tracing:

export LANGSMITH_TRACING="true"

## ​ Create a dataset

The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:

- What should the schema of each datapoint be?
- How many datapoints should I gather?
- How should I gather those datapoints?

**Schema:** Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that’s okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.**How many:** There’s no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don’t worry about getting a large number to start - you can (and should) always add over time!**How to get:** This is maybe the trickiest part. Once you know you want to gather a dataset… how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally _living_ constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling ~10-20 examples.Once you’ve got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).For this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let’s show how to create and upload this dataset to LangSmith!

from langsmith import Client

client = Client()

# Define dataset: these are your test cases
dataset_name = "QA Example Dataset"
dataset = client.create_dataset(dataset_name)

client.create_examples(
dataset_id=dataset.id,
examples=[\
{\
"inputs": {"question": "What is LangChain?"},\
"outputs": {"answer": "A framework for building LLM applications"},\
},\
{\
"inputs": {"question": "What is LangSmith?"},\
"outputs": {"answer": "A platform for observing and evaluating LLM applications"},\
},\
{\
"inputs": {"question": "What is OpenAI?"},\
"outputs": {"answer": "A company that creates Large Language Models"},\
},\
{\
"inputs": {"question": "What is Google?"},\
"outputs": {"answer": "A technology company known for search"},\
},\
{\
"inputs": {"question": "What is Mistral?"},\
"outputs": {"answer": "A company that creates Large Language Models"},\
}\
]
)

Now, if we go the LangSmith UI and look for `QA Example Dataset` in the `Datasets & Testing` page, when we click into it we should see that we have five new examples.!Testing tutorial dataset

## ​ Define metrics

After creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those **exact** answers, but rather something that is similar. This makes our evaluation a little trickier.In addition to evaluating correctness, let’s also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response.Let’s go ahead and define these two metrics.For the first, we will use an LLM to **judge** whether the output is correct (with respect to the expected output). This **LLM-as-a-judge** is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:

import openai
from langsmith import wrappers

openai_client = wrappers.wrap_openai(openai.OpenAI())

eval_instructions = "You are an expert professor specialized in grading students' answers to questions."

user_content = f"""You are grading the following question:
{inputs['question']}
Here is the real answer:
{reference_outputs['answer']}
You are grading the following predicted answer:
{outputs['response']}
Respond with CORRECT or INCORRECT:
Grade:"""
response = openai_client.chat.completions.create(
model="gpt-4o-mini",
temperature=0,
messages=[\
{"role": "system", "content": eval_instructions},\
{"role": "user", "content": user_content},\
],
).choices[0].message.content
return response == "CORRECT"

For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.

return int(len(outputs["response"]) < 2 * len(reference_outputs["answer"]))

## ​ Run Evaluations

Great! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:

default_instructions = "Respond to the users question in a short, concise manner (one short sentence)."

return openai_client.chat.completions.create(
model=model,
temperature=0,
messages=[\
{"role": "system", "content": instructions},\
{"role": "user", "content": question},\
],
).choices[0].message.content

Before running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect.

return {"response": my_app(inputs["question"])}

Great! Now we’re ready to run an evaluation. Let’s do it!

experiment_results = client.evaluate(
ls_target, # Your AI system
data=dataset_name, # The data to predict and grade over
evaluators=[concision, correctness], # The evaluators to score the results
experiment_prefix="openai-4o-mini", # A prefix for your experiment names to easily identify them
)

This will output a URL. If we click on it, we should see results of our evaluation!!Testing tutorial runIf we go Let’s now try it out with a different model! Let’s try `gpt-4-turbo`

return {"response": my_app(inputs["question"], model="gpt-4-turbo")}

experiment_results = client.evaluate(
ls_target_v2,
data=dataset_name,
evaluators=[concision, correctness],
experiment_prefix="openai-4-turbo",
)

And now let’s use GPT-4 but also update the prompt to be a bit more strict in requiring the answer to be short.

instructions_v3 = "Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words."

response = my_app(
inputs["question"],
model="gpt-4-turbo",
instructions=instructions_v3
)
return {"response": response}

experiment_results = client.evaluate(
ls_target_v3,
data=dataset_name,
evaluators=[concision, correctness],
experiment_prefix="strict-openai-4-turbo",
)

If we go

## ​ Comparing results

Awesome, we’ve evaluated three different runs. But how can we compare results? The first way we can do this is just by looking at the runs in the `Experiments` tab. If we do that, we can see a high level view of the metrics for each run:!Testing tutorial compare metricsGreat! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length. But what if we want to explore in more detail?In order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view. We immediately see all three tests side by side. Some of the cells are color coded - this is showing a regression of _a certain metric_ compared to _a certain baseline_. We automatically choose defaults for the baseline and metric, but you can change those yourself. You can also choose which columns and which metrics you see by using the `Display` control. You can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top.!Testing tutorial compare runsIf we want to see more information, we can also select the `Expand` button that appears when hovering over a row to open up a side panel with more detailed information:!Testing tutorial side panel

## ​ Set up automated testing to run in CI/CD

Now that we’ve run this in a one-off manner, we can set it to run in an automated fashion. We can do this pretty easily by just including it as a pytest file that we run in CI/CD. As part of this, we can either just log the results OR set up some criteria to determine if it passes or not. For example, if I wanted to ensure that we always got at least 80% of generated responses passing the `length` check, we could set that up with a test like:

"""Test that the length score is at least 80%."""
experiment_results = evaluate(
ls_target, # Your AI system
data=dataset_name, # The data to predict and grade over
evaluators=[concision, correctness], # The evaluators to score the results
)
# This will be cleaned up in the next release:
feedback = client.list_feedback(
run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],
feedback_key="concision"
)
scores = [f.score for f in feedback]

## ​ Track results over time

Now that we’ve got these experiments running in an automated fashion, we want to track these results over time. We can do this from the overall `Experiments` tab in the datasets page. By default, we show evaluation metrics over time (highlighted in red). We also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow).!Testing tutorial over time

## ​ Conclusion

That’s it for this tutorial!We’ve gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time. Hopefully this can help you iterate with confidence.This is just the start. As mentioned earlier, evaluation is an ongoing process. For example - the datapoints you will want to evaluate on will likely continue to change over time. There are many types of evaluators you may wish to explore. For information on this, check out the how-to guides.Additionally, there are other ways to evaluate data besides in this “offline” manner (e.g. you can evaluate production data). For more information on online evaluation, check out this guide.

## ​ Reference code

Click to see a consolidated code snippet

import openai
from langsmith import Client, wrappers

# Application code

# Define evaluators

# Run evaluations

experiment_results_v1 = client.evaluate(
ls_target, # Your AI system
data=dataset_name, # The data to predict and grade over
evaluators=[concision, correctness], # The evaluators to score the results
experiment_prefix="openai-4o-mini", # A prefix for your experiment names to easily identify them
)

experiment_results_v2 = client.evaluate(
ls_target_v2,
data=dataset_name,
evaluators=[concision, correctness],
experiment_prefix="openai-4-turbo",
)

experiment_results_v3 = client.evaluate(
ls_target_v3,
data=dataset_name,
evaluators=[concision, correctness],
experiment_prefix="strict-openai-4-turbo",
)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Dynamic few shot example selection\\
\\
Previous Evaluate a RAG application\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluation-quickstart)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Evaluation quickstart LangSmith Evaluation Prompt engineering quickstart

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluation-concepts)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Evaluation Evaluation concepts LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluation-approaches)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Evaluation LangSmith docs Implement a CI/CD pipeline using LangSmith Deployment and Evaluation

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/manage-datasets)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Manage datasets Create and manage datasets in the UI LangSmith data plane

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-output-rendering)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Custom output rendering Log LLM calls LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/analyze-an-experiment)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Analyze an experiment LangSmith Evaluation LangSmith Polly

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/compare-experiment-results)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

How to compare experiment results Analyze an experiment How to upload experiments run outside of LangSmith with the REST API

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/filter-experiments-ui)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

How to filter experiments in the UI How to upload experiments run outside of LangSmith with the REST API Self-hosted LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/fetch-perf-metrics-experiment)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Fetch How to fetch performance metrics for an experiment LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/upload-existing-experiments)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

How to upload experiments run outside of LangSmith with the REST API LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/annotation-queues)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Use annotation queues LangSmith Observability Annotate traces and runs inline

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/set-up-feedback-criteria)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Set up feedback criteria Feedback data format LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/annotate-traces-inline)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Annotate traces and runs inline Use annotation queues LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/audit-evaluator-scores)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

How to audit evaluator scores How to return multiple scores in one evaluator How to evaluate an LLM application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/example-data-format)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Example data format LangSmith data plane LangSmith Fetch

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/dataset-json-types)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Dataset prebuilt JSON schema types Dataset transformations LangSmith Fetch

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/dataset-transformations)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Dataset transformations Manage datasets LangSmith data plane

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/llm-as-judge)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

How to define an LLM-as-a-judge evaluator LangSmith docs How to evaluate your agent with trajectory evaluations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluate-pairwise)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

How to run a pairwise evaluation LangSmith Evaluation LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/experiment-configuration)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Experiment configuration Configure LangSmith Agent Server for scale Configure LangSmith for scale

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/manage-datasets),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Manage datasets Create and manage datasets in the UI LangSmith data plane

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluate-llm-application)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

How to evaluate an LLM application LangSmith docs Evaluate a RAG application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Evaluate a chatbot Evaluate a complex agent LangSmith Evaluation

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/create-account-api-key

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Create an account and API key

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- API keys
- Create an API key
- Delete an API key
- Configure the SDK
- Using API keys outside of the SDK

To get started with LangSmith, you need to create an account. You can sign up for a free account in the LangSmith UI. LangSmith supports sign in with Google, GitHub, and email.!Create account

## ​ API keys

LangSmith supports two types of API keys: Service Keys and Personal Access Tokens. Both types of tokens can be used to authenticate requests to the LangSmith API, but they have different use cases.For more details on Service Keys and Personal Access Tokens, refer to the Administration overview page.

## ​ Create an API key

To log traces and run evaluations with LangSmith, you will need to create an API key to authenticate your requests. API keys can be scoped to a set of workspaces, or the entire organization.To create either type of API key:

1. Navigate to the Settings page and scroll to the **API Keys** section.
2. For service keys, choose between an organization-scoped and workspace-scoped key. If the key is workspace-scoped, the workspaces must then be specified.Enterprise users are also able to assign specific roles to the key, which adjusts its permissions.
3. Set the key’s expiration; the key will become unusable after the number of days chosen, or never, if that is selected.
4. Click **Create API Key.**

The API key will be shown only once, so make sure to copy it and store it in a safe place.

## ​ Delete an API key

To delete an API key:

1. Navigate to the Settings page and scroll to the **API Keys** section.
2. Find the API key you need to delete from the table. Toggle **Personal** or **Service** as needed.
3. Select the trash icon in the **Actions** column and confirm deletion.

## ​ Configure the SDK

## ​ Using API keys outside of the SDK

See instructions for managing your organization via API.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Pricing plans\\
\\
Previous Overview\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/administration-overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Account administration

Overview

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- Resource Hierarchy
- Organizations
- Workspaces
- Applications
- Additional info
- Resource tags
- User Management and RBAC
- Users
- API keys
- Expiration Dates
- Personal Access Tokens (PATs)
- Service keys
- Organization roles
- Workspace roles (RBAC)
- Best Practices
- Environment Separation
- Usage and Billing
- Data Retention
- Why retention matters
- How it works
- Billing model
- Rate Limits
- Temporary throughput limit over a 1 minute period at our application load balancer
- Plan-level hourly trace event limit
- Plan-level hourly trace data ingest limit
- Plan-level monthly unique traces limit
- Self-configured monthly usage limits
- Handling 429s responses in your application
- Usage Limits
- Properties of usage limiting
- Side effects of extended data retention traces limit
- Updating usage limits
- Related content
- Additional Resources

This overview covers topics related to managing users, organizations, workspaces, and applications within LangSmith.

## ​ Resource Hierarchy

### ​ Organizations

An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide.When you log in for the first time, a personal organization will be created for you automatically. If you’d like to collaborate with others, you can create a separate organization and invite your team members to join. There are a few important differences between your personal organization and shared organizations:

| Feature | Personal | Shared |
| --- | --- | --- |
| Maximum workspaces | 1 | Variable, depending on plan (see pricing page |
| Collaboration | Cannot invite users | Can invite users |
| Billing: paid plans | Developer plan only | All other plans available |

### ​ Workspaces

Workspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition.

A workspace is a logical grouping of users within an organization. A workspace separates trust boundaries for resources and access control. Users may have permissions in a workspace that grant them access to the resources in that workspace, including tracing projects, datasets, annotation queues, and prompts. For details on setup, see the setup guide and for details on permissions see Workspaces (RBAC).It is recommended to create a separate workspace for each team within your organization. To organize resources even further, you can use Applications to group resources within a workspace.The following image shows a sample workspace settings page: !Sample Workspace

### ​ Applications

An application is a logical grouping of resources within a workspace. Applications are often agents, but they can be used for any project within a team. Applications keep the UI organized by only surfacing the resources associated with the application currently in context.Applications are built on top of resource tags and can be used to control resource access using ABAC, currently in private preview.The following image shows how to manage and switch applications in the main navbar: !Sample Application SelectorAny resource can be created without being tagged to an application. These resources will be visible when the `Show all applications` option is selected.

### ​ Additional info

The following diagram explains the relationship between organizations, workspaces, applications, and resources: !Resource HierarchySee the table below for details on which features are available in which scope(s):

| Resource/Setting | Scope |
| --- | --- |
| Trace Projects | Workspace or Application |
| Annotation Queues | Workspace or Application |
| Deployments | Workspace or Application |
| Datasets & Experiments | Workspace or Application |
| Prompts | Workspace or Application |
| Resource Tags | Workspace |
| API Keys | Workspace |
| Settings including Secrets, Feedback config, Models, Rules, and Shared URLs | Workspace |
| User management: Invite User to Workspace | Workspace |
| RBAC: Assigning Workspace Roles | Workspace |
| Data Retention, Usage Limits | Workspace\* |
| Plans and Billing, Credits, Invoices | Organization |
| User management: Invite User to Organization | Organization\*\* |
| Adding Workspaces | Organization |
| Assigning Organization Roles | Organization |
| RBAC: Creating/Editing/Deleting Custom Roles | Organization |

\\* Data retention settings and usage limits will be available soon for the organization level as well \*\* Self-hosted installations may enable workspace-level invites of users to the organization via a feature flag. See the self-hosted user management docs for details.

### ​ Resource tags

Resource tags allow you to further segregate resources within a workspace for use with ABAC. Each tag is a key-value pair that can be assigned to a resource.LangSmith resource tags are very similar to tags in cloud services like AWS.!Sample Resource Tags

## ​ User Management and RBAC

### ​ Users

A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations.Organization members are managed in organization settings:!Sample Organization MembersAnd workspace members are managed in workspace settings:!Sample Workspace Members

### ​ API keys

We ended support for legacy API keys prefixed with `ls__` on October 22, 2024 in favor of personal access tokens (PATs) and service keys. We require using PATs and service keys for all new integrations. API keys prefixed with `ls__` will no longer work as of October 22, 2024.

#### ​ Expiration Dates

When you create an API key, you have the option to set an expiration date. Adding an expiration date to keys enhances security and minimizes the risk of unauthorized access. For example, you may set expiration dates on keys for temporary tasks that require elevated access.By default, keys never expire. Once expired, an API key is no longer valid and cannot be reactivated or have its expiration modified.

#### ​ Personal Access Tokens (PATs)

Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. We recommend not using these to authenticate requests from your application, but rather using them for personal scripts or tools that interact with the LangSmith API. If the user associated with the PAT is removed from the organization, the PAT will no longer work.PATs are prefixed with `lsv2_pt_`

#### ​ Service keys

Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Only admins can create service keys. We recommend using these for applications / services that need to interact with the LangSmith API, such as LangGraph agents or other integrations. Service keys may be scoped to a single workspace, multiple workspaces, or the entire organization, and can be used to authenticate requests to the LangSmith API for whichever workspace(s) it has access to.Service keys are prefixed with `lsv2_sk_`

Use the `X-Tenant-Id` header to specify the target workspace.

- **When using PATs**: If this header is omitted, requests will run against the default workspace associated with the key.
- **When using organization-scoped service keys**: You must include the `X-Tenant-Id` header when accessing workspace-scoped resources. Without it, the request will fail with a `403 Forbidden` error.

To see how to create a service key or Personal Access Token, see the setup guide

### ​ Organization roles

Organization roles are distinct from the Enterprise feature workspace RBAC and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions.The organization role selected also impacts workspace membership as described here:

- Organization Admingrants full access to manage all organization configuration, users, billing, and workspaces.

- An Organization Admin has `Admin` access to all workspaces in an organization.
- Organization User may read organization information but cannot execute any write actions at the organization level. An Organization User may create Personal Access Tokens.

- An Organization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.
- Organization Viewer is equivalent to Organization User, but **cannot** create Personal Access Tokens. (for self-hosted, available in Helm chart version 0.11.25+).

The Organization User and Organization Viewer roles are only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users have the Organization Admin role.See security settings for instructions on how to disable PAT creation for the entire organization.

For more information on setting up organizations and workspaces, refer to the organization setup guide for more information.The following table provdies an overview of organization level permissions:

| | Organization Viewer | Organization User | Organization Admin |
| --- | --- | --- | --- |
| View organization configuration | ✅ | ✅ | ✅ |
| View organization roles | ✅ | ✅ | ✅ |
| View organization members | ✅ | ✅ | ✅ |
| View data retention settings | ✅ | ✅ | ✅ |
| View usage limits | ✅ | ✅ | ✅ |
| Create personal access tokens (PATs) | ❌ | ✅ | ✅ |
| Admin access to all workspaces | ❌ | ❌ | ✅ |
| Manage billing settings | ❌ | ❌ | ✅ |
| Create workspaces | ❌ | ❌ | ✅ |
| Create, edit, and delete organization roles | ❌ | ❌ | ✅ |
| Invite new users to organization | ❌ | ❌ | ✅ |
| Delete user invites | ❌ | ❌ | ✅ |
| Remove users from an organization | ❌ | ❌ | ✅ |
| Update data retention settings | ❌ | ❌ | ✅ |
| Update usage limits | ❌ | ❌ | ✅ |

For a comprehensive list of required permissions along with the operations and roles that can perform them, refer to the Organization and workspace reference.

### ​ Workspace roles (RBAC)

RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, contact our sales team. Other plans default to using the Admin role for all users.

Roles are used to define the set of permissions that a user has within a workspace. There are three built-in system roles that cannot be edited:

- Workspace Admin has full access to all resources within the workspace.
- Workspace Editor has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys).
- Workspace Viewer has read-only access to all resources within the workspace.

Organization admins can also create/edit custom roles with specific permissions for different resources.Roles can be managed in **Organization Settings** under the **Roles** tab:!The Organization members and roles view showing a list of the roles.

- For comprehensive documentation on roles and permissions, refer to the Role-based access control guide.
- For more details on assigning and creating roles, refer to the User Management guide.
- For a comprehensive list of required permissions along with the operations and roles that can perform them, refer to the Organization and workspace reference.

## ​ Best Practices

### ​ Environment Separation

Use resource tags to organize resources by environment using the default tag key `Environment` and different values for the environment (e.g., `dev`, `staging`, `prod`). We do not recommend using separate workspaces for environment separation because resources cannot be shared across workspaces, which would prevent you from promoting resources (like prompts) between environments.

**Resource tags vs. commit tags for prompt management**While both types of tags can use environment terminology like `dev`, `staging`, and `prod`, they serve different purposes:

- **Resource tags** (`Environment: prod`): Use these to _organize and filter_ resources across your workspace. Apply resource tags to tracing projects, datasets, and other resources (including prompts) to group them by environment, which enables filtering in the UI.
- Commit tags (`prod` tag): Use these to manage which prompt version your code references. Commit tags are labels that point to specific commits in a prompt’s history. When your code pulls a prompt by tag name (e.g., `client.pull_prompt("prompt-name:prod")`), it retrieves whichever commit that tag currently points to. To promote a prompt from `staging` to `prod`, move the commit tag to point to the desired version.

Resource tags organize **which resources** belong to an environment. Commit tags let you control **which version** of a prompt your code references without changing the code itself.

## ​ Usage and Billing

### ​ Data Retention

This section covers how data retention works and how it’s priced in LangSmith.

#### ​ Why retention matters

- **Privacy**: Many data privacy regulations, such as GDPR in Europe or CCPA in California, require organizations to delete personal data once it’s no longer necessary for the purposes for which it was collected. Setting retention periods aids in compliance with such regulations.
- **Cost**: LangSmith charges less for traces that have low data retention. See our tutorial on how to optimize spend for details.

#### ​ How it works

LangSmith has two tiers of traces based on Data Retention with the following characteristics:

| | Base | Extended |
| --- | --- | --- |
| **Price** | $.50 / 1k traces | $5 / 1k traces |
| **Retention Period** | 14 days | 400 days |

**Data deletion after retention ends**After the specified retention period, traces are no longer accessible in the tracing project UI or via the API. All user data associated with the trace (e.g. inputs and outputs) is deleted from our internal systems within a day thereafter. Some metadata associated with each trace may be retained indefinitely for analytics and billing purposes.**Data retention auto-upgrades**

Auto upgrades can have an impact on your bill. Please read this section carefully to fully understand your estimated LangSmith tracing costs.

When you use certain features with `base` tier traces, their data retention will be automatically upgraded to `extended` tier. This will increase both the retention period, and the cost of the trace.The complete list of scenarios in which a trace will upgrade when:

- **Feedback** is added to any run on the trace (or any trace in the thread), whether through manual annotation, automatically with an online evaluator, or programmatically via the SDK.
- An **annotation queue** receives any run from the trace.
- An **automation rule** matches any run within a trace.

**Why auto-upgrade traces?**We have two reasons behind the auto-upgrade model for tracing:

1. We think that traces that match any of these conditions are fundamentally more interesting than other traces, and therefore it is good for users to be able to keep them around longer.
2. We philosophically want to charge customers an order of magnitude lower for traces that may not be interacted with meaningfully. We think auto-upgrades align our pricing model with the value that LangSmith brings, where only traces with meaningful interaction are charged at a higher rate.

If you have questions or concerns about our pricing model, please feel free to contact support via support.langchain.com and let us know your thoughts!**How does data retention affect downstream features?**

- **Annotation Queues, Run Rules, and Feedback**: Traces that use these features will be auto-upgraded.

- **Datasets**: Datasets have an indefinite data retention period. Restated differently, if you add a trace’s inputs and outputs to a dataset, they will never be deleted. We suggest that if you are using LangSmith for data collection, you take advantage of the datasets feature.

#### ​ Billing model

**Billable metrics**On your LangSmith invoice, you will see two metrics that we charge for:

- LangSmith Traces (Base Charge)
- LangSmith Traces (Extended Data Retention Upgrades).

The first metric includes all traces, regardless of tier. The second metric just counts the number of extended retention traces.**Why measure all traces + upgrades instead of base and extended traces?**A natural question to ask when considering our pricing is why not just show the number of `base` tier and `extended` tier traces directly on the invoice?While we understand this would be more straightforward, it doesn’t fit trace upgrades properly. Consider a `base` tier trace that was recorded on June 30, and upgraded to `extended` tier on July 3. The `base` tier trace occurred in the June billing period, but the upgrade occurred in the July billing period. Therefore, we need to be able to measure these two events independently to properly bill our customers.If your trace was recorded as an extended retention trace, then the `base` and `extended` metrics will both be recorded with the same timestamp.**Cost breakdown**The Base Charge for a trace is .05¢ per trace. We priced the upgrade such that an `extended` retention trace costs 10x the price of a base tier trace (.50¢ per trace) including both metrics. Thus, each upgrade costs .45¢.

### ​ Rate Limits

LangSmith has rate limits which are designed to ensure the stability of the service for all users.To ensure access and stability, LangSmith will respond with HTTP Status Code 429 indicating that rate or usage limits have been exceeded under the following circumstances:

#### ​ Temporary throughput limit over a 1 minute period at our application load balancer

This 429 is the the result of exceeding a fixed number of API calls over a 1 minute window on a per API key/access token basis. The start of the window will vary slightly — it is not guaranteed to start at the start of a clock minute — and may change depending on application deployment events.After the max events are received we will respond with a 429 until 60 seconds from the start of the evaluation window has been reached and then the process repeats.This 429 is thrown by our application load balancer and is a mechanism in place for all LangSmith users independent of plan tier to ensure continuity of service for all users.

| Method | Endpoints | Limit | Window |
| --- | --- | --- | --- |
| `DELETE` | `/sessions*` | 30 | 1 minute |
| `POST` OR `PATCH` | `/runs*` | 5000 | 1 minute |
| `GET` | `/runs/:id` | 30 | 1 minute |
| `POST` | `/feedbacks*` | 5000 | 1 minute |
| `*` | `*` | 2000 | 1 minute |

The LangSmith SDK takes steps to minimize the likelihood of reaching these limits on run-related endpoints by batching up to 100 runs from a single session ID into a single API call.

#### ​ Plan-level hourly trace event limit

This 429 is the result of reaching your maximum hourly events ingested and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour.An event in this context is the creation or update of a run. So if run is created, then subsequently updated in the same hourly window, that will count as 2 events against this limit.This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.

| Plan | Limit | Window |
| --- | --- | --- |
| Developer (no payment on file) | 50,000 events | 1 hour |
| Developer (with payment on file) | 250,000 events | 1 hour |
| Startup/Plus | 500,000 events | 1 hour |
| Enterprise | Custom | Custom |

#### ​ Plan-level hourly trace data ingest limit

This 429 is the result of reaching the maximum amount of data ingested across your trace inputs, outputs, and metadata and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour.Typically, inputs, outputs, and metadata are send on both run creation and update events. So if a run is created and is 2.0MB in size at creation, and 3.0MB in size when updated in the same hourly window, that will count as 5.0MB of storage against this limit.This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.

| Plan | Limit | Window |
| --- | --- | --- |
| Developer (no payment on file) | 500MB | 1 hour |
| Developer (with payment on file) | 2.5GB | 1 hour |
| Startup/Plus | 5.0GB | 1 hour |
| Enterprise | Custom | Custom |

#### ​ Plan-level monthly unique traces limit

This 429 is the result of reaching your maximum monthly traces ingested and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month.This is thrown by our application and applies only to the Developer Plan Tier when there is no payment method on file.

| Plan | Limit | Window |
| --- | --- | --- |
| Developer (no payment on file) | 5,000 traces | 1 month |

#### ​ Self-configured monthly usage limits

This 429 is the result of reaching your usage limit as configured by your organization admin and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month.This is thrown by our application and varies by organization based on their configured settings.

#### ​ Handling 429s responses in your application

Since some 429 responses are temporary and may succeed on a successive call, if you are directly calling the LangSmith API in your application we recommend implementing retry logic with exponential backoff and jitter.For convenience, LangChain applications built with the LangSmith SDK has this capability built-in.

It is important to note that if you are saturating the endpoints for extended periods of time, retries may not be effective as your application will eventually run large enough backlogs to exhaust all retries.If that is the case, we would like to discuss your needs more specifically. Please contact support via LangSmith Support with details about your applications throughput needs and sample code and we can work with you to better understand whether the best approach is fixing a bug, changes to your application code, or a different LangSmith plan.

### ​ Usage Limits

LangSmith lets you configure usage limits on tracing. Note that these are _usage_ limits, not _spend_ limits, which mean they let you limit the quantity of occurrences of some event rather than the total amount you will spend.LangSmith lets you set two different monthly limits, mirroring our Billable Metrics discussed in the aforementioned data retention guide:

- All traces limit
- Extended data retention traces limit

These let you limit the number of total traces, and extended data retention traces respectively.

#### ​ Properties of usage limiting

Usage limiting is approximate, meaning that we do not guarantee the exactness of the limit. In rare cases, there may be a small period of time where additional traces are processed above the limit threshold before usage limiting begins to apply.

#### ​ Side effects of extended data retention traces limit

The extended data retention traces limit has side effects. If the limit is already reached, any feature that could cause an auto-upgrade of tracing tiers becomes inaccessible. This is because an auto-upgrade of a trace would cause another extended retention trace to be created, which in turn should not be allowed by the limit. Therefore, you can no longer:

1. match run rules
2. add feed Updating usage limits

Usage limits can be updated from the `Settings` page under `Usage and Billing`. Limit values are cached, so it may take a minute or two before the new limits apply.

### ​ Related content

- Tutorial on how to optimize spend

## ​ Additional Resources

- **Release Versions**: Learn about LangSmith’s version support policy, including Active, Critical, End of Life, and Deprecated support levels.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Create an account and API key\\
\\
Previous Set up hierarchy\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/set-up-hierarchy

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Account administration

Set up hierarchy

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- Set up an organization
- Create an organization
- Manage and navigate workspaces
- Manage users
- Organization roles
- Set up a workspace
- Create a workspace
- Manage users
- Configure workspace settings
- Delete a workspace
- Delete a workspace via the UI
- Set up applications

This page describes setting up and managing your LangSmith _organization_ and _workspaces_:

- Set up an organization: Create and manage organizations for team collaboration, including user management and role assignments.
- Set up a workspace: Set up and configure workspaces to organize your LangSmith resources, manage workspace members, and configure settings for team collaboration.
- Set up applications: Set up applications within a workspace to further organize LangSmith resources, and take advantage of ABAC permissioning.

You may find it helpful to refer to the overview on LangSmith resource hierarchy before you read this setup page.

## ​ Set up an organization

If you’re interested in managing your organization and workspaces programmatically, see this how-to guide.

### ​ Create an organization

When you log in for the first time, LangSmith will create a personal organization for you automatically. If you’d like to collaborate with others, you can create a separate organization and invite your team members to join.To do this, open the Organizations drawer by clicking your profile icon in the bottom left and click **\+ New**. Shared organizations require a credit card before they can be used. You will need to set up billing to proceed.

### ​ Manage and navigate workspaces

Once you’ve subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users. To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the bottom left of LangSmith.

### ​ Manage users

Manage membership in your shared organization in the **Members and roles** tabs on the Settings page. Here you can:

- Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace role.
- Edit a user’s organization role.
- Remove users from your organization.

#### ​ Organization roles

Organization-scoped roles are used to determine access to organization settings. The role selected also impacts workspace membership:

- `Organization Admin` grants full access to manage all organization configuration, users, billing, and workspaces. Any `Organization Admin` has `Admin` access to all workspaces in an organization.

- `Organization User` may read organization information, but cannot execute any write actions at the organization level. You can add an `Organization User` to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.

The `Organization User` role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are `Organization Admins`. Custom organization-scoped roles are not available.

For a full list of permissions associated with each role, refer to the Administration overview page.

## ​ Set up a workspace

When you log in for the first time, a default workspace will be created for you in your personal organization. Workspaces are often used to separate resources between different teams or business units to establish clear trust boundaries between them. Within each workspace, Role-Based Access Control (RBAC) manages permissions and access levels, which ensures that users only have access to the resources and settings necessary for their role. Most LangSmith activity happens in the context of a workspace, each of which has its own settings and access controls.

### ​ Create a workspace

To create a new workspace, navigate to the Settings page **Workspaces** tab in your shared organization and click **Add Workspace**. Once you have created your workspace, you can manage its members and other configuration by selecting it on this page.!Create workspace

Different plans have different limits placed on the number of workspaces that can be used in an organization. For more information, refer to the pricing page.

### ​ Manage users

Only workspace `Admins` can manage workspace membership and, if RBAC is enabled, change a user’s workspace role.

For users that are already members of an organization, a workspace `Admin` may add them to a workspace in the **Workspace members** tab under Workspaces settings page. Users may also be invited directly to one or more workspaces when they are invited to an organization.

### ​ Configure workspace settings

Workspace configuration exists in the Workspaces settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The following example shows the **API keys**, and other configuration options including secrets, models, and shared URLs are available here as well.!Workspace settings

### ​ Delete a workspace

Deleting a workspace will permanently delete the workspace and all associated data. This action cannot be undone.

You can delete a workspace through the LangSmith UI or via API. You must be a workspace `Admin` in order to delete a workspace.

### ​ Delete a workspace via the UI

1. Navigate to **Settings**.
2. Select the workspace you want to delete.
3. Click **Delete** in the top-right corner of the screen.

## ​ Set up applications

Applications can be created within a workspace to further organize resources, such as tracing projects and datasets, within a workspace.A workspace may have zero or more applications.You can view all resources within a workspace by selecting `Show all applications`; resources may be tagged to multiple applications by adding them to the `Application` tag under Resource Tags within the settings page.!Sample Application Selector

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Overview\\
\\
Previous Manage your organization using the API\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/manage-organization-by-api

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Account administration

Manage your organization using the API

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- Workspaces
- User management
- RBAC
- Membership management
- API keys
- Security settings
- User-only endpoints
- Sample code

LangSmith’s API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted in User-only endpoints.

Before diving into this content, it might be helpful to read the following:

- Conceptual guide on organizations and workspaces
- Organization setup how-to guild

There are a few limitations that will be lifted soon:

- The LangSmith SDKs do not support these organization management actions yet.
- Organization-scoped service keys with Organization Admin permission may be used for these actions.

Use the `X-Tenant-Id` header to specify which workspace to target. If the header is not present, operations will default to the workspace the API key was initially created in if it is not organization-scoped.**If `X-Tenant-Id` is not specified when accessing workspace-scoped resources with an organization-scoped API key, the request will fail with `403 Forbidden`.**

Some commonly-used endpoints and use cases are listed below. For a complete list of available endpoints, see the API docs. **The `X-Organization-Id` header should be present on all requests, and `X-Tenant-Id` header should be present on requests that are scoped to a particular workspace.**

## ​ Workspaces

- List workspaces
- Create workspace
- Update workspace name

## ​ User management

### ​ RBAC

- List roles
- List permissions
- Create role
- Update role

### ​ Membership management

`List roles` under RBAC should be used for retrieving role IDs of these operations. `List [organization|workspace] members` endpoints (below) response `"id"`s should be used as `identity_id` in these operations.Organization level:

- List active organization members
- List pending organization members
- Invite a user to the organization and one or more workspaces. This should be used when the user is not already a member in the organization.
- Update a user’s organization role
- Remove someone from the organization

Workspace level:

- List workspace members
- Add a member to a workspace that is already part of the organization
- Update a user’s workspace role
- Remove someone from a workspace

These params should be omitted: `read_only` (deprecated), `password` and `full_name` ( basic auth only)

## ​ API keys

- Create a service key
- Delete a service key

## ​ Security settings

Organization Admin permissions are required to make these changes.

“Shared resources” in this context refer to public prompts, shared runs, and shared datasets.

Updating these settings affects **all resources in the organization**.

- Update organization sharing settings
- use `unshare_all` to unshare **ALL** shared resources in the organization - use `disable_public_sharing` to prevent future sharing of resources

These settings are only editable via API:

- Disable/enable PAT creation(for self-hosted, available in Helm chart version 0.11.25+)

- Use `pat_creation_disabled` to disable PAT creation for the entire organization.
- See the admin guide for information about the Organization Viewer role, which cannot create PATs.

## ​ User-only endpoints

These endpoints are user-scoped and require a logged-in user’s JWT, so they should only be executed through the UI.

- `/api-key/current` endpoints: these are related a user’s PATs
- `/sso/email-verification/send` (Cloud-only): this endpoint is related to SAML SSO

## ​ Sample code

Copy

import os
import requests

def main():
api_key = os.environ["LANGSMITH_API_KEY"]
# LANGSMITH_ORGANIZATION_ID is not a standard environment variable in the SDK, just used for this example
organization_id = os.environ["LANGSMITH_ORGANIZATION_ID"]
base_url = os.environ.get("LANGSMITH_ENDPOINT") # or "https://api.smith.langchain.com". Update appropriately for self-hosted installations or the EU region
headers = {
"Content-Type": "application/json",
"X-API-Key": api_key,
"X-Organization-Id": organization_id,
}
session = requests.Session()
session.headers.update(headers)
workspaces_path = f"{base_url}/api/v1/workspaces"
orgs_path = f"{base_url}/api/v1/orgs/current"
api_keys_path = f"{base_url}/api/v1/api-key"

# Create a workspace
workspace_res = session.post(workspaces_path, json={"display_name": "My Workspace"})
workspace_res.raise_for_status()
workspace = workspace_res.json()
workspace_id = workspace["id"]
new_workspace_headers = {
"X-Tenant-Id": workspace_id,
}

# Grab roles - this includes both organization and workspace roles
roles_res = session.get(f"{orgs_path}/roles")
roles_res.raise_for_status()
roles = roles_res.json()
# system org roles are 'Organization Admin', 'Organization User'
# system workspace roles are 'Admin', 'Editor', 'Viewer'
org_roles_by_name = {role["display_name"]: role for role in roles if role["access_scope"] == "organization"}
ws_roles_by_name = {role["display_name"]: role for role in roles if role["access_scope"] == "workspace"}

# Invite a user to the org and the new workspace, as an Editor.
# workspace_role_id is only allowed if RBAC is enabled (an enterprise feature).

new_user_res = session.post(
f"{orgs_path}/members",
json={
"email": new_user_email,
"role_id": org_roles_by_name["Organization User"]["id"],
"workspace_ids": [workspace_id],
"workspace_role_id": ws_roles_by_name["Editor"]["id"],
},
)
new_user_res.raise_for_status()

# Add a user that already exists in the org to the new workspace, as a Viewer.

org_members_res = session.get(f"{orgs_path}/members")
org_members_res.raise_for_status()
org_members = org_members_res.json()
existing_org_member = next(
(member for member in org_members["members"] if member["email"] == existing_user_email), None
)
existing_user_res = session.post(
f"{workspaces_path}/current/members",
json={
"user_id": existing_org_member["user_id"],
"workspace_ids": [workspace_id],
"workspace_role_id": ws_roles_by_name["Viewer"]["id"],
},
headers=new_workspace_headers,
)
existing_user_res.raise_for_status()

# List all members of the workspace
members_res = session.get(f"{workspaces_path}/current/members", headers=new_workspace_headers)
members_res.raise_for_status()
members = members_res.json()
workspace_member = next(
(member for member in members["members"] if member["email"] == existing_user_email), None
)

# Update the user's workspace role to Admin (enterprise-only)
existing_user_id = workspace_member["id"]
update_res = session.patch(
f"{workspaces_path}/current/members/{existing_user_id}",
json={"role_id": ws_roles_by_name["Admin"]["id"]},
headers=new_workspace_headers,
)
update_res.raise_for_status()

# Update the user's organization role to Organization Admin
update_res = session.patch(
f"{orgs_path}/members/{existing_org_member['id']}",
json={"role_id": org_roles_by_name["Organization Admin"]["id"]},
)
update_res.raise_for_status()

# Create a new Service key
api_key_res = session.post(
api_keys_path,
json={"description": "my key"},
headers=new_workspace_headers,
)
api_key_res.raise_for_status()
api_key_json = api_key_res.json()
api_key = api_key_json["key"]

if __name__ == "__main__":
main()

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Set up hierarchy\\
\\
Previous Manage billing in your account\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/billing

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Account administration

Manage billing in your account

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- Set up billing for your account
- Developer Plan: set up billing on your personal organization
- Plus Plan: set up billing on a shared organization
- Update your information (Paid plans only)
- Invoice email
- Business information and tax ID
- Enforce spend limits
- Understand your current usage
- Usage graph
- Invoices
- Set limits on usage
- Set spend limit for workspace
- Configure trace tier distrubution
- Other methods of managing traces
- Change project-level default retention
- Apply extended data retention to a percentage of traces
- LangSmith Deployment billing
- Summary

This page describes how to manage billing for your LangSmith organization:

- Set up billing for your account: Complete the billing setup process for Developer and Plus plans, including special instructions for legacy accounts.
- Update your information: Modify invoice email addresses, business information, and tax IDs for your organization.
- Enforce spend limits: Learn how to manage your spend through usage limits and data retention.

## ​ Set up billing for your account

Before using this guide, note the following:

- If you are interested in the Enterprise plan, please contact sales. This guide is only for our self-serve billing plans.

To set up billing for your LangSmith organization, navigate to the Billing and Usage page under **Settings**. Depending on your organization’s settings, there are different setup guides:

- Developer plan
- Plus plan

### ​ Developer Plan: set up billing on your personal organization

Personal organizations are limited to 5,000 traces per month until a credit card is added. To add a card:

1. Click **Add card to remove trace limit**.
2. Add your credit card information.
3. Once complete, you will no longer be rate limited to 5,000 traces, and you will be charged for any excess traces at rates specified on the pricing page.

### ​ Plus Plan: set up billing on a shared organization

Team organizations are given an initial 10,000 traces per month. Any excess traces will be charged at rates specified on the pricing page.

New organizations that you manually create are required to be on the Plus Plan. If you see a message about needing to upgrade to Plus to use this organization, follow these steps.

1. Click **Upgrade to Plus**.
2. Invite members to your organization, as desired.
3. Enter your credit card information. Then, enter business information, invoice email, and tax ID. If this organization belongs to a business, check the **This is a business** checkbox and enter the information accordingly. For more information, refer to the Update your information section.

## ​ Update your information (Paid plans only)

To update business information for your LangSmith organization, head to the Billing and Usage page under **Settings**.

### ​ Invoice email

To update the email address for invoices, follow these steps:

1. Navigate to the **Plans and Billing** tab.
2. Locate the section beneath the payment method, where the current invoice email is displayed.
3. Enter the new email address for invoices in the provided field.
4. The new email address will be automatically saved.

You will receive all future invoices to the updated email address.

### ​ Business information and tax ID

In certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your tax ID may qualify you for a sales tax exemption.

To update your organization’s business information, follow these steps:

1. Navigate to the **Plans and Billing** tab.
2. Below the invoice email section, you will find a checkbox labeled **Business**.
3. Check the **Business** checkbox if your organization belongs to a business.
4. A business information section will appear, allowing you to enter or update the following details:
- Business Name
- Address
- Tax ID for applicable jurisdictions
5. A Tax ID field will appear for applicable jurisdictions after you select a country.
6. After entering the necessary information, click the **Save** button to save your changes.

This ensures that your business information is up-to-date and accurate for billing and tax purposes.

## ​ Enforce spend limits

You may find it helpful to read the following pages, before continuing with this section on optimizing your tracing spend:

- Data Retention Conceptual Docs
- Usage Limiting Conceptual Docs

Some of the features mentioned in this guide are not currently available on Enterprise plan due to its custom nature of billing. If you are on the Enterprise plan and have questions about cost optimization, contact your sales rep or support via support.langchain.com.

### ​ Understand your current usage

The first step of any optimization process is to understand current usage. LangSmith provides two ways to do this: Usage graph and Invoices.LangSmith Usage is measured per workspace, because workspaces often represent development environments (as in the example), or teams within an organization.

#### ​ Usage graph

- LangSmith Traces (Base Charge): tracks all traces that you send to LangSmith.
- LangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention.
- LangSmith Deployment Agent Runs: tracks end-to-end invocations of deployed LangGraph agents.

For more details on traces, refer to the data retention conceptual docs. For more details on Agent Runs, refer to LangSmith Deployment billing.

#### ​ Invoices

To understand how your usage translates to spend, navigate to the **Invoices** tab. The first invoice that will appear on screen is a draft of your current month’s invoice, which shows your running spend thus far this month.

LangSmith’s Usage Graph and Invoice use the term `tenant_id` to refer to a workspace ID. They are interchangeable.

#### ​ Set spend limit for workspace

2. Input a spend limit for your selected workspace. LangSmith will determine an appropriate number of base and extended trace limits to match that spend. The trace limits include the free trace allocation that comes with your plan (see details on pricing page).

For organizations with **multiple workspaces only**: For simplicity, LangSmith incorporates the free traces into the cost calculation of the **first workspace only**. In actuality, the free traces can be “consumed” by any workspace. Therefore, although workspace-level spend limits are approximate for multi-workspace organizations, the organization-level spend limit is absolute.

#### ​ Configure trace tier distrubution

LangSmith has two trace tiers: base traces and extended traces. Base traces have the base retention and are short-lived (14 days), while extended traces have extended retention and are long-lived (400 days). For more information, refer to the data retention conceptual docs.Set the desired default trace tier by selecting an option below the **Default data retention** label. All traces will have this tier by default when they are registered. Note that because extended traces cost more than base traces, selecting **Extended** as your default data retention option will result in less overall traces allowed in the billing period. By default, updating this setting will only apply to future incoming traces. To apply to all existing traces in the workspace, select the checkbox.If the default data retention is set to **Base** you can optionally use the slider to distribute trace limits across base and extended tracess. LangSmith automatically provides a suggestion for this distribution but you can tailor this to your needs. For example, if you are running lots of automations or other features that may upgrade a trace to extended, you may want to increase your extended trace limits. To see the complete list of features that may upgrade a trace, see here.

The extended data retention limit can cause features other than tracing to stop working once reached. If you plan to use this feature, read more about its functionality and side effects.

#### ​ Apply extended data retention to a percentage of traces

You may not want all traces to expire after 14 days. You can automatically extend the retention of traces that match some criteria by creating an automation rule. You might want to apply extended data retention to specific types of traces, such as:

- 10% of all traces: For general analysis or analyzing trends long term.
- Errored traces: To investigate and debug issues thoroughly.
- Traces with specific metadata: For long-term examination of particular features or user flows.

To configure this:

2. Name your rule and optionally apply filters or a sample rate. For more information on configuring filters, refer to filtering techniques.

When an automation rule matches any run within a trace, then all runs within the trace are upgraded to be retained for 400 days.

For example, this is the expected configuration to keep 10% of all traces for extended data retention:!P2sampletracesIf you want to keep a subset of traces for **longer than 400 days** for data collection purposes, you can create another run rule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.

### ​ LangSmith Deployment billing

In addition to traces, LangSmith charges for deployed agents via LangSmith Deployment (formerly LangGraph Platform).

- **Agent Runs**: An Agent Run is one end-to-end invocation of a deployed LangGraph agent and is billed at $0.005 each. Nodes and subgraphs within a single agent execution are not charged separately. Calls to other LangGraph agents are charged separately to the deployment hosting the called agent. When using human-in-the-loop with interrupts, resuming after an interrupt creates a separate Agent Run.
- **Deployment Uptime**: You are also charged for the time your deployment’s database is live and persisting state. See the pricing page for uptime costs by deployment type (Development vs Production).

For high-volume deployment usage, please contact our sales team to discuss custom pricing options.

### ​ Summary

If you have questions about further managing your spend, please contact support via support.langchain.com.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Manage your organization using the API\\
\\
Previous Set up resource tags\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/set-up-resource-tags

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Account administration

Set up resource tags

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- Create a tag
- Assign a tag to a resource
- Delete a tag

Before diving into this content, it might be helpful to read the following:

- Conceptual guide on organizations and workspaces

Resource tags are available for Plus and Enterprise plans.

While workspaces help separate trust boundaries and access control, tags help you organize resources within a workspace. Tags are key-value pairs that you can attach to resources.

**Not to be confused with commit tags**: Resource tags are key-value pairs used to organize and filter workspace resources (projects, datasets, prompts, etc.). Commit tags are labels that reference specific versions in a prompt’s commit history. While both types of tags can use similar terminology (like `prod` or `staging`), resource tags help you _organize resources_ across your workspace, while commit tags control _which version_ of a prompt is used in your code.

## ​ Create a tag

To create a tag, head to the workspace settings and click on the “Resource Tags” tab. Here, you’ll be able to see the existing tag values, grouped by key. Two keys `Application` and `Environment` are created by default; the `Application` key is used to filter resources shown in the UI.To create a new tag, click on the “New Tag” button. You’ll be prompted to enter a key and a value for the tag. Note that you can use an existing key or create a new one.!Create tag

## ​ Assign a tag to a resource

Within the same side panel for creating a new tag, you can also create assign resources to tags. Search for corresponding resources in the “Assign Resources” section and select the resources you want to tag.

You can only tag workspace-scoped resources with resource tags. This includes Tracing Projects, Annotation Queues, Deployments, Experiments, Datasets, and Prompts.

To un-assign a tag from a resource, click on the Trash icon next to the tag, both in the tag panel and the resource tag panel.

## ​ Delete a tag

You can delete either a key or a value of a tag from the workspace settings page. To delete a key, click on the Trash icon next to the key. To delete a value, click on the Trash icon next to the value.Note that if you delete a key, all values associated with that key will also be deleted. When you delete a value, you will lose all associations between that value and resources.!Delete tag

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Manage billing in your account\\
\\
Previous User management\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/user-management

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Account administration

User management

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- Set up access control
- Create a role
- Assign a role to a user
- Set up SAML SSO for your organization
- Just-in-time (JIT) provisioning
- Login methods and access
- Enforce SAML SSO only
- Prerequisites
- Initial configuration
- Entra ID (Azure)
- Google
- Okta
- Supported features
- Configuration steps
- SP-initiated SSO
- Set up SCIM for your organization
- Requirements
- Prerequisites
- Role Precedence
- Email verification
- Attributes and Mapping
- Group Naming Convention
- Mapping
- User Attributes
- Group Attributes
- Step 1 - Configure SAML SSO (Cloud only)
- NameID Format
- Step 2 - Disable JIT provisioning
- Disabling JIT for Cloud
- Disabling JIT for Self-Hosted
- Step 3 - Generate SCIM bearer token
- Step 4 - Configure your Identity Provider
- Azure Entra ID configuration steps
- Okta configuration steps
- Other Identity Providers

This page covers user management features in LangSmith, including access control, authentication, and automated user provisioning:

- Set up access control: Configure role-based access control (RBAC) to manage user permissions within workspaces, including creating custom roles and assigning them to users.
- SAML SSO (Enterprise plan): Set up Single Sign-On authentication for Enterprise customers using SAML 2.0, including configuration for popular identity providers.
- SCIM User Provisioning (Enterprise plan): Automate user provisioning and deprovisioning between your identity provider and LangSmith using SCIM.

## ​ Set up access control

RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, contact our sales team. Other plans default to using the `Admin` role for all users.

You may find it helpful to read the Administration overview page before setting up access control.

LangSmith relies on RBAC to manage user permissions within a workspace. This allows you to control who can access your LangSmith workspace and what they can do within it. Only users with the `workspace:manage` permission can manage access control settings for a workspace.For a complete reference of workspace roles and their permissions, refer to the Role-based access control guide. For specific operations each role can perform, refer to the Organization and workspace operations reference.

### ​ Create a role

By default, LangSmith comes with a set of system roles:

- `Admin`: has full access to all resources within the workspace.
- `Viewer`: has read-only access to all resources within the workspace.
- `Editor`: has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys).

If these do not fit your access model, `Organization Admins` can create custom roles to suit your needs.To create a role, navigate to the **Roles** tab in the **Members and roles** section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization.Click on the **Create Role** button to create a new role. A **Create role** form will open.!Create RoleAssign permissions for the different LangSmith resources that you want to control access to.

### ​ Assign a role to a user

Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the `Workspace members` tab in the `Workspaces` section of the Organization settings pageEach user will have a **Role** dropdown that you can use to assign a role to them.!Assign RoleYou can also invite new users with a given role.!Invite User

## ​ Set up SAML SSO for your organization

Single Sign-On (SSO) functionality is **available for Enterprise Cloud** customers to access LangSmith through a single authentication source. This allows administrators to centrally manage team access and keeps information more secure.LangSmith’s SSO configuration is built using the SAML (Security Assertion Markup Language) 2.0 standard. SAML 2.0 enables connecting an Identity Provider (IdP) to your organization for an easier, more secure login experience.SSO services permit a user to use one set of credentials (for example, a name or email address and password) to access multiple applications. The service authenticates the end user only once for all the applications the user has been given rights to and eliminates further prompts when the user switches applications during the same session. The benefits of SSO include:

- Streamlines user management across systems for organization owners.
- Enables organizations to enforce their own security policies (e.g., MFA).
- Removes the need for end users to remember and manage multiple passwords. Simplifies the end-user experience, by allowing sign in at one single access point across multiple applications.

### ​ Just-in-time (JIT) provisioning

LangSmith supports Just-in-time provisioning when using SAML SSO. This allows someone signing in via SAML SSO to join the organization and selected workspaces automatically as a member.

JIT provisioning only runs for new users, that is, users who do not already have access to the organization with the same email address via a different login method.

### ​ Login methods and access

Once you have completed your configuration of SAML SSO for your organization, users will be able to log in via SAML SSO in addition to other login methods, such as username/password or Google Authentication”:

- When logged in via SAML SSO, users can only access the corresponding organization with SAML SSO configured.
- Users with SAML SSO as their only login method do not have personal organizations.
- When logged in via any other method, users can access the organization with SAML SSO configured along with any other organizations they are a part of.

### ​ Enforce SAML SSO only

User invites are not supported in organizations enforcing SAML SSO only. Initial workspace membership and role is determined by JIT provisioning, and changes afterwards can be managed in the UI.
For additional flexibility in automated user management, LangSmith supports SCIM.

To ensure users can only access the organization when logged in using SAML SSO and no other method, check the **Login via SSO only** checkbox and click **Save**. Once this happens, users accessing the organization that are logged-in via a non-SSO login method are required to log back in using SAML SSO. This setting can be switched . If you have issues setting up SAML SSO, contact the LangChain support team via support.langchain.com.

### ​ Prerequisites

SAML SSO is available for organizations on the Enterprise plan. Please contact sales to learn more.

- Your organization must be on an Enterprise plan.
- Your Identity Provider (IdP) must support the SAML 2.0 standard.
- Only `Organization Admins` can configure SAML SSO.

For instructions on using SCIM along with SAML for user provisioning and deprovisioning, refer to the SCIM setup.

### ​ Initial configuration

For IdP-specific configuration steps, refer to one of the following:

- Entra ID
- Google
- Okta

1. In your IdP: Configure a SAML application with the following details, then copy the metadata URL or XML for step 3.

The following URLs are different for the US and EU regions. Ensure you select the correct link.

1. Single sign-on URL (or ACS URL):
- US:
- EU:
2. Audience URI (or SP Entity ID):
- US:
- EU:
3. Name ID format: email address.
4. Application username: email address.
5. Required claims: `sub` and `email`.

2. Select the `Default workspace role` and `Default workspaces`. New users logging in via SSO will be added to the specified workspaces with the selected role.

- `Default workspace role` and `Default workspaces` are editable. The updated settings will apply to new users only, not existing users.
- (Coming soon) `SAML metadata URL` and `SAML metadata XML` are editable. This is usually only necessary when cryptographic keys are rotated/expired or the metadata URL has changed but the same IdP is still used.

### ​ Entra ID (Azure)

For additional information, see Microsoft’s documentation.

**Step 1: Create a new Entra ID application integration**

1. Log in to the Azure portal with a privileged role (e.g., `Global Administrator`). On the left navigation pane, select the `Entra ID` service.
2. Navigate to **Enterprise Applications** and then select **All Applications**.
3. Click **Create your own application**.
4. In the **Create your own application** window: 1. Enter a name for your application (e.g., `LangSmith`).
2. Select **Integrate any other application you don’t find in the gallery (Non-gallery)**.
5. Click **Create**.

**Step 2: Configure the Entra ID application and obtain the SAML Metadata**

1. Open the enterprise application that you created.

3. On the Single sign-on page, click **SAML**.
4. Update the **Basic SAML Configuration**: 1. `Identifier (Entity ID)`:

- US:
- EU:
2. `Reply URL (Assertion Consumer Service URL)`:

- US:
- EU:
3. Leave `Relay State`, `Logout Url`, and `Sign on URL` empty.
4. Click **Save**.
5. Ensure required claims are present with **Namespace**: `http://schemas.xmlsoap.org/ws/2005/05/identity/claims`: 1. `sub`: `user.objectid`.
2. `emailaddress`: `user.userprincipalname` or `user.mail` (if using the latter, ensure all users have the `Email` field filled in under `Contact Information`).
3. (Optional) For SCIM, see the setup documentation for specific instructions about `Unique User Identifier (Name ID)`.
6. On the SAML-based Sign-on page, under **SAML Certificates**, copy the **App Federation Metadata URL**.

**Step 3: Set up LangSmith SSO Configuration**Follow the instructions under initial configuration in the `Fill in required information` step, using the metadata URL from the previous step.**Step 4: Verify the SSO setup**

2. Click **Add user/group**.
3. In the **Add Assignment** window: 1. Under **Users**, click **None Selected**.
2. Search for the user you want to assign to the enterprise application, and then click **Select**.
3. Verify that the user is selected, and click **Assign**.

### ​ Google

For additional information, see Google’s documentation.**Step 1: Create and configure the Google Workspace SAML application**

1. Make sure you’re signed into an administrator account with the appropriate permissions.

3. Click **Add App** and then **Add custom SAML app**.
4. Enter the app name and, optionally, upload an icon. Click **Continue**.
5. On the Google Identity Provider details page, download the **IDP metadata** and save it for Step 2. Click **Continue**.
6. In the `Service Provider Details` window, enter: 1. `ACS URL`:

- US:
- EU:
2. `Entity ID`:

- US:
- EU:
3. Leave `Start URL` and the `Signed response` box empty.

5. Click `Continue`.

**Step 2: Set up LangSmith SSO Configuration**Follow the instructions under initial configuration in the `Fill in required information` step, using the `IDP metadata` from the previous step as the metadata XML.**Step 3: Turn on the SAML app in Google**

2. Click `User access`.
3. Turn on the service: 1. To turn the service on for everyone in your organization, click `On for everyone`, and then click `Save`.
2. To turn the service on for an organizational unit: 1. At the left, select the organizational unit then `On`.
2. If the Service status is set to `Inherited` and you want to keep the updated setting, even if the parent setting changes, click `Override`.
3. If the Service status is set to `Overridden`, either click `Inherit` to revert to the same setting as its parent, or click `Save` to keep the new setting, even if the parent setting changes.
3. To turn on a service for a set of users across or within organizational units, select an access group. For details, go to Use groups to customize service access.
4. Ensure that the email addresses your users use to sign in to LangSmith match the email addresses they use to sign in to your Google domain.

**Step 4: Verify the SSO setup**Have a user with access sign in via the unique login URL from the **SSO Configuration** page, or go to the SAML application page in Google and click **TEST SAML LOGIN**.

#### ​ Supported features

- IdP-initiated SSO (Single Sign-On)
- SP-initiated SSO
- Just-In-Time provisioning
- Enforce SSO only

#### ​ Configuration steps

For additional information, see Okta’s documentation.**Step 1: Create and configure the Okta SAML application**

**Via Okta Integration Network (recommended)**

01. Sign in to Okta.
02. In the upper-right corner, select Admin. The button is not visible from the Admin area.
03. Select `Browse App Integration Catalog`.
04. Find and select the LangSmith application.
05. On the application overview page, select Add Integration.
06. Leave `ApiUrlBase` empty.
07. Fill in `AuthHost`:

- US: `auth.langchain.com`
- EU: `eu.auth.langchain.com`
08. (Optional, if planning to use SCIM as well) Fill in `LangSmithUrl`:

- US: `api.smith.langchain.com`
- EU: `eu.api.smith.langchain.com`
09. Under Application Visibility, keep the box unchecked.
10. Select Next.
11. Select `SAML 2.0`.
12. Fill in `Sign-On Options`:

- `Application username format`: `Email`
- `Update application username on`: `Create and update`
- `Allow users to securely see their password`: leave **unchecked**.
13. Copy the **Metadata URL** from the **Sign On Options** page to use in the next step.

**Via Custom App Integration**

SCIM is not compatible with this method of configuration. Refer to **Via Okta Integration Network**.

1. Log in to Okta as an administrator, and go to the **Okta Admin console**.

3. Select **SAML 2.0**.
4. Enter an `App name` (e.g., `LangSmith`) and optionally an **App logo**, then click **Next**.
5. Enter the following information in the **Configure SAML** page: 1. `Single sign-on URL` (`ACS URL`). Keep `Use this for Recipient URL and Destination URL` checked:

- US:
- EU:
2. `Audience URI (SP Entity ID)`:

- US:
- EU:
3. `Name ID format`: **Persistent**.
4. `Application username`: `email`.
5. Leave the rest of the fields empty or set to their default.
6. Click **Next**.
6. Click **Finish**.
7. Copy the **Metadata URL** from the **Sign On** page to use in the next step.

**Step 2: Set up LangSmith SSO Configuration**Follow the instructions under initial configuration in the **Fill in required information** step, using the metadata URL from the previous step.**Step 3: Assign users to LangSmith in Okta**

2. Under the **Assignments** tab, click **Assign** then either **Assign to People** or **Assign to Groups**.
3. Make the desired selection(s), then **Assign** and **Done**.

**Step 4: Verify the SSO setup**Have a user with access sign in via the unique login URL from the `SSO Configuration` page, or have a user select the application from their Okta dashboard.

#### ​ SP-initiated SSO

Once service-provider–initiated SSO is configured, users can sign in using a unique login URL. You can find this in the LangSmith UI under **Organization members and roles** then **SSO configuration**.

## ​ Set up SCIM for your organization

System for Cross-domain Identity Management (SCIM) is an open standard that allows for the automation of user provisioning. Using SCIM, you can automatically provision and de-provision users in your LangSmith organization and workspaces, keeping user access synchronized with your organization’s identity provider.

SCIM is available for organizations on the Enterprise plan. Contact sales to learn more.SCIM is available on Helm chart versions 0.10.41 (application version 0.10.108) and later.SCIM support is API-only (see instructions below).

SCIM eliminates the need for manual user management and ensures that user access is always up-to-date with your organization’s identity system. This allows for:

- **Automated user management**: Users are automatically added, updated, and removed from LangSmith based on their status in your IdP.
- **Reduced administrative overhead**: No need to manage user access manually across multiple systems.
- **Improved security**: Users who leave your organization are automatically deprovisioned from LangSmith.
- **Consistent access control**: User attributes and group memberships are synchronized between systems.
- **Scaling team access control**: Efficiently manage large teams with many workspaces and custom roles.
- **Role assignment**: Select specific Organization Roles and Workspace Roles for groups of users.

#### ​ Prerequisites

- Your organization must be on an Enterprise plan.
- Your Identity Provider (IdP) must support SCIM 2.0.
- Only Organization Admins can configure SCIM.
- For cloud customers: SAML SSO must be configurable for your organization.
- For self-hosted customers: OAuth with Client Secret authentication mode must be enabled.
- For self-hosted customers, network traffic must be allowed from the identity provider to LangSmith:
- Microsoft Entra ID supports allowlisting IP ranges or an agent-based solution to provide connectivity.
( details).
- Okta supports allow-listing IPs or domains ( details)
or an agent-based solution ( details) to provide connectivity.

#### ​ Role Precedence

When a user belongs to multiple groups for the same workspace, the following precedence applies:

1. **Organization Admin groups** take highest precedence. Users in these groups will be `Admin` in all workspaces.
2. **Most recently created workspace-specific group** takes precedence over other workspace groups.

When a group is deleted or a user is removed from a group, their access is updated according to their remaining group membership, following the precedence rules.SCIM group membership will override manually assigned roles or roles assigned via Just-in-time (JIT) provisioning. We recommend disabling JIT provisioning to avoid conflicts.

#### ​ Email verification

In cloud only, creating a new user with SCIM triggers an email to the user.
They must verify their email address by clicking the link in this email.
The link expires in 24 hours, and can be resent if needed by removing and re-adding the user via SCIM.

#### ​ Group Naming Convention

Renaming groups is **not** supported via SCIM. Group names are persistent because they must match role names and/or workspace names in LangSmith.

- `LS:Organization Admins`
- `Groups-Organization Admins`
- `Organization Admin`

- `LS:Organization User:Production:Annotators`
- `Groups-Organization User:Engineering:Developers`
- `Organization User:Marketing:Viewers`

### ​ Mapping

While specific instructions depending on the identity provider may vary, these mappings show what is supported by the LangSmith SCIM integration:

#### ​ User Attributes

| **LangSmith App Attribute** | **Identity Provider Attribute** | **Matching Precedence** |
| --- | --- | --- |
| `userName`1 | email address | |
| `active` | `!deactivated` | |
| `emails[type eq "work"].value` | email address2 | |
| `name.formatted` | `displayName` OR `givenName + familyName`3 | |
| `givenName` | `givenName` | |
| `familyName` | `familyName` | |
| `externalId` | `sub`4 | 1 |

1. `userName` is not required by LangSmith
2. Email address is required
3. Use the computed expression if your `displayName` does not match the format of `Firstname Lastname`
4. To avoid inconsistency, this should match the SAML `NameID` assertion for cloud customers, or the `sub` OAuth2.0 claim for self-hosted.

#### ​ Group Attributes

| **LangSmith App Attribute** | **Identity Provider Attribute** | **Matching Precedence** |
| --- | --- | --- |
| `displayName` | `displayName`1 | 1 |
| `externalId` | `objectId` | |
| `members` | `members` | |

1. Groups must follow the naming convention described in the Group Naming Convention section.
If your company has a group naming policy, you should instead map from the `description` identity provider attribute and
set the description based on the Group Naming Convention section.

### ​ Step 1 - Configure SAML SSO (Cloud only)

There are two scenarios for SAML SSO configuration:

1. If SAML SSO is already configured for your organization, you should skip the steps to initially add the application ( Add application from Okta Integration Network or Create a new Entra ID application integration), as you already have an application configured and just need to enable provisioning.
2. If you are configuring SAML SSO for the first time alongside SCIM, first follow the instructions to set up SAML SSO, _then_ follow the instructions here to enable SCIM.

#### ​ NameID Format

LangSmith uses the SAML NameID to identify users. The NameID is a required field in the SAML response and is case-insensitive.The NameID must:

1. Be unique to each user.
2. Be a persistent value that never changes, such as a randomly generated unique user ID.
3. Match exactly on each sign-in attempt. It should not rely on user input.

The NameID should not be an email address or username because email addresses and usernames are more likely to change over time and can be case-sensitive.The NameID format must be `Persistent`, unless you are using a field, like email, that requires a different format.

### ​ Step 2 - Disable JIT provisioning

Before enabling SCIM, disable Just-in-time (JIT) provisioning to prevent conflicts between automatic and manual user provisioning.

#### ​ Disabling JIT for Cloud

Use the `PATCH /orgs/current/info` endpoint:

Copy

curl -X PATCH $LANGCHAIN_ENDPOINT/orgs/current/info \
-H "X-Api-Key: $LANGCHAIN_API_KEY" \
-H "Content-Type: application/json" \
-d '{"jit_provisioning_enabled": false}'

#### ​ Disabling JIT for Self-Hosted

As of LangSmith chart version **0.11.14**, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:

commonEnv:
- name: SELF_HOSTED_JIT_PROVISIONING_ENABLED
value: "false"

### ​ Step 3 - Generate SCIM bearer token

In self-hosted environments, the full URL below may look like `https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens` (without a subdomain, note the `/api/v1` path prefix) or `https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens` (with a subdomain) - see the ingress docs for more details.

Generate a SCIM Bearer Token for your organization. This token will be used by your IdP to authenticate SCIM API requests. Ensure env vars are set appropriately, for example:

curl -X POST $LANGCHAIN_ENDPOINT/v1/platform/orgs/current/scim/tokens \
-H "X-Api-Key: $LANGCHAIN_API_KEY" \
-H "Content-Type: application/json" \
-d '{"description": "Your description here"}'

Note that the SCIM Bearer Token value is not available outside of the response to this request. These additional endpoints are present:

- `GET /v1/platform/orgs/current/scim/tokens`
- `GET /v1/platform/orgs/current/scim/tokens/{scim_token_id}`
- `PATCH /v1/platform/orgs/current/scim/tokens/{scim_token_id}` (only the `description` field is supported)
- `DELETE /v1/platform/orgs/current/scim/tokens/{scim_token_id}`

### ​ Step 4 - Configure your Identity Provider

If you use Azure Entra ID (formerly Azure AD) or Okta, there are specific instructions for identity provider setup (refer to Azure Entra ID, Okta). The requirements and steps above are applicable for all identity providers.

#### ​ Azure Entra ID configuration steps

In self-hosted installations, the `oid` JWT claim is used as the `sub`.
See this Microsoft Learn link
and the related configuration instructions for additional details.

**Step 1: Configure SCIM in your Enterprise Application**

1. Log in to the Azure portal with a privileged role (e.g., `Global Administrator`).
2. Navigate to your existing LangSmith Enterprise Application.

4. Click **Get started**.

**Step 2: Configure Admin credentials**

1. Under **Admin Credentials**: - **Tenant URL**: - US: `https://api.smith.langchain.com/scim/v2`
- EU: `https://eu.api.smith.langchain.com/scim/v2`

- **Secret Token**: Enter the SCIM Bearer Token generated in Step 3.
2. Click **Test Connection** to verify the configuration.
3. Click **Save**.

**Step 3: Configure Attribute Mappings**Configure the following attribute mappings under `Mappings`:**User Attributes**Set **Target Object Actions** to `Create` and `Update` (start with `Delete` disabled for safety):

| **LangSmith App Attribute** | **Microsoft Entra ID Attribute** | **Matching Precedence** |
| --- | --- | --- |
| `userName` | `userPrincipalName` | |
| `active` | `Not([IsSoftDeleted])` | |
| `emails[type eq "work"].value` | `mail`1 | |
| `name.formatted` | `displayName` OR `Join(" ", [givenName], [surname])`2 | |
| `externalId` | `objectId`3 | 1 |

1. User’s email address must be present in Entra ID.
2. Use the `Join` expression if your `displayName` does not match the format of `Firstname Lastname`.
3. To avoid inconsistency, this should match the SAML NameID assertion and the `sub` OAuth2.0 claim. For SAML SSO in cloud, the `Unique User Identifier (Name ID)` required claim should be `user.objectID` and the `Name identifier format` should be `persistent`.

**Group Attributes**Set **Target Object Actions** to `Create` and `Update` only (start with `Delete` disabled for safety):

| **LangSmith App Attribute** | **Microsoft Entra ID Attribute** | **Matching Precedence** |
| --- | --- | --- |
| `displayName` | `displayName`1 | 1 |
| `externalId` | `objectId` | |
| `members` | `members` | |

1. Groups must follow the naming convention described in the Group Naming Convention section.
If your company has a group naming policy, you should instead map from the `description` Microsoft Entra ID Attribute and
set the description based on the Group Naming Convention section.

**Step 4: Assign Users and Groups**

**Step 5: Enable Provisioning**

1. Set **Provisioning Status** to `On` under **Provisioning**.
2. Monitor the initial sync to ensure users and groups are provisioned correctly.
3. Once verified, enable `Delete` actions for both User and Group mappings.

For troubleshooting, refer to the SAML SSO FAQs. If you have issues setting up SCIM, contact the LangChain support team via support.langchain.com.

#### ​ Okta configuration steps

You must use the Okta Lifecycle Management product. This product tier is required to use SCIM on Okta.

**Supported features**

- Create users
- Update user attributes
- Deactivate users
- Group push ( **without group renaming**)
- Import users
- Import groups

**Step 1: Add application from Okta Integration Network**

If you have already configured SSO login via SAML (cloud) or OAuth2.0 with OIDC (self-hosted), skip this step.

See SAML SSO setup for cloud or OAuth2.0 setup for self-hosted.**Step 2: Configure API Integration**

01. In the General tab, ensure the `LangSmithUrl` is filled in according to the instructions from Step 1
02. In the Provisioning tab, select `Integration`.
03. Select `Edit` then `Enable API integration`.
04. For API Token, paste the SCIM token you generated above.
05. Keep `Import Groups` checked.
06. To verify the configuration, select Test API Credentials.
07. Select Save.
08. After saving the API integration details, new settings tabs appear on the left. Select `To App`.
09. Select Edit.
10. Select the Enable checkbox for Create Users, Update Users, and Deactivate Users.
11. Select Save.
12. Assign users and/or groups in the Assignments tab. Assigned users are created and managed in your LangSmith group.

**Step 3: Configure User Provisioning Settings**

Okta does not support group attributes besides the group name itself, so group name _must_ follow the naming convention described in the Group Naming Convention section.

Follow Okta’s Enable Group Push instructions to configure groups to push by name or by rule.

#### ​ Other Identity Providers

Other identity providers have not been tested but may function depending on their SCIM implementation.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Set up resource tags\\
\\
Previous LangSmith Polly\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/scalability-and-resilience

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Additional resources

Scalability & resilience

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- Server scalability
- Queue scalability
- Resilience
- Postgres resilience
- Redis resilience

LangSmith is designed to scale horizontally with your workload. Each instance of the service is stateless, and keeps no resources in memory. The service is designed to gracefully handle new instances being added or removed, including hard shutdown cases.

## ​ Server scalability

As you add more instances to a service, they will share the HTTP load as long as an appropriate load balancer mechanism is placed in front of them. In most deployment modalities we configure a load balancer for the service automatically. In the “self-hosted without control plane” modality it’s your responsibility to add a load balancer. Since the instances are stateless any load balancing strategy will work, no session stickiness is needed, or recommended. Any instance of the server can communicate with any queue instance (through Redis PubSub), meaning that requests to cancel or stream an in-progress run can be handled by any arbitrary instance.

## ​ Queue scalability

As you add more instances to a service, they will increase run throughput linearly, as each instance is configured to handle a set number of concurrent runs (by default 10). Each attempt for each run will be handled by a single instance, with exactly-once semantics enforced through Postgres’s MVCC model (refer to section below for crash resilience details). Attempts that fail due to transient database errors are retried up to 3 times. We do not make use of long-lived transactions or locks, this enables us to make more efficient use of Postgres resources.

## ​ Resilience

While a run is being handled by a queue instance, a periodic heartbeat timestamp will be recorded in Redis by that queue worker.When a graceful shutdown request is received (SIGINT) an instance enters shutdown mode, which

- stops accepting new HTTP requests
- gives any in-progress runs a limited number of seconds to finish (if not finished it will be put back in the queue)
- stops the instance from picking up more runs from the queue

If a hard shutdown occurs due to a server crash or an infrastructure failure, any runs that were in progress will be picked up by an internal sweeper task that looks for in-progress runs that have breached their heartbeat window. The sweeper runs every 2 minutes and will put the runs back in the queue for another instance to pick them up.

## ​ Postgres resilience

For deployment modalities where we manage the Postgres database, we have periodic backups and continuously replicated standby replicas for automatic failover. This Postgres configuration is available in the Cloud deployment option for `Production` deployment types only.All communication with Postgres implements retries for retry-able errors. If Postgres is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Postgres will render the Agent Server unavailable.

## ​ Redis resilience

All data that requires durable storage is stored in Postgres, not Redis. Redis is used only for ephemeral metadata, and communication between instances. Therefore we place no durability requirements on Redis.All communication with Redis implements retries for retry-able errors. If Redis is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Redis will render the Agent Server unavailable.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Authentication methods\\
\\
Previous Frequently asked questions\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/faq

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Additional resources

Frequently asked questions

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- Observability
- I can’t create API keys or manage users in the UI, what’s wrong?
- How does load balancing/ingress work?
- How can we authenticate to the application?
- Can I use external storage services?
- Does my application need egress to function properly?
- Resource requirements for the application?
- SAML SSO FAQs
- How do I change a SAML SSO user’s email address?
- Can I change identity providers?
- How do I fix “405 method not allowed”?
- SCIM FAQs
- Can I use SCIM without SAML SSO?
- What happens if I have both JIT provisioning and SCIM enabled?
- How do I change a user’s role or workspace access?
- What happens when a user is removed from all groups?
- Can I use custom group names?
- Why is my Okta integration not working?
- Deployment
- Do I need to use LangChain to use LangGraph? What’s the difference?
- How is LangGraph different from other agent frameworks?
- Does LangGraph impact the performance of my app?
- Is LangGraph open source? Is it free?
- How are LangGraph and LangSmith different?
- Is LangSmith open source?
- Does LangGraph work with LLMs that don’t support tool calling?
- Does LangGraph work with OSS LLMs?
- Can I use Studio without logging in to LangSmith?
- What is an Agent Run?

## ​ Observability

### ​ _I can’t create API keys or manage users in the UI, what’s wrong?_

- You have likely deployed LangSmith without setting up SSO. LangSmith requires SSO to manage users and API keys. You can find more information on setting up SSO in the configuration section.

### ​ _How does load balancing/ingress work_?

- You will need to expose the frontend container/service to your applications/users. This will handle routing to all downstream services.
- You will need to terminate SSL at the ingress level. We recommend using a managed service like AWS ALB, GCP Load Balancer, or Nginx.

### ​ _How can we authenticate to the application?_

- Currently, our self-hosted solution supports SSO with OAuth2.0 and OIDC as an authn solution. Note, we do offer a no-auth solution but highly recommend setting up oauth before moving into production.

You can find more information on setting up SSO in the configuration section.

### ​ _Can I use external storage services?_

- You can configure LangSmith to use external versions of all storage services. In a production setting, we strongly recommend using external storage services. Check out the configuration section for more information.

### ​ _Does my application need egress to function properly?_

Our deployment only needs egress for a few things (most of which can reside within your VPC):

- Fetching images (If mirroring your images, this may not be needed)
- Talking to any LLM endpoints
- Talking to any external storage services you may have configured
- Fetching OAuth information
- Subscription Metrics and Operational Metadata (if not running in offline mode) - Requires egress to `https://beacon.langchain.com`
- See Egress for more information

Your VPC can set up rules to limit any other access. Note: We require the `X-Organization-Id` and `X-Tenant-Id` headers to be allowed to be passed through to the backend service. These are used to determine which organization and workspace (previously called “tenant”) the request is for.

### ​ _Resource requirements for the application?_

- In kubernetes, we recommend a minimum helm configuration which can be found in here. For docker, we recommend a minimum of 16GB of RAM and 4 CPUs.
- For Postgres, we recommend a minimum of 8GB of RAM and 2 CPUs.
- For Redis, we recommend 4GB of RAM and 2 CPUs.
- For Clickhouse, we recommend 32GB of RAM and 8 CPUs.

#### ​ _How do I change a SAML SSO user’s email address?_

Some identity providers retain the original `User ID` through an email change while others do not, so we recommend that you follow these steps to avoid duplicate users in LangSmith:

1. Remove the user from the organization (see here)
2. Change their email address in the IdP
3. Have them login to LangSmith again via SAML SSO - this will trigger the usual JIT provisioning flow with their new email address

Changing email address via SCIM or otherwise is not currently supported for users with multiple linked login methods. This error message is shown: `email update not supported with linked login methods`. For example, if a user previously logged in via email/password or Google social login, and then is added with the same email address via SSO, changing their email address is not supported. This applies to both self-hosted and cloud.

#### ​ _Can I change identity providers?_

Reach out to the LangChain support team through our portal at for support on migration.

#### ​ _How do I fix “405 method not allowed”?_

Ensure you’re using the correct ACS URL:

#### ​ _Can I use SCIM without SAML SSO?_

- **Cloud**: No, SAML SSO is required for SCIM in cloud deployments
- **Self-hosted**: Yes, SCIM works with OAuth with Client Secret authentication mode

#### ​ _What happens if I have both JIT provisioning and SCIM enabled?_

JIT provisioning and SCIM can conflict with each other. We recommend disabling JIT provisioning before enabling SCIM to ensure consistent user provisioning behavior.

#### ​ _How do I change a user’s role or workspace access?_

Update the user’s group membership in your IdP. The changes will be synchronized to LangSmith according to the role precedence rules.

#### ​ _What happens when a user is removed from all groups?_

The user will be deprovisioned from your LangSmith organization according to your IdP’s deprovisioning settings.

#### ​ _Can I use custom group names?_

Yes. If your identity provider supports syncing alternate fields to the `displayName` group attribute, you may use an alternate attribute (like `description`) as the `displayName` in LangSmith and retain full customizability of the identity provider group name. Otherwise, groups must follow the specific naming convention described in the Group Naming Convention section to properly map to LangSmith roles and workspaces.

#### ​ _Why is my Okta integration not working?_

See Okta’s troubleshooting guide here:

## ​ Deployment

### ​ Do I need to use LangChain to use LangGraph? What’s the difference?

No. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.

### ​ How is LangGraph different from other agent frameworks?

Other agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company’s needs. LangGraph provides a more expressive framework to handle companies’ unique tasks without restricting users to a single black-box cognitive architecture.

### ​ Does LangGraph impact the performance of my app?

LangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.

### ​ Is LangGraph open source? Is it free?

Yes. LangGraph is an MIT-licensed open-source library and is free to use.

### ​ How are LangGraph and LangSmith different?

LangGraph is a stateful, orchestration framework that brings added control to agent workflows. LangSmith is a service for deploying and scaling agentic applications, with an opinionated API for building agent UXs, plus an integrated developer UI.

| Features | LangGraph (open source) | LangSmith |
| --- | --- | --- |
| Description | Stateful orchestration framework for agentic applications | Scalable infrastructure for deploying LangGraph applications |
| SDKs | Python and JavaScript | Python and JavaScript |
| HTTP APIs | None | Yes - useful for retrieving & updating state or long-term memory, or creating a configurable assistant |
| Streaming | Basic | Dedicated mode for token-by-token messages |
| Checkpointer | Community contributed | Supported out-of-the-box |
| Persistence Layer | Self-managed | Managed Postgres with efficient storage |

| Scalability | Self-managed | Auto-scaling of task queues and servers |
| Fault-tolerance | Self-managed | Automated retries |
| Concurrency Control | Simple threading | Supports double-texting |
| Scheduling | None | Cron scheduling |
| Monitoring | None | Integrated with LangSmith for observability |
| IDE integration | Studio | Studio |

### ​ Is LangSmith open source?

No. LangSmith is proprietary software.There is a free, self-hosted version of LangSmith with access to basic features. The Cloud deployment option and the Self-Hosted deployment options are paid services. Contact our sales team to learn more.For more information, see our LangSmith pricing page.

### ​ Does LangGraph work with LLMs that don’t support tool calling?

Yes! You can use LangGraph with any LLMs. The main reason we use LLMs that support tool calling is that this is often the most convenient way to have the LLM make its decision about what to do. If your LLM does not support tool calling, you can still use it - you just need to write a bit of logic to convert the raw LLM string response to a decision about what to do.

### ​ Does LangGraph work with OSS LLMs?

Yes! LangGraph is totally ambivalent to what LLMs are used under the hood. The main reason we use closed LLMs in most of the tutorials is that they seamlessly support tool calling, while OSS LLMs often don’t. But tool calling is not necessary (see this section) so you can totally use LangGraph with OSS LLMs.

### ​ Can I use Studio without logging in to LangSmith?

Yes! You can use the development version of Agent Server to run the backend locally.
This will connect to the Studio frontend hosted as part of LangSmith.
If you set an environment variable of `LANGSMITH_TRACING=false`, then no traces will be sent to LangSmith.

### ​ What is an Agent Run?

An Agent Run is one end-to-end invocation of a LangGraph agent deployed via LangSmith Deployment. Nodes and subgraphs are not charged separately. Calls to other LangGraph agents (through RemoteGraph or the LangGraph SDK or the API directly) are charged separately, to the deployment that hosts the agent being called. An interrupt for human-in-the-loop creates a separate Agent Run when resuming.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Scalability & resilience\\
\\
Previous Regions FAQ\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/regions-faq

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Additional resources

Regions FAQ

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- Legal and compliance
- What privacy and data protection frameworks does LangSmith, including its EU instance, comply with?
- My company isn’t based in the EU, can I still have my data hosted there?
- Do you have a legal entity in the EU that we can contract with?
- Do different legal terms apply if I choose the EU region?
- Features
- How do I use the EU instance?
- Are there any functional differences between US and EU cloud-managed LangSmith?
- Can an organization have workspaces in different regions?
- Can I connect an EU organization to a US organization and share billing?
- What data will be stored in my selected region?
- How can I see my organization’s region?
- Can I switch my organization from the US to EU or vice versa?
- Plans and pricing
- Is the EU region available on all LangSmith plans?
- Is pricing different for the EU region compared to the US region?
- What currency is used for payment if I use the EU region?

See the cloud architecture reference for additional details.

## ​ Legal and compliance

#### ​ _What privacy and data protection frameworks does LangSmith, including its EU instance, comply with?_

LangSmith complies with the General Data Protection Regulation (GDPR) and other laws and regulations applicable to the LangSmith service. We are also SOC 2 Type 2 certified and are HIPAA compliant. You can request more information about our security policies and posture at trust.langchain.com. If you would like to sign a Data Processing Addendum (DPA) with us, please contact support via support.langchain.com. Please note we only enter into Business Associate Agreements (BAAs) with customers on our Enterprise plan.

#### ​ _My company isn’t based in the EU, can I still have my data hosted there?_

Yes, you can host your LangSmith data in the EU instance independent of your location.

#### ​ _Do you have a legal entity in the EU that we can contract with?_

We do not have a legal entity in the EU for customer contracting today.

#### ​ _Do different legal terms apply if I choose the EU region?_

The terms are the same for the EU and US regions.

## ​ Features

#### ​ _How do I use the EU instance?_

Follow the instructions here to create an account and an API key (make sure to change the region to EU in the dropdown)

#### ​ _Are there any functional differences between US and EU cloud-managed LangSmith?_

There may be a small delay between launches to each region depending on the feature. Besides that, they are functionally equivalent - all features supported in the US are supported in the EU and vice versa.

#### ​ _Can an organization have workspaces in different regions?_

LangSmith does not support this at the moment, but if you are interested, please contact support via support.langchain.com and share your use case.

#### ​ _Can I connect an EU organization to a US organization and share billing?_

#### ​ _What data will be stored in my selected region?_

See the cloud architecture reference for details.

#### ​ _How can I see my organization’s region?_

Check your URL - any organizations on are in the EU, and any on are in the US.

#### ​ _Can I switch my organization from the US to EU or vice versa?_

We do not support migration between regions at this time, but if you are interested in this feature, please contact support via support.langchain.com.

## ​ Plans and pricing

#### ​ _Is the EU region available on all LangSmith plans?_

Yes, you can sign up for the EU region on all plans including free plans.

#### ​ _Is pricing different for the EU region compared to the US region?_

No, pricing is the same for the EU and US regions.

#### ​ _What currency is used for payment if I use the EU region?_

All LangSmith plans are paid in USD.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Frequently asked questions\\
\\
Previous Frequently Asked Questions\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/pricing-faq

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Additional resources

Frequently Asked Questions

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

On this page

- Questions and Answers
- I’ve been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?
- Which plan is right for me?
- What is a seat?
- What is a trace?
- What is an Agent Run?
- What is an ingested event?
- I’ve hit my rate or usage limits. What can I do?
- I have a developer account, can I upgrade my account to the Plus or Enterprise plan?
- How does billing work?
- Can I limit how much I spend on tracing?
- How can I track my usage so far this month?
- I have a question about my bill…
- What can I expect from Support?
- Where is my data stored?
- Which security frameworks is LangSmith compliant with?
- Will you train on the data that I send LangSmith?

## ​ Questions and Answers

### ​ I’ve been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?

If you’ve been using LangSmith already, your usage will be billable starting in July 2024. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by contacting our sales team.

### ​ Which plan is right for me?

If you’re an individual developer, the Developer plan is a great choice for small projects.For teams that want to collaborate in LangSmith, check out the Plus plan. **If you are an early-stage startup building an AI application**, you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our Startup Contact Form for more details.If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our Sales Contact Form for more details.

### ​ What is a seat?

A seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill.

### ​ What is a trace?

A trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an example of a single trace.

### ​ What is an Agent Run?

An Agent Run is one end-to-end invocation of a LangGraph agent deployed via LangSmith Deployment. Nodes and subgraphs within a single agent execution are not charged separately. Calls to other LangGraph agents (through RemoteGraph or the LangGraph SDK or the API directly) are charged separately, to the deployment that hosts the agent being called. An interrupt for human-in-the-loop creates a separate Agent Run when resuming.Agent Runs are billed at $0.005 each. For high-volume usage, please contact our sales team to discuss custom pricing options.

### ​ What is an ingested event?

An ingested event is any distinct, trace-related data sent to LangSmith. This includes:

- Inputs, outputs and metadata sent at the start of a run step within a trace
- Inputs, outputs and metadata sent at the end of a run step within a trace
- Feedback on run steps or traces

### ​ I’ve hit my rate or usage limits. What can I do?

When you first sign up for a LangSmith account, you get a Personal organization that is limited to 5000 monthly traces. To continue sending traces after reaching this limit, upgrade to the Developer or Plus plans by adding a credit card. Head to Plans and Billing to upgrade.Similarly, if you’ve hit the rate limits on your current plan, you can upgrade to a higher plan to get higher limits, or contact support via support.langchain.com with questions.

### ​ I have a developer account, can I upgrade my account to the Plus or Enterprise plan?

Yes, Developer plan users can easily upgrade to the Plus plan on the Plans and Billing page. For the Enterprise plan, please contact our sales team to discuss your needs.

### ​ How does billing work?

**Seats**Seats are billed monthly on the first of the month. Additional seats purchased mid-month are pro-rated and billed within one day of the purchase. Seats removed mid-month will not be credited.**Traces**As long as you have a card on file in your account, we’ll service your traces and bill you on the first of the month for traces that you submitted in the previous month. You will be able to set usage limits if you so choose to limit the maximum charges you could incur in any given month.

### ​ Can I limit how much I spend on tracing?

You can set limits on the number of traces that can be sent to LangSmith per month on the Usage configuration page.

While we do show you the dollar value of your usage limit for convenience, this limit evaluated in terms of number of traces instead of dollar amount. For example, if you are approved for our startup plan tier where you are given a generous allotment of free traces, your usage limit will not automatically change.You are not currently able to set a spend limit in the product.

### ​ How can I track my usage so far this month?

Under the Settings section for your Organization you will see subsection for **Usage**. There, you will be able to see a graph of the daily number of billable LangSmith traces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail your actual number of runs slightly for the current day.

### ​ I have a question about my bill…

Customers on the Developer and Plus plan tiers should contact support via support.langchain.com. Customers on the Enterprise plan should contact their sales representative directly.Enterprise plan customers are billed annually by invoice.

### ​ What can I expect from Support?

On the Developer plan, community-based support is available on LangChain community Slack.On the Plus plan, you will also receive preferential support via support.langchain.com for LangSmith-related questions only and we’ll do our best to respond within the next business day.On the Enterprise plan, you’ll get white-glove support with a Slack channel, a dedicated customer success manager, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, we’ll also support deployments and new releases with our infra engineering team on-call.

### ​ Where is my data stored?

You may choose to sign up in either the US or EU region. See the cloud architecture reference for more details. If you’re on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment.

### ​ Which security frameworks is LangSmith compliant with?

We are SOC 2 Type II, GDPR, and HIPAA compliant.You can request more information about our security policies and posture at trust.langchain.com. Please note we only enter into BAAs with customers on our Enterprise plan.

### ​ Will you train on the data that I send LangSmith?

We will not train on your data, and you own all rights to your data. See LangSmith for more information.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Regions FAQ\\
\\
Previous LangSmith status\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/quick-start-studio

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Studio

Get started with Studio

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Deployed graphs
- Local development server
- Prerequisites
- Setup
- (Optional) Attach a debugger
- Next steps

Studio in the LangSmith Deployment UI supports connecting to two types of graphs:

- Graphs deployed on cloud or self-hosted.
- Graphs running locally with Agent Server.

## ​ Deployed graphs

Studio is accessed in the LangSmith UI from the **Deployments** navigation.For applications that are deployed, you can access Studio as part of that deployment. To do so, navigate to the deployment in the UI and select **Studio**.This will load Studio connected to your live deployment, allowing you to create, read, and update the threads, assistants, and memory in that deployment.

## ​ Local development server

### ​ Prerequisites

To test your application locally using Studio:

- Follow the local application quickstart first.
- If you don’t want data traced to LangSmith, set `LANGSMITH_TRACING=false` in your application’s `.env` file. With tracing disabled, no data leaves your local server.

### ​ Setup

1. Install the LangGraph CLI:

pip

uv

npm

Copy

pip install -U "langgraph-cli[inmem]"
langgraph dev

**Browser Compatibility**
Safari blocks `localhost` connections to Studio. To work around this, run the command with `--tunnel` to access Studio via a secure tunnel.

This will start the Agent Server locally, running in-memory. The server will run in watch mode, listening for and automatically restarting on code changes. Read this reference to learn about all the options for starting the API server.You will see the following logs:

>
> - API:
>
> - Docs:
>
> - LangSmith Studio Web UI:

Once running, you will automatically be directed to Studio.
2. For a running server, access the Dbugger with one of the following:

1. Directly navigate to the following URL: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.
2. Navigate to **Deployments** in the UI, click the **Studio** button on a deployment, enter `http://127.0.0.1:2024` and click **Connect**.

If running your server at a different host or port, update the `baseUrl` to match.

### ​ (Optional) Attach a debugger

For step-by-step debugging with breakpoints and variable inspection, run the following:

# Install debugpy package
pip install debugpy
# Start server with debugging enabled
langgraph dev --debug-port 5678

Then attach your preferred debugger:

- VS Code

- PyCharm

Add this configuration to `launch.json`:

{
"name": "Attach to LangGraph",
"type": "debugpy",
"request": "attach",
"connect": {
"host": "0.0.0.0",
"port": 5678
}
}

1. Go to Run → Edit Configurations
2. Click + and select “Python Debug Server”
3. Set IDE host name: `localhost`
4. Set port: `5678` (or the port number you chose in the previous step)
5. Click “OK” and start debugging

For issues getting started, refer to the troubleshooting guide.

## ​ Next steps

For more information on how to run Studio, refer to the following guides:

- Run application
- Manage assistants
- Manage threads
- Iterate on prompts
- Debug LangSmith traces
- Add node to dataset

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Studio\\
\\
Previous How to use Studio\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/prompt-engineering-quickstart

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Prompt engineering quickstart

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- Prerequisites
- 1\. Set workspace secret
- 2\. Create a prompt
- 3\. Test a prompt
- 4\. Iterate on a prompt
- Next steps
- Video guide

Prompts guide the behavior of Large Language Models (LLM). _Prompt engineering_ is the process of crafting, testing, and refining the instructions you give to an LLM so it produces reliable and useful responses.LangSmith provides tools to create, version, test, and collaborate on prompts. You’ll also encounter common concepts like _prompt templates_, which let you reuse structured prompts, and _variables_, which allow you to dynamically insert values (such as a user’s question) into a prompt.In this quickstart, you’ll create, test, and improve prompts using either the UI or the SDK. This quickstart will use OpenAI as the example LLM provider, but the same workflow applies across other providers.

If you prefer to watch a video on getting started with prompt engineering, refer to the quickstart Video guide.

## ​ Prerequisites

Before you begin, make sure you have:

- **A LangSmith account**: Sign up or log in at smith.langchain.com.
- **A LangSmith API key**: Follow the Create an API key guide.
- **An OpenAI API key**: Generate this from the OpenAI dashboard.

Select the tab for UI or SDK workflows:

- UI

- SDK

## ​ 1\. Set workspace secret

In the LangSmith UI, ensure that your OpenAI API key is set as a workspace secret.

1. Navigate to **Settings** and then move to the **Secrets** tab.
2. Select **Add secret** and enter the `OPENAI_API_KEY` and your API key as the **Value**.
3. Select **Save secret**.

When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.

## ​ 2\. Create a prompt

1. In the LangSmith UI, navigate to the **Prompts** section in the left-hand menu.
2. Click on **\+ Prompt** to create a prompt.
3. Modify the prompt by editing or adding prompts and input variables as needed.

## ​ 3\. Test a prompt

1. Under the **Prompts** heading select the gear icon next to the model name, which will launch the **Prompt Settings** window on the **Model Configuration** tab.
2. Set the model configuration you want to use. The **Provider** and **Model** you select will determine the parameters that are configurable on this configuration page. Once set, click **Save as**.

3. Specify the input variables you would like to test in the **Inputs** box and then click **Start**.

To learn about more options for configuring your prompt in the Playground, refer to Configure prompt settings.
4. After testing and refining your prompt, click **Save** to store it for future use.

## ​ 4\. Iterate on a prompt

LangSmith allows for team-based prompt iteration. Workspace members can experiment with prompts in the playground and save their changes as a new _commit_ when ready.To improve your prompts:

- Reference the documentation provided by your model provider for best practices in prompt creation, such as: - Best practices for prompt engineering with the OpenAI API
- Gemini’s Introduction to prompt design
- Build and refine your prompts with the Prompt Canvas—an interactive tool in LangSmith. Learn more in the Prompt Canvas guide.
- Tag specific commits to mark important moments in your commit history.

1. To create a commit, navigate to the **Playground** and select **Commit**. Choose the prompt to commit changes to and then **Commit**.
2. Navigate to **Prompts** in the left-hand menu. Select the prompt. Once on the prompt’s detail page, move to the **Commits** tab. Find the tag icon to **Add a Commit Tag**.

## ​ 1\. Set up your environment

1. In your terminal, prepare your environment:

Python

TypeScript

Copy

mkdir ls-prompt-quickstart && cd ls-prompt-quickstart
python -m venv .venv
source .venv/bin/activate
pip install -qU langsmith openai langchain_core

2. Set your API keys:

To create a prompt, you’ll define a list of messages that you want in your prompt and then push to LangSmith.Use the language-specific constructor and push method:

- Python: `ChatPromptTemplate` → `client.push_prompt(...)`
- TypeScript: `ChatPromptTemplate.fromMessages(...)` → `client.pushPrompt(...)`

1. Add the following code to a `create_prompt` file:

from langsmith import Client
from langchain_core.prompts import ChatPromptTemplate

client = Client()

prompt = ChatPromptTemplate([\
("system", "You are a helpful chatbot."),\
("user", "{question}"),\
])

client.push_prompt("prompt-quickstart", object=prompt)

This creates an ordered list of messages, wraps them in `ChatPromptTemplate`, and then pushes the prompt by name to your workspace for versioning and reuse.
2. Run `create_prompt`:

python create_prompt.py

Follow the resulting link to view the newly created Prompt Hub prompt in the LangSmith UI.

In this step, you’ll pull the prompt you created in step 2 by name (`"prompt-quickstart"`), format it with a test input, convert it to OpenAI’s chat format, and call the OpenAI Chat Completions API.Then, you’ll iterate on the prompt by creating a new version. Members of your workspace can open an existing prompt, experiment with changes in the UI, and save those changes as a new commit on the same prompt, which preserves history for the whole team.

1. Add the following to a `test_prompt` file:

from langsmith import Client
from openai import OpenAI
from langchain_core.messages import convert_to_openai_messages

client = Client()
oai_client = OpenAI()

prompt = client.pull_prompt("prompt-quickstart")

# Since the prompt only has one variable you could also pass in the value directly
# Equivalent to formatted_prompt = prompt.invoke("What is the color of the sky?")
formatted_prompt = prompt.invoke({"question": "What is the color of the sky?"})

response = oai_client.chat.completions.create(
model="gpt-4o",
messages=convert_to_openai_messages(formatted_prompt.messages),
)

2. Run `test_prompt` :

python test_prompt.py

3. To create a new version of a prompt, call the same push method you used initially with the same prompt name and your updated template. LangSmith will record it as a new commit and preserve prior versions.Copy the following code to an `iterate_prompt` file:

new_prompt = ChatPromptTemplate([\
("system", "You are a helpful chatbot. Respond in Spanish."),\
("user", "{question}"),\
])

client.push_prompt("prompt-quickstart", object=new_prompt)

4. Run `iterate_prompt` :

python iterate_prompt.py

Now your prompt will contain two commits.

To improve your prompts:

- Reference the documentation provided by your model provider for best practices in prompt creation, such as:
- Best practices for prompt engineering with the OpenAI API
- Gemini’s Introduction to prompt design
- Build and refine your prompts with the Prompt Canvas—an interactive tool in LangSmith. Learn more in the Prompt Canvas guide.

## ​ Next steps

- Learn more about how to store and manage prompts using the Prompt Hub in the Create a prompt guide.
- Learn how to set up the Playground to Test multi-turn conversations in this tutorial.
- Learn how to test your prompt’s performance over a dataset instead of individual examples, refer to Run an evaluation from the Prompt Playground.

Use **Polly** in the Playground to help optimize your prompts, generate tools, and create output schemas.

## ​ Video guide

Getting Started with LangSmith (4/8): Playground & Prompts - YouTube

Photo image of LangChain

LangChain

165K subscribers

Getting Started with LangSmith (4/8): Playground & Prompts

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Why am I seeing this?

Watch on

0:00

0:00 / 7:59

•Live

•

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Prompt engineering\\
\\
Previous Prompt engineering concepts\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/pricing-plans

Logo that links to Homepage

## Compare plans

Features

Developer

Plus

Enterprise

Pricing

Users

Maximum of 1 seat

(free)

Up to 10 seats

$39 per seat/month

Custom

5k base traces / mo included

Pay as you go thereafter ⓘ

10k base traces / mo included

See hourly trace ingestion and trace event limits here.

Agent runs

(Deployment)

N/A

1 free Dev deployment with unlimited agent runs included.

For additional deployments:

$0.005 / agent run

Uptime cost

$0.0007 / min per Development deployment

$0.0036 / min per Production deployment

LangSmith Observability & Evaluation:

Debug, monitor, and improve your AI applications with tracing and evals

Tracing

Monitoring

Insights (beta)

Online and offline evals

Dataset collection

Annotation queue (human feedback)

Prompt Hub and Playground

Bulk data export

LangSmith Deployment:

Deploy and manage long-running agents with our purpose-built infrastructure

LangSmith Studio

Expose agent as MCP server

Real-time streaming of intermediary steps and final output

Agent Authorization (beta)

1-Click Deploy

Horizontally-scalable service for production-sized deployments

Assistants API

30+ API endpoints including state and memory

Cron scheduling

Authentication & authorization for LangGraph APIs

Hosting options:

Choose where to host the LangSmith platform

Platform hosting option(s)

Cloud

Cloud, Hybrid, or Self-Hosted

Infra

Fully managed by LangChain

Cloud: Fully managed by LangChain

Hybrid: SaaS control plane, Self-hosted data plane

Self-Hosted: Fully self-managed

Data location

LangChain's Cloud (US or EU)

Cloud: LangChain's Cloud (US or EU)

Hybrid: LangChain's Cloud (US or EU)

Self-Hosted: Your VPC

Security Controls

SSO

Google, GitHub

Custom SSO

Role-Based Access Control

Organization Roles (User and Admin)

Support

Community Slack

Email Support

Team trainings

Architectural guidance for your applications

Access to deployed engineers

SLA

Procurement

Billing

Monthly, self-serve

Annual invoice

Custom Terms

Infosec Review

## FAQs

#### General Questions

Which plan is right for me?

Our Developer plan is a great choice for personal projects. You will have 1 free seat with access to LangSmith (5k base traces/month included).

‍

The Plus plan is for teams that want to self-serve with moderate usage and collaboration needs. You can purchase up to 10 seats with access to LangSmith (10k base traces/month included). You will be able to ship agents with our managed LangSmith Deployment service, with 1 free dev-sized deployment included.

The Enterprise plan is for teams that need more advanced administration, security, support, or deployment options. Contact our sales team to learn more.

Do you offer a plan for startups?

Yes! We offer a Startup Plan for LangSmith, designed for early-stage companies building agentic applications. You’ll get discounted rates and generous free trace allotments to build with confidence from day one.

‍ Apply here to get started with startup pricing. Customers can stay on the Startup Plan for 1 year before graduating to the Plus Plan.

When will I be billed?

For the Developer or Plus Plan, seats are billed monthly on the 1st or pro-rated if added mid-month (no credit for removed seats); traces are billed monthly in arrears for your usage. Enterprise plans are invoiced annually upfront.

Will you train on the data that I send to LangSmith?

We will not train on your data, and you own all rights to your data. See our [](https://www.langchain.com/terms-of-service) for more information.

#### LangSmith Observability & Evaluation Questions

What is a trace? Can it contain multiple events?

A trace represents a single execution of your application—whether it’s an agent, evaluator, or playground session. It can include many individual steps, such as LLM calls and other tracked events. Here's an example of a single trace.

What is the difference between a base trace and an extended trace?

Base traces have a shorter retention period of 14 days and cost $0.50 per 1k traces. Extended traces have a longer retention period of 400 days and cost $5.00 per 1k traces. You can "upgrade" base traces to extended traces at $4.50 per 1k traces.

Why might I want to upgrade a base trace to an extended trace?

Base traces are short-lived (14-day retention) and ideal for quick debugging or ad-hoc analysis. They’re priced for volume and short-term utility.

Extended traces, on the other hand, are retained for 400 days and often include valuable feedback—whether from users, evaluators, or human labelers. This feedback makes them essential for ongoing improvement and model tuning (higher utility), which comes with a higher price point.

LangSmith lets you choose the right retention for each trace, helping you balance cost and value.

Why would I upgrade a base trace to an extended trace?

#### LangSmith Deployment Questions

Does LangSmith Deployment include any free deployments?

Plus plans include 1 free dev-sized deployment. If you spin up additional dev-sized or production-sized deployments, you’ll be charged by usage (agent runs and uptime).

For my free Dev agent deployment for LangSmith Deployment, is there a cap on the number of agent runs?

If you’re on the Plus plan, you get 1 free dev-sized agent deployment – all usage in this deployment will be free no matter how many agent runs you execute.

How do you define agent runs?

An agent run is one end-to-end invocation of a LangGraph agent deployed via LangSmith Deployment. Nodes and subgraphs within a single agent execution are not charged separately. Calls to other LangGraph agents (through RemoteGraph or the LangGraph SDK or the API directly) are charged separately, to the deployment that hosts the agent being called. An interrupt for human-in-the-loop creates a separate agent run when resuming.

What does uptime mean for LangSmith Deployment usage?

Uptime is the duration your deployment’s database is live and persisting state. Uptime will be tracked as soon as your deployment is live and ends when you shut it down. Dev agent deployments are typically short-lived (used during iteration, then deleted) – whereas Production agent deployments stay live and are updated via revisions (rather than being deleted).

When should I use a dev-sized deployment vs. a production-sized agent deployment?

We recommend using the production-sized deployment for any customer-facing agent. Dev-sized agent deployments are intended for testing and do not support horizontal scaling, backups, or performance optimizations needed in production.

## Ready to start shipping  reliable agents faster?

Get started with tools from the LangChain product suite for every step of the agent development lifecycle.

Talk to sales Try LangSmith

---

# https://docs.langchain.com/langsmith/create-account-api-key)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

Create an account and API key Control plane API reference for LangSmith Deployment LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/administration-overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

Self-hosted LangSmith LangSmith Deployment LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/set-up-hierarchy)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

Set up hierarchy Set up LangSmith Set up hybrid LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/manage-organization-by-api)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

Manage your organization using the API Control plane API reference for LangSmith Deployment Self-hosted LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/billing)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith docs LangSmith Deployment Self-hosted LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/set-up-resource-tags)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

Set up resource tags Overview Set up LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/user-management)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

User management Customize user management Self-hosted LangSmith changelog

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/polly)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Polly LangSmith docs LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/scalability-and-resilience)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

Scalability & resilience LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/faq)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Fetch Self-hosted LangSmith LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/regions-faq)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Fetch Self-hosted LangSmith LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/pricing-faq)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

Self-hosted LangSmith LangSmith Fetch LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/overview).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK LangGraph overview LangSmith reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deployments).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment App development in LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/quick-start-studio)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Studio

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/prompt-engineering-quickstart)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Prompt engineering quickstart Tracing quickstart Evaluation quickstart

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/pricing-plans)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

Self-hosted LangSmith Self-hosted LangSmith changelog LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/install

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Install LangChain

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

To install the LangChain package:

npm

pnpm

yarn

bun

Copy

npm install langchain @langchain/core
# Requires Node.js 20+

LangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages.

# Installing the OpenAI integration
npm install @langchain/openai
# Installing the Anthropic integration
npm install @langchain/anthropic

See the Integrations tab for a full list of available integrations.

Now that you have LangChain installed, you can get started by following the Quickstart guide.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangChain overview\\
\\
Previous Quickstart\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/quickstart

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Quickstart

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Requirements
- Build a basic agent
- Build a real-world agent

This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes.

**LangChain Docs MCP server**If you’re using an AI coding assistant or IDE (e.g. Claude Code or Cursor), you should install the LangChain Docs MCP server to get the most out of it. This ensures your agent has access to up-to-date LangChain documentation and examples.

## ​ Requirements

For these examples, you will need to:

- Install the LangChain package
- Set up a Claude (Anthropic) account and get an API key
- Set the `ANTHROPIC_API_KEY` environment variable in your terminal

Although these examples use Claude, you can use any supported model by changing the model name in the code and setting up the appropriate API key.

## ​ Build a basic agent

Start by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.

Copy

import { createAgent, tool } from "langchain";
import * as z from "zod";

const getWeather = tool(

{
name: "get_weather",
description: "Get the weather for a given city",
schema: z.object({
city: z.string().describe("The city to get the weather for"),
}),
}
);

const agent = createAgent({
model: "claude-sonnet-4-5-20250929",
tools: [getWeather],
});

console.log(
await agent.invoke({
messages: [{ role: "user", content: "What's the weather in Tokyo?" }],
})
);

To learn how to trace your agent with LangSmith, see the LangSmith documentation.

## ​ Build a real-world agent

Next, build a practical weather forecasting agent that demonstrates key production concepts:

1. **Detailed system prompts** for better agent behavior
2. **Create tools** that integrate with external data
3. **Model configuration** for consistent responses
4. **Structured output** for predictable results
5. **Conversational memory** for chat-like interactions
6. **Create and run the agent** create a fully functional agent

Let’s walk through each step:

1

Define the system prompt

The system prompt defines your agent’s role and behavior. Keep it specific and actionable:

const systemPrompt = `You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;

2

Create tools

Tools are functions your agent can call. Oftentimes tools will want to connect to external systems, and will rely on runtime configuration to do so. Notice here how the `getUserLocation` tool does exactly that:

import { tool, type ToolRuntime } from "langchain";
import * as z from "zod";

{
name: "get_weather_for_location",
description: "Get the weather for a given city",
schema: z.object({
city: z.string().describe("The city to get the weather for"),
}),
}
);

const getUserLocation = tool(

const { user_id } = config.context;
return user_id === "1" ? "Florida" : "SF";
},
{
name: "get_user_location",
description: "Retrieve user information based on user ID",
}
);

Zod is a library for validating and parsing pre-defined schemas. You can use it to define the input schema for your tools to make sure the agent only calls the tool with the correct arguments.Alternatively, you can define the `schema` property as a JSON schema object. Keep in mind that JSON schemas **won’t** be validated at runtime.

Example: Using JSON schema for tool input

{
name: "get_weather_for_location",
description: "Get the weather for a given city",
schema: {
type: "object",
properties: {
city: {
type: "string",
description: "The city to get the weather for"
}
},
required: ["city"]
},
}
);

3

Configure your model

Set up your language model with the right parameters for your use case:

import { initChatModel } from "langchain";

const model = await initChatModel(
"claude-sonnet-4-5-20250929",
{ temperature: 0.5, timeout: 10, maxTokens: 1000 }
);

Depending on the model and provider chosen, initialization parameters may vary; refer to their reference pages for details.

4

Define response format

Optionally, define a structured response format if you need the agent responses to match
a specific schema.

const responseFormat = z.object({
punny_response: z.string(),
weather_conditions: z.string().optional(),
});

5

Add memory

Add memory to your agent to maintain state across interactions. This allows
the agent to remember previous conversations and context.

import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

In production, use a persistent checkpointer that saves to a database.
See Add and manage memory for more details.

6

Create and run the agent

Now assemble your agent with all the components and run it!

import { createAgent } from "langchain";

const agent = createAgent({
model: "claude-sonnet-4-5-20250929",
systemPrompt: systemPrompt,
tools: [getUserLocation, getWeather],
responseFormat,
checkpointer,
});

// `thread_id` is a unique identifier for a given conversation.
const config = {
configurable: { thread_id: "1" },
context: { user_id: "1" },
};

const response = await agent.invoke(
{ messages: [{ role: "user", content: "what is the weather outside?" }] },
config
);
console.log(response.structuredResponse);
// {
// punny_response: "Florida is still having a 'sun-derful' day ...",
// weather_conditions: "It's always sunny in Florida!"
// }

// Note that we can continue the conversation using the same `thread_id`.
const thankYouResponse = await agent.invoke(
{ messages: [{ role: "user", content: "thank you!" }] },
config
);
console.log(thankYouResponse.structuredResponse);
// {
// punny_response: "You're 'thund-erfully' welcome! ...",
// weather_conditions: undefined
// }

Show Full example code

import { createAgent, tool, initChatModel, type ToolRuntime } from "langchain";
import { MemorySaver } from "@langchain/langgraph";
import * as z from "zod";

// Define system prompt
const systemPrompt = `You are an expert weather forecaster, who speaks in puns.

// Define tools
const getWeather = tool(

{
name: "get_weather_for_location",
description: "Get the weather for a given city",
schema: z.object({
city: z.string(),
}),
}
);

const { user_id } = config.context;
return user_id === "1" ? "Florida" : "SF";
},
{
name: "get_user_location",
description: "Retrieve user information based on user ID",
schema: z.object({}),
}
);

// Configure model
const model = await initChatModel(
"claude-sonnet-4-5-20250929",
{ temperature: 0 }
);

// Define response format
const responseFormat = z.object({
punny_response: z.string(),
weather_conditions: z.string().optional(),
});

// Set up memory
const checkpointer = new MemorySaver();

// Create agent
const agent = createAgent({
model,
systemPrompt,
responseFormat,
checkpointer,
tools: [getUserLocation, getWeather],
});

// Run agent
// `thread_id` is a unique identifier for a given conversation.
const config = {
configurable: { thread_id: "1" },
context: { user_id: "1" },
};

const response = await agent.invoke(
{ messages: [{ role: "user", content: "what is the weather outside?" }] },
config
);
console.log(response.structuredResponse);
// {
// punny_response: "Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!",
// weather_conditions: "It's always sunny in Florida!"
// }

// Note that we can continue the conversation using the same `thread_id`.
const thankYouResponse = await agent.invoke(
{ messages: [{ role: "user", content: "thank you!" }] },
config
);
console.log(thankYouResponse.structuredResponse);
// {
// punny_response: "You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!",
// weather_conditions: undefined
// }

Congratulations! You now have an AI agent that can:

- **Understand context** and remember conversations
- **Use multiple tools** intelligently
- **Provide structured responses** in a consistent format
- **Handle user-specific information** through context
- **Maintain conversation state** across interactions

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Install LangChain\\
\\
Previous Changelog\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/philosophy

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Philosophy

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

LangChain is driven by a few core beliefs:

- Large Language Models (LLMs) are great, powerful new technology.
- LLMs are even better when you combine them with external sources of data.
- LLMs will transform what the applications of the future look like. Specifically, the applications of the future will look more and more agentic.
- It is still very early on in that transformation.
- While it’s easy to build a prototype of those agentic applications, it’s still really hard to build agents that are reliable enough to put into production.

With LangChain, we have two core focuses:

1

We want to enable developers to build with the best models.

Different providers expose different APIs, with different model parameters and different message formats.
Standardizing these model inputs and outputs is a core focus, making it easy for developer to easily change to the most recent state-of-the-art model, avoiding lock-in.

2

We want to make it easy to use models to orchestrate more complex flows that interact with other data and computation.

Models should be used for more than just _text generation_ \- they should also be used to orchestrate more complex flows that interact with other data. LangChain makes it easy to define tools that LLMs can use dynamically, as well as help with parsing of and access to unstructured data.

## ​ History

Given the constant rate of change in the field, LangChain has also evolved over time. Below is a brief timeline of how LangChain has changed over the years, evolving alongside what it means to build with LLMs:

​

2022-10-24

v0.0.1

A month before ChatGPT, **LangChain was launched as a Python package**. It consisted of two main components:

- LLM abstractions
- “Chains”, or predetermined steps of computation to run, for common use cases. For example - RAG: run a retrieval step, then run a generation step.

The name LangChain comes from “Language” (like Language models) and “Chains”.

2022-12

The first general purpose agents were added to LangChain.These general purpose agents were based on the ReAct paper (ReAct standing for Reasoning and Acting). They used LLMs to generate JSON that represented tool calls, and then parsed that JSON to determine what tools to call.

2023-01

OpenAI releases a ‘Chat Completion’ API.Previously, models took in strings and returned a string. In the ChatCompletions API, they evolved to take in a list of messages and return a message. Other model providers followed suit, and LangChain updated to work with lists of messages.

LangChain releases a JavaScript version.LLMs and agents will change how applications are built and JavaScript is the language of application developers.

2023-02

**LangChain Inc. was formed as a company** around the open source LangChain project.The main goal was to “make intelligent agents ubiquitous”. The team recognized that while LangChain was a key part (LangChain made it simple to get started with LLMs), there was also a need for other components.

2023-03

OpenAI releases ‘function calling’ in their API.This allowed the API to explicitly generate payloads that represented tool calls. Other model providers followed suit, and LangChain was updated to use this as the preferred method for tool calling (rather than parsing JSON).

2023-06

**LangSmith is released** as closed source platform by LangChain Inc., providing observability and evalsThe main issue with building agents is getting them to be reliable, and LangSmith, which provides observability and evals, was built to solve that need. LangChain was updated to integrate seamlessly with LangSmith.

2024-01

v0.1.0

**LangChain releases 0.1.0**, its first non-0.0.x.The industry matured from prototypes to production, and as such, LangChain increased its focus on stability.

2024-02

**LangGraph is released** as an open-source library.The original LangChain had two focuses: LLM abstractions, and high-level interfaces for getting started with common applications; however, it was missing a low-level orchestration layer that allowed developers to control the exact flow of their agent. Enter: LangGraph.When building LangGraph, we learned from lessons when building LangChain and added functionality we discovered was needed: streaming, durable execution, short-term memory, human-in-the-loop, and more.

2024-06

**LangChain has over 700 integrations.**Integrations were split out of the core LangChain package, and either moved into their own standalone packages (for the core integrations) or `@langchain/community`.

2024-10

LangGraph becomes the preferred way to build any AI application that is more than a single LLM call.As developers tried to improve the reliability of their applications, they needed more control than the high-level interfaces provided. LangGraph provided that low-level flexibility. Most chains and agents were marked as deprecated in LangChain with guides on how to migrate them to LangGraph. There is still one high-level abstraction created in LangGraph: an agent abstraction. It is built on top of low-level LangGraph and has the same interface as the ReAct agents from LangChain.

2025-04

Model APIs become more multimodal.Models started to accept files, images, videos, and more. We updated the `@langchain/core` message format accordingly to allow developers to specify these multimodal inputs in a standard way.

2025-10-20

v1.0.0

**LangChain releases 1.0** with two major changes:

1. Complete revamp of all chains and agents in `langchain`. All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain.For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the `@langchain/classic` package.
2. A standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Changelog\\
\\
Previous Agents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/messages

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Messages

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Basic usage
- Text prompts
- Message prompts
- Dictionary format
- Message types
- System Message
- Human Message
- Text content
- Message metadata
- AI Message
- Tool calls
- Token usage
- Streaming and chunks
- Tool Message
- Message content
- Standard content blocks
- Multimodal
- Content block reference
- Use with chat models

Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.Messages are objects that contain:

- **Role** \- Identifies the message type (e.g. `system`, `user`)
- **Content** \- Represents the actual content of the message (like text, images, audio, documents, etc.)
- **Metadata** \- Optional fields such as response information, message IDs, and token usage

LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.

## ​ Basic usage

The simplest way to use messages is to create message objects and pass them to a model when invoking.

Copy

import { initChatModel, HumanMessage, SystemMessage } from "langchain";

const model = await initChatModel("gpt-5-nano");

const systemMsg = new SystemMessage("You are a helpful assistant.");
const humanMsg = new HumanMessage("Hello, how are you?");

const messages = [systemMsg, humanMsg];
const response = await model.invoke(messages); // Returns AIMessage

### ​ Text prompts

Text prompts are strings - ideal for straightforward generation tasks where you don’t need to retain conversation history.

const response = await model.invoke("Write a haiku about spring");

**Use text prompts when:**

- You have a single, standalone request
- You don’t need conversation history
- You want minimal code complexity

### ​ Message prompts

Alternatively, you can pass in a list of messages to the model by providing a list of message objects.

import { SystemMessage, HumanMessage, AIMessage } from "langchain";

const messages = [\
new SystemMessage("You are a poetry expert"),\
new HumanMessage("Write a haiku about spring"),\
new AIMessage("Cherry blossoms bloom..."),\
];
const response = await model.invoke(messages);

**Use message prompts when:**

- Managing multi-turn conversations
- Working with multimodal content (images, audio, files)
- Including system instructions

### ​ Dictionary format

You can also specify messages directly in OpenAI chat completions format.

const messages = [\
{ role: "system", content: "You are a poetry expert" },\
{ role: "user", content: "Write a haiku about spring" },\
{ role: "assistant", content: "Cherry blossoms bloom..." },\
];
const response = await model.invoke(messages);

## ​ Message types

- System message \- Tells the model how to behave and provide context for interactions
- Human message \- Represents user input and interactions with the model
- AI message \- Responses generated by the model, including text content, tool calls, and metadata
- Tool message \- Represents the outputs of tool calls

### ​ System Message

A `SystemMessage` represent an initial set of instructions that primes the model’s behavior. You can use a system message to set the tone, define the model’s role, and establish guidelines for responses.

Basic instructions

const systemMsg = new SystemMessage("You are a helpful coding assistant.");

const messages = [\
systemMsg,\
new HumanMessage("How do I create a REST API?"),\
];
const response = await model.invoke(messages);

Detailed persona

import { SystemMessage, HumanMessage } from "langchain";

const systemMsg = new SystemMessage(`
You are a senior TypeScript developer with expertise in web frameworks.
Always provide code examples and explain your reasoning.
Be concise but thorough in your explanations.
`);

* * *

### ​ Human Message

A `HumanMessage` represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal content.

#### ​ Text content

Message object

const response = await model.invoke([\
new HumanMessage("What is machine learning?"),\
]);

String shortcut

const response = await model.invoke("What is machine learning?");

#### ​ Message metadata

Add metadata

const humanMsg = new HumanMessage({
content: "Hello!",
name: "alice",
id: "msg_123",
});

The `name` field behavior varies by provider – some use it for user identification, others ignore it. To check, refer to the model provider’s reference.

### ​ AI Message

An `AIMessage` represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.

const response = await model.invoke("Explain AI");
console.log(typeof response); // AIMessage

`AIMessage` objects are returned by the model when calling it, which contains all of the associated metadata in the response.Providers weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new `AIMessage` object and insert it into the message history as if it came from the model.

import { AIMessage, SystemMessage, HumanMessage } from "langchain";

const aiMsg = new AIMessage("I'd be happy to help you with that question!");

const messages = [\
new SystemMessage("You are a helpful assistant"),\
new HumanMessage("Can you help me?"),\
aiMsg, // Insert as if it came from the model\
new HumanMessage("Great! What's 2+2?")\
]

const response = await model.invoke(messages);

Attributes

​

text

string

The text content of the message.

content

string \| ContentBlock\[\]

The raw content of the message.

content\_blocks

ContentBlock.Standard\[\]

The standardized content blocks of the message. (See content)

tool\_calls

ToolCall\[\] \| None

The tool calls made by the model.Empty if no tools are called.

id

A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)

usage\_metadata

UsageMetadata \| None

The usage metadata of the message, which can contain token counts when available. See `UsageMetadata`.

response\_metadata

ResponseMetadata \| None

The response metadata of the message.

#### ​ Tool calls

When models make tool calls, they’re included in the `AIMessage`:

const modelWithTools = model.bindTools([getWeather]);
const response = await modelWithTools.invoke("What's the weather in Paris?");

for (const toolCall of response.tool_calls) {
console.log(`Tool: ${toolCall.name}`);
console.log(`Args: ${toolCall.args}`);
console.log(`ID: ${toolCall.id}`);
}

Other structured data, such as reasoning or citations, can also appear in message content.

#### ​ Token usage

An `AIMessage` can hold token counts and other usage metadata in its `usage_metadata` field:

import { initChatModel } from "langchain";

const response = await model.invoke("Hello!");
console.log(response.usage_metadata);

{
"output_tokens": 304,
"input_tokens": 8,
"total_tokens": 312,
"input_token_details": {
"cache_read": 0
},
"output_token_details": {
"reasoning": 256
}
}

See `UsageMetadata` for details.

#### ​ Streaming and chunks

During streaming, you’ll receive `AIMessageChunk` objects that can be combined into a full message object:

import { AIMessageChunk } from "langchain";

let finalChunk: AIMessageChunk | undefined;
for (const chunk of chunks) {
finalChunk = finalChunk ? finalChunk.concat(chunk) : chunk;
}

Learn more:

- Streaming tokens from chat models
- Streaming tokens and/or steps from agents

### ​ Tool Message

For models that support tool calling, AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution can generate `ToolMessage` objects directly. Below, we show a simple example. Read more in the tools guide.

import { AIMessage, ToolMessage } from "langchain";

const aiMessage = new AIMessage({
content: [],
tool_calls: [{\
name: "get_weather",\
args: { location: "San Francisco" },\
id: "call_123"\
}]
});

const toolMessage = new ToolMessage({
content: "Sunny, 72°F",
tool_call_id: "call_123"
});

const messages = [\
new HumanMessage("What's the weather in San Francisco?"),\
aiMessage, // Model's tool call\
toolMessage, // Tool execution result\
];

const response = await model.invoke(messages); // Model processes the result

required

The stringified output of the tool call.

tool\_call\_id

The ID of the tool call that this message is responding to. Must match the ID of the tool call in the `AIMessage`.

name

The name of the tool that was called.

artifact

dict

Additional data not sent to the model but can be accessed programmatically.

The `artifact` field stores supplementary data that won’t be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model’s context.

Example: Using artifact for retrieval metadata

For example, a retrieval tool could retrieve a passage from a document for reference by a model. Where message `content` contains text that the model will reference, an `artifact` can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:

import { ToolMessage } from "langchain";

// Artifact available downstream
const artifact = { document_id: "doc_123", page: 0 };

const toolMessage = new ToolMessage({
content: "It was the best of times, it was the worst of times.",
tool_call_id: "call_123",
name: "search_books",
artifact
});

See the RAG tutorial for an end-to-end example of building retrieval agents with LangChain.

## ​ Message content

You can think of a message’s content as the payload of data that gets sent to the model. Messages have a `content` attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as multimodal content and other data.Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See content blocks below.LangChain chat models accept message content in the `content` attribute.This may contain either:

1. A string
2. A list of content blocks in a provider-native format
3. A list of LangChain’s standard content blocks

See below for an example using multimodal inputs:

import { HumanMessage } from "langchain";

// String content
const humanMessage = new HumanMessage("Hello, how are you?");

// Provider-native format (e.g., OpenAI)
const humanMessage = new HumanMessage({
content: [\
{ type: "text", text: "Hello, how are you?" },\
{\
type: "image_url",\
image_url: { url: "https://example.com/image.jpg" },\
},\
],
});

// List of standard content blocks
const humanMessage = new HumanMessage({
contentBlocks: [\
{ type: "text", text: "Hello, how are you?" },\
{ type: "image", url: "https://example.com/image.jpg" },\
],
});

### ​ Standard content blocks

LangChain provides a standard representation for message content that works across providers.Message objects implement a `contentBlocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from `ChatAnthropic` or `ChatOpenAI` will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent `ReasoningContentBlock` representation:

- Anthropic

- OpenAI

import { AIMessage } from "@langchain/core/messages";

const message = new AIMessage({
content: [\
{\
"type": "thinking",\
"thinking": "...",\
"signature": "WaUjzkyp...",\
},\
{\
"type":"text",\
"text": "...",\
"id": "msg_abc123",\
},\
],
response_metadata: { model_provider: "anthropic" },
});

console.log(message.contentBlocks);

const message = new AIMessage({
content: [\
{\
"type": "reasoning",\
"id": "rs_abc123",\
"summary": [\
{"type": "summary_text", "text": "summary 1"},\
{"type": "summary_text", "text": "summary 2"},\
],\
},\
{"type": "text", "text": "..."},\
],
response_metadata: { model_provider: "openai" },
});

See the integrations guides to get started with the
inference provider of your choice.

**Serializing standard content**If an application outside of LangChain needs access to the standard content block
representation, you can opt-in to storing content blocks in message content.To do this, you can set the `LC_OUTPUT_VERSION` environment variable to `v1`. Or,
initialize any chat model with `outputVersion: "v1"`:

const model = await initChatModel(
"gpt-5-nano",
{ outputVersion: "v1" }
);

### ​ Multimodal

**Multimodality** refers to the ability to work with data that comes in different
forms, such as text, audio, images, and video. LangChain includes standard types
for these data that can be used across providers.Chat models can accept multimodal data as input and generate
it as output. Below we show short examples of input messages featuring multimodal data.

Extra keys can be included top-level in the content block or nested in `"extras": {"key": value}`.OpenAI and AWS Bedrock Converse,
for example, require a filename for PDFs. See the provider page
for your chosen model for specifics.

Image input

PDF document input

Audio input

Video input

// From URL
const message = new HumanMessage({
content: [\
{ type: "text", text: "Describe the content of this image." },\
{\
type: "image",\
source_type: "url",\
url: "https://example.com/path/to/image.jpg"\
},\
],
});

// From base64 data
const message = new HumanMessage({
content: [\
{ type: "text", text: "Describe the content of this image." },\
{\
type: "image",\
source_type: "base64",\
data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",\
},\
],
});

// From provider-managed File ID
const message = new HumanMessage({
content: [\
{ type: "text", text: "Describe the content of this image." },\
{ type: "image", source_type: "id", id: "file-abc123" },\
],
});

Not all models support all file types. Check the model provider’s reference for supported formats and size limits.

### ​ Content block reference

Content blocks are represented (either when creating a message or accessing the `contentBlocks` field) as a list of typed objects. Each item in the list must adhere to one of the following block types:

Core

ContentBlock.Text

**Purpose:** Standard text output

type

Always `"text"`

The text content

annotations

Citation\[\]

List of annotations for the text

**Example:**

{
type: "text",
text: "Hello world",
annotations: []
}

ContentBlock.Reasoning

**Purpose:** Model reasoning steps

Always `"reasoning"`

reasoning

The reasoning content

{
type: "reasoning",
reasoning: "The user is asking about..."
}

Multimodal

ContentBlock.Multimodal.Image

**Purpose:** Image data

Always `"image"`

url

URL pointing to the image location.

data

Base64-encoded image data.

fileId

Reference to the image in an external file storage system (e.g., OpenAI or Anthropic’s Files API).

mimeType

Image MIME type (e.g., `image/jpeg`, `image/png`). Required for base64 data.

ContentBlock.Multimodal.Audio

**Purpose:** Audio data

Always `"audio"`

URL pointing to the audio location.

Base64-encoded audio data.

Reference to the audio file in an external file storage system (e.g., OpenAI or Anthropic’s Files API).

Audio MIME type (e.g., `audio/mpeg`, `audio/wav`). Required for base64 data.

ContentBlock.Multimodal.Video

**Purpose:** Video data

Always `"video"`

URL pointing to the video location.

Base64-encoded video data.

Reference to the video file in an external file storage system (e.g., OpenAI or Anthropic’s Files API).

Video MIME type (e.g., `video/mp4`, `video/webm`). Required for base64 data.

ContentBlock.Multimodal.File

**Purpose:** Generic files (PDF, etc)

Always `"file"`

URL pointing to the file location.

Base64-encoded file data.

Reference to the file in an external file storage system (e.g., OpenAI or Anthropic’s Files API).

File MIME type (e.g., `application/pdf`). Required for base64 data.

ContentBlock.Multimodal.PlainText

**Purpose:** Document text (`.txt`, `.md`)

Always `"text-plain"`

title

Title of the text content

MIME type of the text (e.g., `text/plain`, `text/markdown`)

Tool Calling

ContentBlock.Tools.ToolCall

**Purpose:** Function calls

Always `"tool_call"`

Name of the tool to call

args

object

Arguments to pass to the tool

Unique identifier for this tool call

{
type: "tool_call",
name: "search",
args: { query: "weather" },
id: "call_123"
}

ContentBlock.Tools.ToolCallChunk

**Purpose:** Streaming tool fragments

Always `"tool_call_chunk"`

Name of the tool being called

Partial tool arguments (may be incomplete JSON)

Tool call identifier

index

number \| string

Position of this chunk in the stream

ContentBlock.Tools.InvalidToolCall

**Purpose:** Malformed calls

Always `"invalid_tool_call"`

Name of the tool that failed to be called

Raw arguments that failed to parse

error

Description of what went wrong

**Common errors:** Invalid JSON, missing required fields

Server-Side Tool Execution

ContentBlock.Tools.ServerToolCall

**Purpose:** Tool call that is executed server-side.

Always `"server_tool_call"`

An identifier associated with the tool call.

The name of the tool to be called.

ContentBlock.Tools.ServerToolCallChunk

**Purpose:** Streaming server-side tool call fragments

Always `"server_tool_call_chunk"`

ContentBlock.Tools.ServerToolResult

**Purpose:** Search results

Always `"server_tool_result"`

Identifier of the corresponding server tool call.

Identifier associated with the server tool result.

status

Execution status of the server-side tool. `"success"` or `"error"`.

output

Output of the executed tool.

Provider-Specific Blocks

ContentBlock.NonStandard

**Purpose:** Provider-specific escape hatch

Always `"non_standard"`

value

Provider-specific data structure

**Usage:** For experimental or provider-unique features

Additional provider-specific content types may be found within the reference documentation of each model provider.

Each of these content blocks mentioned above are indvidually addressable as types when importing the `ContentBlock` type.

import { ContentBlock } from "langchain";

// Text block
const textBlock: ContentBlock.Text = {
type: "text",
text: "Hello world",
}

// Image block
const imageBlock: ContentBlock.Multimodal.Image = {
type: "image",
url: "https://example.com/image.png",
mimeType: "image/png",
}

View the canonical type definitions in the API reference.

Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code.Content blocks are not a replacement for the `content` property, but rather a new property that can be used to access the content of a message in a standardized format.

## ​ Use with chat models

Chat models accept a sequence of message objects as input and return an `AIMessage` as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.Refer to the below guides to learn more:

- Built-in features for persisting and managing conversation histories
- Strategies for managing context windows, including trimming and summarizing messages

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Models\\
\\
Previous Tools\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/short-term-memory

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Short-term memory

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Overview
- Usage
- In production
- Customizing agent memory
- Common patterns
- Trim messages
- Delete messages
- Summarize messages
- Access memory
- Tools
- Read short-term memory in a tool
- Write short-term memory from tools
- Prompt
- Before model
- After model

## ​ Overview

Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.Short term memory lets your application remember previous interactions within a single thread or conversation.

A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.

Conversation history is the most common form of short-term memory. Long conversations pose a challenge to today’s LLMs; a full history may not fit inside an LLM’s context window, resulting in an context loss or errors.Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get “distracted” by stale or off-topic content, all while suffering from slower response times and higher costs.Chat models accept context using messages, which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or “forget” stale information.

## ​ Usage

To add short-term memory (thread-level persistence) to an agent, you need to specify a `checkpointer` when creating an agent.

LangChain’s agent manages short-term memory as a part of your agent’s state.By storing these in the graph’s state, the agent can access the full context for a given conversation while maintaining separation between different threads.State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.

Copy

import { createAgent } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const agent = createAgent({
model: "claude-sonnet-4-5-20250929",
tools: [],
checkpointer,
});

await agent.invoke(
{ messages: [{ role: "user", content: "hi! i am Bob" }] },
{ configurable: { thread_id: "1" } }
);

### ​ In production

In production, use a checkpointer backed by a database:

import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres";

const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";
const checkpointer = PostgresSaver.fromConnString(DB_URI);

## ​ Customizing agent memory

You can extend the agent state by creating custom middleware with a state schema. Custom state schemas can be passed using the `stateSchema` parameter in middleware.

import * as z from "zod";
import { createAgent, createMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const customStateSchema = z.object({
userId: z.string(),
preferences: z.record(z.string(), z.any()),
});

const stateExtensionMiddleware = createMiddleware({
name: "StateExtension",
stateSchema: customStateSchema,
});

const checkpointer = new MemorySaver();
const agent = createAgent({
model: "gpt-5",
tools: [],
middleware: [stateExtensionMiddleware],
checkpointer,
});

// Custom state can be passed in invoke
const result = await agent.invoke({
messages: [{ role: "user", content: "Hello" }],
userId: "user_123",
preferences: { theme: "dark" },
});

## ​ Common patterns

With short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:

**Trim messages** \\
\\
Remove first or last N messages (before calling LLM) **Delete messages** \\
\\
Delete messages from LangGraph state permanently **Summarize messages** \\
\\
Summarize earlier messages in the history and replace them with a summary

## Custom strategies

Custom strategies (e.g., message filtering, etc.)

This allows the agent to keep track of the conversation without exceeding the LLM’s context window.

### ​ Trim messages

Most LLMs have a maximum supported context window (denominated in tokens).One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `maxTokens`) to use for handling the boundary.To trim message history in an agent, use @\[`createMiddleware`\] with a `beforeModel` hook:

import { RemoveMessage } from "@langchain/core/messages";
import { createAgent, createMiddleware } from "langchain";
import { MemorySaver, REMOVE_ALL_MESSAGES } from "@langchain/langgraph";

const trimMessages = createMiddleware({
name: "TrimMessages",

const messages = state.messages;

if (messages.length <= 3) {
return; // No changes needed
}

const firstMsg = messages[0];
const recentMessages =
messages.length % 2 === 0 ? messages.slice(-3) : messages.slice(-4);
const newMessages = [firstMsg, ...recentMessages];

return {
messages: [\
new RemoveMessage({ id: REMOVE_ALL_MESSAGES }),\
...newMessages,\
],
};
},
});

const checkpointer = new MemorySaver();
const agent = createAgent({
model: "gpt-4o",
tools: [],
middleware: [trimMessages],
checkpointer,
});

### ​ Delete messages

You can delete messages from the graph state to manage the message history.This is useful when you want to remove specific messages or clear the entire message history.To delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with `messagesStateReducer` reducer, like `MessagesZodState`.To remove specific messages:

import { RemoveMessage } from "@langchain/core/messages";

// remove the earliest two messages
return {
messages: messages
.slice(0, 2)

};
}
};

When deleting messages, **make sure** that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:

- Some providers expect message history to start with a `user` message
- Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.

import { RemoveMessage } from "@langchain/core/messages";
import { createAgent, createMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const deleteOldMessages = createMiddleware({
name: "DeleteOldMessages",

};
}
return;
},
});

const agent = createAgent({
model: "gpt-4o",
tools: [],
systemPrompt: "Please be concise and to the point.",
middleware: [deleteOldMessages],
checkpointer: new MemorySaver(),
});

const config = { configurable: { thread_id: "1" } };

const streamA = await agent.stream(
{ messages: [{ role: "user", content: "hi! I'm bob" }] },
{ ...config, streamMode: "values" }
);
for await (const event of streamA) {

message.getType(),\
message.content,\
]);
console.log(messageDetails);
}

const streamB = await agent.stream(
{
messages: [{ role: "user", content: "what's my name?" }],
},
{ ...config, streamMode: "values" }
);
for await (const event of streamB) {

[[ "human", "hi! I'm bob" ]]
[[ "human", "hi! I'm bob" ], [ "ai", "Hello, Bob! How can I assist you today?" ]]
[[ "human", "hi! I'm bob" ], [ "ai", "Hello, Bob! How can I assist you today?" ]]
[[ "human", "hi! I'm bob" ], [ "ai", "Hello, Bob! How can I assist you today" ], ["human", "what's my name?" ]]
[[ "human", "hi! I'm bob" ], [ "ai", "Hello, Bob! How can I assist you today?" ], ["human", "what's my name?"], [ "ai", "Your name is Bob, as you mentioned. How can I help you further?" ]]
[[ "human", "what's my name?" ], [ "ai", "Your name is Bob, as you mentioned. How can I help you further?" ]]

### ​ Summarize messages

The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue.
Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.!SummaryTo summarize message history in an agent, use the built-in `summarizationMiddleware`:

import { createAgent, summarizationMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const agent = createAgent({
model: "gpt-4o",
tools: [],
middleware: [\
summarizationMiddleware({\
model: "gpt-4o-mini",\
trigger: { tokens: 4000 },\
keep: { messages: 20 },\
}),\
],
checkpointer,
});

const config = { configurable: { thread_id: "1" } };
await agent.invoke({ messages: "hi, my name is bob" }, config);
await agent.invoke({ messages: "write a short poem about cats" }, config);
await agent.invoke({ messages: "now do the same but for dogs" }, config);
const finalResponse = await agent.invoke({ messages: "what's my name?" }, config);

console.log(finalResponse.messages.at(-1)?.content);
// Your name is Bob!

See `summarizationMiddleware` for more configuration options.

## ​ Access memory

You can access and modify the short-term memory (state) of an agent in several ways:

#### ​ Read short-term memory in a tool

Access short term memory (state) in a tool using the `ToolRuntime` parameter.The `tool_runtime` parameter is hidden from the tool signature (so the model doesn’t see it), but the tool can access the state through it.

import * as z from "zod";
import { createAgent, tool, type ToolRuntime } from "langchain";

const stateSchema = z.object({
userId: z.string(),
});

const getUserInfo = tool(
async (_, config: ToolRuntime<z.infer<typeof stateSchema>>) => {
const userId = config.state.userId;
return userId === "user_123" ? "John Doe" : "Unknown User";
},
{
name: "get_user_info",
description: "Get user info",
schema: z.object({}),
}
);

const agent = createAgent({
model: "gpt-5-nano",
tools: [getUserInfo],
stateSchema,
});

const result = await agent.invoke(
{
messages: [{ role: "user", content: "what's my name?" }],
userId: "user_123",
},
{
context: {},
}
);

console.log(result.messages.at(-1)?.content);
// Outputs: "Your name is John Doe."

#### ​ Write short-term memory from tools

To modify the agent’s short-term memory (state) during execution, you can return state updates directly from the tools.This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.

import * as z from "zod";
import { tool, createAgent, ToolMessage, type ToolRuntime } from "langchain";
import { Command } from "@langchain/langgraph";

const CustomState = z.object({
userId: z.string().optional(),
});

const updateUserInfo = tool(

const userId = config.state.userId;
const name = userId === "user_123" ? "John Smith" : "Unknown user";
return new Command({
update: {
userName: name,
// update the message history
messages: [\
new ToolMessage({\
content: "Successfully looked up user information",\
tool_call_id: config.toolCall?.id ?? "",\
}),\
],
},
});
},
{
name: "update_user_info",
description: "Look up and update user info.",
schema: z.object({}),
}
);

const greet = tool(

const userName = config.context?.userName;
return `Hello ${userName}!`;
},
{
name: "greet",
description: "Use this to greet the user once you found their info.",
schema: z.object({}),
}
);

const agent = createAgent({
model: "openai:gpt-5-mini",
tools: [updateUserInfo, greet],
stateSchema: CustomState,
});

const result = await agent.invoke({
messages: [{ role: "user", content: "greet the user" }],
userId: "user_123",
});

console.log(result.messages.at(-1)?.content);
// Output: "Hello! I’m here to help — what would you like to do today?"

### ​ Prompt

Access short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.

import * as z from "zod";
import { createAgent, tool, dynamicSystemPromptMiddleware } from "langchain";

const contextSchema = z.object({
userName: z.string(),
});

const getWeather = tool(

return `The weather in ${city} is always sunny!`;
},
{
name: "get_weather",
description: "Get user info",
schema: z.object({
city: z.string(),
}),
}
);

const agent = createAgent({
model: "gpt-5-nano",
tools: [getWeather],
contextSchema,
middleware: [\

return `You are a helpful assistant. Address the user as ${config.context?.userName}.`;\
}),\
],
});

const result = await agent.invoke(
{
messages: [{ role: "user", content: "What is the weather in SF?" }],
},
{
context: {
userName: "John Smith",
},
}
);

for (const message of result.messages) {
console.log(message);
}
/**
* HumanMessage {
* "content": "What is the weather in SF?",
* // ...
* }
* AIMessage {
* // ...
* "tool_calls": [\
* {\
* "name": "get_weather",\
* "args": {\
* "city": "San Francisco"\
* },\
* "type": "tool_call",\
* "id": "call_tCidbv0apTpQpEWb3O2zQ4Yx"\
* }\
* ],
* // ...
* }
* ToolMessage {
* "content": "The weather in San Francisco is always sunny!",
* "tool_call_id": "call_tCidbv0apTpQpEWb3O2zQ4Yx"
* // ...
* }
* AIMessage {
* "content": "John Smith, here's the latest: The weather in San Francisco is always sunny!\n\nIf you'd like more details (temperature, wind, humidity) or a forecast for the next few days, I can pull that up. What would you like?",
* // ...
* }
*/

### ​ Before model

\_\_start\_\_

before\_model

model

tools

\_\_end\_\_

import { RemoveMessage } from "@langchain/core/messages";
import { createAgent, createMiddleware, trimMessages } from "langchain";
import { MemorySaver } from "@langchain/langgraph";
import { REMOVE_ALL_MESSAGES } from "@langchain/langgraph";

const trimMessageHistory = createMiddleware({
name: "TrimMessages",

const trimmed = await trimMessages(state.messages, {
maxTokens: 384,
strategy: "last",
startOn: "human",
endOn: ["human", "tool"],

});
return {
messages: [new RemoveMessage({ id: REMOVE_ALL_MESSAGES }), ...trimmed],
};
},
});

const checkpointer = new MemorySaver();
const agent = createAgent({
model: "gpt-5-nano",
tools: [],
middleware: [trimMessageHistory],
checkpointer,
});

### ​ After model

after\_model

import { RemoveMessage } from "@langchain/core/messages";
import { createAgent, createMiddleware } from "langchain";
import { REMOVE_ALL_MESSAGES } from "@langchain/langgraph";

const validateResponse = createMiddleware({
name: "ValidateResponse",

const lastMessage = state.messages.at(-1)?.content;
if (
typeof lastMessage === "string" &&
lastMessage.toLowerCase().includes("confidential")
) {
return {
messages: [\
new RemoveMessage({ id: REMOVE_ALL_MESSAGES }),\
...state.messages,\
],
};
}
return;
},
});

const agent = createAgent({
model: "gpt-5-nano",
tools: [],
middleware: [validateResponse],
});

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Tools\\
\\
Previous Streaming\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/streaming

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Streaming

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Overview
- Supported stream modes
- Agent progress
- LLM tokens
- Custom updates
- Stream multiple modes
- Disable streaming
- Related

LangChain implements a streaming system to surface real-time updates.Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.

## ​ Overview

LangChain’s streaming system lets you surface live feedback from agent runs to your application.What’s possible with LangChain streaming:

- **Stream agent progress** — get state updates after each agent step.
- **Stream LLM tokens** — stream language model tokens as they’re generated.
- **Stream custom updates** — emit user-defined signals (e.g., `"Fetched 10/100 records"`).
- **Stream multiple modes** — choose from `updates` (agent progress), `messages` (LLM tokens + metadata), or `custom` (arbitrary user data).

See the common patterns section below for additional end-to-end examples.

## ​ Supported stream modes

Pass one or more of the following stream modes as a list to the `stream` method:

| Mode | Description |
| --- | --- |
| `updates` | Streams state updates after each agent step. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |
| `messages` | Streams tuples of `(token, metadata)` from any graph nodes where an LLM is invoked. |
| `custom` | Streams custom data from inside your graph nodes using the stream writer. |

## ​ Agent progress

To stream agent progress, use the `stream` method with `streamMode: "updates"`. This emits an event after every agent step.For example, if you have an agent that calls a tool once, you should see the following updates:

- **LLM node**: `AIMessage` with tool call requests
- **Tool node**: `ToolMessage` with execution result
- **LLM node**: Final AI response

Copy

import z from "zod";
import { createAgent, tool } from "langchain";

const getWeather = tool(

return `The weather in ${city} is always sunny!`;
},
{
name: "get_weather",
description: "Get weather for a given city.",
schema: z.object({
city: z.string(),
}),
}
);

const agent = createAgent({
model: "gpt-5-nano",
tools: [getWeather],
});

for await (const chunk of await agent.stream(
{ messages: [{ role: "user", content: "what is the weather in sf" }] },
{ streamMode: "updates" }
)) {
const [step, content] = Object.entries(chunk)[0];
console.log(`step: ${step}`);
console.log(`content: ${JSON.stringify(content, null, 2)}`);
}
/**
* step: model
* content: {
* "messages": [\
* {\
* "kwargs": {\
* // ...\
* "tool_calls": [\
* {\
* "name": "get_weather",\
* "args": {\
* "city": "San Francisco"\
* },\
* "type": "tool_call",\
* "id": "call_0qLS2Jp3MCmaKJ5MAYtr4jJd"\
* }\
* ],\
* // ...\
* }\
* }\
* ]
* }
* step: tools
* content: {
* "messages": [\
* {\
* "kwargs": {\
* "content": "The weather in San Francisco is always sunny!",\
* "name": "get_weather",\
* // ...\
* }\
* }\
* ]
* }
* step: model
* content: {
* "messages": [\
* {\
* "kwargs": {\
* "content": "The latest update says: The weather in San Francisco is always sunny!\n\nIf you'd like real-time details (current temperature, humidity, wind, and today's forecast), I can pull the latest data for you. Want me to fetch that?",\
* // ...\
* }\
* }\
* ]
* }
*/

## ​ LLM tokens

To stream tokens as they are produced by the LLM, use `streamMode: "messages"`:

const agent = createAgent({
model: "gpt-4o-mini",
tools: [getWeather],
});

for await (const [token, metadata] of await agent.stream(
{ messages: [{ role: "user", content: "what is the weather in sf" }] },
{ streamMode: "messages" }
)) {
console.log(`node: ${metadata.langgraph_node}`);
console.log(`content: ${JSON.stringify(token.contentBlocks, null, 2)}`);
}

## ​ Custom updates

To stream updates from tools as they are executed, you can use the `writer` parameter from the configuration.

import z from "zod";
import { tool, createAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

// Stream any arbitrary data
config.writer?.(`Looking up data for city: ${input.city}`);
// ... fetch city data
config.writer?.(`Acquired data for city: ${input.city}`);
return `It's always sunny in ${input.city}!`;
},
{
name: "get_weather",
description: "Get weather for a given city.",
schema: z.object({
city: z.string().describe("The city to get weather for."),
}),
}
);

for await (const chunk of await agent.stream(
{ messages: [{ role: "user", content: "what is the weather in sf" }] },
{ streamMode: "custom" }
)) {
console.log(chunk);
}

Output

Looking up data for city: San Francisco
Acquired data for city: San Francisco

If you add the `writer` parameter to your tool, you won’t be able to invoke the tool outside of a LangGraph execution context without providing a writer function.

## ​ Stream multiple modes

You can specify multiple streaming modes by passing streamMode as an array: `streamMode: ["updates", "messages", "custom"]`.The streamed outputs will be tuples of `[mode, chunk]` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

for await (const [streamMode, chunk] of await agent.stream(
{ messages: [{ role: "user", content: "what is the weather in sf" }] },
{ streamMode: ["updates", "messages", "custom"] }
)) {
console.log(`${streamMode}: ${JSON.stringify(chunk, null, 2)}`);
}

## ​ Disable streaming

In some applications you might need to disable streaming of individual tokens for a given model. This is useful when:

- Working with multi-agent systems to control which agents stream their output
- Mixing models that support streaming with those that do not
- Deploying to LangSmith and wanting to prevent certain model outputs from being streamed to the client

Set `streaming: false` when initializing the model.

import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
model: "gpt-4o",
streaming: false,
});

When deploying to LangSmith, set `streaming=False` on any models whose output you don’t want streamed to the client. This is configured in your graph code before deployment.

Not all chat model integrations support the `streaming` parameter. If your model doesn’t support it, use `disableStreaming: true` instead. This parameter is available on all chat models via the base class.

See the LangGraph streaming guide for more details.

## ​ Related

- Streaming with chat models — Stream tokens directly from a chat model without using an agent or graph
- Streaming with human-in-the-loop — Stream agent progress while handling interrupts for human review
- LangGraph streaming — Advanced streaming options including `values`, `debug` modes, and subgraph streaming

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Short-term memory\\
\\
Previous Structured output\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/structured-output

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core components

Structured output

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Response Format
- Provider strategy
- Tool calling strategy
- Custom tool message content
- Error handling
- Multiple structured outputs error
- Schema validation error
- Error handling strategies

Structured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get typed structured data.LangChain’s prebuilt ReAct agent `createAgent` handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it’s captured, validated, and returned in the `structuredResponse` key of the agent’s state.

Copy

type ResponseFormat = (

)

const agent = createAgent({
// ...
responseFormat: ResponseFormat | ResponseFormat[]
})

## ​ Response Format

Controls how the agent returns structured data. You can provide either a Zod object or JSON schema. By default, the agent uses a tool calling strategy, in which the output is created by an additional tool call. Certain models support native structured output, in which case the agent will use that strategy instead.You can control the behavior by wrapping `ResponseFormat` in a `toolStrategy` or `providerStrategy` function call:

import { toolStrategy, providerStrategy } from "langchain";

const agent = createAgent({
// use a provider strategy if supported by the model
responseFormat: providerStrategy(z.object({ ... }))
// or enforce a tool strategy
responseFormat: toolStrategy(z.object({ ... }))
})

The structured response is returned in the `structuredResponse` key of the agent’s final state.

const customProfile: ModelProfile = {
structuredOutput: true,
// ...
}
const model = await initChatModel("...", { profile: customProfile });

If tools are specified, the model must support simultaneous use of tools and structured output.

## ​ Provider strategy

Some model providers support structured output natively through their APIs (e.g. OpenAI, Grok, Gemini). This is the most reliable method when available.To use this strategy, configure a `ProviderStrategy`:

​

schema

required

The schema defining the structured output format. Supports:

- **Zod Schema**: A zod schema
- **JSON Schema**: A JSON schema object

LangChain automatically uses `ProviderStrategy` when you pass a schema type directly to `createAgent.responseFormat` and the model supports native structured output:

Zod Schema

JSON Schema

import * as z from "zod";
import { createAgent, providerStrategy } from "langchain";

const ContactInfo = z.object({
name: z.string().describe("The name of the person"),
email: z.string().describe("The email address of the person"),
phone: z.string().describe("The phone number of the person"),
});

const agent = createAgent({
model: "gpt-5",
tools: [],
responseFormat: providerStrategy(ContactInfo)
});

const result = await agent.invoke({
messages: [{"role": "user", "content": "Extract contact info from: John Doe, john@example.com, (555) 123-4567"}]
});

console.log(result.structuredResponse);
// { name: "John Doe", email: "john@example.com", phone: "(555) 123-4567" }

Provider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.

If the provider natively supports structured output for your model choice, it is functionally equivalent to write `responseFormat: contactInfoSchema` instead of `responseFormat: providerStrategy(contactInfoSchema)`. In either case, if structured output is not supported, the agent will fall Tool calling strategy

For models that don’t support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models.To use this strategy, configure a `ToolStrategy`:

responseFormat:
| JsonSchemaFormat

options?: ToolStrategyOptions

options.toolMessageContent

Custom content for the tool message returned when structured output is generated.
If not provided, defaults to a message showing the structured response data.

options.handleError

Options parameter containing an optional `handleError` parameter for customizing the error handling strategy.

- **`true`**: Catch all errors with default error template (default)
- **`False`**: No retry, let exceptions propagate

Union Types

import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ProductReview = z.object({
rating: z.number().min(1).max(5).optional(),
sentiment: z.enum(["positive", "negative"]),
keyPoints: z.array(z.string()).describe("The key points of the review. Lowercase, 1-3 words each."),
});

const agent = createAgent({
model: "gpt-5",
tools: [],
responseFormat: toolStrategy(ProductReview)
})

const result = await agent.invoke({
"messages": [{"role": "user", "content": "Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'"}]
})

console.log(result.structuredResponse);
// { "rating": 5, "sentiment": "positive", "keyPoints": ["fast shipping", "expensive"] }

### ​ Custom tool message content

The `toolMessageContent` parameter allows you to customize the message that appears in the conversation history when structured output is generated:

const MeetingAction = z.object({
task: z.string().describe("The specific task to be completed"),
assignee: z.string().describe("Person responsible for the task"),
priority: z.enum(["low", "medium", "high"]).describe("Priority level"),
});

const agent = createAgent({
model: "gpt-5",
tools: [],
responseFormat: toolStrategy(MeetingAction, {
toolMessageContent: "Action item captured and added to meeting notes!"
})
});

const result = await agent.invoke({
messages: [{"role": "user", "content": "From our meeting: Sarah needs to update the project timeline as soon as possible"}]
});

console.log(result);
/**
* {
* messages: [\
* { role: "user", content: "From our meeting: Sarah needs to update the project timeline as soon as possible" },\
* { role: "assistant", content: "Action item captured and added to meeting notes!", tool_calls: [ { name: "MeetingAction", args: { task: "update the project timeline", assignee: "Sarah", priority: "high" }, id: "call_456" } ] },\
* { role: "tool", content: "Action item captured and added to meeting notes!", tool_call_id: "call_456", name: "MeetingAction" }\
* ],
* structuredResponse: { task: "update the project timeline", assignee: "Sarah", priority: "high" }
* }
*/

Without `toolMessageContent`, we’d see:

# console.log(result);
/**
* {
* messages: [\
* ...\
* { role: "tool", content: "Returning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}", tool_call_id: "call_456", name: "MeetingAction" }\
* ],
* structuredResponse: { task: "update the project timeline", assignee: "Sarah", priority: "high" }
* }
*/

### ​ Error handling

Models can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.

#### ​ Multiple structured outputs error

When a model incorrectly calls multiple structured output tools, the agent provides error feedback in a `ToolMessage` and prompts the model to retry:

const ContactInfo = z.object({
name: z.string().describe("Person's name"),
email: z.string().describe("Email address"),
});

const EventDetails = z.object({
event_name: z.string().describe("Name of the event"),
date: z.string().describe("Event date"),
});

const agent = createAgent({
model: "gpt-5",
tools: [],
responseFormat: toolStrategy([ContactInfo, EventDetails]),
});

const result = await agent.invoke({
messages: [\
{\
role: "user",\
content:\
"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th",\
},\
],
});

console.log(result);

/**
* {
* messages: [\
* { role: "user", content: "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th" },\
* { role: "assistant", content: "", tool_calls: [ { name: "ContactInfo", args: { name: "John Doe", email: "john@email.com" }, id: "call_1" }, { name: "EventDetails", args: { event_name: "Tech Conference", date: "March 15th" }, id: "call_2" } ] },\
* { role: "tool", content: "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.", tool_call_id: "call_1", name: "ContactInfo" },\
* { role: "tool", content: "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.", tool_call_id: "call_2", name: "EventDetails" },\
* { role: "assistant", content: "", tool_calls: [ { name: "ContactInfo", args: { name: "John Doe", email: "john@email.com" }, id: "call_3" } ] },\
* { role: "tool", content: "Returning structured response: {'name': 'John Doe', 'email': 'john@email.com'}", tool_call_id: "call_3", name: "ContactInfo" }\
* ],
* structuredResponse: { name: "John Doe", email: "john@email.com" }
* }
*/

#### ​ Schema validation error

When structured output doesn’t match the expected schema, the agent provides specific error feedback:

const ProductRating = z.object({
rating: z.number().min(1).max(5).describe("Rating from 1-5"),
comment: z.string().describe("Review comment"),
});

const agent = createAgent({
model: "gpt-5",
tools: [],
responseFormat: toolStrategy(ProductRating),
});

const result = await agent.invoke({
messages: [\
{\
role: "user",\
content: "Parse this: Amazing product, 10/10!",\
},\
],
});

/**
* {
* messages: [\
* { role: "user", content: "Parse this: Amazing product, 10/10!" },\
* { role: "assistant", content: "", tool_calls: [ { name: "ProductRating", args: { rating: 10, comment: "Amazing product" }, id: "call_1" } ] },\
* { role: "tool", content: "Error: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating\nrating\n Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].\n Please fix your mistakes.", tool_call_id: "call_1", name: "ProductRating" },\
* { role: "assistant", content: "", tool_calls: [ { name: "ProductRating", args: { rating: 5, comment: "Amazing product" }, id: "call_2" } ] },\
* { role: "tool", content: "Returning structured response: {'rating': 5, 'comment': 'Amazing product'}", tool_call_id: "call_2", name: "ProductRating" }\
* ],
* structuredResponse: { rating: 5, comment: "Amazing product" }
* }
*/

#### ​ Error handling strategies

You can customize how errors are handled using the `handleErrors` parameter:**Custom error message:**

const responseFormat = toolStrategy(ProductRating, {
handleError: "Please provide a valid rating between 1-5 and include a comment."
)

// Error message becomes:
// { role: "tool", content: "Please provide a valid rating between 1-5 and include a comment." }

**Handle specific exceptions only:**

import { ToolInputParsingException } from "@langchain/core/tools";

const responseFormat = toolStrategy(ProductRating, {

if (error instanceof ToolInputParsingException) {
return "Please provide a valid rating between 1-5 and include a comment.";
}
return error.message;
}
)

// Only validation errors get retried with default message:
// { role: "tool", content: "Error: Failed to parse structured output for tool 'ProductRating': ...\n Please fix your mistakes." }

**Handle multiple exception types:**

if (error instanceof ToolInputParsingException) {
return "Please provide a valid rating between 1-5 and include a comment.";
}
if (error instanceof CustomUserError) {
return "This is a custom user error.";
}
return error.message;
}
)

**No error handling:**

const responseFormat = toolStrategy(ProductRating, {
handleError: false // All errors raised
)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Streaming\\
\\
Previous Overview\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/middleware/overview

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Middleware

Overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- The agent loop
- Additional resources

Middleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following:

- Tracking agent behavior with logging, analytics, and debugging.
- Transforming prompts, tool selection, and output formatting.
- Adding retries, fallbacks, and early termination logic.
- Applying rate limits, guardrails, and PII detection.

Add middleware by passing them to `createAgent`:

Copy

import {
createAgent,
summarizationMiddleware,
humanInTheLoopMiddleware,
} from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [...],
middleware: [summarizationMiddleware, humanInTheLoopMiddleware],
});

## ​ The agent loop

The core agent loop involves calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:!Core agent loop diagramMiddleware exposes hooks before and after each of those steps:!Middleware flow diagram

## ​ Additional resources

**Built-in middleware** \\
\\
Explore built-in middleware for common use cases. **Custom middleware** \\
\\
Build your own middleware with hooks and decorators. **Middleware API reference** \\
\\
Complete API reference for middleware. **Testing agents** \\
\\
Test your agents with LangSmith.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Structured output\\
\\
Previous Built-in middleware\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/middleware/built-in

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Middleware

Built-in middleware

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Provider-agnostic middleware
- Summarization
- Human-in-the-loop
- Model call limit
- Tool call limit
- Model fallback
- PII detection
- Custom PII types
- To-do list
- LLM tool selector
- Tool retry
- Model retry
- LLM tool emulator
- Context editing
- Provider-specific middleware

LangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.

## ​ Provider-agnostic middleware

The following middleware work with any LLM provider:

| Middleware | Description |
| --- | --- |
| Summarization | Automatically summarize conversation history when approaching token limits. |
| Human-in-the-loop | Pause execution for human approval of tool calls. |
| Model call limit | Limit the number of model calls to prevent excessive costs. |
| Tool call limit | Control tool execution by limiting call counts. |
| Model fallback | Automatically fall | Detect and handle Personally Identifiable Information (PII). |
| To-do list | Equip agents with task planning and tracking capabilities. |
| LLM tool selector | Use an LLM to select relevant tools before calling main model. |
| Tool retry | Automatically retry failed tool calls with exponential backoff. |
| Model retry | Automatically retry failed model calls with exponential backoff. |
| LLM tool emulator | Emulate tool execution using an LLM for testing purposes. |
| Context editing | Manage conversation context by trimming or clearing tool uses. |

### ​ Summarization

Automatically summarize conversation history when approaching token limits, preserving recent messages while compressing older context. Summarization is useful for the following:

- Long-running conversations that exceed context windows.
- Multi-turn dialogues with extensive history.
- Applications where preserving full conversation context matters.

Copy

import { createAgent, summarizationMiddleware } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [weatherTool, calculatorTool],
middleware: [\
summarizationMiddleware({\
model: "gpt-4o-mini",\
trigger: { tokens: 4000 },\
keep: { messages: 20 },\
}),\
],
});

Configuration options

The `fraction` conditions for `trigger` and `keep` (shown below) rely on a chat model’s profile data if using `langchain@1.1.0`. If data are not available, use another condition or specify manually:

const customProfile: ModelProfile = {
maxInputTokens: 100_000,
// ...
}
model = await initChatModel("...", {
profile: customProfile,
});

​

model

string \| BaseChatModel

required

Model for generating summaries. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance.

trigger

object \| object\[\]

Conditions for triggering summarization. Can be:

- A single condition object (all properties must be met - AND logic)
- An array of condition objects (any condition must be met - OR logic)

Each condition can include:

- `fraction` (number): Fraction of model’s context size (0-1)
- `tokens` (number): Absolute token count
- `messages` (number): Message count

At least one property must be specified per condition. If not provided, summarization will not trigger automatically.

keep

object

default:"{messages: 20}"

How much context to preserve after summarization. Specify exactly one of:

- `fraction` (number): Fraction of model’s context size to keep (0-1)
- `tokens` (number): Absolute token count to keep
- `messages` (number): Number of recent messages to keep

tokenCounter

function

Custom token counting function. Defaults to character-based counting.

summaryPrompt

string

Custom prompt template for summarization. Uses built-in template if not specified. The template should include `{messages}` placeholder where conversation history will be inserted.

trimTokensToSummarize

number

default:"4000"

Maximum number of tokens to include when generating the summary. Messages will be trimmed to fit this limit before summarization.

summaryPrefix

Prefix to add to the summary message. If not provided, a default prefix is used.

maxTokensBeforeSummary

deprecated

**Deprecated:** Use `trigger: { tokens: value }` instead. Token threshold for triggering summarization.

messagesToKeep

**Deprecated:** Use `keep: { messages: value }` instead. Recent messages to preserve.

Full example

The summarization middleware monitors message token counts and automatically summarizes older messages when thresholds are reached.**Trigger conditions** control when summarization runs:

- Single condition object (specified must be met)
- Array of conditions (any condition must be met - OR logic)
- Each condition can use `fraction` (of model’s context size), `tokens` (absolute count), or `messages` (message count)

**Keep condition** control how much context to preserve (specify exactly one):

- `fraction` \- Fraction of model’s context size to keep
- `tokens` \- Absolute token count to keep
- `messages` \- Number of recent messages to keep

// Single condition
const agent = createAgent({
model: "gpt-4o",
tools: [weatherTool, calculatorTool],
middleware: [\
summarizationMiddleware({\
model: "gpt-4o-mini",\
trigger: { tokens: 4000, messages: 10 },\
keep: { messages: 20 },\
}),\
],
});

// Multiple conditions
const agent2 = createAgent({
model: "gpt-4o",
tools: [weatherTool, calculatorTool],
middleware: [\
summarizationMiddleware({\
model: "gpt-4o-mini",\
trigger: [\
{ tokens: 3000, messages: 6 },\
],\
keep: { messages: 20 },\
}),\
],
});

// Using fractional limits
const agent3 = createAgent({
model: "gpt-4o",
tools: [weatherTool, calculatorTool],
middleware: [\
summarizationMiddleware({\
model: "gpt-4o-mini",\
trigger: { fraction: 0.8 },\
keep: { fraction: 0.3 },\
}),\
],
});

### ​ Human-in-the-loop

Pause agent execution for human approval, editing, or rejection of tool calls before they execute. Human-in-the-loop is useful for the following:

- High-stakes operations requiring human approval (e.g. database writes, financial transactions).
- Compliance workflows where human oversight is mandatory.
- Long-running conversations where human feedback guides the agent.

Human-in-the-loop middleware requires a checkpointer to maintain state across interruptions.

import { createAgent, humanInTheLoopMiddleware } from "langchain";

function readEmailTool(emailId: string): string {
/** Mock function to read an email by its ID. */
return `Email content for ID: ${emailId}`;
}

function sendEmailTool(recipient: string, subject: string, body: string): string {
/** Mock function to send an email. */
return `Email sent to ${recipient} with subject '${subject}'`;
}

const agent = createAgent({
model: "gpt-4o",
tools: [readEmailTool, sendEmailTool],
middleware: [\
humanInTheLoopMiddleware({\
interruptOn: {\
sendEmailTool: {\
allowedDecisions: ["approve", "edit", "reject"],\
},\
readEmailTool: false,\
}\
})\
]
});

For complete examples, configuration options, and integration patterns, see the Human-in-the-loop documentation.

Watch this video guide demonstrating Human-in-the-loop middleware behavior.

### ​ Model call limit

Limit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following:

- Preventing runaway agents from making too many API calls.
- Enforcing cost controls on production deployments.
- Testing agent behavior within specific call budgets.

import { createAgent, modelCallLimitMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const agent = createAgent({
model: "gpt-4o",
checkpointer: new MemorySaver(), // Required for thread limiting
tools: [],
middleware: [\
modelCallLimitMiddleware({\
threadLimit: 10,\
runLimit: 5,\
exitBehavior: "end",\
}),\
],
});

Watch this video guide demonstrating Model Call Limit middleware behavior.

threadLimit

Maximum model calls across all runs in a thread. Defaults to no limit.

runLimit

Maximum model calls per single invocation. Defaults to no limit.

exitBehavior

default:"end"

Behavior when limit is reached. Options: `'end'` (graceful termination) or `'error'` (throw exception)

### ​ Tool call limit

Control agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following:

- Preventing excessive calls to expensive external APIs.
- Limiting web searches or database queries.
- Enforcing rate limits on specific tool usage.
- Protecting against runaway agent loops.

import { createAgent, toolCallLimitMiddleware } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [searchTool, databaseTool],
middleware: [\
toolCallLimitMiddleware({ threadLimit: 20, runLimit: 10 }),\
toolCallLimitMiddleware({\
toolName: "search",\
threadLimit: 5,\
runLimit: 3,\
}),\
],
});

Watch this video guide demonstrating Tool Call Limit middleware behavior.

toolName

Name of specific tool to limit. If not provided, limits apply to **all tools globally**.

Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. `undefined` means no thread limit.

Maximum tool calls per single invocation (one user message → response cycle). Resets with each new user message. `undefined` means no run limit.**Note:** At least one of `threadLimit` or `runLimit` must be specified.

default:"continue"

Behavior when limit is reached:

- `'continue'` (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages.
- `'error'` \- Throw a `ToolCallLimitExceededError` exception, stopping execution immediately
- `'end'` \- Stop execution immediately with a ToolMessage and AI message for the exceeded tool call. Only works when limiting a single tool; throws error if other tools have pending calls.

Specify limits with:

- **Thread limit** \- Max calls across all runs in a conversation (requires checkpointer)
- **Run limit** \- Max calls per single invocation (resets each turn)

Exit behaviors:

- `'continue'` (default) - Block exceeded calls with error messages, agent continues
- `'error'` \- Raise exception immediately
- `'end'` \- Stop with ToolMessage + AI message (single-tool scenarios only)

const globalLimiter = toolCallLimitMiddleware({ threadLimit: 20, runLimit: 10 });
const searchLimiter = toolCallLimitMiddleware({ toolName: "search", threadLimit: 5, runLimit: 3 });
const databaseLimiter = toolCallLimitMiddleware({ toolName: "query_database", threadLimit: 10 });
const strictLimiter = toolCallLimitMiddleware({ toolName: "scrape_webpage", runLimit: 2, exitBehavior: "error" });

const agent = createAgent({
model: "gpt-4o",
tools: [searchTool, databaseTool, scraperTool],
middleware: [globalLimiter, searchLimiter, databaseLimiter, strictLimiter],
});

### ​ Model fallback

Automatically fall,\
],
});

The middleware accepts a variable number of string arguments representing fallback models in order:

...models

string\[\]

One or more fallback model strings to try in order when the primary model fails

modelFallbackMiddleware(
"first-fallback-model",
"second-fallback-model",
// ... more models
)

### ​ PII detection

Detect and handle Personally Identifiable Information (PII) in conversations using configurable strategies. PII detection is useful for the following:

- Healthcare and financial applications with compliance requirements.
- Customer service agents that need to sanitize logs.
- Any application handling sensitive user data.

import { createAgent, piiMiddleware } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [],
middleware: [\
piiMiddleware("email", { strategy: "redact", applyToInput: true }),\
piiMiddleware("credit_card", { strategy: "mask", applyToInput: true }),\
],
});

#### ​ Custom PII types

You can create custom PII types by providing a `detector` parameter. This allows you to detect patterns specific to your use case beyond the built-in types.**Three ways to create custom detectors:**

1. **Regex pattern string** \- Simple pattern matching
2. **RegExp object** \- More control over regex flags
3. **Custom function** \- Complex detection logic with validation

import { createAgent, piiMiddleware, type PIIMatch } from "langchain";

// Method 1: Regex pattern string
const agent1 = createAgent({
model: "gpt-4o",
tools: [],
middleware: [\
piiMiddleware("api_key", {\
detector: "sk-[a-zA-Z0-9]{32}",\
strategy: "block",\
}),\
],
});

// Method 2: RegExp object
const agent2 = createAgent({
model: "gpt-4o",
tools: [],
middleware: [\
piiMiddleware("phone_number", {\
detector: /\+?\d{1,3}[\s.-]?\d{3,4}[\s.-]?\d{4}/,\
strategy: "mask",\
}),\
],
});

// Method 3: Custom detector function
function detectSSN(content: string): PIIMatch[] {
const matches: PIIMatch[] = [];
const pattern = /\d{3}-\d{2}-\d{4}/g;
let match: RegExpExecArray | null;

while ((match = pattern.exec(content)) !== null) {
const ssn = match[0];
// Validate: first 3 digits shouldn't be 000, 666, or 900-999
const firstThree = parseInt(ssn.substring(0, 3), 10);

matches.push({
text: ssn,
start: match.index ?? 0,
end: (match.index ?? 0) + ssn.length,
});
}
}
return matches;
}

const agent3 = createAgent({
model: "gpt-4o",
tools: [],
middleware: [\
piiMiddleware("ssn", {\
detector: detectSSN,\
strategy: "hash",\
}),\
],
});

**Custom detector function signature:**The detector function must accept a string (content) and return matches:Returns an array of `PIIMatch` objects:

interface PIIMatch {
text: string; // The matched text
start: number; // Start index in content
end: number; // End index in content
}

function detector(content: string): PIIMatch[] {
return [\
{ text: "matched_text", start: 0, end: 12 },\
// ... more matches\
];
}

For custom detectors:

- Use regex strings for simple patterns
- Use RegExp objects when you need flags (e.g., case-insensitive matching)
- Use custom functions when you need validation logic beyond pattern matching
- Custom functions give you full control over detection logic and can implement complex validation rules

piiType

Type of PII to detect. Can be a built-in type (`email`, `credit_card`, `ip`, `mac_address`, `url`) or a custom type name.

strategy

default:"redact"

How to handle detected PII. Options:

- `'block'` \- Throw error when detected
- `'redact'` \- Replace with `[REDACTED_TYPE]`
- `'mask'` \- Partially mask (e.g., `****-****-****-1234`)

detector

RegExp \| string \| function

Custom detector. Can be:

- `RegExp` \- Regex pattern for matching
- `string` \- Regex pattern string (e.g., `"sk-[a-zA-Z0-9]{32}"`)

If not provided, uses built-in detector for the PII type.

applyToInput

boolean

default:"true"

Check user messages before model call

applyToOutput

default:"false"

Check AI messages after model call

applyToToolResults

Check tool result messages after execution

### ​ To-do list

Equip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:

- Complex multi-step tasks requiring coordination across multiple tools.
- Long-running operations where progress visibility is important.

This middleware automatically provides agents with a `write_todos` tool and system prompts to guide effective task planning.

import { createAgent, todoListMiddleware } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [readFile, writeFile, runTests],
middleware: [todoListMiddleware()],
});

Watch this video guide demonstrating To-do List middleware behavior.

No configuration options available (uses defaults).

### ​ LLM tool selector

Use an LLM to intelligently select relevant tools before calling the main model. LLM tool selectors are useful for the following:

- Agents with many tools (10+) where most aren’t relevant per query.
- Reducing token usage by filtering irrelevant tools.
- Improving model focus and accuracy.

This middleware uses structured output to ask an LLM which tools are most relevant for the current query. The structured output schema defines the available tool names and descriptions. Model providers often add this structured output information to the system prompt behind the scenes.

import { createAgent, llmToolSelectorMiddleware } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [tool1, tool2, tool3, tool4, tool5, ...],
middleware: [\
llmToolSelectorMiddleware({\
model: "gpt-4o-mini",\
maxTools: 3,\
alwaysInclude: ["search"],\
}),\
],
});

Model for tool selection. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance. Defaults to the agent’s main model.

systemPrompt

Instructions for the selection model. Uses built-in prompt if not specified.

maxTools

Maximum number of tools to select. If the model selects more, only the first maxTools will be used. No limit if not specified.

alwaysInclude

Tool names to always include regardless of selection. These do not count against the maxTools limit.

### ​ Tool retry

Automatically retry failed tool calls with configurable exponential backoff. Tool retry is useful for the following:

- Handling transient failures in external API calls.
- Improving reliability of network-dependent tools.
- Building resilient agents that gracefully handle temporary errors.

**API reference:** `toolRetryMiddleware`

import { createAgent, toolRetryMiddleware } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [searchTool, databaseTool],
middleware: [\
toolRetryMiddleware({\
maxRetries: 3,\
backoffFactor: 2.0,\
initialDelayMs: 1000,\
}),\
],
});

maxRetries

default:"2"

tools

(ClientTool \| ServerTool \| string)\[\]

Optional array of tools or tool names to apply retry logic to. Can be a list of `BaseTool` instances or tool name strings. If `undefined`, applies to all tools.

retryOn

Either an array of error constructors to retry on, or a function that takes an error and returns `true` if it should be retried. Default is to retry on all errors.

onFailure

Behavior when all retries are exhausted. Options:

- `'continue'` (default) - Return a `ToolMessage` with error details, allowing the LLM to handle the failure and potentially recover
- `'error'` \- Re-raise the exception, stopping agent execution
- Custom function - Function that takes the exception and returns a string for the `ToolMessage` content, allowing custom error formatting

**Deprecated values:**`'raise'` (use `'error'` instead) and `'return_message'` (use `'continue'` instead). These deprecated values still work but will show a warning.

backoffFactor

default:"2.0"

initialDelayMs

default:"1000"

maxDelayMs

default:"60000"

jitter

Whether to add random jitter (`±25%`) to delay to avoid thundering herd

The middleware automatically retries failed tool calls with exponential backoff.**Key configuration:**

- `maxRetries` \- Number of retry attempts (default: 2)
- `backoffFactor` \- Multiplier for exponential backoff (default: 2.0)
- `initialDelayMs` \- Starting delay in milliseconds (default: 1000ms)
- `maxDelayMs` \- Cap on delay growth (default: 60000ms)
- `jitter` \- Add random variation (default: true)

**Failure handling:**

- `onFailure: "continue"` (default) - Return error message
- `onFailure: "error"` \- Re-raise exception
- Custom function - Function returning error message

import { createAgent, toolRetryMiddleware } from "langchain";
import { tool } from "@langchain/core/tools";
import { z } from "zod";

// Basic usage with default settings (2 retries, exponential backoff)
const agent = createAgent({
model: "gpt-4o",
tools: [searchTool, databaseTool],
middleware: [toolRetryMiddleware()],
});

// Retry specific exceptions only
const retry = toolRetryMiddleware({
maxRetries: 4,
retryOn: [TimeoutError, NetworkError],
backoffFactor: 1.5,
});

// Custom exception filtering
function shouldRetry(error: Error): boolean {
// Only retry on 5xx errors
if (error.name === "HTTPError" && "statusCode" in error) {
const statusCode = (error as any).statusCode;
return 500 <= statusCode && statusCode < 600;
}
return false;
}

const retryWithFilter = toolRetryMiddleware({
maxRetries: 3,
retryOn: shouldRetry,
});

// Apply to specific tools with custom error handling

const retrySpecificTools = toolRetryMiddleware({
maxRetries: 4,
tools: ["search_database"],
onFailure: formatError,
});

// Apply to specific tools using BaseTool instances
const searchDatabase = tool(

// Search implementation
return results;
},
{
name: "search_database",
description: "Search the database",
schema: z.object({ query: z.string() }),
}
);

const retryWithToolInstance = toolRetryMiddleware({
maxRetries: 4,
tools: [searchDatabase], // Pass BaseTool instance
});

// Constant backoff (no exponential growth)
const constantBackoff = toolRetryMiddleware({
maxRetries: 5,
backoffFactor: 0.0, // No exponential growth
initialDelayMs: 2000, // Always wait 2 seconds
});

// Raise exception on failure
const strictRetry = toolRetryMiddleware({
maxRetries: 2,
onFailure: "error", // Re-raise exception instead of returning message
});

### ​ Model retry

Automatically retry failed model calls with configurable exponential backoff. Model retry is useful for the following:

- Handling transient failures in model API calls.
- Improving reliability of network-dependent model requests.
- Building resilient agents that gracefully handle temporary model errors.

**API reference:** `modelRetryMiddleware`

import { createAgent, modelRetryMiddleware } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [searchTool, databaseTool],
middleware: [\
modelRetryMiddleware({\
maxRetries: 3,\
backoffFactor: 2.0,\
initialDelayMs: 1000,\
}),\
],
});

- `'continue'` (default) - Return an `AIMessage` with error details, allowing the agent to potentially handle the failure gracefully
- `'error'` \- Re-raise the exception, stopping agent execution
- Custom function - Function that takes the exception and returns a string for the `AIMessage` content, allowing custom error formatting

The middleware automatically retries failed model calls with exponential backoff.

// Basic usage with default settings (2 retries, exponential backoff)
const agent = createAgent({
model: "gpt-4o",
tools: [searchTool],
middleware: [modelRetryMiddleware()],
});

class TimeoutError extends Error {
// ...
}
class NetworkError extends Error {
// ...
}

// Retry specific exceptions only
const retry = modelRetryMiddleware({
maxRetries: 4,
retryOn: [TimeoutError, NetworkError],
backoffFactor: 1.5,
});

// Custom exception filtering
function shouldRetry(error: Error): boolean {
// Only retry on rate limit errors
if (error.name === "RateLimitError") {
return true;
}
// Or check for specific HTTP status codes
if (error.name === "HTTPError" && "statusCode" in error) {
const statusCode = (error as any).statusCode;
return statusCode === 429 || statusCode === 503;
}
return false;
}

const retryWithFilter = modelRetryMiddleware({
maxRetries: 3,
retryOn: shouldRetry,
});

// Return error message instead of raising
const retryContinue = modelRetryMiddleware({
maxRetries: 4,
onFailure: "continue", // Return AIMessage with error instead of throwing
});

// Custom error message formatting

const retryWithFormatter = modelRetryMiddleware({
maxRetries: 4,
onFailure: formatError,
});

// Constant backoff (no exponential growth)
const constantBackoff = modelRetryMiddleware({
maxRetries: 5,
backoffFactor: 0.0, // No exponential growth
initialDelayMs: 2000, // Always wait 2 seconds
});

// Raise exception on failure
const strictRetry = modelRetryMiddleware({
maxRetries: 2,
onFailure: "error", // Re-raise exception instead of returning message
});

### ​ LLM tool emulator

Emulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses. LLM tool emulators are useful for the following:

- Testing agent behavior without executing real tools.
- Developing agents when external tools are unavailable or expensive.
- Prototyping agent workflows before implementing actual tools.

import { createAgent, toolEmulatorMiddleware } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [getWeather, searchDatabase, sendEmail],
middleware: [\
toolEmulatorMiddleware(), // Emulate all tools\
],
});

(string \| ClientTool \| ServerTool)\[\]

List of tool names (string) or tool instances to emulate. If `undefined` (default), ALL tools will be emulated. If empty array `[]`, no tools will be emulated. If array with tool names/instances, only those tools will be emulated.

Model to use for generating emulated tool responses. Can be a model identifier string (e.g., `'anthropic:claude-sonnet-4-5-20250929'`) or a `BaseChatModel` instance. Defaults to the agent’s model if not specified.

The middleware uses an LLM to generate plausible responses for tool calls instead of executing the actual tools.

import { createAgent, toolEmulatorMiddleware, tool } from "langchain";
import * as z from "zod";

const getWeather = tool(

{
name: "get_weather",
description: "Get the current weather for a location",
schema: z.object({ location: z.string() }),
}
);

const sendEmail = tool(

{
name: "send_email",
description: "Send an email",
schema: z.object({
to: z.string(),
subject: z.string(),
body: z.string(),
}),
}
);

// Emulate all tools (default behavior)
const agent = createAgent({
model: "gpt-4o",
tools: [getWeather, sendEmail],
middleware: [toolEmulatorMiddleware()],
});

// Emulate specific tools by name
const agent2 = createAgent({
model: "gpt-4o",
tools: [getWeather, sendEmail],
middleware: [\
toolEmulatorMiddleware({\
tools: ["get_weather"],\
}),\
],
});

// Emulate specific tools by passing tool instances
const agent3 = createAgent({
model: "gpt-4o",
tools: [getWeather, sendEmail],
middleware: [\
toolEmulatorMiddleware({\
tools: [getWeather],\
}),\
],
});

// Use custom model for emulation
const agent5 = createAgent({
model: "gpt-4o",
tools: [getWeather, sendEmail],
middleware: [\
toolEmulatorMiddleware({\
model: "claude-sonnet-4-5-20250929",\
}),\
],
});

### ​ Context editing

Manage conversation context by clearing older tool call outputs when token limits are reached, while preserving recent results. This helps keep context windows manageable in long conversations with many tool calls. Context editing is useful for the following:

- Long conversations with many tool calls that exceed token limits
- Reducing token costs by removing older tool outputs that are no longer relevant
- Maintaining only the most recent N tool results in context

import { createAgent, contextEditingMiddleware, ClearToolUsesEdit } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [],
middleware: [\
contextEditingMiddleware({\
edits: [\
new ClearToolUsesEdit({\
triggerTokens: 100000,\
keep: 3,\
}),\
],\
}),\
],
});

edits

ContextEdit\[\]

default:"\[new ClearToolUsesEdit()\]"

Array of `ContextEdit` strategies to apply

**`ClearToolUsesEdit` options:**

triggerTokens

default:"100000"

Token count that triggers the edit. When the conversation exceeds this token count, older tool outputs will be cleared.

clearAtLeast

default:"0"

Minimum number of tokens to reclaim when the edit runs. If set to 0, clears as much as needed.

default:"3"

Number of most recent tool results that must be preserved. These will never be cleared.

clearToolInputs

Whether to clear the originating tool call parameters on the AI message. When `true`, tool call arguments are replaced with empty objects.

excludeTools

default:"\[\]"

List of tool names to exclude from clearing. These tools will never have their outputs cleared.

placeholder

default:"\[cleared\]"

Placeholder text inserted for cleared tool outputs. This replaces the original tool message content.

The middleware applies context editing strategies when token limits are reached. The most common strategy is `ClearToolUsesEdit`, which clears older tool results while preserving recent ones.**How it works:**

1. Monitor token count in conversation
2. When threshold is reached, clear older tool outputs
3. Keep most recent N tool results
4. Optionally preserve tool call arguments for context

const agent = createAgent({
model: "gpt-4o",
tools: [searchTool, calculatorTool, databaseTool],
middleware: [\
contextEditingMiddleware({\
edits: [\
new ClearToolUsesEdit({\
triggerTokens: 2000,\
keep: 3,\
clearToolInputs: false,\
excludeTools: [],\
placeholder: "[cleared]",\
}),\
],\
}),\
],
});

## ​ Provider-specific middleware

These middleware are optimized for specific LLM providers. See each provider’s documentation for full details and examples.

**Anthropic** \\
\\
Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Overview\\
\\
Previous Custom middleware\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/middleware/custom

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Middleware

Custom middleware

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Hooks
- Node-style hooks
- Wrap-style hooks
- Create middleware
- Custom state schema
- Custom context
- Execution order
- Agent jumps
- Best practices
- Examples
- Dynamic model selection
- Tool call monitoring
- Dynamically selecting tools
- Working with system messages
- Additional resources

Build custom middleware by implementing hooks that run at specific points in the agent execution flow.

## ​ Hooks

Middleware provides two styles of hooks to intercept agent execution:

**Node-style hooks** \\
\\
Run sequentially at specific execution points. **Wrap-style hooks** \\
\\
Run around each model or tool call.

### ​ Node-style hooks

Run sequentially at specific execution points. Use for logging, validation, and state updates.**Available hooks:**

- `beforeAgent` \- Before agent starts (once per invocation)
- `beforeModel` \- Before each model call
- `afterModel` \- After each model response
- `afterAgent` \- After agent completes (once per invocation)

**Example:**

Copy

import { createMiddleware, AIMessage } from "langchain";

return createMiddleware({
name: "MessageLimitMiddleware",

if (state.messages.length === maxMessages) {
return {
messages: [new AIMessage("Conversation limit reached.")],
jumpTo: "end",
};
}
return;
},

const lastMessage = state.messages[state.messages.length - 1];
console.log(`Model returned: ${lastMessage.content}`);
return;
},
});
};

### ​ Wrap-style hooks

Intercept execution and control when the handler is called. Use for retries, caching, and transformation.You decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).**Available hooks:**

- `wrapModelCall` \- Around each model call
- `wrapToolCall` \- Around each tool call

import { createMiddleware } from "langchain";

return createMiddleware({
name: "RetryMiddleware",

for (let attempt = 0; attempt < maxRetries; attempt++) {
try {
return handler(request);
} catch (e) {
if (attempt === maxRetries - 1) {
throw e;
}
console.log(`Retry ${attempt + 1}/${maxRetries} after error: ${e}`);
}
}
throw new Error("Unreachable");
},
});
};

## ​ Create middleware

Use the `createMiddleware` function to define custom middleware:

const loggingMiddleware = createMiddleware({
name: "LoggingMiddleware",

console.log(`About to call model with ${state.messages.length} messages`);
return;
},

const lastMessage = state.messages[state.messages.length - 1];
console.log(`Model returned: ${lastMessage.content}`);
return;
},
});

## ​ Custom state schema

Middleware can extend the agent’s state with custom properties. This enables middleware to:

- **Track state across execution**: Maintain counters, flags, or other values that persist throughout the agent’s execution lifecycle
- **Share data between hooks**: Pass information from `beforeModel` to `afterModel` or between different middleware instances
- **Implement cross-cutting concerns**: Add functionality like rate limiting, usage tracking, user context, or audit logging without modifying the core agent logic
- **Make conditional decisions**: Use accumulated state to determine whether to continue execution, jump to different nodes, or modify behavior dynamically

import { createMiddleware, createAgent, HumanMessage } from "langchain";
import * as z from "zod";

const callCounterMiddleware = createMiddleware({
name: "CallCounterMiddleware",
stateSchema: z.object({
modelCallCount: z.number().default(0),
userId: z.string().optional(),
}),

return { jumpTo: "end" };
}
return;
},

return { modelCallCount: state.modelCallCount + 1 };
},
});

const agent = createAgent({
model: "gpt-4o",
tools: [...],
middleware: [callCounterMiddleware],
});

const result = await agent.invoke({
messages: [new HumanMessage("Hello")],
modelCallCount: 0,
userId: "user-123",
});

State fields can be either public or private. Fields that start with an underscore (`_`) are considered private and will not be included in the agent’s result. Only public fields (those without a leading underscore) are returned.This is useful for storing internal middleware state that shouldn’t be exposed to the caller, such as temporary tracking variables or internal flags:

const middleware = createMiddleware({
name: "ExampleMiddleware",
stateSchema: z.object({
// Public field - included in invoke result
publicCounter: z.number().default(0),
// Private field - excluded from invoke result
_internalFlag: z.boolean().default(false),
}),

// Both fields are accessible during execution
if (state._internalFlag) {
return { publicCounter: state.publicCounter + 1 };
}
return { _internalFlag: true };
},
});

const result = await agent.invoke({
messages: [new HumanMessage("Hello")],
publicCounter: 0
});

// result only contains publicCounter, not _internalFlag
console.log(result.publicCounter); // 1
console.log(result._internalFlag); // undefined

## ​ Custom context

Middleware can define a custom context schema to access per-invocation metadata. Unlike state, context is read-only and not persisted between invocations. This makes it ideal for:

- **User information**: Pass user ID, roles, or preferences that don’t change during execution
- **Configuration overrides**: Provide per-invocation settings like rate limits or feature flags
- **Tenant/workspace context**: Include organization-specific data for multi-tenant applications
- **Request metadata**: Pass request IDs, API keys, or other metadata needed by middleware

Define a context schema using Zod and access it via `runtime.context` in middleware hooks. Required fields in the context schema will be enforced at the TypeScript level, ensuring you must provide them when calling `agent.invoke()`.

import { createAgent, createMiddleware, HumanMessage } from "langchain";
import * as z from "zod";

const contextSchema = z.object({
userId: z.string(),
tenantId: z.string(),
apiKey: z.string().optional(),
});

const userContextMiddleware = createMiddleware({
name: "UserContextMiddleware",
contextSchema,

// Access context from runtime
const { userId, tenantId } = request.runtime.context;

// Add user context to system message
const contextText = `User ID: ${userId}, Tenant: ${tenantId}`;
const newSystemMessage = request.systemMessage.concat(contextText);

return handler({
...request,
systemMessage: newSystemMessage,
});
},
});

const agent = createAgent({
model: "gpt-4o",
middleware: [userContextMiddleware],
tools: [],
contextSchema,
});

const result = await agent.invoke(
{ messages: [new HumanMessage("Hello")] },
// Required fields (userId, tenantId) must be provided
{
context: {
userId: "user-123",
tenantId: "acme-corp",
},
}
);

**Required context fields**: When you define required fields in your `contextSchema` (fields without `.optional()` or `.default()`), TypeScript will enforce that these fields must be provided during `agent.invoke()` calls. This ensures type safety and prevents runtime errors from missing required context.

// This will cause a TypeScript error if userId or tenantId are missing
const result = await agent.invoke(
{ messages: [new HumanMessage("Hello")] },
{ context: { userId: "user-123" } } // Error: tenantId is required
);

## ​ Execution order

When using multiple middleware, understand how they execute:

const agent = createAgent({
model: "gpt-4o",
middleware: [middleware1, middleware2, middleware3],
tools: [...],
});

Execution flow

**Before hooks run in order:**

1. `middleware1.before_agent()`
2. `middleware2.before_agent()`
3. `middleware3.before_agent()`

**Agent loop starts**

4. `middleware1.before_model()`
5. `middleware2.before_model()`
6. `middleware3.before_model()`

**Wrap hooks nest like function calls:**

7. `middleware1.wrap_model_call()` → `middleware2.wrap_model_call()` → `middleware3.wrap_model_call()` → model

**After hooks run in reverse order:**

8. `middleware3.after_model()`
9. `middleware2.after_model()`
10. `middleware1.after_model()`

**Agent loop ends**

11. `middleware3.after_agent()`
12. `middleware2.after_agent()`
13. `middleware1.after_agent()`

**Key rules:**

- `before_*` hooks: First to last
- `after_*` hooks: Last to first (reverse)
- `wrap_*` hooks: Nested (first middleware wraps all others)

## ​ Agent jumps

To exit early from middleware, return a dictionary with `jump_to`:**Available jump targets:**

- `'end'`: Jump to the end of the agent execution (or the first `after_agent` hook)
- `'tools'`: Jump to the tools node
- `'model'`: Jump to the model node (or the first `before_model` hook)

import { createAgent, createMiddleware, AIMessage } from "langchain";

const agent = createAgent({
model: "gpt-4o",
middleware: [\
createMiddleware({\
name: "BlockedContentMiddleware",\
beforeModel: {\
canJumpTo: ["end"],\

if (state.messages.at(-1)?.content.includes("BLOCKED")) {\
return {\
messages: [new AIMessage("I cannot respond to that request.")],\
jumpTo: "end" as const,\
};\
}\
return;\
},\
},\
}),\
],
});

const result = await agent.invoke({
messages: "Hello, world! BLOCKED"
});

/**
* Expected output:
* I cannot respond to that request.
*/
console.log(result.messages.at(-1)?.content);

## ​ Best practices

1. Keep middleware focused - each should do one thing well
2. Handle errors gracefully - don’t let middleware errors crash the agent
3. **Use appropriate hook types**:

- Node-style for sequential logic (logging, validation)
- Wrap-style for control flow (retry, fallback, caching)
4. Clearly document any custom state properties
5. Unit test middleware independently before integrating
6. Consider execution order - place critical middleware first in the list
7. Use built-in middleware when possible

## ​ Examples

### ​ Dynamic model selection

import { createMiddleware, initChatModel } from "langchain";

const dynamicModelMiddleware = createMiddleware({
name: "DynamicModelMiddleware",

const modifiedRequest = { ...request };

modifiedRequest.model = initChatModel("gpt-4o");
} else {
modifiedRequest.model = initChatModel("gpt-4o-mini");
}
return handler(modifiedRequest);
},
});

### ​ Tool call monitoring

const toolMonitoringMiddleware = createMiddleware({
name: "ToolMonitoringMiddleware",

console.log(`Executing tool: ${request.toolCall.name}`);
console.log(`Arguments: ${JSON.stringify(request.toolCall.args)}`);
try {
const result = handler(request);
console.log("Tool completed successfully");
return result;
} catch (e) {
console.log(`Tool failed: ${e}`);
throw e;
}
},
});

### ​ Dynamically selecting tools

Select relevant tools at runtime to improve performance and accuracy.**Benefits:**

- **Shorter prompts** \- Reduce complexity by exposing only relevant tools
- **Better accuracy** \- Models choose correctly from fewer options
- **Permission control** \- Dynamically filter tools based on user access

import { createAgent, createMiddleware } from "langchain";

const toolSelectorMiddleware = createMiddleware({
name: "ToolSelector",

// Select a small, relevant subset of tools based on state/context
const relevantTools = selectRelevantTools(request.state, request.runtime);
const modifiedRequest = { ...request, tools: relevantTools };
return handler(modifiedRequest);
},
});

const agent = createAgent({
model: "gpt-4o",
tools: allTools,
middleware: [toolSelectorMiddleware],
});

### ​ Working with system messages

Modify system messages in middleware using the `systemMessage` field in `ModelRequest`. It contains a `SystemMessage` object (even if the agent was created with a string `systemPrompt`).**Example: Chaining middleware** \- Different middleware can use different approaches:

import { createMiddleware, SystemMessage, createAgent } from "langchain";

// Middleware 1: Uses systemMessage with simple concatenation
const myMiddleware = createMiddleware({
name: "MyMiddleware",

return handler({
...request,
systemMessage: request.systemMessage.concat(`Additional context.`),
});
},
});

// Middleware 2: Uses systemMessage with structured content (preserves structure)
const myOtherMiddleware = createMiddleware({
name: "MyOtherMiddleware",

return handler({
...request,
systemMessage: request.systemMessage.concat(
new SystemMessage({
content: [\
{\
type: "text",\
text: " More additional context. This will be cached.",\
cache_control: { type: "ephemeral", ttl: "5m" },\
},\
],
})
),
});
},
});

const agent = createAgent({
model: "anthropic:claude-3-5-sonnet",
systemPrompt: "You are a helpful assistant.",
middleware: [myMiddleware, myOtherMiddleware],
});

The resulting system message will be:

new SystemMessage({
content: [\
{ type: "text", text: "You are a helpful assistant." },\
{ type: "text", text: "Additional context." },\
{\
type: "text",\
text: " More additional context. This will be cached.",\
cache_control: { type: "ephemeral", ttl: "5m" },\
},\
],
});

Use `SystemMessage.concat` to preserve cache control metadata or structured content blocks created by other middleware.

## ​ Additional resources

- Middleware API reference
- Built-in middleware
- Testing agents

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Built-in middleware\\
\\
Previous Guardrails\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/guardrails

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Guardrails

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Built-in guardrails
- PII detection
- Human-in-the-loop
- Custom guardrails
- Before agent guardrails
- After agent guardrails
- Combine multiple guardrails
- Additional resources

Guardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent’s execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems.Common use cases include:

- Preventing PII leakage
- Detecting and blocking prompt injection attacks
- Blocking inappropriate or harmful content
- Enforcing business rules and compliance requirements
- Validating output quality and accuracy

You can implement guardrails using middleware to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.

Guardrails can be implemented using two complementary approaches:

## Deterministic guardrails

Use rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.

## Model-based guardrails

Use LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.

LangChain provides both built-in guardrails (e.g., PII detection, human-in-the-loop) and a flexible middleware system for building custom guardrails using either approach.

## ​ Built-in guardrails

### ​ PII detection

LangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more.PII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data.The PII middleware supports multiple strategies for handling detected PII:

| Strategy | Description | Example |
| --- | --- | --- |
| `redact` | Replace with `[REDACTED_{PII_TYPE}]` | `[REDACTED_EMAIL]` |
| `mask` | Partially obscure (e.g., last 4 digits) | `****-****-****-1234` |
| `hash` | Replace with deterministic hash | `a8f5f167...` |
| `block` | Raise exception when detected | Error thrown |

Copy

import { createAgent, piiRedactionMiddleware } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [customerServiceTool, emailTool],
middleware: [\
// Redact emails in user input before sending to model\
piiRedactionMiddleware({\
piiType: "email",\
strategy: "redact",\
applyToInput: true,\
}),\
// Mask credit cards in user input\
piiRedactionMiddleware({\
piiType: "credit_card",\
strategy: "mask",\
applyToInput: true,\
}),\
// Block API keys - raise error if detected\
piiRedactionMiddleware({\
piiType: "api_key",\
detector: /sk-[a-zA-Z0-9]{32}/,\
strategy: "block",\
applyToInput: true,\
}),\
],
});

// When user provides PII, it will be handled according to the strategy
const result = await agent.invoke({
messages: [{\
role: "user",\
content: "My email is john.doe@example.com and card is 5105-1051-0510-5100"\
}]
});

Built-in PII types and configuration

**Built-in PII types:**

- `email` \- Email addresses
- `credit_card` \- Credit card numbers (Luhn validated)
- `ip` \- IP addresses
- `mac_address` \- MAC addresses
- `url` \- URLs

**Configuration options:**

| Parameter | Description | Default |
| --- | --- | --- |
| `piiType` | Type of PII to detect (built-in or custom) | Required |
| `strategy` | How to handle detected PII (`"block"`, `"redact"`, `"mask"`, `"hash"`) | `"redact"` |
| `detector` | Custom detector regex pattern | `undefined` (uses built-in) |
| `applyToInput` | Check user messages before model call | `true` |
| `applyToOutput` | Check AI messages after model call | `false` |
| `applyToToolResults` | Check tool result messages after execution | `false` |

See the middleware documentation for complete details on PII detection capabilities.

### ​ Human-in-the-loop

LangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions.Human-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.

import { createAgent, humanInTheLoopMiddleware } from "langchain";
import { MemorySaver, Command } from "@langchain/langgraph";

const agent = createAgent({
model: "gpt-4o",
tools: [searchTool, sendEmailTool, deleteDatabaseTool],
middleware: [\
humanInTheLoopMiddleware({\
interruptOn: {\
// Require approval for sensitive operations\
send_email: { allowAccept: true, allowEdit: true, allowRespond: true },\
delete_database: { allowAccept: true, allowEdit: true, allowRespond: true },\
// Auto-approve safe operations\
search: false,\
}\
}),\
],
checkpointer: new MemorySaver(),
});

// Human-in-the-loop requires a thread ID for persistence
const config = { configurable: { thread_id: "some_id" } };

// Agent will pause and wait for approval before executing sensitive tools
let result = await agent.invoke(
{ messages: [{ role: "user", content: "Send an email to the team" }] },
config
);

result = await agent.invoke(
new Command({ resume: { decisions: [{ type: "approve" }] } }),
config // Same thread ID to resume the paused conversation
);

See the human-in-the-loop documentation for complete details on implementing approval workflows.

## ​ Custom guardrails

For more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.

### ​ Before agent guardrails

Use “before agent” hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.

import { createMiddleware, AIMessage } from "langchain";

return createMiddleware({
name: "ContentFilterMiddleware",
beforeAgent: {

// Get the first user message
if (!state.messages || state.messages.length === 0) {
return;
}

const firstMessage = state.messages[0];
if (firstMessage._getType() !== "human") {
return;
}

const content = firstMessage.content.toString().toLowerCase();

// Check for banned keywords
for (const keyword of keywords) {
if (content.includes(keyword)) {
// Block execution before any processing
return {
messages: [\
new AIMessage(\
"I cannot process requests containing inappropriate content. Please rephrase your request."\
)\
],
jumpTo: "end",
};
}
}

return;
},
canJumpTo: ['end']
}
});
};

// Use the custom guardrail
import { createAgent } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [searchTool, calculatorTool],
middleware: [\
contentFilterMiddleware(["hack", "exploit", "malware"]),\
],
});

// This request will be blocked before any processing
const result = await agent.invoke({
messages: [{ role: "user", content: "How do I hack into a database?" }]
});

### ​ After agent guardrails

Use “after agent” hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.

import { createMiddleware, AIMessage, initChatModel } from "langchain";

const safetyModel = initChatModel("gpt-4o-mini");

return createMiddleware({
name: "SafetyGuardrailMiddleware",
afterAgent: {

// Get the final AI response
if (!state.messages || state.messages.length === 0) {
return;
}

const lastMessage = state.messages[state.messages.length - 1];
if (lastMessage._getType() !== "ai") {
return;
}

// Use a model to evaluate safety
const safetyPrompt = `Evaluate if this response is safe and appropriate.
Respond with only 'SAFE' or 'UNSAFE'.

Response: ${lastMessage.content.toString()}`;

const result = await safetyModel.invoke([\
{ role: "user", content: safetyPrompt }\
]);

if (result.content.toString().includes("UNSAFE")) {
return {
messages: [\
new AIMessage(\
"I cannot provide that response. Please rephrase your request."\
)\
],
jumpTo: "end",
};
}

// Use the safety guardrail
import { createAgent } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [searchTool, calculatorTool],
middleware: [safetyGuardrailMiddleware()],
});

const result = await agent.invoke({
messages: [{ role: "user", content: "How do I make explosives?" }]
});

### ​ Combine multiple guardrails

You can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:

import { createAgent, piiRedactionMiddleware, humanInTheLoopMiddleware } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [searchTool, sendEmailTool],
middleware: [\
// Layer 1: Deterministic input filter (before agent)\
contentFilterMiddleware(["hack", "exploit"]),\
\
// Layer 2: PII protection (before and after model)\
piiRedactionMiddleware({\
piiType: "email",\
strategy: "redact",\
applyToInput: true,\
}),\
piiRedactionMiddleware({\
piiType: "email",\
strategy: "redact",\
applyToOutput: true,\
}),\
\
// Layer 3: Human approval for sensitive tools\
humanInTheLoopMiddleware({\
interruptOn: {\
send_email: { allowAccept: true, allowEdit: true, allowRespond: true },\
}\
}),\
\
// Layer 4: Model-based safety check (after agent)\
safetyGuardrailMiddleware(),\
],
});

## ​ Additional resources

- Middleware documentation \- Complete guide to custom middleware
- Middleware API reference \- Complete guide to custom middleware
- Human-in-the-loop \- Add human review for sensitive operations
- Testing agents \- Strategies for testing safety mechanisms

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Custom middleware\\
\\
Previous Runtime\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/runtime

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Runtime

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Overview
- Access
- Inside tools
- Inside middleware

## ​ Overview

LangChain’s `createAgent` runs on LangGraph’s runtime under the hood.LangGraph exposes a `Runtime` object with the following information:

1. **Context**: static information like user id, db connections, or other dependencies for an agent invocation
2. **Store**: a BaseStore instance used for long-term memory
3. **Stream writer**: an object used for streaming information via the `"custom"` stream mode

The runtime context is how you thread data through your agent. Rather than storing things in global state, you can attach values — like a database connection, user session, or configuration — to the context and access them inside tools and middleware. This keeps things stateless, testable, and reusable.

You can access the runtime information within tools and middleware.

## ​ Access

When creating an agent with `createAgent`, you can specify a `contextSchema` to define the structure of the `context` stored in the agent `Runtime`.When invoking the agent, pass the `context` argument with the relevant configuration for the run:

Copy

import * as z from "zod";
import { createAgent } from "langchain";

const contextSchema = z.object({
userName: z.string(),
});

const agent = createAgent({
model: "gpt-4o",
tools: [\
/* ... */\
],
contextSchema,
});

const result = await agent.invoke(
{ messages: [{ role: "user", content: "What's my name?" }] },
{ context: { userName: "John Smith" } }
);

### ​ Inside tools

You can access the runtime information inside tools to:

- Access the context
- Read or write long-term memory
- Write to the custom stream (ex, tool progress / updates)

Use the `runtime` parameter to access the `Runtime` object inside a tool.

import * as z from "zod";
import { tool } from "langchain";
import { type ToolRuntime } from "@langchain/core/tools";

const fetchUserEmailPreferences = tool(

const userName = runtime.context?.userName;
if (!userName) {
throw new Error("userName is required");
}

let preferences = "The user prefers you to write a brief and polite email.";
if (runtime.store) {
const memory = await runtime.store?.get(["users"], userName);
if (memory) {
preferences = memory.value.preferences;
}
}
return preferences;
},
{
name: "fetch_user_email_preferences",
description: "Fetch the user's email preferences.",
schema: z.object({}),
}
);

### ​ Inside middleware

You can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context.Use the `runtime` parameter to access the `Runtime` object inside middleware.

import * as z from "zod";
import { createAgent, createMiddleware, SystemMessage } from "langchain";

// Dynamic prompt middleware
const dynamicPromptMiddleware = createMiddleware({
name: "DynamicPrompt",
contextSchema,

const systemMsg = `You are a helpful assistant. Address the user as ${userName}.`;
return {
messages: [new SystemMessage(systemMsg), ...state.messages],
};
},
});

// Logging middleware
const loggingMiddleware = createMiddleware({
name: "Logging",
contextSchema,

console.log(`Processing request for user: ${runtime.context?.userName}`);
return;
},

console.log(`Completed request for user: ${runtime.context?.userName}`);
return;
},
});

const agent = createAgent({
model: "gpt-4o",
tools: [\
/* ... */\
],
middleware: [dynamicPromptMiddleware, loggingMiddleware],
contextSchema,
});

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Guardrails\\
\\
Previous Context engineering in agents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/context-engineering

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Context engineering in agents

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Overview
- Why do agents fail?
- The agent loop
- What you can control
- Data sources
- How it works
- Model Context
- System Prompt
- Messages
- Tools
- Defining tools
- Selecting tools
- Model
- Response Format
- Defining formats
- Selecting formats
- Tool Context
- Reads
- Writes
- Life-cycle Context
- Example: Summarization
- Best practices
- Related resources

## ​ Overview

The hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.

### ​ Why do agents fail?

When agents fail, it’s usually because the LLM call inside the agent took the wrong action / didn’t do what we expected. LLMs fail for one of two reasons:

1. The underlying LLM is not capable enough
2. The “right” context was not passed to the LLM

More often than not - it’s actually the second reason that causes agents to not be reliable.**Context engineering** is providing the right information and tools in the right format so the LLM can accomplish a task. This is the number one job of AI Engineers. This lack of “right” context is the number one blocker for more reliable agents, and LangChain’s agent abstractions are uniquely designed to facilitate context engineering.

New to context engineering? Start with the conceptual overview to understand the different types of context and when to use them.

### ​ The agent loop

A typical agent loop consists of two main steps:

1. **Model call** \- calls the LLM with a prompt and available tools, returns either a response or a request to execute tools
2. **Tool execution** \- executes the tools that the LLM requested, returns tool results

This loop continues until the LLM decides to finish.

### ​ What you can control

To build reliable agents, you need to control what happens at each step of the agent loop, as well as what happens between steps.

| Context Type | What You Control | Transient or Persistent |
| --- | --- | --- |
| **Model Context** | What goes into model calls (instructions, message history, tools, response format) | Transient |
| **Tool Context** | What tools can access and produce (reads/writes to state, store, runtime context) | Persistent |
| **Life-cycle Context** | What happens between model and tool calls (summarization, guardrails, logging, etc.) | Persistent |

## Transient context

What the LLM sees for a single call. You can modify messages, tools, or prompts without changing what’s saved in state.

## Persistent context

What gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently.

### ​ Data sources

Throughout this process, your agent accesses (reads / writes) different sources of data:

| Data Source | Also Known As | Scope | Examples |
| --- | --- | --- | --- |
| **Runtime Context** | Static configuration | Conversation-scoped | User ID, API keys, database connections, permissions, environment settings |
| **State** | Short-term memory | Conversation-scoped | Current messages, uploaded files, authentication status, tool results |
| **Store** | Long-term memory | Cross-conversation | User preferences, extracted insights, memories, historical data |

### ​ How it works

LangChain middleware is the mechanism under the hood that makes context engineering practical for developers using LangChain.Middleware allows you to hook into any step in the agent lifecycle and:

- Update context
- Jump to a different step in the agent lifecycle

Throughout this guide, you’ll see frequent use of the middleware API as a means to the context engineering end.

## ​ Model Context

Control what goes into each model call - instructions, available tools, which model to use, and output format. These decisions directly impact reliability and cost.

**System Prompt** \\
\\
Base instructions from the developer to the LLM. **Messages** \\
\\
The full list of messages (conversation history) sent to the LLM. **Tools** \\
\\
Utilities the agent has access to to take actions. **Model** \\
\\
The actual model (including configuration) to be called. **Response Format** \\
\\
Schema specification for the model’s final response.

All of these types of model context can draw from **state** (short-term memory), **store** (long-term memory), or **runtime context** (static configuration).

### ​ System Prompt

The system prompt sets the LLM’s behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.

- State

- Store

- Runtime Context

Access message count or conversation context from state:

Copy

import { createAgent } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [...],
middleware: [\

// Read from State: check conversation length\
const messageCount = state.messages.length;\
\
let base = "You are a helpful assistant.";\
\

base += "\nThis is a long conversation - be extra concise.";\
}\
\
return base;\
}),\
],
});

Access user preferences from long-term memory:

import * as z from "zod";
import { createAgent, dynamicSystemPromptMiddleware } from "langchain";

const contextSchema = z.object({
userId: z.string(),
});

const agent = createAgent({
model: "gpt-4o",
tools: [...],
contextSchema,
middleware: [\

const userId = runtime.context.userId;\
\
// Read from Store: get user preferences\
const store = runtime.store;\
const userPrefs = await store.get(["preferences"], userId);\
\
let base = "You are a helpful assistant.";\
\
if (userPrefs) {\
const style = userPrefs.value?.communicationStyle || "balanced";\
base += `\nUser prefers ${style} responses.`;\
}\
\
return base;\
}),\
],
});

Access user ID or configuration from Runtime Context:

const contextSchema = z.object({
userRole: z.string(),
deploymentEnv: z.string(),
});

// Read from Runtime Context: user role and environment\
const userRole = runtime.context.userRole;\
const env = runtime.context.deploymentEnv;\
\
let base = "You are a helpful assistant.";\
\
if (userRole === "admin") {\
base += "\nYou have admin access. You can perform all operations.";\
} else if (userRole === "viewer") {\
base += "\nYou have read-only access. Guide users to read operations only.";\
}\
\
if (env === "production") {\
base += "\nBe extra careful with any data modifications.";\
}\
\
return base;\
}),\
],
});

### ​ Messages

Messages make up the prompt that is sent to the LLM.
It’s critical to manage the content of messages to ensure that the LLM has the right information to respond well.

Inject uploaded file context from State when relevant to current query:

import { createMiddleware } from "langchain";

const injectFileContext = createMiddleware({
name: "InjectFileContext",

// request.state is a shortcut for request.state.messages
const uploadedFiles = request.state.uploadedFiles || [];

// Build context about available files

);

const fileContext = `Files you have access to in this conversation:
${fileDescriptions.join("\n")}

Reference these files when answering questions.`;

// Inject file context before recent messages
const messages = [\
...request.messages, // Rest of conversation\
{ role: "user", content: fileContext }\
];
request = request.override({ messages });
}

return handler(request);
},
});

const agent = createAgent({
model: "gpt-4o",
tools: [...],
middleware: [injectFileContext],
});

Inject user’s email writing style from Store to guide drafting:

import * as z from "zod";
import { createMiddleware } from "langchain";

const injectWritingStyle = createMiddleware({
name: "InjectWritingStyle",
contextSchema,

const userId = request.runtime.context.userId;

// Read from Store: get user's writing style examples
const store = request.runtime.store;
const writingStyle = await store.get(["writing_style"], userId);

if (writingStyle) {
const style = writingStyle.value;
// Build style guide from stored examples
const styleContext = `Your writing style:
- Tone: ${style.tone || 'professional'}
- Typical greeting: "${style.greeting || 'Hi'}"
- Typical sign-off: "${style.signOff || 'Best'}"
- Example email you've written:
${style.exampleEmail || ''}`;

// Append at end - models pay more attention to final messages
const messages = [\
...request.messages,\
{ role: "user", content: styleContext }\
];
request = request.override({ messages });
}

Inject compliance rules from Runtime Context based on user’s jurisdiction:

const contextSchema = z.object({
userJurisdiction: z.string(),
industry: z.string(),
complianceFrameworks: z.array(z.string()),
});

name: "InjectComplianceRules",
contextSchema,

// Read from Runtime Context: get compliance requirements
const { userJurisdiction, industry, complianceFrameworks } = request.runtime.context;

// Build compliance constraints
const rules = [];
if (complianceFrameworks.includes("GDPR")) {
rules.push("- Must obtain explicit consent before processing personal data");
rules.push("- Users have right to data deletion");
}
if (complianceFrameworks.includes("HIPAA")) {
rules.push("- Cannot share patient health information without authorization");
rules.push("- Must use secure, encrypted communication");
}
if (industry === "finance") {
rules.push("- Cannot provide financial advice without proper disclaimers");
}

const complianceContext = `Compliance requirements for ${userJurisdiction}:
${rules.join("\n")}`;

// Append at end - models pay more attention to final messages
const messages = [\
...request.messages,\
{ role: "user", content: complianceContext }\
];
request = request.override({ messages });
}

**Transient vs Persistent Message Updates:**The examples above use `wrap_model_call` to make **transient** updates - modifying what messages are sent to the model for a single call without changing what’s saved in state.For **persistent** updates that modify state (like the summarization example in Life-cycle Context), use life-cycle hooks like `before_model` or `after_model` to permanently update the conversation history. See the middleware documentation for more details.

### ​ Tools

Tools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.

#### ​ Defining tools

Each tool needs a clear name, description, argument names, and argument descriptions. These aren’t just metadata—they guide the model’s reasoning about when and how to use the tool.

import { tool } from "@langchain/core/tools";
import { z } from "zod";

const searchOrders = tool(

// Implementation here
},
{
name: "search_orders",
description: `Search for user orders by status.

Use this when the user asks about order history or wants to check
order status. Always filter by the provided status.`,
schema: z.object({
userId: z.string().describe("Unique identifier for the user"),
status: z.enum(["pending", "shipped", "delivered"]).describe("Order status to filter by"),
limit: z.number().default(10).describe("Maximum number of results to return"),
}),
}
);

#### ​ Selecting tools

Not every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection adapts the available toolset based on authentication state, user permissions, feature flags, or conversation stage.

Enable advanced tools only after certain conversation milestones:

const stateBasedTools = createMiddleware({
name: "StateBasedTools",

// Read from State: check authentication and conversation length
const state = request.state;
const isAuthenticated = state.authenticated || false;
const messageCount = state.messages.length;

let filteredTools = request.tools;

// Only enable sensitive tools after authentication
if (!isAuthenticated) {

} else if (messageCount < 5) {

}

return handler({ ...request, tools: filteredTools });
},
});

Filter tools based on user preferences or feature flags in Store:

const storeBasedTools = createMiddleware({
name: "StoreBasedTools",
contextSchema,

// Read from Store: get user's enabled features
const store = request.runtime.store;
const featureFlags = await store.get(["features"], userId);

if (featureFlags) {
const enabledFeatures = featureFlags.value?.enabledTools || [];

Filter tools based on user permissions from Runtime Context:

const contextSchema = z.object({
userRole: z.string(),
});

const contextBasedTools = createMiddleware({
name: "ContextBasedTools",
contextSchema,

// Read from Runtime Context: get user role
const userRole = request.runtime.context.userRole;

if (userRole === "admin") {
// Admins get all tools
} else if (userRole === "editor") {

} else {

See Dynamically selecting tools for more examples.

### ​ Model

Different models have different strengths, costs, and context windows. Select the right model for the task at hand, which
might change during an agent run.

Use different models based on conversation length from State:

import { createMiddleware, initChatModel } from "langchain";

// Initialize models once outside the middleware
const largeModel = initChatModel("claude-sonnet-4-5-20250929");
const standardModel = initChatModel("gpt-4o");
const efficientModel = initChatModel("gpt-4o-mini");

const stateBasedModel = createMiddleware({
name: "StateBasedModel",

// request.messages is a shortcut for request.state.messages
const messageCount = request.messages.length;
let model;

model = largeModel;

model = standardModel;
} else {
model = efficientModel;
}

return handler({ ...request, model });
},
});

Use user’s preferred model from Store:

import * as z from "zod";
import { createMiddleware, initChatModel } from "langchain";

// Initialize available models once
const MODEL_MAP = {
"gpt-4o": initChatModel("gpt-4o"),
"gpt-4o-mini": initChatModel("gpt-4o-mini"),
"claude-sonnet": initChatModel("claude-sonnet-4-5-20250929"),
};

const storeBasedModel = createMiddleware({
name: "StoreBasedModel",
contextSchema,

// Read from Store: get user's preferred model
const store = request.runtime.store;
const userPrefs = await store.get(["preferences"], userId);

let model = request.model;

if (userPrefs) {
const preferredModel = userPrefs.value?.preferredModel;
if (preferredModel && MODEL_MAP[preferredModel]) {
model = MODEL_MAP[preferredModel];
}
}

Select model based on cost limits or environment from Runtime Context:

const contextSchema = z.object({
costTier: z.string(),
environment: z.string(),
});

// Initialize models once outside the middleware
const premiumModel = initChatModel("claude-sonnet-4-5-20250929");
const standardModel = initChatModel("gpt-4o");
const budgetModel = initChatModel("gpt-4o-mini");

const contextBasedModel = createMiddleware({
name: "ContextBasedModel",
contextSchema,

// Read from Runtime Context: cost tier and environment
const costTier = request.runtime.context.costTier;
const environment = request.runtime.context.environment;

let model;

if (environment === "production" && costTier === "premium") {
model = premiumModel;
} else if (costTier === "budget") {
model = budgetModel;
} else {
model = standardModel;
}

See Dynamic model for more examples.

### ​ Response Format

Structured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isn’t sufficient.**How it works:** When you provide a schema as the response format, the model’s final response is guaranteed to conform to that schema. The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format.

#### ​ Defining formats

Schema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to.

import { z } from "zod";

const customerSupportTicket = z.object({
category: z.enum(["billing", "technical", "account", "product"]).describe(
"Issue category"
),
priority: z.enum(["low", "medium", "high", "critical"]).describe(
"Urgency level"
),
summary: z.string().describe(
"One-sentence summary of the customer's issue"
),
customerSentiment: z.enum(["frustrated", "neutral", "satisfied"]).describe(
"Customer's emotional tone"
),
}).describe("Structured ticket information extracted from customer message");

#### ​ Selecting formats

Dynamic response format selection adapts schemas based on user preferences, conversation stage, or role—returning simple formats early and detailed formats as complexity increases.

Configure structured output based on conversation state:

import { createMiddleware } from "langchain";
import { z } from "zod";

const simpleResponse = z.object({
answer: z.string().describe("A brief answer"),
});

const detailedResponse = z.object({
answer: z.string().describe("A detailed answer"),
reasoning: z.string().describe("Explanation of reasoning"),
confidence: z.number().describe("Confidence score 0-1"),
});

const stateBasedOutput = createMiddleware({
name: "StateBasedOutput",

// request.state is a shortcut for request.state.messages
const messageCount = request.messages.length;

let responseFormat;
if (messageCount < 3) {
// Early conversation - use simple format
responseFormat = simpleResponse;
} else {
// Established conversation - use detailed format
responseFormat = detailedResponse;
}

return handler({ ...request, responseFormat });
},
});

Configure output format based on user preferences in Store:

const verboseResponse = z.object({
answer: z.string().describe("Detailed answer"),
sources: z.array(z.string()).describe("Sources used"),
});

const conciseResponse = z.object({
answer: z.string().describe("Brief answer"),
});

const storeBasedOutput = createMiddleware({
name: "StoreBasedOutput",

// Read from Store: get user's preferred response style
const store = request.runtime.store;
const userPrefs = await store.get(["preferences"], userId);

if (userPrefs) {
const style = userPrefs.value?.responseStyle || "concise";
if (style === "verbose") {
request.responseFormat = verboseResponse;
} else {
request.responseFormat = conciseResponse;
}
}

Configure output format based on Runtime Context like user role or environment:

const contextSchema = z.object({
userRole: z.string(),
environment: z.string(),
});

const adminResponse = z.object({
answer: z.string().describe("Answer"),
debugInfo: z.record(z.any()).describe("Debug information"),
systemStatus: z.string().describe("System status"),
});

const userResponse = z.object({
answer: z.string().describe("Answer"),
});

const contextBasedOutput = createMiddleware({
name: "ContextBasedOutput",

// Read from Runtime Context: user role and environment
const userRole = request.runtime.context.userRole;
const environment = request.runtime.context.environment;

let responseFormat;
if (userRole === "admin" && environment === "production") {
responseFormat = adminResponse;
} else {
responseFormat = userResponse;
}

## ​ Tool Context

Tools are special in that they both read and write context.In the most basic case, when a tool executes, it receives the LLM’s request parameters and returns a tool message back. The tool does its work and produces a result.Tools can also fetch important information for the model that allows it to perform and complete tasks.

### ​ Reads

Most real-world tools need more than just the LLM’s parameters. They need user IDs for database queries, API keys for external services, or current session state to make decisions. Tools read from state, store, and runtime context to access this information.

Read from State to check current session information:

import * as z from "zod";
import { createAgent, tool, type ToolRuntime } from "langchain";

const checkAuthentication = tool(

// Read from State: check current auth status
const currentState = runtime.state;
const isAuthenticated = currentState.authenticated || false;

if (isAuthenticated) {
return "User is authenticated";
} else {
return "User is not authenticated";
}
},
{
name: "check_authentication",
description: "Check if user is authenticated",
schema: z.object({}),
}
);

Read from Store to access persisted user preferences:

const getPreference = tool(

const userId = runtime.context.userId;

// Read from Store: get existing preferences
const store = runtime.store;
const existingPrefs = await store.get(["preferences"], userId);

if (existingPrefs) {
const value = existingPrefs.value?.[preferenceKey];
return value ? `${preferenceKey}: ${value}` : `No preference set for ${preferenceKey}`;
} else {
return "No preferences found";
}
},
{
name: "get_preference",
description: "Get user preference from Store",
schema: z.object({
preferenceKey: z.string(),
}),
}
);

Read from Runtime Context for configuration like API keys and user IDs:

import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";

const contextSchema = z.object({
userId: z.string(),
apiKey: z.string(),
dbConnection: z.string(),
});

const fetchUserData = tool(

// Read from Runtime Context: get API key and DB connection
const { userId, apiKey, dbConnection } = runtime.context;

// Use configuration to fetch data
const results = await performDatabaseQuery(dbConnection, query, apiKey);

return `Found ${results.length} results for user ${userId}`;
},
{
name: "fetch_user_data",
description: "Fetch data using Runtime Context configuration",
schema: z.object({
query: z.string(),
}),
}
);

const agent = createAgent({
model: "gpt-4o",
tools: [fetchUserData],
contextSchema,
});

### ​ Writes

Tool results can be used to help an agent complete a given task. Tools can both return results directly to the model
and update the memory of the agent to make important context available to future steps.

Write to State to track session-specific information using Command:

import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";
import { Command } from "@langchain/langgraph";

const authenticateUser = tool(

// Perform authentication
if (password === "correct") {
// Write to State: mark as authenticated using Command
return new Command({
update: { authenticated: true },
});
} else {
return new Command({ update: { authenticated: false } });
}
},
{
name: "authenticate_user",
description: "Authenticate user and update State",
schema: z.object({
password: z.string(),
}),
}
);

Write to Store to persist data across sessions:

const savePreference = tool(

// Read existing preferences
const store = runtime.store;
const existingPrefs = await store.get(["preferences"], userId);

// Merge with new preference
const prefs = existingPrefs?.value || {};
prefs[preferenceKey] = preferenceValue;

// Write to Store: save updated preferences
await store.put(["preferences"], userId, prefs);

return `Saved preference: ${preferenceKey} = ${preferenceValue}`;
},
{
name: "save_preference",
description: "Save user preference to Store",
schema: z.object({
preferenceKey: z.string(),
preferenceValue: z.string(),
}),
}
);

See Tools for comprehensive examples of accessing state, store, and runtime context in tools.

## ​ Life-cycle Context

Control what happens **between** the core agent steps - intercepting data flow to implement cross-cutting concerns like summarization, guardrails, and logging.As you’ve seen in Model Context and Tool Context, middleware is the mechanism that makes context engineering practical. Middleware allows you to hook into any step in the agent lifecycle and either:

1. **Update context** \- Modify state and store to persist changes, update conversation history, or save insights
2. **Jump in the lifecycle** \- Move to different steps in the agent cycle based on context (e.g., skip tool execution if a condition is met, repeat model call with modified context)

### ​ Example: Summarization

One of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in Model Context, summarization **persistently updates state** \- permanently replacing old messages with a summary that’s saved for all future turns.LangChain offers built-in middleware for this:

import { createAgent, summarizationMiddleware } from "langchain";

const agent = createAgent({
model: "gpt-4o",
tools: [...],
middleware: [\
summarizationMiddleware({\
model: "gpt-4o-mini",\
trigger: { tokens: 4000 },\
keep: { messages: 20 },\
}),\
],
});

When the conversation exceeds the token limit, `SummarizationMiddleware` automatically:

1. Summarizes older messages using a separate LLM call
2. Replaces them with a summary message in State (permanently)
3. Keeps recent messages intact for context

The summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.

For a complete list of built-in middleware, available hooks, and how to create custom middleware, see the Middleware documentation.

## ​ Best practices

1. **Start simple** \- Begin with static prompts and tools, add dynamics only when needed
2. **Test incrementally** \- Add one context engineering feature at a time
3. **Monitor performance** \- Track model calls, token usage, and latency
4. **Use built-in middleware** \- Leverage `SummarizationMiddleware`, `LLMToolSelectorMiddleware`, etc.
5. **Document your context strategy** \- Make it clear what context is being passed and why
6. **Understand transient vs persistent**: Model context changes are transient (per-call), while life-cycle context changes persist to state

## ​ Related resources

- Context conceptual overview \- Understand context types and when to use them
- Middleware \- Complete middleware guide
- Tools \- Tool creation and context access
- Memory \- Short-term and long-term memory patterns
- Agents \- Core agent concepts

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Runtime\\
\\
Previous Model Context Protocol (MCP)\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/mcp

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Model Context Protocol (MCP)

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Quickstart
- Custom servers
- Transports
- HTTP
- Passing headers
- Authentication
- stdio
- Core features
- Tools
- Loading tools
- Additional resources

Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the `@langchain/mcp-adapters` library.

## ​ Quickstart

Install the `@langchain/mcp-adapters` library:

npm

pnpm

yarn

bun

Copy

npm install @langchain/mcp-adapters

`@langchain/mcp-adapters` enables agents to use tools defined across one or more MCP servers.

`MultiServerMCPClient` is **stateless by default**. Each tool invocation creates a fresh MCP `ClientSession`, executes the tool, and then cleans up. See the stateful sessions section for more details.

Accessing multiple MCP servers

import { MultiServerMCPClient } from "@langchain/mcp-adapters";
import { ChatAnthropic } from "@langchain/anthropic";
import { createAgent } from "langchain";

const client = new MultiServerMCPClient({
math: {
transport: "stdio", // Local subprocess communication
command: "node",
// Replace with absolute path to your math_server.js file
args: ["/path/to/math_server.js"],
},
weather: {
transport: "http", // HTTP-based remote server
// Ensure you start your weather server on port 8000
url: "http://localhost:8000/mcp",
},
});

const tools = await client.getTools();
const agent = createAgent({
model: "claude-sonnet-4-5-20250929",
tools,
});

const mathResponse = await agent.invoke({
messages: [{ role: "user", content: "what's (3 + 5) x 12?" }],
});

const weatherResponse = await agent.invoke({
messages: [{ role: "user", content: "what is the weather in nyc?" }],
});

## ​ Custom servers

To create your own MCP servers, you can use the `@modelcontextprotocol/sdk` library. This library provides a simple way to define tools and run them as servers.

npm install @modelcontextprotocol/sdk

To test your agent with MCP tool servers, use the following examples:

Math server (stdio transport)

import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import {
CallToolRequestSchema,
ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";

const server = new Server(
{
name: "math-server",
version: "0.1.0",
},
{
capabilities: {
tools: {},
},
}
);

return {
tools: [\
{\
name: "add",\
description: "Add two numbers",\
inputSchema: {\
type: "object",\
properties: {\
a: {\
type: "number",\
description: "First number",\
},\
b: {\
type: "number",\
description: "Second number",\
},\
},\
required: ["a", "b"],\
},\
},\
{\
name: "multiply",\
description: "Multiply two numbers",\
inputSchema: {\
type: "object",\
properties: {\
a: {\
type: "number",\
description: "First number",\
},\
b: {\
type: "number",\
description: "Second number",\
},\
},\
required: ["a", "b"],\
},\
},\
],
};
});

switch (request.params.name) {
case "add": {
const { a, b } = request.params.arguments as { a: number; b: number };
return {
content: [\
{\
type: "text",\
text: String(a + b),\
},\
],
};
}
case "multiply": {
const { a, b } = request.params.arguments as { a: number; b: number };
return {
content: [\
{\
type: "text",\
text: String(a * b),\
},\
],
};
}
default:
throw new Error(`Unknown tool: ${request.params.name}`);
}
});

async function main() {
const transport = new StdioServerTransport();
await server.connect(transport);
console.error("Math MCP server running on stdio");
}

main();

Weather server (SSE transport)

import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { SSEServerTransport } from "@modelcontextprotocol/sdk/server/sse.js";
import {
CallToolRequestSchema,
ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";
import express from "express";

const app = express();
app.use(express.json());

const server = new Server(
{
name: "weather-server",
version: "0.1.0",
},
{
capabilities: {
tools: {},
},
}
);

return {
tools: [\
{\
name: "get_weather",\
description: "Get weather for location",\
inputSchema: {\
type: "object",\
properties: {\
location: {\
type: "string",\
description: "Location to get weather for",\
},\
},\
required: ["location"],\
},\
},\
],
};
});

switch (request.params.name) {
case "get_weather": {
const { location } = request.params.arguments as { location: string };
return {
content: [\
{\
type: "text",\
text: `It's always sunny in ${location}`,\
},\
],
};
}
default:
throw new Error(`Unknown tool: ${request.params.name}`);
}
});

const transport = new SSEServerTransport("/mcp", res);
await server.connect(transport);
});

const PORT = process.env.PORT || 8000;

console.log(`Weather MCP server running on port ${PORT}`);
});

## ​ Transports

MCP supports different transport mechanisms for client-server communication.

### ​ HTTP

The `http` transport (also referred to as `streamable-http`) uses HTTP requests for client-server communication. See the MCP HTTP transport specification for more details.

const client = new MultiServerMCPClient({
weather: {
transport: "sse",
url: "http://localhost:8000/mcp",
},
});

### ​ stdio

Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.

Unlike HTTP transports, `stdio` connections are inherently **stateful**—the subprocess persists for the lifetime of the client connection. However, when using `MultiServerMCPClient` without explicit session management, each tool call still creates a new session. See stateful sessions for managing persistent connections.

const client = new MultiServerMCPClient({
math: {
transport: "stdio",
command: "node",
args: ["/path/to/math_server.js"],
},
});

## ​ Core features

### ​ Tools

Tools allow MCP servers to expose executable functions that LLMs can invoke to perform actions—such as querying databases, calling APIs, or interacting with external systems. LangChain converts MCP tools into LangChain tools, making them directly usable in any LangChain agent or workflow.

#### ​ Loading tools

Use `client.get_tools()` to retrieve tools from MCP servers and pass them to your agent:

import { MultiServerMCPClient } from "@langchain/mcp-adapters";
import { createAgent } from "langchain";

const client = new MultiServerMCPClient({...});
const tools = await client.getTools();
const agent = createAgent({ model: "claude-sonnet-4-5-20250929", tools });

## ​ Additional resources

- MCP documentation
- MCP Transport documentation
- `@langchain/mcp-adapters`

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Context engineering in agents\\
\\
Previous Human-in-the-loop\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Human-in-the-loop

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Interrupt decision types
- Configuring interrupts
- Responding to interrupts
- Decision types
- Streaming with human-in-the-loop
- Execution lifecycle
- Custom HITL logic

The Human-in-the-Loop (HITL) middleware lets you add human oversight to agent tool calls.
When a model proposes an action that might require review — for example, writing to a file or executing SQL — the middleware can pause execution and wait for a decision.It does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an interrupt that halts execution. The graph state is saved using LangGraph’s persistence layer, so execution can pause safely and resume later.A human decision then determines what happens next: the action can be approved as-is (`approve`), modified before running (`edit`), or rejected with feedback (`reject`).

## ​ Interrupt decision types

The middleware defines three built-in ways a human can respond to an interrupt:

| Decision Type | Description | Example Use Case |
| --- | --- | --- |
| ✅ `approve` | The action is approved as-is and executed without changes. | Send an email draft exactly as written |
| ✏️ `edit` | The tool call is executed with modifications. | Change the recipient before sending an email |
| ❌ `reject` | The tool call is rejected, with an explanation added to the conversation. | Reject an email draft and explain how to rewrite it |

The available decision types for each tool depend on the policy you configure in `interrupt_on`.
When multiple tool calls are paused at the same time, each action requires a separate decision.
Decisions must be provided in the same order as the actions appear in the interrupt request.

When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.

## ​ Configuring interrupts

To use HITL, add the middleware to the agent’s `middleware` list when creating the agent.You configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.

Copy

import { createAgent, humanInTheLoopMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const agent = createAgent({
model: "gpt-4o",
tools: [writeFileTool, executeSQLTool, readDataTool],
middleware: [\
humanInTheLoopMiddleware({\
interruptOn: {\
write_file: true, // All decisions (approve, edit, reject) allowed\
execute_sql: {\
allowedDecisions: ["approve", "reject"],\
// No editing allowed\
description: "🚨 SQL execution requires DBA approval",\
},\
// Safe operation, no approval needed\
read_data: false,\
},\
// Prefix for interrupt messages - combined with tool name and args to form the full message\
// e.g., "Tool execution pending approval: execute_sql with query='DELETE FROM...'"\
// Individual tools can override this by specifying a "description" in their interrupt config\
descriptionPrefix: "Tool execution pending approval",\
}),\
],
// Human-in-the-loop requires checkpointing to handle interrupts.
// In production, use a persistent checkpointer like AsyncPostgresSaver.
checkpointer: new MemorySaver(),
});

You must configure a checkpointer to persist the graph state across interrupts.
In production, use a persistent checkpointer like `AsyncPostgresSaver`. For testing or prototyping, use `InMemorySaver`.When invoking the agent, pass a `config` that includes the **thread ID** to associate execution with a conversation thread.
See the LangGraph interrupts documentation for details.

Configuration options

​

interruptOn

object

required

Mapping of tool names to approval configs

**Tool approval config options:**

allowAccept

boolean

default:"false"

Whether approval is allowed

allowEdit

Whether editing is allowed

allowRespond

Whether responding/rejection is allowed

## ​ Responding to interrupts

When you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in `interrupt_on`. In that case, the invocation result will include an `__interrupt__` field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.

import { HumanMessage } from "@langchain/core/messages";
import { Command } from "@langchain/langgraph";

// You must provide a thread ID to associate the execution with a conversation thread,
// so the conversation can be paused and resumed (as is needed for human review).
const config = { configurable: { thread_id: "some_id" } };

// Run the graph until the interrupt is hit.
const result = await agent.invoke(
{
messages: [new HumanMessage("Delete old records from the database")],
},
config
);

// The interrupt contains the full HITL request with action_requests and review_configs
console.log(result.__interrupt__);

// Resume with approval decision
await agent.invoke(
new Command({
resume: { decisions: [{ type: "approve" }] }, // or "reject"
}),
config // Same thread ID to resume the paused conversation
);

### ​ Decision types

- ✅ approve

- ✏️ edit

- ❌ reject

Use `approve` to approve the tool call as-is and execute it without changes.

await agent.invoke(
new Command({
// Decisions are provided as a list, one per action under review.
// The order of decisions must match the order of actions
// listed in the `__interrupt__` request.
resume: {
decisions: [\
{\
type: "approve",\
}\
]
}
}),
config // Same thread ID to resume the paused conversation
);

Use `edit` to modify the tool call before execution.
Provide the edited action with the new tool name and arguments.

await agent.invoke(
new Command({
// Decisions are provided as a list, one per action under review.
// The order of decisions must match the order of actions
// listed in the `__interrupt__` request.
resume: {
decisions: [\
{\
type: "edit",\
// Edited action with tool name and args\
editedAction: {\
// Tool name to call.\
// Will usually be the same as the original action.\
name: "new_tool_name",\
// Arguments to pass to the tool.\
args: { key1: "new_value", key2: "original_value" },\
}\
}\
]
}
}),
config // Same thread ID to resume the paused conversation
);

Use `reject` to reject the tool call and provide feedback instead of execution.

await agent.invoke(
new Command({
// Decisions are provided as a list, one per action under review.
// The order of decisions must match the order of actions
// listed in the `__interrupt__` request.
resume: {
decisions: [\
{\
type: "reject",\
// An explanation about why the action was rejected\
message: "No, this is wrong because ..., instead do this ...",\
}\
]
}
}),
config // Same thread ID to resume the paused conversation
);

The `message` is added to the conversation as feed Multiple decisions

When multiple actions are under review, provide a decision for each action in the same order as they appear in the interrupt:

{
decisions: [\
{ type: "approve" },\
{\
type: "edit",\
editedAction: {\
name: "tool_name",\
args: { param: "new_value" }\
}\
},\
{\
type: "reject",\
message: "This action is not allowed"\
}\
]
}

## ​ Streaming with human-in-the-loop

You can use `stream()` instead of `invoke()` to get real-time updates while the agent runs and handles interrupts. Use `stream_mode=['updates', 'messages']` to stream both agent progress and LLM tokens.

import { Command } from "@langchain/langgraph";

const config = { configurable: { thread_id: "some_id" } };

// Stream agent progress and LLM tokens until interrupt
for await (const [mode, chunk] of await agent.stream(
{ messages: [{ role: "user", content: "Delete old records from the database" }] },
{ ...config, streamMode: ["updates", "messages"] }
)) {
if (mode === "messages") {
// LLM token
const [token, metadata] = chunk;
if (token.content) {
process.stdout.write(token.content);
}
} else if (mode === "updates") {
// Check for interrupt
if ("__interrupt__" in chunk) {
console.log(`\n\nInterrupt: ${JSON.stringify(chunk.__interrupt__)}`);
}
}
}

// Resume with streaming after human decision
for await (const [mode, chunk] of await agent.stream(
new Command({ resume: { decisions: [{ type: "approve" }] } }),
{ ...config, streamMode: ["updates", "messages"] }
)) {
if (mode === "messages") {
const [token, metadata] = chunk;
if (token.content) {
process.stdout.write(token.content);
}
}
}

See the Streaming guide for more details on stream modes.

## ​ Execution lifecycle

The middleware defines an `after_model` hook that runs after the model generates a response but before any tool calls are executed:

1. The agent invokes the model to generate a response.
2. The middleware inspects the response for tool calls.
3. If any calls require human input, the middleware builds a `HITLRequest` with `action_requests` and `review_configs` and calls interrupt.
4. The agent waits for human decisions.
5. Based on the `HITLResponse` decisions, the middleware executes approved or edited calls, synthesizes ToolMessage’s for rejected calls, and resumes execution.

## ​ Custom HITL logic

For more specialized workflows, you can build custom HITL logic directly using the interrupt primitive and middleware abstraction.Review the execution lifecycle above to understand how to integrate interrupts into the agent’s operation.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Model Context Protocol (MCP)\\
\\
Previous Multi-agent\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/retrieval

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Retrieval

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Building a knowledge base
- From retrieval to RAG
- Retrieval Pipeline
- Building Blocks
- RAG Architectures
- 2-step RAG
- Agentic RAG
- Hybrid RAG

Large Language Models (LLMs) are powerful, but they have two key limitations:

- **Finite context** — they can’t ingest entire corpora at once.
- **Static knowledge** — their training data is frozen at a point in time.

Retrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of **Retrieval-Augmented Generation (RAG)**: enhancing an LLM’s answers with context-specific information.

## ​ Building a knowledge base

A **knowledge base** is a repository of documents or structured data used during retrieval.If you need a custom knowledge base, you can use LangChain’s document loaders and vector stores to build one from your own data.

If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do **not** need to rebuild it. You can:

- Connect it as a **tool** for an agent in Agentic RAG.
- Query it and supply the retrieved content as context to the LLM (2-Step RAG).

See the following tutorial to build a searchable knowledge base and minimal RAG workflow: **Tutorial: Semantic search** \\
\\
Learn how to create a searchable knowledge base from your own data using LangChain’s document loaders, embeddings, and vector stores.\\
In this tutorial, you’ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You’ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.\\
\\
Learn more

### ​ From retrieval to RAG

Retrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they **integrate retrieval with generation** to produce grounded, context-aware answers.This is the core idea behind **Retrieval-Augmented Generation (RAG)**. The retrieval pipeline becomes a foundation for a broader system that combines search with generation.

### ​ Retrieval Pipeline

A typical retrieval workflow looks like this:

Sources

(Google Drive, Slack, Notion, etc.)

Document Loaders

Documents

Split into chunks

Turn into embeddings

Vector Store

User Query

Query embedding

Retriever

LLM uses retrieved info

Answer

Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.

### ​ Building Blocks

**Document loaders**

Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized `Document` objects.

Learn more

**Embedding models** \\
\\
An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\\
\\
Learn more **Vector stores** \\
\\
Specialized databases for storing and searching embeddings.\\
\\
Learn more **Retrievers** \\
\\
A retriever is an interface that returns documents given an unstructured query.\\
\\
Learn more

## ​ RAG Architectures

RAG can be implemented in multiple ways, depending on your system’s needs. We outline each type in the sections below.

| Architecture | Description | Control | Flexibility | Latency | Example Use Case |
| --- | --- | --- | --- | --- | --- |
| **2-Step RAG** | Retrieval always happens before generation. Simple and predictable | ✅ High | ❌ Low | ⚡ Fast | FAQs, documentation bots |
| **Agentic RAG** | An LLM-powered agent decides _when_ and _how_ to retrieve during reasoning | ❌ Low | ✅ High | ⏳ Variable | Research assistants with access to multiple tools |
| **Hybrid** | Combines characteristics of both approaches with validation steps | ⚖️ Medium | ⚖️ Medium | ⏳ Variable | Domain-specific Q&A with quality validation |

**Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.

### ​ 2-step RAG

In **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.

User Question

Retrieve Relevant Documents

Generate Answer

Return Answer to User

**Tutorial: Retrieval-Augmented Generation (RAG)** \\
\\
See how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\\
This tutorial walks through two approaches:\\
\\
- A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.\\
- A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.\\
\\
Learn more

### ​ Agentic RAG

**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.

The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.

Yes

No

User Input / Question

Agent (LLM)

Need external info?

Search using tool(s)

Enough to answer?

Generate final answer

;

Show Extended example: Agentic RAG for LangGraph's llms.txt

This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading llms.txt, which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user’s question.

Copy

import { tool, createAgent, HumanMessage } from "langchain";
import * as z from "zod";

const ALLOWED_DOMAINS = ["https://langchain-ai.github.io/"];
const LLMS_TXT = "https://langchain-ai.github.io/langgraph/llms.txt";

const fetchDocumentation = tool(

return `Error: URL not allowed. Must start with one of: ${ALLOWED_DOMAINS.join(", ")}`;
}
const response = await fetch(input.url);
if (!response.ok) {
throw new Error(`HTTP error! status: ${response.status}`);
}
return response.text();
},
{
name: "fetch_documentation",
description: "Fetch and convert documentation from a URL",
schema: z.object({
url: z.string().describe("The URL of the documentation to fetch"),
}),
}
);

const llmsTxtResponse = await fetch(LLMS_TXT);
const llmsTxtContent = await llmsTxtResponse.text();

const systemPrompt = `
You are an expert TypeScript developer and technical assistant.
Your primary role is to help users with questions about LangGraph and related tools.

Instructions:

1. If a user asks a question you're unsure about — or one that likely involves API usage,
behavior, or configuration — you MUST use the \`fetch_documentation\` tool to consult the relevant docs.
2. When citing documentation, summarize clearly and include relevant context from the content.
3. Do not use any URLs outside of the allowed domain.
4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.

You can access official documentation from the following approved sources:

${llmsTxtContent}

You MUST consult the documentation to get up to date documentation
before answering a user's question about LangGraph.

Your answers should be clear, concise, and technically accurate.
`;

const tools = [fetchDocumentation];

const agent = createAgent({
model: "claude-sonnet-4-0"
tools,
systemPrompt,
name: "Agentic RAG",
});

const response = await agent.invoke({
messages: [\
new HumanMessage(\
"Write a short example of a langgraph agent using the " +\
"prebuilt create react agent. the agent should be able " +\
"to look up stock pricing information."\
),\
],
});

console.log(response.messages.at(-1)?.content);

### ​ Hybrid RAG

Hybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.Typical components include:

- **Query enhancement**: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.
- **Retrieval validation**: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.
- **Answer validation**: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.

The architecture often supports multiple iterations between these steps:

Query Enhancement

Retrieve Documents

Sufficient Info?

Refine Query

Answer Quality OK?

Try Different Approach?

Return Best Answer

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Custom workflow\\
\\
Previous Long-term memory\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/long-term-memory

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Advanced usage

Long-term memory

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Overview
- Memory storage
- Read long-term memory in tools
- Write long-term memory from tools

## ​ Overview

LangChain agents use LangGraph persistence to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use.

## ​ Memory storage

LangGraph stores long-term memories as JSON documents in a store.Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information.This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.

Copy

import { InMemoryStore } from "@langchain/langgraph";

// Replace with an actual embedding function or LangChain embeddings object

};

// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.
const store = new InMemoryStore({ index: { embed, dims: 2 } });
const userId = "my-user";
const applicationContext = "chitchat";
const namespace = [userId, applicationContext];

await store.put(
namespace,
"a-memory",
{
rules: [\
"User likes short, direct language",\
"User only speaks English & TypeScript",\
],
"my-key": "my-value",
}
);

// get the "memory" by ID
const item = await store.get(namespace, "a-memory");

// search for "memories" within this namespace, filtering on content equivalence, sorted by vector similarity
const items = await store.search(
namespace,
{
filter: { "my-key": "my-value" },
query: "language preferences"
}
);

For more information about the memory store, see the Persistence guide.

## ​ Read long-term memory in tools

A tool the agent can use to look up user information

import * as z from "zod";
import { createAgent, tool, type ToolRuntime } from "langchain";
import { InMemoryStore } from "@langchain/langgraph";

// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.
const store = new InMemoryStore();
const contextSchema = z.object({
userId: z.string(),
});

// Write sample data to the store using the put method
await store.put(
["users"], // Namespace to group related data together (users namespace for user data)
"user_123", // Key within the namespace (user ID as key)
{
name: "John Smith",
language: "English",
} // Data to store for the given user
);

const getUserInfo = tool(
// Look up user info.
async (_, runtime: ToolRuntime<unknown, z.infer<typeof contextSchema>>) => {
// Access the store - same as that provided to `createAgent`
const userId = runtime.context.userId;
if (!userId) {
throw new Error("userId is required");
}
// Retrieve data from store - returns StoreValue object with value and metadata
const userInfo = await runtime.store.get(["users"], userId);
return userInfo?.value ? JSON.stringify(userInfo.value) : "Unknown user";
},
{
name: "getUserInfo",
description: "Look up user info by userId from the store.",
schema: z.object({}),
}
);

const agent = createAgent({
model: "gpt-4o-mini",
tools: [getUserInfo],
contextSchema,
// Pass store to agent - enables agent to access store when running tools
store,
});

// Run the agent
const result = await agent.invoke(
{ messages: [{ role: "user", content: "look up user information" }] },
{ context: { userId: "user_123" } }
);

console.log(result.messages.at(-1)?.content);

/**
* Outputs:
* User Information:
* - Name: John Smith
* - Language: English
*/

## ​ Write long-term memory from tools

Example of a tool that updates user information

import * as z from "zod";
import { tool, createAgent, type ToolRuntime } from "langchain";
import { InMemoryStore } from "@langchain/langgraph";

// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.
const store = new InMemoryStore();

const contextSchema = z.object({
userId: z.string(),
});

// Schema defines the structure of user information for the LLM
const UserInfo = z.object({
name: z.string(),
});

// Tool that allows agent to update user information (useful for chat applications)
const saveUserInfo = tool(
async (

runtime: ToolRuntime<unknown, z.infer<typeof contextSchema>>

const userId = runtime.context.userId;
if (!userId) {
throw new Error("userId is required");
}
// Store data in the store (namespace, key, data)
await runtime.store.put(["users"], userId, userInfo);
return "Successfully saved user info.";
},
{
name: "save_user_info",
description: "Save user info",
schema: UserInfo,
}
);

const agent = createAgent({
model: "gpt-4o-mini",
tools: [saveUserInfo],
contextSchema,
store,
});

// Run the agent
await agent.invoke(
{ messages: [{ role: "user", content: "My name is John Smith" }] },
// userId passed in context to identify whose information is being updated
{ context: { userId: "user_123" } }
);

// You can access the store directly to get the value
const result = await store.get(["users"], "user_123");
console.log(result?.value); // Output: { name: "John Smith" }

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Retrieval\\
\\
Previous LangSmith Studio\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/studio

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Agent development

LangSmith Studio

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Prerequisites
- Set up local Agent server
- 1\. Install the LangGraph CLI
- 2\. Prepare your agent
- 3\. Environment variables
- 4\. Create a LangGraph config file
- 5\. Install dependencies
- 6\. View your agent in Studio
- Video guide

When building agents with LangChain locally, it’s helpful to visualize what’s happening inside your agent, interact with it in real-time, and debug issues as they occur. **LangSmith Studio** is a free visual interface for developing and testing your LangChain agents from your local machine.Studio connects to your locally running agent to show you each step your agent takes: the prompts sent to the model, tool calls and their results, and the final output. You can test different inputs, inspect intermediate states, and iterate on your agent’s behavior without additional code or deployment.This pages describes how to set up Studio with your local LangChain agent.

## ​ Prerequisites

Before you begin, ensure you have the following:

- **A LangSmith account**: Sign up (for free) or log in at smith.langchain.com.
- **A LangSmith API key**: Follow the Create an API key guide.
- If you don’t want data traced to LangSmith, set `LANGSMITH_TRACING=false` in your application’s `.env` file. With tracing disabled, no data leaves your local server.

## ​ Set up local Agent server

### ​ 1\. Install the LangGraph CLI

The LangGraph CLI provides a local development server (also called Agent Server) that connects your agent to Studio.

Copy

pip install --upgrade "langgraph-cli[inmem]"

### ​ 2\. Prepare your agent

If you already have a LangChain agent, you can use it directly. This example uses a simple email agent:

agent.py

from langchain.agents import create_agent

def send_email(to: str, subject: str, body: str):
"""Send an email"""
email = {
"to": to,
"subject": subject,
"body": body
}
# ... email sending logic

return f"Email sent to {to}"

agent = create_agent(
"gpt-4o",
tools=[send_email],
system_prompt="You are an email assistant. Always use the send_email tool.",
)

### ​ 3\. Environment variables

Studio requires a LangSmith API key to connect your local agent. Create a `.env` file in the root of your project and add your API key from LangSmith.

Ensure your `.env` file is not committed to version control, such as Git.

.env

LANGSMITH_API_KEY=lsv2...

### ​ 4\. Create a LangGraph config file

The LangGraph CLI uses a configuration file to locate your agent and manage dependencies. Create a `langgraph.json` file in your app’s directory:

langgraph.json

{
"dependencies": ["."],
"graphs": {
"agent": "./src/agent.py:agent"
},
"env": ".env"
}

The `create_agent` function automatically returns a compiled LangGraph graph, which is what the `graphs` key expects in the configuration file.

For detailed explanations of each key in the JSON object of the configuration file, refer to the LangGraph configuration file reference.

At this point, the project structure will look like this:

my-app/
├── src
│ └── agent.py
├── .env
└── langgraph.json

### ​ 5\. Install dependencies

Install your project dependencies from the root directory:

pip

uv

pip install langchain langchain-openai

### ​ 6\. View your agent in Studio

Start the development server to connect your agent to Studio:

langgraph dev

Safari blocks `localhost` connections to Studio. To work around this, run the above command with `--tunnel` to access Studio via a secure tunnel.

Once the server is running, your agent is accessible both via API at `http://127.0.0.1:2024` and through the Studio UI at `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`:

With Studio connected to your local agent, you can iterate quickly on your agent’s behavior. Run a test input, inspect the full execution trace including prompts, tool arguments, return values, and token/latency metrics. When something goes wrong, Studio captures exceptions with the surrounding state to help you understand what happened.The development server supports hot-reloading—make changes to prompts or tool signatures in your code, and Studio reflects them immediately. Re-run conversation threads from any step to test your changes without starting over. This workflow scales from simple single-tool agents to complex multi-node graphs.For more information on how to run Studio, refer to the following guides in the LangSmith docs:

- Run application
- Manage assistants
- Manage threads
- Iterate on prompts
- Debug LangSmith traces
- Add node to dataset

## ​ Video guide

LangSmith Studio v2: The Ultimate Agent Development Environment - YouTube

Photo image of LangChain

LangChain

165K subscribers

LangSmith Studio v2: The Ultimate Agent Development Environment

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Watch on

0:00

0:00 / 8:09

•Live

•

For more information about local and deployed agents, see Set up local Agent Server and Deploy.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Long-term memory\\
\\
Previous Test\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/test

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Agent development

Test

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Integration Testing
- Installing AgentEvals
- Trajectory Match Evaluator
- LLM-as-Judge Evaluator
- LangSmith Integration

Agentic applications let an LLM decide its own next steps to solve a problem. That flexibility is powerful, but the model’s black-box nature makes it hard to predict how a tweak in one part of your agent will affect the rest. To build production-ready agents, thorough testing is essential.There are a few approaches to testing your agents:

- Unit tests exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically.
- Integration tests test the agent using real network calls to confirm that components work together, credentials and schemas line up, and latency is acceptable.

Agentic applications tend to lean more on integration because they chain multiple components together and must deal with flakiness due to the nondeterministic nature of LLMs.

## ​ Integration Testing

Many agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain’s `agentevals` package provides evaluators specifically designed for testing agent trajectories with live models.AgentEvals lets you easily evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a **trajectory match** or by using an **LLM judge**: **Trajectory match** \\
\\
Hard-code a reference trajectory for a given input and validate the run via a step-by-step comparison.Ideal for testing well-defined workflows where you know the expected behavior. Use when you have specific expectations about which tools should be called and in what order. This approach is deterministic, fast, and cost-effective since it doesn’t require additional LLM calls. **LLM-as-judge** \\
\\
Use a LLM to qualitatively validate your agent’s execution trajectory. The “judge” LLM reviews the agent’s decisions against a prompt rubric (which can include a reference trajectory).More flexible and can assess nuanced aspects like efficiency and appropriateness, but requires an LLM call and is less deterministic. Use when you want to evaluate the overall quality and reasonableness of the agent’s trajectory without strict tool call or ordering requirements.

### ​ Installing AgentEvals

Copy

npm install agentevals @langchain/core

Or, clone the AgentEvals repository directly.

### ​ Trajectory Match Evaluator

AgentEvals offers the `createTrajectoryMatchEvaluator` function to match your agent’s trajectory against a reference trajectory. There are four modes to choose from:

| Mode | Description | Use Case |
| --- | --- | --- |
| `strict` | Exact match of messages and tool calls in the same order | Testing specific sequences (e.g., policy lookup before authorization) |
| `unordered` | Same tool calls allowed in any order | Verifying information retrieval when order doesn’t matter |
| `subset` | Agent calls only tools from reference (no extras) | Ensuring agent doesn’t exceed expected scope |
| `superset` | Agent calls at least the reference tools (extras allowed) | Verifying minimum required actions are taken |

Strict match

The `strict` mode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action.

import { createAgent, tool, HumanMessage, AIMessage, ToolMessage } from "langchain"
import { createTrajectoryMatchEvaluator } from "agentevals";
import * as z from "zod";

const getWeather = tool(

return `It's 75 degrees and sunny in ${city}.`;
},
{
name: "get_weather",
description: "Get weather information for a city.",
schema: z.object({
city: z.string(),
}),
}
);

const agent = createAgent({
model: "gpt-4o",
tools: [getWeather]
});

const evaluator = createTrajectoryMatchEvaluator({
trajectoryMatchMode: "strict",
});

async function testWeatherToolCalledStrict() {
const result = await agent.invoke({
messages: [new HumanMessage("What's the weather in San Francisco?")]
});

const referenceTrajectory = [\
new HumanMessage("What's the weather in San Francisco?"),\
new AIMessage({\
content: "",\
tool_calls: [\
{ id: "call_1", name: "get_weather", args: { city: "San Francisco" } }\
]\
}),\
new ToolMessage({\
content: "It's 75 degrees and sunny in San Francisco.",\
tool_call_id: "call_1"\
}),\
new AIMessage("The weather in San Francisco is 75 degrees and sunny."),\
];

const evaluation = await evaluator({
outputs: result.messages,
referenceOutputs: referenceTrajectory
});
// {
// 'key': 'trajectory_strict_match',
// 'score': true,
// 'comment': null,
// }
expect(evaluation.score).toBe(true);
}

Unordered match

The `unordered` mode allows the same tool calls in any order, which is helpful when you want to verify that specific information was retrieved but don’t care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn’t matter.

return `It's 75 degrees and sunny in ${city}.`;
},
{
name: "get_weather",
description: "Get weather information for a city.",
schema: z.object({ city: z.string() }),
}
);

const getEvents = tool(

return `Concert at the park in ${city} tonight.`;
},
{
name: "get_events",
description: "Get events happening in a city.",
schema: z.object({ city: z.string() }),
}
);

const agent = createAgent({
model: "gpt-4o",
tools: [getWeather, getEvents]
});

const evaluator = createTrajectoryMatchEvaluator({
trajectoryMatchMode: "unordered",
});

async function testMultipleToolsAnyOrder() {
const result = await agent.invoke({
messages: [new HumanMessage("What's happening in SF today?")]
});

// Reference shows tools called in different order than actual execution
const referenceTrajectory = [\
new HumanMessage("What's happening in SF today?"),\
new AIMessage({\
content: "",\
tool_calls: [\
{ id: "call_1", name: "get_events", args: { city: "SF" } },\
{ id: "call_2", name: "get_weather", args: { city: "SF" } },\
]\
}),\
new ToolMessage({\
content: "Concert at the park in SF tonight.",\
tool_call_id: "call_1"\
}),\
new ToolMessage({\
content: "It's 75 degrees and sunny in SF.",\
tool_call_id: "call_2"\
}),\
new AIMessage("Today in SF: 75 degrees and sunny with a concert at the park tonight."),\
];

const evaluation = await evaluator({
outputs: result.messages,
referenceOutputs: referenceTrajectory,
});
// {
// 'key': 'trajectory_unordered_match',
// 'score': true,
// }
expect(evaluation.score).toBe(true);
}

Subset and superset match

The `superset` and `subset` modes match partial trajectories. The `superset` mode verifies that the agent called at least the tools in the reference trajectory, allowing additional tool calls. The `subset` mode ensures the agent did not call any tools beyond those in the reference.

import { createAgent } from "langchain"
import { tool } from "@langchain/core/tools";
import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { createTrajectoryMatchEvaluator } from "agentevals";
import * as z from "zod";

const getDetailedForecast = tool(

return `Detailed forecast for ${city}: sunny all week.`;
},
{
name: "get_detailed_forecast",
description: "Get detailed weather forecast for a city.",
schema: z.object({ city: z.string() }),
}
);

const agent = createAgent({
model: "gpt-4o",
tools: [getWeather, getDetailedForecast]
});

const evaluator = createTrajectoryMatchEvaluator({
trajectoryMatchMode: "superset",
});

async function testAgentCallsRequiredToolsPlusExtra() {
const result = await agent.invoke({
messages: [new HumanMessage("What's the weather in Boston?")]
});

// Reference only requires getWeather, but agent may call additional tools
const referenceTrajectory = [\
new HumanMessage("What's the weather in Boston?"),\
new AIMessage({\
content: "",\
tool_calls: [\
{ id: "call_1", name: "get_weather", args: { city: "Boston" } },\
]\
}),\
new ToolMessage({\
content: "It's 75 degrees and sunny in Boston.",\
tool_call_id: "call_1"\
}),\
new AIMessage("The weather in Boston is 75 degrees and sunny."),\
];

const evaluation = await evaluator({
outputs: result.messages,
referenceOutputs: referenceTrajectory,
});
// {
// 'key': 'trajectory_superset_match',
// 'score': true,
// 'comment': null,
// }
expect(evaluation.score).toBe(true);
}

You can also set the `toolArgsMatchMode` property and/or `toolArgsMatchOverrides` to customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the repository for more details.

### ​ LLM-as-Judge Evaluator

You can also use an LLM to evaluate the agent’s execution path with the `createTrajectoryLLMAsJudge` function. Unlike the trajectory match evaluators, it doesn’t require a reference trajectory, but one can be provided if available.

Without reference trajectory

import { createAgent } from "langchain"
import { tool } from "@langchain/core/tools";
import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from "agentevals";
import * as z from "zod";

const evaluator = createTrajectoryLLMAsJudge({
model: "openai:o3-mini",
prompt: TRAJECTORY_ACCURACY_PROMPT,
});

async function testTrajectoryQuality() {
const result = await agent.invoke({
messages: [new HumanMessage("What's the weather in Seattle?")]
});

const evaluation = await evaluator({
outputs: result.messages,
});
// {
// 'key': 'trajectory_accuracy',
// 'score': true,
// 'comment': 'The provided agent trajectory is reasonable...'
// }
expect(evaluation.score).toBe(true);
}

With reference trajectory

If you have a reference trajectory, you can add an extra variable to your prompt and pass in the reference trajectory. Below, we use the prebuilt `TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE` prompt and configure the `reference_outputs` variable:

import { TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE } from "agentevals";

const evaluator = createTrajectoryLLMAsJudge({
model: "openai:o3-mini",
prompt: TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,
});

const evaluation = await evaluator({
outputs: result.messages,
referenceOutputs: referenceTrajectory,
});

For more configurability over how the LLM evaluates the trajectory, visit the repository.

## ​ LangSmith Integration

For tracking experiments over time, you can log evaluator results to LangSmith, a platform for building production-grade LLM applications that includes tracing, evaluation, and experimentation tools.First, set up LangSmith by setting the required environment variables:

export LANGSMITH_API_KEY="your_langsmith_api_key"
export LANGSMITH_TRACING="true"

LangSmith offers two main approaches for running evaluations: Vitest/Jest integration and the `evaluate` function.

Using vitest/jest integration

import * as ls from "langsmith/vitest";
// import * as ls from "langsmith/jest";

import { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from "agentevals";

const trajectoryEvaluator = createTrajectoryLLMAsJudge({
model: "openai:o3-mini",
prompt: TRAJECTORY_ACCURACY_PROMPT,
});

ls.test("accurate trajectory", {
inputs: {
messages: [\
{\
role: "user",\
content: "What is the weather in SF?"\
}\
]
},
referenceOutputs: {
messages: [\
new HumanMessage("What is the weather in SF?"),\
new AIMessage({\
content: "",\
tool_calls: [\
{ id: "call_1", name: "get_weather", args: { city: "SF" } }\
]\
}),\
new ToolMessage({\
content: "It's 75 degrees and sunny in SF.",\
tool_call_id: "call_1"\
}),\
new AIMessage("The weather in SF is 75 degrees and sunny."),\
],
},

const result = await agent.invoke({
messages: [new HumanMessage("What is the weather in SF?")]
});

ls.logOutputs({ messages: result.messages });

await trajectoryEvaluator({
inputs,
outputs: result.messages,
referenceOutputs,
});
});
});

Run the evaluation with your test runner:

vitest run test_trajectory.eval.ts
# or
jest test_trajectory.eval.ts

Using the evaluate function

Alternatively, you can create a dataset in LangSmith and use the `evaluate` function:

import { evaluate } from "langsmith/evaluation";
import { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from "agentevals";

async function runAgent(inputs: any) {
const result = await agent.invoke(inputs);
return result.messages;
}

await evaluate(
runAgent,
{
data: "your_dataset_name",
evaluators: [trajectoryEvaluator],
}
);

Results will be automatically logged to LangSmith.

To learn more about evaluating your agent, see the LangSmith docs.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Studio\\
\\
Previous Agent Chat UI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/ui

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Agent development

Agent Chat UI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Quick start
- Local development
- Connect to your agent

Agent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI works seamlessly with agents created using `create_agent` and provides interactive experiences for your agents with minimal setup, whether you’re running locally or in a deployed context (such as LangSmith).Agent Chat UI is open source and can be adapted to your application needs.

Introducing Agent Chat UI - YouTube

Photo image of LangChain

LangChain

165K subscribers

Introducing Agent Chat UI

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Why am I seeing this?

Watch on

0:00

0:00 / 5:59

•Live

•

You can use generative UI in the Agent Chat UI. For more information, see Implement generative user interfaces with LangGraph.

### ​ Quick start

The fastest way to get started is using the hosted version:

1. **Visit Agent Chat UI**
2. **Connect your agent** by entering your deployment URL or local server address
3. **Start chatting** \- the UI will automatically detect and render tool calls and interrupts

### ​ Local development

For customization or local development, you can run Agent Chat UI locally:

Use npx

Clone repository

Copy

# Create a new Agent Chat UI project
npx create-agent-chat-app --project-name my-chat-ui
cd my-chat-ui

# Install dependencies and start
pnpm install
pnpm dev

### ​ Connect to your agent

Agent Chat UI can connect to both local and deployed agents.After starting Agent Chat UI, you’ll need to configure it to connect to your agent:

1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)
2. **Deployment URL**: Your Agent server’s endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent’s URL)
3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you’re using a local Agent server)

Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.

Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see Hiding Messages in the Chat.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Test\\
\\
Previous LangSmith Deployment\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/deploy

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Deploy with LangSmith

LangSmith Deployment

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Prerequisites
- Deploy your agent
- 1\. Create a repository on GitHub
- 2\. Deploy to LangSmith
- 3\. Test your application in Studio
- 4\. Get the API URL for your deployment
- 5\. Test the API

When you’re ready to deploy your LangChain agent to production, LangSmith provides a managed hosting platform designed for agent workloads. Traditional hosting platforms are built for stateless, short-lived web applications, while LangGraph is **purpose-built for stateful, long-running agents** that require persistent state and background execution. LangSmith handles the infrastructure, scaling, and operational concerns so you can deploy directly from your repository.

## ​ Prerequisites

Before you begin, ensure you have the following:

- A GitHub account
- A LangSmith account (free to sign up)

## ​ Deploy your agent

### ​ 1\. Create a repository on GitHub

Your application’s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide. Then, push your code to the repository.

### ​ 2\. Deploy to LangSmith

1

Navigate to LangSmith Deployment

Log in to LangSmith. In the left sidebar, select **Deployments**.

2

Create new deployment

Click the **\+ New Deployment** button. A pane will open where you can fill in the required fields.

3

Link repository

If you are a first time user or adding a private repository that has not been previously connected, click the **Add new account** button and follow the instructions to connect your GitHub account.

4

Deploy repository

Select your application’s repository. Click **Submit** to deploy. This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.

### ​ 3\. Test your application in Studio

Once your application is deployed:

1. Select the deployment you just created to view more details.
2. Click the **Studio** button in the top right corner. Studio will open to display your graph.

### ​ 4\. Get the API URL for your deployment

1. In the **Deployment details** view in LangGraph, click the **API URL** to copy it to your clipboard.
2. Click the `URL` to copy it to the clipboard.

### ​ 5\. Test the API

You can now test the API:

- Python

- Rest API

1. Install LangGraph Python:

Copy

pip install langgraph-sdk

2. Send a message to the agent:

from langgraph_sdk import get_sync_client # or get_client for async

client = get_sync_client(url="your-deployment-url", api_key="your-langsmith-api-key")

for chunk in client.runs.stream(
None, # Threadless run
"agent", # Name of agent. Defined in langgraph.json.
input={
"messages": [{\
"role": "human",\
"content": "What is LangGraph?",\
}],
},
stream_mode="updates",
):
print(f"Receiving new event of type: {chunk.event}...")
print(chunk.data)
print("\n\n")

curl -s --request POST \

--header 'Content-Type: application/json' \

--data "{
\"assistant_id\": \"agent\", `# Name of agent. Defined in langgraph.json.`
\"input\": {
\"messages\": [\
{\
\"role\": \"human\",\
\"content\": \"What is LangGraph?\"\
}\
]
},
\"stream_mode\": \"updates\"
}"

LangSmith offers additional hosting options, including self-hosted and hybrid. For more information, please see the Platform setup overview.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Agent Chat UI\\
\\
Previous LangSmith Observability\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/observability

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Deploy with LangSmith

LangSmith Observability

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Prerequisites
- Enable tracing
- Quickstart
- Trace selectively
- Log to a project
- Add metadata to traces

As you build and run agents with LangChain, you need visibility into how they behave: which tools they call, what prompts they generate, and how they make decisions. LangChain agents built with `createAgent` automatically support tracing through LangSmith, a platform for capturing, debugging, evaluating, and monitoring LLM application behavior._Traces_ record every step of your agent’s execution, from the initial user input to the final response, including all tool calls, model interactions, and decision points. This execution data helps you debug issues, evaluate performance across different inputs, and monitor usage patterns in production.This guide shows you how to enable tracing for your LangChain agents and use LangSmith to analyze their execution.

## ​ Prerequisites

Before you begin, ensure you have the following:

- **A LangSmith account**: Sign up (for free) or log in at smith.langchain.com.
- **A LangSmith API key**: Follow the Create an API key guide.

## ​ Enable tracing

All LangChain agents automatically support LangSmith tracing. To enable it, set the following environment variables:

Copy

export LANGSMITH_TRACING=true

## ​ Quickstart

No extra code is needed to log a trace to LangSmith. Just run your agent code as you normally would:

import { createAgent } from "@langchain/agents";

function sendEmail(to: string, subject: string, body: string): string {
// ... email sending logic
return `Email sent to ${to}`;
}

function searchWeb(query: string): string {
// ... web search logic
return `Search results for: ${query}`;
}

const agent = createAgent({
model: "gpt-4o",
tools: [sendEmail, searchWeb],
systemPrompt: "You are a helpful assistant that can send emails and search the web."
});

// Run the agent - all steps will be traced automatically
const response = await agent.invoke({
messages: [{ role: "user", content: "Search for the latest AI news and email a summary to john@example.com" }]
});

By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see Log to a project.

## ​ Trace selectively

You may opt to trace specific invocations or parts of your application using LangSmith’s `tracing_context` context manager:

import langsmith as ls

# This WILL be traced
with ls.tracing_context(enabled=True):
agent.invoke({"messages": [{"role": "user", "content": "Send a test email to alice@example.com"}]})

# This will NOT be traced (if LANGSMITH_TRACING is not set)
agent.invoke({"messages": [{"role": "user", "content": "Send another email"}]})

## ​ Log to a project

Statically

You can set a custom project name for your entire application by setting the `LANGSMITH_PROJECT` environment variable:

export LANGSMITH_PROJECT=my-agent-project

Dynamically

You can set the project name programmatically for specific operations:

with ls.tracing_context(project_name="email-agent-test", enabled=True):
response = agent.invoke({
"messages": [{"role": "user", "content": "Send a welcome email"}]
})

## ​ Add metadata to traces

You can annotate your traces with custom metadata and tags:

response = agent.invoke(
{"messages": [{"role": "user", "content": "Send a welcome email"}]},
config={
"tags": ["production", "email-assistant", "v1.0"],
"metadata": {
"user_id": "user_123",
"session_id": "session_456",
"environment": "production"
}
}
)

`tracing_context` also accepts tags and metadata for fine-grained control:

with ls.tracing_context(
project_name="email-agent-test",
enabled=True,
tags=["production", "email-assistant", "v1.0"],
metadata={"user_id": "user_123", "session_id": "session_456", "environment": "production"}):
response = agent.invoke(
{"messages": [{"role": "user", "content": "Send a welcome email"}]}
)

This custom metadata and tags will be attached to the trace in LangSmith.

To learn more about how to use traces to debug, evaluate, and monitor your agents, see the LangSmith documentation.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Deployment\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/install)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/quickstart)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Quickstart Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/philosophy)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/messages)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Messages

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/tools)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain integrations packages All integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/short-term-memory)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Memory overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/streaming)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Streaming LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/structured-output)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Structured output Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/middleware/overview)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

LangChain overview Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/middleware/built-in)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Built-in middleware Trace with LangChain (Python and JS/TS) LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/middleware/custom)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

How to add custom middleware Trace with LangChain (Python and JS/TS) Built-in middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/guardrails)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Guardrails

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/runtime)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Runtime

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/context-engineering)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/mcp)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Model Context Protocol (MCP) Trace with LangChain (Python and JS/TS) Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Human-in-the-loop using server API LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/retrieval)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/long-term-memory)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Long-term memory Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/studio)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangSmith Studio

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/test)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/ui)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) All integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/deploy)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/observability)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/integrations/providers/overview).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

- All providers

##### Popular Providers

- OpenAI

- Anthropic

- Google

- AWS

- Microsoft

##### General integrations

- Chat models
- Tools and Toolkits
- LLMs
- Middleware
- Key-value stores
- Document transformers
- Model caches
- Callbacks

##### RAG integrations

- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integrations All integration providers Integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/overview),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph overview LangGraph CLI

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/prompt-engineering-concepts

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Prompt engineering concepts

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- Why prompt engineering?
- Prompts vs. prompt templates
- Prompts in LangSmith
- Chat vs Completion
- F-string vs. mustache
- Tools
- Structured output
- Model
- Prompt versioning
- Commits
- Tags
- Prompt playground
- Testing multiple prompts
- Testing over a dataset
- Video guide

While traditional software applications are built by writing code, AI applications often derive their logic from prompts.This guide will walk through the key concepts of prompt engineering in LangSmith.

## ​ Why prompt engineering?

A prompt sets the stage for the model, like an audience member at an improv show directing the actor’s next performance - it guides the model’s behavior without changing its underlying capabilities. Just as telling an actor to “be a pirate” determines how they act, a prompt provides instructions, examples, and context that shape how the model responds.Prompt engineering is important because it allows you to change the way the model behaves. While there are other ways to change the model’s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with and often provides the highest ROI.We often see that prompt engineering is multi-disciplinary. Sometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager or another domain expert. It is important to have the proper tooling and infrastructure to support this cross-disciplinary building.

## ​ Prompts vs. prompt templates

Although we often use these terms interchangably, it is important to understand the difference between “prompts” and “prompt templates”.Prompts refer to the messages that are passed into the language model.Prompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates can include variables for few shot examples, outside context, or any other external data that is needed in your prompt.!Prompt vs prompt template

## ​ Prompts in LangSmith

You can store and version prompts templates in LangSmith. There are few key aspects of a prompt template to understand.

### ​ Chat vs Completion

There are two different types of prompts: `chat` style prompts and `completion` style prompts.Chat style prompts are a **list of messages**. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.Completion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.

### ​ F-string vs. mustache

You can format your prompt with input variables using either f-string or mustache format. Here is an example prompt with f-string format:

Copy

Hello, {name}!

And here is one with mustache:

Hello, {{name}}!

To add a conditional mustache prompt:

{{#is_logged_in}} Welcome back, {{name}}!{{else}} Please log in.{{/is_logged_in}}

- The playground UI will pick up `is_logged_in` variable, but any nested variables you’ll need to specify yourself. Paste the following into inputs to ensure the above conditional prompt works:

{ "name": "Alice"}

The LangSmith Playground uses `f-string` as the default template format, but you can switch to `mustache` format in the prompt settings/template format section. `mustache` gives you more flexibility around conditional variables, loops, and nested keys. For conditional variables, you’ll need to manually add json variables in the ‘inputs’ section. Read the documentation

### ​ Tools

Tools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description, and JSON schema of arguments used to call the tool.

### ​ Structured output

Structured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they stick to a specified schema. This may or may not use Tools under the hood.

Structured output is similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM **always** responds in this format. With tools, the LLM may select **multiple** tools; with structured output, only one response is generate.

### ​ Model

Optionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).

## ​ Prompt versioning

Verisioning is a key part of iterating and collaborating on your different prompts.

### ​ Commits

Every saved update to a prompt creates a new commit with a unique commit hash. This allows you to:

- View the full history of changes to a prompt.
- Review earlier versions.
- Revert to a previous state if needed.
- Reference specific versions in your code using the commit hash (e.g., `client.pull_prompt("prompt_name:commit_hash")`).

In the UI, you can compare a commit with its previous version by toggling **Show diff** in the top-right corner of the **Commits** tab.!The commit hashes list for a prompt with the diff of one commit.

### ​ Tags

Commit tags are human-readable labels that point to specific commits in your prompt’s history. Unlike commit hashes, tags can be moved to point to different commits, allowing you to update which version your code references without changing the code itself.Use cases for commit tags can include:

- **Environment-specific tags**: Mark commits for `production` or `staging` environments, which allows you to switch between different versions without changing your code.
- **Version control**: Mark stable versions of your prompts, for example, `v1`, `v2`, which lets you reference specific versions in your code and track changes over time.
- **Collaboration**: Mark versions ready for review, which enables you to share specific versions with collaborators and get feedback.

**Not to be confused with resource tags**: Commit tags reference specific prompt versions. Resource tags are key-value pairs used to organize workspace resources.

For detailed information on creating and managing commit tags, see Manage prompts.

## ​ Prompt playground

The prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.In the playground you can:

- Change the model being used
- Change prompt template being used
- Change the output schema
- Change the tools available
- Enter the input variables to run through the prompt template
- Run the prompt through the model
- Observe the outputs

Use **Polly** in the Playground to optimize prompts, generate tools, and create output schemas with AI assistance.

## ​ Testing multiple prompts

You can add more prompts to your playground to easily compare outputs and decide which version is better:!Add prompt to playground

## ​ Testing over a dataset

To test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results are streamed back as well as how many repitions there are in the test.!Test over dataset in playgroundYou can click on the “View Experiment” button to dive deeper into the results of the test.

## ​ Video guide

Getting Started with LangSmith (4/8): Playground & Prompts - YouTube

Photo image of LangChain

LangChain

165K subscribers

Getting Started with LangSmith (4/8): Playground & Prompts

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Why am I seeing this?

Watch on

0:00

0:00 / 7:59

•Live

•

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Prompt engineering quickstart\\
\\
Previous Create a prompt\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/create-a-prompt

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Create and update prompts

Create a prompt

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- Compose your prompt
- Template format
- Add a template variable
- Structured output
- Tools
- Run the prompt
- Save your prompt
- View your prompts
- Add metadata
- Next steps

Navigate to the in the left-hand sidebar or from the application homepage.!Empty playground

## ​ Compose your prompt

On the left is an editable view of the prompt.The prompt is made up of messages, each of which has a “role” - including `system`, `human`, and `ai`.

### ​ Add a template variable

The power of prompts comes from the ability to use variables in your prompt. You can use variables to add dynamic content to your prompt. Add a template variable in one of two ways:

1. Add `{{variable_name}}` to your prompt (with one curly brace on each side for `f-string` and two for `mustache`). !Prompt with variable
2. Highlight text you want to templatize and click the tooltip button that shows up. Enter a name for your variable, and convert. !Convert to variable

When we add a variable, we see a place to enter sample inputs for our prompt variables. Fill these in with values to test the prompt. !Prompt inputs

### ​ Structured output

Adding an output schema to your prompt will get output in a structured format. Learn more about structured output here. !Structured output

### ​ Tools

You can also add a tool by clicking the `+ Tool` button at the bottom of the prompt editor. See here for more information on how to use tools.

Use **Polly** in the Playground to generate tools, create output schemas, and optimize your prompts with AI assistance.

## ​ Run the prompt

Click “Start” to run the prompt.!Create a prompt run

## ​ Save your prompt

To save your prompt, click the “Save” button, name your prompt, and decide if you want it to be “private” or “public”. Private prompts are only visible to your workspace, while public prompts are discoverable to anyone.The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. !Save prompt

The first time you create a public prompt, you’ll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace.

## ​ View your prompts

You’ve just created your first prompt! View a table of your prompts in the prompts tab.!Prompt table

## ​ Add metadata

To add metadata to your prompt, click the prompt and then click the “Edit” pencil icon next to the name. This brings you to where you can add additional information about the prompt, including a description, a README, and use cases. For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub.!Pencil!Edit prompt

# ​ Next steps

Now that you’ve created a prompt, you can use it in your application code. See how to pull a prompt programmatically.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Prompt engineering concepts\\
\\
Previous Manage prompts\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/manage-prompts

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Create and update prompts

Manage prompts

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- Commit tags
- Create a tag
- Move a tag
- Delete a tag
- Use tags in code
- Trigger a webhook on prompt commit
- Configure a webhook
- Trigger the webhook
- Use the Playground
- Using the API
- Public prompt hub

LangSmith provides several tools to help you manage your _prompts_ effectively. This page describes the following features:

- Commit tags for version control and environment management.
- Webhook triggers for automating workflows when prompts are updated.
- Public prompt hub for discovering and using community-created prompts.

## ​ Commit tags

_Commit tags_ are labels that reference a specific _commit_ in your prompt’s version history. They help you mark significant versions and control which versions run in different environments. By referencing tags rather than commit IDs in your code, you can update which version is being used without modifying the code itself.Each tag references exactly one commit, though you can reassign a tag to point to a different commit.

**Not to be confused with resource tags**: Commit tags are specific to prompt versioning and reference individual commits in a prompt’s history. Resource tags are key-value pairs used to organize workspace resources like projects, datasets, and prompts. While both can use similar naming conventions (like `prod` or `staging`), commit tags control **which version** of a prompt runs, while resource tags help you **organize and filter** resources across your workspace.

### ​ Create a tag

To create a tag, navigate to the **Commits** tab for a prompt. Click on the tag icon next to the commit you want to tag. Click **New Tag** and enter a name for the tag.!Commits tab!Create new prompt tag

### ​ Move a tag

To point a tag to a different commit, click on the tag icon next to the destination commit, and select the tag you want to move. This will automatically update the tag to point to the new commit.!Move prompt tag

### ​ Delete a tag

To delete a tag, click on the delete icon next to the tag you want to delete. This will delete the tag altogether and it will no longer be associated with any commit.

### ​ Use tags in code

Tags provide a stable way to reference specific versions of your prompts in code. Instead of using commit hashes directly, you can reference tags that can be updated without changing your code.Here is an example of pulling a prompt by tag in Python:

Copy

prompt = client.pull_prompt("joke-generator:prod")
# If prod tag points to commit a1b2c3d4, this is equivalent to:
prompt = client.pull_prompt("joke-generator:a1b2c3d4")

For more information on how to use prompts in code, refer to Managing prompts programmatically.

## ​ Trigger a webhook on prompt commit

You can configure a webhook to be triggered whenever a commit is made to a prompt.Some common use cases of this include:

- Triggering a CI/CD pipeline when prompts are updated.
- Synchronizing prompts with a GitHub repository.
- Notifying team members about prompt modifications.

### ​ Configure a webhook

Navigate to the **Prompts** section in the left-hand sidebar or from the application homepage. In the top right corner, click on the `+ Webhook` button.Add a webhook URL and any required headers.

You can only configure one webhook per workspace. If you want to configure multiple per workspace or set up a different webhook for each prompt, let us know in the LangChain Forum.

To test out your webhook, click the **Send test notification** button. This will send a test notification to the webhook URL you provided with a sample payload.The sample payload is a JSON object with the following fields:

- `prompt_id`: The ID of the prompt that was committed.
- `prompt_name`: The name of the prompt that was committed.
- `commit_hash`: The commit hash of the prompt.
- `created_at`: The date of the commit.
- `created_by`: The author of the commit.
- `manifest`: The manifest of the prompt.

### ​ Trigger the webhook

Commit to a prompt to trigger the webhook you’ve configured.

#### ​ Use the Playground

If you do this in the Playground, you’ll be prompted to deselect the webhooks you’d like to avoid triggering.!Commit prompt playground

#### ​ Using the API

If you commit via the API, you can specify to skip triggering the webhook by setting the `skip_webhooks` parameter to `true` or to an array of webhook ids to ignore. Refer to the API docs for more information.

## ​ Public prompt hub

LangSmith’s public prompt hub is a collection of prompts that have been created by the LangChain community that you can use for reference.

Note that prompts are user-generated and unverified. LangChain does not review or endorse public prompts, use these at your own risk. Use of Prompt Hub is subject to our [](https://www.langchain.com/terms-of-service).

Navigate to the **Prompts** section of the left-hand sidebar and click on **Browse all Public Prompts in the LangChain Hub**.Here you’ll find all of the publicly listed prompts in the LangChain Hub. You can search for prompts by name, handle, use cases, descriptions, or models. You can fork prompts to your personal organization, view the prompt’s details, and run the prompt in the Playground. You can pull any public prompt into your code using the SDK.To view prompts tied to your workspace, visit the **Prompts** tab in the sidebar.!Prompts tab

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Create a prompt\\
\\
Previous Manage prompts programmatically\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/manage-prompts-programmatically

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Create and update prompts

Manage prompts programmatically

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- Install packages
- Configure environment variables
- Push a prompt
- Pull a prompt
- Use a prompt without LangChain
- OpenAI
- Anthropic
- List, delete, and like prompts

You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.

Previously this functionality lived in the `langchainhub` package which is now deprecated. All functionality going forward will live in the `langsmith` package.

## ​ Install packages

In Python, you can directly use the LangSmith SDK ( _recommended, full functionality_) or you can use through the LangChain package (limited to pushing and pulling prompts).In TypeScript, you must use the LangChain npm package for pulling prompts (it also allows pushing). For all other functionality, use the LangSmith package.

pip

uv

TypeScript

Copy

## ​ Configure environment variables

export LANGSMITH_API_KEY="lsv2_..."

What we refer to as “prompts” used to be called “repos”, so any references to “repo” in the code are referring to a prompt.

## ​ Push a prompt

To create a new prompt or update an existing prompt, you can use the `push prompt` method.

Python

LangChain (Python)

from langsmith import Client
from langchain_core.prompts import ChatPromptTemplate

client = Client()
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
url = client.push_prompt("joke-generator", object=prompt)
# url is a link to the prompt in the UI
print(url)

You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground. (see settings here: Supported Providers)

from langsmith import Client
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

client = Client()
model = ChatOpenAI(model="gpt-4o-mini")
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
chain = prompt | model
client.push_prompt("joke-generator-with-model", object=chain)

## ​ Pull a prompt

To pull a prompt, you can use the `pull prompt` method, which returns a the prompt as a langchain `PromptTemplate`.To pull a **private prompt** you do not need to specify the owner handle (though you can, if you have one set).To pull a **public prompt** from the LangChain Hub, you need to specify the handle of the prompt’s author.

from langsmith import Client
from langchain_openai import ChatOpenAI

client = Client()
prompt = client.pull_prompt("joke-generator")
model = ChatOpenAI(model="gpt-4o-mini")
chain = prompt | model
chain.invoke({"topic": "cats"})

Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model. Just specify include\_model when pulling the prompt. If the stored prompt includes a model, it will be returned as a RunnableSequence. Make sure you have the proper environment variables set for the model you are using.

from langsmith import Client

client = Client()
chain = client.pull_prompt("joke-generator-with-model", include_model=True)
chain.invoke({"topic": "cats"})

When pulling a prompt, you can also specify a specific commit hash or commit tag to pull a specific version of the prompt.

prompt = client.pull_prompt("joke-generator:12344e88")

To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt’s author.

prompt = client.pull_prompt("efriis/my-first-prompt")

For pulling prompts, if you are using Node.js or an environment that supports dynamic imports, we recommend using the `langchain/hub/node` entrypoint, as it handles deserialization of models associated with your prompt configuration automatically.If you are in a non-Node environment, “includeModel” is not supported for non-OpenAI models and you should use the base `langchain/hub` entrypoint.

## ​ Use a prompt without LangChain

If you want to store your prompts in LangSmith but use them directly with a model provider’s API, you can use our conversion methods. These convert your prompt into the payload required for the OpenAI or Anthropic API.These conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency in addition to your official SDK of choice. Here are some examples:

### ​ OpenAI

pip install -U langchain_openai

from openai import OpenAI
from langsmith.client import Client, convert_prompt_to_openai_format

# langsmith client
client = Client()
# openai client
oai_client = OpenAI()

# pull prompt and invoke to populate the variables
prompt = client.pull_prompt("joke-generator")
prompt_value = prompt.invoke({"topic": "cats"})
openai_payload = convert_prompt_to_openai_format(prompt_value)
openai_response = oai_client.chat.completions.create(**openai_payload)

### ​ Anthropic

pip install -U langchain_anthropic

from anthropic import Anthropic
from langsmith.client import Client, convert_prompt_to_anthropic_format

# anthropic client
client = Client()
anthropic_client = Anthropic()

prompt = client.pull_prompt("joke-generator")
prompt_value = prompt.invoke({"topic": "cats"})
anthropic_payload = convert_prompt_to_anthropic_format(prompt_value)
anthropic_response = anthropic_client.messages.create(**anthropic_payload)

## ​ List, delete, and like prompts

You can also list, delete, and like/unlike prompts using the `list prompts`, `delete prompt`, `like prompt` and `unlike prompt` methods. See the LangSmith SDK client for extensive documentation on these methods.

# List all prompts in my workspace
prompts = client.list_prompts()

# List my private prompts that include "joke"
prompts = client.list_prompts(query="joke", is_public=False)

# Delete a prompt
client.delete_prompt("joke-generator")

# Like a prompt
client.like_prompt("efriis/my-first-prompt")

# Unlike a prompt
client.unlike_prompt("efriis/my-first-prompt")

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Manage prompts\\
\\
Previous Configure prompt settings\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/managing-model-configurations

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Create and update prompts

Configure prompt settings

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- Model configurations
- Create saved configurations
- Edit configurations
- Delete configurations
- Extra parameters
- Tool settings
- Prompt formatting

The LangSmith playground enables you to control various settings for your prompts. The **Prompt Settings** window contains:

- Model configuration
- Tool settings
- Prompt formatting

To access **Prompt Settings**:

1. Navigate to the **Playground** in the left sidebar.
2. Under the **Prompts** heading select the gear icon next to the model name, which will launch the **Prompt Settings** window.

## ​ Model configurations

Model configurations define the parameters your prompt runs against. In the LangSmith Playground, you can save and manage these configurations, which allows you to reuse your preferred settings across prompts and sessions. For details on specific settings, refer to your model provider’s documentation (for example, Anthropic, OpenAI).

### ​ Create saved configurations

1. In the **Model Configurations** tab, adjust the model configuration as needed—you can select a saved configuration to edit.
2. Click the **Save As** button in the top bar.
3. Enter a name and optional description for your configuration and confirm.
4. Now that you’ve saved the configuration, anyone in your organization’s workspace can access it. All saved configurations are available in the **Model Configuration** dropdown.
5. Once you have created a saved configuration, you can set it as your default, so any new prompt you create will automatically use this configuration. To set a configuration as your default, click the **Set as default** icon next to the model name in the dropdown.

### ​ Edit configurations

1. To rename a saved configuration, or update the description, select the configuration name or description and make the necessary changes.
2. Update the current configuration’s parameters as needed and click the **Save** button at the top.

### ​ Delete configurations

1. Select the configuration you want to remove.
2. Click the trash icon to delete it.

### ​ Extra parameters

The **Extra Parameters** field allows you to pass additional model parameters that aren’t directly supported in the LangSmith interface. This is particularly useful in two scenarios:

1. When model providers release new parameters that haven’t yet been integrated into the LangSmith interface. You can specify these parameters in JSON format to use them right away. For example:

Copy

{
"reasoning_effort": "medium"
}

2. When troubleshooting parameter-related errors in the playground, such as:

TypeError: AsyncCompletions.create() got an unexpected keyword argument 'max_concurrency'

If you receive an error about unnecessary parameters (which is more common when using LangChain JS for run tracing), you can use this field to remove the extra parameters.

## ​ Tool settings

_Tools_ enable your LLM to perform tasks like searching the web, looking up information, and so on. In the **Tools Settings** tab, you can manage the ways your LLM uses and accesses the tools you have defined in your prompt, including:

- **Parallel Tool Calls**: Calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously. (Dependent on model support for parallel execution.)
- **Tool Choice**: Select the tools that the model can access. For more details, refer to Use tools in a prompt.

## ​ Prompt formatting

The **Prompt Format** tab allows you to specify:

- The **Prompt type**. For details on chat and completion prompts, refer to Prompt engineering concepts.
- The **Template format**. For details on prompt templating and using variables, refer to F-string vs. mustache.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Manage prompts programmatically\\
\\
Previous Use tools in a prompt\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/use-tools

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Create and update prompts

Use tools in a prompt

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- When to use tools
- Built-in tools
- OpenAI Tools
- Anthropic Tools
- Adding and using tools
- Add a tool
- Use a built-in tool
- Create a custom tool
- Tool choice settings

Tools allow language models to interact with external systems and perform actions beyond just generating text. In the LangSmith playground, you can use two types of tools:

1. **Built-in tools**: Pre-configured tools provided by model providers (like OpenAI and Anthropic) that are ready to use. These include capabilities like web search, code interpretation, and more.
2. **Custom tools**: Functions you define to perform specific tasks. These are useful when you need to integrate with your own systems or create specialized functionality. When you define custom tools within the LangSmith Playground, you can verify that the model correctly identifies and calls these tools with the correct arguments. Soon we plan to support executing these custom tool calls directly.

## ​ When to use tools

- Use **built-in tools** when you need common capabilities like web search or code interpretation. These are built and maintained by the model providers.
- Use **custom tools** when you want to test and validate your own tool designs, including: - Validating which tools the model chooses to use and seeing the specific arguments it provides in tool calls
- Simulating tool interactions

## ​ Built-in tools

The LangSmith Playground has native support for a variety of tools from OpenAI and Anthropic. If you want to use a tool that isn’t explicitly listed in the Playground, you can still add it by manually specifying its `type` and any required arguments.

### ​ OpenAI Tools

- **Web search**: Search the web for real-time information
- **Image generation**: Generate images based on a text prompt
- **MCP**: Gives the model access to tools hosted on a remote MCP server
- View all OpenAI tools

### ​ Anthropic Tools

- **Web search**: Search the web for up-to-date information
- View all Anthropic tools

## ​ Adding and using tools

### ​ Add a tool

To add a tool to your prompt, click the `+ Tool` button at the bottom of the prompt editor. !Add tool

### ​ Use a built-in tool

1. In the tool section, select the built-in tool you want to use. You’ll only see the tools that are compatible with the provider and model you’ve chosen.
2. When the model calls the tool, the playground will display the response

### ​ Create a custom tool

To create a custom tool, you’ll need to provide:

- Name: A descriptive name for your tool
- Description: Clear explanation of what the tool does
- Arguments: The inputs your tool requires

## ​ Tool choice settings

Some models provide control over which tools are called. To configure this:

1. Go to prompt settings
2. Navigate to tool settings
3. Select tool choice

To understand the available tool choice options, check the documentation for your specific provider. For example, OpenAI’s documentation on tool choice.!Tool choice

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Configure prompt settings\\
\\
Previous Include multimodal content in a prompt\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/multimodal-content

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Create and update prompts

Include multimodal content in a prompt

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- Inline content
- Template variables
- Populate the template variable
- Run an evaluation

Some applications are based around multimodal content, like a chatbot that can answer questions about a PDF or image. In these cases, you’ll want to include multimodal content in your prompt and test the model’s ability to answer questions about the content.The LangSmith Playground supports two methods for incorporating multimodal content in your prompts:

1. Inline content: Embed static files (images, PDFs, audio) directly in your prompt. This is ideal when you want to consistently include the same multimodal content across all uses of the prompt. For example, you might include a reference image that helps ground the model’s responses.
2. Template variables: Create dynamic placeholders for attachments that can be populated with different content each time. This approach offers more flexibility, allowing you to: - Test how the model handles different inputs
- Create reusable prompts that work with varying content

Not all models support multimodal content. Before using multimodal features in the playground, make sure your selected model supports the file types you want to use.

## ​ Inline content

Click the file icon in the message where you want to add multimodal content. Under the `Upload content` tab, you can upload a file and include it inline in the prompt.!Upload inline multimodal content

## ​ Template variables

Click the file icon in the message where you want to add multimodal content. Under the `Template variables` tab, you can create a template variable for a specific attachment type. Currently, only images, PDFs, and audio files (.wav, .mp3) are supported.!Template variable multimodal content

## ​ Populate the template variable

Once you’ve added a template variable, you can provide content for it using the panel on the right side of the screen. Simply click the `+` button to upload or select content that will be used to populate the template variable.!Manual prompt multimodal

## ​ Run an evaluation

After testing out your prompt manually, you can run an evaluation to see how the prompt performs over a golden dataset of examples.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Use tools in a prompt\\
\\
Previous Write your prompt with AI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/write-prompt-with-ai

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Create and update prompts

Write your prompt with AI

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- Chat sidebar
- Quick actions
- Custom quick actions
- Diffing
- Saving and using prompts

The prompt canvas makes it easy to edit a prompt with the help of an LLM. This allows you to iterate faster on long prompts and also makes it easier to make overarching stylisting or tonal changes to your prompt. You can enter the promp canvas by clicking the glowing wand over any message in your prompt:!Prompt canvas open

## ​ Chat sidebar

You can use the chat sidebar to ask questions about your prompt, or to give instructions in natural language to the LLM for how to rewrite your prompt.!Prompt canvas rewrite

You can also edit the prompt directly - you don’t **need** to use the LLM. This is useful if you know what edits you want to make and just want to make them directly

## ​ Quick actions

There are quick actions to change the reading level or length of the prompt with a single mouse click:!Prompt canvas quick actions

## ​ Custom quick actions

You can also save your own custom quick actions, for ease of use across all the prompts you are working on in LangSmith:!Prompt canvas custom quick action

## ​ Diffing

You can also see the specific differences between each version of your prompt by selecting the diff slider in the top right of the canvas:!Prompt canvas diff

## ​ Saving and using prompts

Lastly, you can save the prompt you have created in the canvas by clicking the “Use this Version” button in the bottom right:!Prompt canvas save

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Include multimodal content in a prompt\\
\\
Previous Connect to an OpenAI compliant model provider/proxy\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/optimize-classifier

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Tutorials

Optimize a classifier

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- The objective
- Getting started
- Set up automations
- Update the application
- Semantic search over examples

This tutorial walks through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback. That is exactly what we will do in this example.

## ​ The objective

In this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.

## ​ Getting started

To get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:

Copy

import os
os.environ["LANGSMITH_PROJECT"] = "classifier"

We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.

import openai
from langsmith import traceable, Client
import uuid

client = openai.Client()

available_topics = [\
"bug",\
"improvement",\
"new_feature",\
"documentation",\
"integration",\
]

prompt_template = """Classify the type of the issue as one of {topics}.
Issue: {text}"""

@traceable(
run_type="chain",
name="Classifier",
)
def topic_classifier(
topic: str):
return client.chat.completions.create(
model="gpt-4o-mini",
temperature=0,
messages=[\
{\
"role": "user",\
"content": prompt_template.format(\
topics=','.join(available_topics),\
text=topic,\
)\
}\
],
).choices[0].message.content

We can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.Here’s how we can invoke the application:

from langsmith import uuid7

run_id = uuid7()
topic_classifier(
"fix bug in LCEL",
langsmith_extra={"run_id": run_id})

Here’s how we can attach feedback after. We can collect feedback in two forms.First, we can collect “positive” feedback - this is for examples that the model got right.

ls_client = Client()
run_id = uuid7()
topic_classifier(
"fix bug in LCEL",
langsmith_extra={"run_id": run_id})
ls_client.create_feedback(
run_id,
key="user-score",
score=1.0,
)

Next, we can focus on collecting feedback that corresponds to a “correction” to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.

ls_client = Client()
run_id = uuid7()
topic_classifier(
"fix bug in documentation",
langsmith_extra={"run_id": run_id})
ls_client.create_feedback(
run_id,
key="correction",
correction="documentation")

## ​ Set up automations

We can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.The first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Let’s create a dataset called `classifier-github-issues` to add this data to.!Optimization NegativeThe second will take all runs with a correction and use a webhook to add them to a dataset. When creating this webhook, we will select the option to “Use Corrections”. This option will make it so that when creating a dataset from a run, rather than using the output of the run as the gold-truth output of the datapoint, it will use the correction.!Optimization Positive

## ​ Update the application

We can now update our code to pull down the dataset we are sending runs to. Once we pull it down, we can create a string with the examples in it. We can then put this string as part of the prompt!

# Initialize the LangSmith Client so we can use to get the dataset
ls_client = Client()

# Create a function that will take in a list of examples and format them into a string
def create_example_string(examples):
final_strings = []
for e in examples:

return "\n\n".join(final_strings)
### NEW CODE ###

prompt_template = """Classify the type of the issue as one of {topics}.

Here are some examples:
{examples}

Begin!

@traceable(
run_type="chain",
name="Classifier",
)
def topic_classifier(
topic: str):
# We can now pull down the examples from the dataset
# We do this inside the function so it always get the most up-to-date examples,
# But this can be done outside and cached for speed if desired
examples = list(ls_client.list_examples(dataset_name="classifier-github-issues")) # <- New Code
example_string = create_example_string(examples)
return client.chat.completions.create(
model="gpt-4o-mini",
temperature=0,
messages=[\
{\
"role": "user",\
"content": prompt_template.format(\
topics=','.join(available_topics),\
text=topic,\
examples=example_string,\
)\
}\
],
).choices[0].message.content

If now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as `documentation`

ls_client = Client()
run_id = uuid7()
topic_classifier(
"address bug in documentation",
langsmith_extra={"run_id": run_id})

## ​ Semantic search over examples

One additional thing we can do is only use the most semantically similar examples. This is useful when you start to build up a lot of examples.In order to do this, we can first define an example to find the `k` most similar examples:

import numpy as np

def find_similar(examples, topic, k=5):
inputs = [e.inputs['topic'] for e in examples] + [topic]
vectors = client.embeddings.create(input=inputs, model="text-embedding-3-small")
vectors = [e.embedding for e in vectors.data]
vectors = np.array(vectors)
args = np.argsort(-vectors.dot(vectors[-1])[:-1])[:5]
examples = [examples[i] for i in args]
return examples

We can then use that in the application

@traceable(
run_type="chain",
name="Classifier",
)
def topic_classifier(
topic: str):
examples = list(ls_client.list_examples(dataset_name="classifier-github-issues"))
examples = find_similar(examples, topic)
example_string = create_example_string(examples)
return client.chat.completions.create(
model="gpt-4o-mini",
temperature=0,
messages=[\
{\
"role": "user",\
"content": prompt_template.format(\
topics=','.join(available_topics),\
text=topic,\
examples=example_string,\
)\
}\
],
).choices[0].message.content

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Connect to a custom model\\
\\
Previous How to sync prompts with GitHub\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/prompt-commit

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Tutorials

How to sync prompts with GitHub

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- Prerequisites
- Understanding LangSmith “Prompt Commits” and webhooks
- Implementing a FastAPI server for webhook reception
- Configuring the webhook in LangSmith
- The workflow in action
- Beyond a simple commit

LangSmith provides a collaborative interface to create, test, and iterate on prompts.While you can dynamically fetch prompts from LangSmith into your application at runtime, you may prefer to sync prompts with your own database or version control system. To support this workflow, LangSmith allows you to receive notifications of prompt updates via webhooks.**Why sync prompts with GitHub?**

- **Version Control:** Keep your prompts versioned alongside your application code in a familiar system.
- **CI/CD Integration:** Trigger automated staging or production deployments when critical prompts change.

## ​ Prerequisites

Before we begin, ensure you have the following set up:

1. **GitHub Account:** A standard GitHub account.
2. **GitHub Repository:** Create a new (or choose an existing) repository where your LangSmith prompt manifests will be stored. This could be the same repository as your application code or a dedicated one for prompts.
3. **GitHub Personal Access Token (PAT):** - LangSmith webhooks don’t directly interact with GitHub—they call an intermediary server that _you_ create.
- This server requires a GitHub PAT to authenticate and make commits to your repository.
- Must include the `repo` scope (`public_repo` is sufficient for public repositories).

- Click **Generate new token (classic)**.
- Name it (e.g., “LangSmith Prompt Sync”), set an expiration, and select the required scopes.
- Click **Generate token** and **copy it immediately** — it won’t be shown again.
- Store the token securely and provide it as an environment variable to your server.

## ​ Understanding LangSmith “Prompt Commits” and webhooks

In LangSmith, when you save changes to a prompt, you’re essentially creating a new version or a “Prompt Commit.” These commits are what can trigger webhooks.The webhook will send a JSON payload containing the new **prompt manifest**.

Sample Webhook Payload

Copy

{
"prompt_id": "f33dcb51-eb17-47a5-83ca-64ac8a027a29",
"prompt_name": "My Prompt",
"commit_hash": "commit_hash_1234567890",
"created_at": "2021-01-01T00:00:00Z",
"created_by": "Jane Doe",
"manifest": {
"lc": 1,
"type": "constructor",
"id": ["langchain", "schema", "runnable", "RunnableSequence"],
"kwargs": {
"first": {
"lc": 1,
"type": "constructor",
"id": ["langchain", "prompts", "chat", "ChatPromptTemplate"],
"kwargs": {
"messages": [\
{\
"lc": 1,\
"type": "constructor",\
"id": [\
"langchain_core",\
"prompts",\
"chat",\
"SystemMessagePromptTemplate"\
],\
"kwargs": {\
"prompt": {\
"lc": 1,\
"type": "constructor",\
"id": [\
"langchain_core",\
"prompts",\
"prompt",\
"PromptTemplate"\
],\
"kwargs": {\
"input_variables": [],\
"template_format": "mustache",\
"template": "You are a chatbot."\
}\
}\
}\
},\
{\
"lc": 1,\
"type": "constructor",\
"id": [\
"langchain_core",\
"prompts",\
"chat",\
"HumanMessagePromptTemplate"\
],\
"kwargs": {\
"prompt": {\
"lc": 1,\
"type": "constructor",\
"id": [\
"langchain_core",\
"prompts",\
"prompt",\
"PromptTemplate"\
],\
"kwargs": {\
"input_variables": ["question"],\
"template_format": "mustache",\
"template": "{{question}}"\
}\
}\
}\
}\
],
"input_variables": ["question"]
}
},
"last": {
"lc": 1,
"type": "constructor",
"id": ["langchain", "schema", "runnable", "RunnableBinding"],
"kwargs": {
"bound": {
"lc": 1,
"type": "constructor",
"id": ["langchain", "chat_models", "openai", "ChatOpenAI"],
"kwargs": {
"temperature": 1,
"top_p": 1,
"presence_penalty": 0,
"frequency_penalty": 0,
"model": "gpt-4.1-mini",
"extra_headers": {},
"openai_api_key": {
"id": ["OPENAI_API_KEY"],
"lc": 1,
"type": "secret"
}
}
},
"kwargs": {}
}
}
}
}
}

It’s important to understand that LangSmith webhooks for prompt commits are generally triggered at the **workspace level**. This means if _any_ prompt within your LangSmith workspace is modified and a “prompt commit” is saved, the webhook will fire and send the updated manifest of the prompt. The payloads are identifiable by prompt id. Your receiving server should be designed with this in mind.

## ​ Implementing a FastAPI server for webhook reception

To effectively process webhook notifications from LangSmith when prompts are updated, an intermediary server application is necessary. This server will act as the receiver for HTTP POST requests sent by LangSmith. For demonstration purposes in this guide, we will outline the creation of a simple FastAPI application to fulfill this role.This publicly accessible server will be responsible for:

1. **Receiving Webhook Requests:** Listening for incoming HTTP POST requests.
2. **Parsing Payloads:** Extracting and interpreting the JSON-formatted prompt manifest from the request body.
3. **Committing to GitHub:** Programmatically creating a new commit in your specified GitHub repository, containing the updated prompt manifest. This ensures your prompts remain version-controlled and synchronized with changes made in LangSmith.

For deployment, platforms like Render.com (offering a suitable free tier), Vercel, Fly.io, or other cloud providers (AWS, GCP, Azure) can be utilized to host the FastAPI application and obtain a public URL.The server’s core functionality will include an endpoint for webhook reception, logic for parsing the manifest, and integration with the GitHub API (using a Personal Access Token for authentication) to manage commits.

Minimal FastAPI Server Code ()

`main.py`This server will listen for incoming webhooks from LangSmith and commit the received prompt manifest to your GitHub repository.

import base64
import json
import uuid
from typing import Any, Dict
import httpx
from fastapi import FastAPI, HTTPException, Body
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings, SettingsConfigDict

# --- Configuration ---
class AppConfig(BaseSettings):
"""
Application configuration model.
Loads settings from environment variables.
"""
GITHUB_TOKEN: str
GITHUB_REPO_OWNER: str
GITHUB_REPO_NAME: str
GITHUB_FILE_PATH: str = "prompt_manifest.json"
GITHUB_BRANCH: str = "main"
model_config = SettingsConfigDict(
env_file=".env",
env_file_encoding='utf-8',
extra='ignore'
)

settings = AppConfig()

# --- Pydantic Models ---
class WebhookPayload(BaseModel):
"""
Defines the expected structure of the incoming webhook payload.
"""
prompt_id: UUID = Field(
...,
description="The unique identifier for the prompt."
)
prompt_name: str = Field(
...,
description="The name/title of the prompt."
)
commit_hash: str = Field(
...,
description="An identifier for the commit event that triggered the webhook."
)
created_at: str = Field(
...,
description="Timestamp indicating when the event was created (ISO format preferred)."
)
created_by: str = Field(
...,
description="The name of the user who created the event."
)
manifest: Dict[str, Any] = Field(
...,
description="The main content or configuration data to be committed to GitHub."
)

# --- GitHub Helper Function ---

"""
Helper function to commit the manifest directly to the configured branch.
"""
github_api_base_url = "https://api.github.com"
repo_file_url = (
f"{github_api_base_url}/repos/{settings.GITHUB_REPO_OWNER}/"
f"{settings.GITHUB_REPO_NAME}/contents/{settings.GITHUB_FILE_PATH}"
)
headers = {
"Authorization": f"Bearer {settings.GITHUB_TOKEN}",
"Accept": "application/vnd.github.v3+json",
"X-GitHub-Api-Version": "2022-11-28",
}
manifest_json_string = json.dumps(payload.manifest, indent=2)
content_base64 = base64.b64encode(manifest_json_string.encode('utf-8')).decode('utf-8')
commit_message = f"feat: Update {settings.GITHUB_FILE_PATH} via webhook - commit {payload.commit_hash}"
data_to_commit = {
"message": commit_message,
"content": content_base64,
"branch": settings.GITHUB_BRANCH,
}
async with httpx.AsyncClient() as client:
current_file_sha = None
try:
params_get = {"ref": settings.GITHUB_BRANCH}
response_get = await client.get(repo_file_url, headers=headers, params=params_get)
if response_get.status_code == 200:
current_file_sha = response_get.json().get("sha")
elif response_get.status_code != 404: # If not 404 (not found), it's an unexpected error
response_get.raise_for_status()
except httpx.HTTPStatusError as e:
error_detail = f"GitHub API error (GET file SHA): {e.response.status_code} - {e.response.text}"
print(f"[ERROR] {error_detail}")
raise HTTPException(status_code=e.response.status_code, detail=error_detail)
except httpx.RequestError as e:
error_detail = f"Network error connecting to GitHub (GET file SHA): {str(e)}"
print(f"[ERROR] {error_detail}")
raise HTTPException(status_code=503, detail=error_detail)
if current_file_sha:
data_to_commit["sha"] = current_file_sha
try:
response_put = await client.put(repo_file_url, headers=headers, json=data_to_commit)
response_put.raise_for_status()
return response_put.json()
except httpx.HTTPStatusError as e:
error_detail = f"GitHub API error (PUT content): {e.response.status_code} - {e.response.text}"
if e.response.status_code == 409: # Conflict
error_detail = (
f"GitHub API conflict (PUT content): {e.response.text}. "
"This might be due to an outdated SHA or branch protection rules."
)
elif e.response.status_code == 422: # Unprocessable Entity
error_detail = (
f"GitHub API Unprocessable Entity (PUT content): {e.response.text}. "
f"Ensure the branch '{settings.GITHUB_BRANCH}' exists and the payload is correctly formatted."
)
print(f"[ERROR] {error_detail}")
raise HTTPException(status_code=e.response.status_code, detail=error_detail)
except httpx.RequestError as e:
error_detail = f"Network error connecting to GitHub (PUT content): {str(e)}"
print(f"[ERROR] {error_detail}")
raise HTTPException(status_code=503, detail=error_detail)

# --- FastAPI Application ---
app = FastAPI(
title="Minimal Webhook to GitHub Commit Service",
description="Receives a webhook and commits its 'manifest' part directly to a GitHub repository.",
version="0.1.0",
)

@app.post("/webhook/commit", status_code=201, tags=["GitHub Webhooks"])
async def handle_webhook_direct_commit(payload: WebhookPayload = Body(...)):
"""
Webhook endpoint to receive events and commit DIRECTLY to the configured branch.
"""
try:
github_response = await commit_manifest_to_github(payload)
return {
"message": "Webhook received and manifest committed directly to GitHub successfully.",
"github_commit_details": github_response.get("commit", {}),
"github_content_details": github_response.get("content", {})
}
except HTTPException:
raise # Re-raise if it's an HTTPException from the helper
except Exception as e:
error_message = f"An unexpected error occurred: {str(e)}"
print(f"[ERROR] {error_message}")
raise HTTPException(status_code=500, detail="An internal server error occurred.")

@app.get("/health", status_code=200, tags=["Health"])
async def health_check():
"""
A simple health check endpoint.
"""
return {"status": "ok", "message": "Service is running."}

# To run this server (save as main.py):
# 1. Install dependencies: pip install fastapi uvicorn pydantic pydantic-settings httpx python-dotenv
# 2. Create a .env file with your GitHub token and repo details.
# 3. Run with Uvicorn: uvicorn main:app --reload
# 4. Deploy to a public platform like Render.com.

**Key aspects of this server:**

- **Configuration (`.env`):** It expects a `.env` file with your `GITHUB_TOKEN`, `GITHUB_REPO_OWNER`, and `GITHUB_REPO_NAME`. You can also customize `GITHUB_FILE_PATH` (default: `LangSmith_prompt_manifest.json`) and `GITHUB_BRANCH` (default: `main`).
- **GitHub Interaction:** The `commit_manifest_to_github` function handles the logic of fetching the current file’s SHA (to update it) and then committing the new manifest content.
- **Webhook Endpoint (`/webhook/commit`):** This is the URL path your LangSmith webhook will target.
- **Error Handling:** Basic error handling for GitHub API interactions is included.

**Deploy this server to your chosen platform (e.g., Render) and note down its public URL (e.g., `https://prompt-commit-webhook.onrender.com`).**

## ​ Configuring the webhook in LangSmith

Once your FastAPI server is deployed and you have its public URL, you can configure the webhook in LangSmith:

1. Navigate to your LangSmith workspace.
2. Go to the **Prompts** section. Here you’ll see a list of your prompts.!LangSmith Prompts section
3. On the top right of the Prompts page, click the **\+ Webhook** button.
4. You’ll be presented with a form to configure your webhook:!LangSmith Webhook configuration modal - **Webhook URL:** Enter the full public URL of your deployed FastAPI server’s endpoint. For our example server, this would be `https://prompt-commit-webhook.onrender.com/webhook/commit`.
- **Headers (Optional):**
- You can add custom headers that LangSmith will send with each webhook request.
5. **Test the Webhook:** LangSmith provides a “Send Test Notification” button. Use this to send a sample payload to your server. Check your server logs (e.g., on Render) to ensure it receives the request and processes it successfully (or to debug any issues).
6. **Save** the webhook configuration.

## ​ The workflow in action

1. **Prompt Modification:** A user (developer or non-technical team member) modifies a prompt in the LangSmith UI and saves it, creating a new “prompt commit.”
2. **Webhook Trigger:** LangSmith detects this new prompt commit and triggers the configured webhook.
3. **HTTP Request:** LangSmith sends an HTTP POST request to the public URL of your FastAPI server (e.g., `https://prompt-commit-webhook.onrender.com/webhook/commit`). The body of this request contains the JSON prompt manifest for the entire workspace.
4. **Server Receives Payload:** Your FastAPI server’s endpoint receives the request.
5. **GitHub Commit:** The server parses the JSON manifest from the request body. It then uses the configured GitHub Personal Access Token, repository owner, repository name, file path, and branch to: - Check if the manifest file already exists in the repository on the specified branch to get its SHA (this is necessary for updating an existing file).
- Create a new commit with the latest prompt manifest, either creating the file or updating it if it already exists. The commit message will indicate that it’s an update from LangSmith.
6. **Confirmation:** You should see the new commit appear in your GitHub repository.!Manifest commited to GitHub

You’ve now successfully synced your LangSmith prompts with GitHub!

## ​ Beyond a simple commit

Our example FastAPI server performs a direct commit of the entire prompt manifest. However, this is just the starting point. You can extend the server’s functionality to perform more sophisticated actions:

- **Granular Commits:** Parse the manifest and commit changes to individual prompt files if you prefer a more granular structure in your repository.
- **Trigger CI/CD:** Instead of (or in addition to) committing, have the server trigger a CI/CD pipeline (e.g., Jenkins, GitHub Actions, GitLab CI) to deploy a staging environment, run tests, or build new application versions.
- **Update Databases/Caches:** If your application loads prompts from a database or cache, update these stores directly.
- **Notifications:** Send notifications to Slack, email, or other communication channels about prompt changes.
- **Selective Processing:** Based on metadata within the LangSmith payload (if available, e.g., which specific prompt changed or by whom), you could apply different logic.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Optimize a classifier\\
\\
Previous Test multi-turn conversations\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/multiple-messages

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Tutorials

Test multi-turn conversations

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

On this page

- From an existing run
- From a dataset
- Manually
- Next steps

This how-to guide walks you through the various ways you can set up the playground for multi-turn conversations, which will allow you to test different tool configurations and system prompts against longer threads of messages.!Multiturn diagram

## ​ From an existing run

First, ensure you have properly traced a multi-turn conversation, and then navigate to your tracing project. Once you get to your tracing project simply open the run, select the LLM call, and open it in the playground as follows:!Multiturn from runYou can then edit the system prompt, tweak the tools and/or output schema and observe how the output of the multi-turn conversation changes.

## ​ From a dataset

Before starting, make sure you have set up your dataset. Since you want to evaluate multi-turn conversations, make sure there is a key in your inputs that contains a list of messages.Once you have created your dataset, head to the playground and load your dataset to evaluate.Then, add a messages list variable to your prompt, making sure to name it the same as the key in your inputs that contains the list of messages:!Multiturn from datasetWhen you run your prompt, the messages from each example will be added as a list in place of the ‘Messages List’ variable.

## ​ Manually

There are two ways to manually create multi-turn conversations. The first way is by simply appending messages to the prompt:!Multiturn manualThis is helpful for quick iteration, but is rigid since the multi-turn conversation is hardcoded. Instead, if you want your prompt to work with any multi-turn conversation you can add a ‘Messages List’ variable and add your multi-turn conversation there:!Multiturn manual listThis allows you to just tweak the system prompt or the tools, while allowing any multi-turn conversation to take the place of the `Messages List` variable, allowing you to reuse this prompt across various runs.

## ​ Next steps

Now that you know how to set up the playground for multi-turn interactions, you can either manually inspect and judge the outputs, or you can add evaluators to classify results.You can also read these how-to guides to learn more about how to use the playground to run evaluations.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to sync prompts with GitHub\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/prompt-engineering-concepts)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Prompt engineering quickstart Prompt engineering concepts Prompt engineering

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/create-a-prompt)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability Prompt engineering quickstart LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/manage-prompts)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Manage prompts Manage prompts programmatically LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/manage-prompts-programmatically)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Manage prompts programmatically Manage prompts LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/managing-model-configurations)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Self-hosted LangSmith LangSmith docs App development in LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/use-tools)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server Test a ReAct agent with Pytest/Vitest and LangSmith LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/multimodal-content)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Include multimodal content in a prompt Run an evaluation with multimodal content Model Context Protocol (MCP)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/write-prompt-with-ai)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Prompt engineering concepts LangSmith docs LangSmith Polly

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/optimize-classifier)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Optimize a classifier LangSmith Polly LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/prompt-commit)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Manage prompts How to sync prompts with GitHub LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/multiple-messages)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Self-hosted LangSmith LangSmith docs LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/polly)**

Skip to main content**#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Polly LangSmith docs LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/quickstart

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Quickstart

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

On this page

- Prerequisites
- Step 1: Install dependencies
- Step 2: Set up your API keys
- Step 3: Create a search tool
- Step 4: Create a deep agent
- Step 5: Run the agent
- What happened?
- Next steps

This guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. You’ll build a research agent that can conduct research and write reports.

## ​ Prerequisites

Before you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI).

### ​ Step 1: Install dependencies

pip

uv

poetry

Copy

pip install deepagents tavily-python

### ​ Step 2: Set up your API keys

export ANTHROPIC_API_KEY="your-api-key"
export TAVILY_API_KEY="your-tavily-api-key"

### ​ Step 3: Create a search tool

import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
query: str,
max_results: int = 5,
topic: Literal["general", "news", "finance"] = "general",
include_raw_content: bool = False,
):
"""Run a web search"""
return tavily_client.search(
query,
max_results=max_results,
include_raw_content=include_raw_content,
topic=topic,
)

### ​ Step 4: Create a deep agent

# System prompt to steer the agent to be an expert researcher
research_instructions = """You are an expert researcher. Your job is to conduct thorough research and then write a polished report.

You have access to an internet search tool as your primary means of gathering information.

## `internet_search`

Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included.
"""

agent = create_deep_agent(
tools=[internet_search],
system_prompt=research_instructions
)

### ​ Step 5: Run the agent

result = agent.invoke({"messages": [{"role": "user", "content": "What is langgraph?"}]})

# Print the agent's response
print(result["messages"][-1].content)

## ​ What happened?

Your deep agent automatically:

1. **Planned its approach**: Used the built-in `write_todos` tool to break down the research task
2. **Conducted research**: Called the `internet_search` tool to gather information
3. **Managed context**: Used file system tools (`write_file`, `read_file`) to offload large search results
4. **Spawned subagents** (if needed): Delegated complex subtasks to specialized subagents
5. **Synthesized a report**: Compiled findings into a coherent response

## ​ Next steps

Now that you’ve built your first deep agent:

- **Customize your agent**: Learn about customization options, including custom system prompts, tools, and subagents.
- **Understand middleware**: Dive into the middleware architecture that powers deep agents.
- **Add long-term memory**: Enable persistent memory across conversations.
- **Deploy to production**: Learn about deployment options for LangGraph applications.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Deep Agents overview\\
\\
Previous Customize Deep Agents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/customization

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Get started

Customize Deep Agents

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

On this page

- Model
- System prompt
- Tools

create\_deep\_agent

Core Config

Features

Model

System Prompt

Tools

Backend

Subagents

Interrupts

Customized Agent

## ​ Model

By default, `deepagents` uses `claude-sonnet-4-5-20250929`. You can customize the model used by passing any supported model identifier string or LangChain model object.

Model string

LangChain model object

Copy

from langchain.chat_models import init_chat_model
from deepagents import create_deep_agent

model = init_chat_model(model="gpt-5")
agent = create_deep_agent(model=model)

## ​ System prompt

Deep agents come with a built-in system prompt inspired by Claude Code’s system prompt. The default system prompt contains detailed instructions for using the built-in planning tool, file system tools, and subagents.Each deep agent tailored to a use case should include a custom system prompt specific to that use case.

from deepagents import create_deep_agent

research_instructions = """\
You are an expert researcher. Your job is to conduct \
thorough research, and then write a polished report. \
"""

agent = create_deep_agent(
system_prompt=research_instructions,
)

## ​ Tools

Just like tool-calling agents, a deep agent gets a set of top level tools that it has access to.

import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
query: str,
max_results: int = 5,
topic: Literal["general", "news", "finance"] = "general",
include_raw_content: bool = False,
):
"""Run a web search"""
return tavily_client.search(
query,
max_results=max_results,
include_raw_content=include_raw_content,
topic=topic,
)

agent = create_deep_agent(
tools=[internet_search]
)

In addition to any tools that you provide, deep agents also get access to a number of default tools:

- `write_todos` – Update the agent’s to-do list
- `ls` – List all files in the agent’s filesystem
- `read_file` – Read a file from the agent’s filesystem
- `write_file` – Write a new file in the agent’s filesystem
- `edit_file` – Edit an existing file in the agent’s filesystem
- `task` – Spawn a subagent to handle a specific task

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Quickstart\\
\\
Previous Agent harness capabilities\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/harness

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Agent harness capabilities

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

On this page

- File system access
- Large tool result eviction
- Pluggable storage backends
- Task delegation (subagents)
- Conversation history summarization
- Dangling tool call repair
- To-do list tracking
- Human-in-the-Loop
- Prompt caching (Anthropic)

We think of `deepagents` as an “agent harness”. It is the same core tool calling loop as other agent frameworks, but with built-in tools and capabilities.

isolated work

Deep Agent

File System Tools

To-Do List

Subagents

Storage Backend

State

Filesystem

Store

Final Result

This page lists out the components that make up the agent harness.

## ​ File system access

The harness provides six tools for file system operations, making files first-class citizens in the agent’s environment:

| Tool | Description |
| --- | --- |
| `ls` | List files in a directory with metadata (size, modified time) |
| `read_file` | Read file contents with line numbers, supports offset/limit for large files |
| `write_file` | Create new files |
| `edit_file` | Perform exact string replacements in files (with global replace mode) |
| `glob` | Find files matching patterns (e.g., `**/*.py`) |
| `grep` | Search file contents with multiple output modes (files only, content with context, or counts) |

## ​ Large tool result eviction

The harness automatically dumps large tool results to the file system when they exceed a token threshold, preventing context window saturation.**How it works:**

- Monitors tool call results for size (default threshold: 20,000 tokens)
- When exceeded, writes the result to a file instead
- Replaces the tool result with a concise reference to the file
- Agent can later read the file if needed

## ​ Pluggable storage backends

The harness abstracts file system operations behind a protocol, allowing different storage strategies for different use cases.**Available backends:**

1. **StateBackend** \- Ephemeral in-memory storage - Files live in the agent’s state (checkpointed with conversation)
- Persists within a thread but not across threads
- Useful for temporary working files
2. **FilesystemBackend** \- Real filesystem access - Read/write from actual disk
- Supports virtual mode (sandboxed to a root directory)
- Integrates with system tools (ripgrep for grep)
- Security features: path validation, size limits, symlink prevention
3. **StoreBackend** \- Persistent cross-conversation storage - Uses LangGraph’s BaseStore for durability
- Namespaced per assistant\_id
- Files persist across conversations
- Useful for long-term memory or knowledge bases
4. **CompositeBackend** \- Route different paths to different backends - Example: `/` → StateBackend, `/memories/` → StoreBackend
- Longest-prefix matching for routing
- Enables hybrid storage strategies

## ​ Task delegation (subagents)

The harness allows the main agent to create ephemeral “subagents” for isolated multi-step tasks.**Why it’s useful:**

- **Context isolation** \- Subagent’s work doesn’t clutter main agent’s context
- **Parallel execution** \- Multiple subagents can run concurrently
- **Specialization** \- Subagents can have different tools/configurations
- **Token efficiency** \- Large subtask context is compressed into a single result

**How it works:**

- Main agent has a `task` tool
- When invoked, creates a fresh agent instance with its own context
- Subagent executes autonomously until completion
- Returns a single final report to the main agent
- Subagents are stateless (can’t send multiple messages back)

**Default subagent:**

- “general-purpose” subagent automatically available
- Has filesystem tools by default
- Can be customized with additional tools/middleware

**Custom subagents:**

- Define specialized subagents with specific tools
- Example: code-reviewer, web-researcher, test-runner
- Configure via `subagents` parameter

## ​ Conversation history summarization

The harness automatically compresses old conversation history when token usage becomes excessive.**Configuration:**

- Triggers at 170,000 tokens
- Keeps the most recent 6 messages intact
- Older messages are summarized by the model

**Why it’s useful:**

- Enables very long conversations without hitting context limits
- Preserves recent context while compressing ancient history
- Transparent to the agent (appears as a special system message)

## ​ Dangling tool call repair

The harness fixes message history when tool calls are interrupted or cancelled before receiving results.**The problem:**

- Agent requests tool call: “Please run X”
- Tool call is interrupted (user cancels, error, etc.)
- Agent sees tool\_call in AIMessage but no corresponding ToolMessage
- This creates an invalid message sequence

**The solution:**

- Detects AIMessages with tool\_calls that have no results
- Creates synthetic ToolMessage responses indicating the call was cancelled
- Repairs the message history before agent execution

- Prevents agent confusion from incomplete message chains
- Gracefully handles interruptions and errors
- Maintains conversation coherence

## ​ To-do list tracking

The harness provides a `write_todos` tool that agents can use to maintain a structured task list.**Features:**

- Track multiple tasks with statuses (pending, in\_progress, completed)
- Persisted in agent state
- Helps agent organize complex multi-step work
- Useful for long-running tasks and planning

## ​ Human-in-the-Loop

The harness pauses agent execution at specified tool calls to allow human approval/modification.**Configuration:**

- Map tool names to interrupt configurations
- Example: `{"edit_file": True}` \- pause before every edit
- Can provide approval messages or modify tool inputs

- Safety gates for destructive operations
- User verification before expensive API calls
- Interactive debugging and guidance

## ​ Prompt caching (Anthropic)

The harness enables Anthropic’s prompt caching feature to reduce redundant token processing.**How it works:**

- Caches portions of the prompt that repeat across turns
- Significantly reduces latency and cost for long system prompts
- Automatically skips for non-Anthropic models

- System prompts (especially with filesystem docs) can be 5k+ tokens
- These repeat every turn without caching
- Caching provides ~10x speedup and cost reduction

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Customize Deep Agents\\
\\
Previous Backends\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/backends

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Backends

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

On this page

- Quickstart
- Built-in backends
- StateBackend (ephemeral)
- FilesystemBackend (local disk)
- StoreBackend (LangGraph Store)
- CompositeBackend (router)
- Specify a backend
- Route to different backends
- Use a virtual filesystem
- Add policy hooks
- Protocol reference

Deep agents expose a filesystem surface to the agent via tools like `ls`, `read_file`, `write_file`, `edit_file`, `glob`, and `grep`. These tools operate through a pluggable backend.

Filesystem Tools

Backend

State

Filesystem

Store

Composite

Custom

Routes

This page explains how to choose a backend, route different paths to different backends, implement your own virtual filesystem (e.g., S3 or Postgres), add policy hooks, and comply with the backend protocol.

## ​ Quickstart

Here are a few pre-built filesystem backends that you can quickly use with your deep agent:

| Built-in backend | Description |
| --- | --- |

| Composite | Ephemeral by default, `/memories/` persisted. The Composite backend is maximally flexible. You can specify different routes in the filesystem to point towards different backends. See Composite routing below for a ready-to-paste example. |

## ​ Built-in backends

### ​ StateBackend (ephemeral)

Copy

# By default we provide a StateBackend
agent = create_deep_agent()

# Under the hood, it looks like
from deepagents.backends import StateBackend

agent = create_deep_agent(
backend=(lambda rt: StateBackend(rt)) # Note that the tools access State through the runtime.state
)

**How it works:**

- Stores files in LangGraph agent state for the current thread.
- Persists across multiple agent turns on the same thread via checkpoints.

**Best for:**

- A scratch pad for the agent to write intermediate results.
- Automatic eviction of large tool outputs which the agent can then read back in piece by piece.

### ​ FilesystemBackend (local disk)

from deepagents.backends import FilesystemBackend

agent = create_deep_agent(
backend=FilesystemBackend(root_dir=".", virtual_mode=True)
)

- Reads/writes real files under a configurable `root_dir`.
- You can optionally set `virtual_mode=True` to sandbox and normalize paths under `root_dir`.
- Uses secure path resolution, prevents unsafe symlink traversal when possible, can use ripgrep for fast `grep`.

- Local projects on your machine
- CI sandboxes
- Mounted persistent volumes

### ​ StoreBackend (LangGraph Store)

from langgraph.store.memory import InMemoryStore
from deepagents.backends import StoreBackend

agent = create_deep_agent(
backend=(lambda rt: StoreBackend(rt)), # Note that the tools access Store through the runtime.store
store=InMemoryStore()
)

- Stores files in a LangGraph `BaseStore` provided by the runtime, enabling cross‑thread durable storage.

- When you already run with a configured LangGraph store (for example, Redis, Postgres, or cloud implementations behind `BaseStore`).
- When you’re deploying your agent through LangSmith Deployment (a store is automatically provisioned for your agent).

### ​ CompositeBackend (router)

from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, StateBackend, StoreBackend
from langgraph.store.memory import InMemoryStore

composite_backend = lambda rt: CompositeBackend(
default=StateBackend(rt),
routes={
"/memories/": StoreBackend(rt),
}
)

agent = create_deep_agent(
backend=composite_backend,
store=InMemoryStore() # Store passed to create_deep_agent, not backend
)

- Routes file operations to different backends based on path prefix.
- Preserves the original path prefixes in listings and search results.

- When you want to give your agent both ephemeral and cross-thread storage, a CompositeBackend allows you provide both a StateBackend and StoreBackend
- When you have multiple sources of information that you want to provide to your agent as part of a single filesystem.
- e.g. You have long-term memories stored under /memories/ in one Store and you also have a custom backend that has documentation accessible at /docs/.

## ​ Specify a backend

- Pass a backend to `create_deep_agent(backend=...)`. The filesystem middleware uses it for all tooling.
- You can pass either:
- An instance implementing `BackendProtocol` (for example, `FilesystemBackend(root_dir=".")`), or
- A factory `BackendFactory = Callable[[ToolRuntime], BackendProtocol]` (for backends that need runtime like `StateBackend` or `StoreBackend`).
- If omitted, the default is `lambda rt: StateBackend(rt)`.

## ​ Route to different backends

Route parts of the namespace to different backends. Commonly used to persist `/memories/*` and keep everything else ephemeral.

from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, StateBackend, FilesystemBackend

composite_backend = lambda rt: CompositeBackend(
default=StateBackend(rt),
routes={
"/memories/": FilesystemBackend(root_dir="/deepagents/myagent", virtual_mode=True),
},
)

agent = create_deep_agent(backend=composite_backend)

Behavior:

- `/workspace/plan.md` → StateBackend (ephemeral)
- `/memories/agent.md` → FilesystemBackend under `/deepagents/myagent`
- `ls`, `glob`, `grep` aggregate results and show original path prefixes.

Notes:

- Longer prefixes win (for example, route `"/memories/projects/"` can override `"/memories/"`).
- For StoreBackend routing, ensure the agent runtime provides a store (`runtime.store`).

## ​ Use a virtual filesystem

Build a custom backend to project a remote or database filesystem (e.g., S3 or Postgres) into the tools namespace.Design guidelines:

- Paths are absolute (`/x/y.txt`). Decide how to map them to your storage keys/rows.
- Implement `ls_info` and `glob_info` efficiently (server-side listing where available, otherwise local filter).
- Return user-readable error strings for missing files or invalid regex patterns.
- For external persistence, set `files_update=None` in results; only in-state backends should return a `files_update` dict.

S3-style outline:

from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult
from deepagents.backends.utils import FileInfo, GrepMatch

class S3Backend(BackendProtocol):
def __init__(self, bucket: str, prefix: str = ""):
self.bucket = bucket
self.prefix = prefix.rstrip("/")

return f"{self.prefix}{path}"

# List objects under _key(path); build FileInfo entries (path, size, modified_at)
...

# Fetch object; return numbered content or an error string

# Optionally filter server‑side; else list and scan content

# Apply glob relative to path across keys

# Enforce create‑only semantics; return WriteResult(path=file_path, files_update=None)

# Read → replace (respect uniqueness vs replace_all) → write → return occurrences

Postgres-style outline:

- Table `files(path text primary key, content text, created_at timestamptz, modified_at timestamptz)`
- Map tool operations onto SQL:
- `ls_info` uses `WHERE path LIKE $1 || '%'`
- `glob_info` filter in SQL or fetch then apply glob in Python
- `grep_raw` can fetch candidate rows by extension or last modified time, then scan lines

## ​ Add policy hooks

Enforce enterprise rules by subclassing or wrapping a backend.Block writes/edits under selected prefixes (subclass):

from deepagents.backends.filesystem import FilesystemBackend
from deepagents.backends.protocol import WriteResult, EditResult

class GuardedBackend(FilesystemBackend):
def __init__(self, *, deny_prefixes: list[str], **kwargs):
super().__init__(**kwargs)
self.deny_prefixes = [p if p.endswith("/") else p + "/" for p in deny_prefixes]

if any(file_path.startswith(p) for p in self.deny_prefixes):
return WriteResult(error=f"Writes are not allowed under {file_path}")
return super().write(file_path, content)

if any(file_path.startswith(p) for p in self.deny_prefixes):
return EditResult(error=f"Edits are not allowed under {file_path}")
return super().edit(file_path, old_string, new_string, replace_all)

Generic wrapper (works with any backend):

class PolicyWrapper(BackendProtocol):
def __init__(self, inner: BackendProtocol, deny_prefixes: list[str] | None = None):
self.inner = inner
self.deny_prefixes = [p if p.endswith("/") else p + "/" for p in (deny_prefixes or [])]

return any(path.startswith(p) for p in self.deny_prefixes)

return self.inner.ls_info(path)

return self.inner.read(file_path, offset=offset, limit=limit)

return self.inner.grep_raw(pattern, path, glob)

return self.inner.glob_info(pattern, path)

if self._deny(file_path):
return WriteResult(error=f"Writes are not allowed under {file_path}")
return self.inner.write(file_path, content)

if self._deny(file_path):
return EditResult(error=f"Edits are not allowed under {file_path}")
return self.inner.edit(file_path, old_string, new_string, replace_all)

## ​ Protocol reference

Backends must implement the `BackendProtocol`.Required endpoints:

- Return entries with at least `path`. Include `is_dir`, `size`, `modified_at` when available. Sort by `path` for deterministic output.

- Return numbered content. On missing file, return `"Error: File '/x' not found"`.

- Return structured matches. For an invalid regex, return a string like `"Invalid regex pattern: ..."` (do not raise).

- Return matched files as `FileInfo` entries (empty list if none).

- Create-only. On conflict, return `WriteResult(error=...)`. On success, set `path` and for state backends set `files_update={...}`; external backends should use `files_update=None`.

- Enforce uniqueness of `old_string` unless `replace_all=True`. If not found, return error. Include `occurrences` on success.

Supporting types:

- `WriteResult(error, path, files_update)`
- `EditResult(error, path, files_update, occurrences)`
- `FileInfo` with fields: `path` (required), optionally `is_dir`, `size`, `modified_at`.
- `GrepMatch` with fields: `path`, `line`, `text`.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Agent harness capabilities\\
\\
Previous Subagents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/subagents

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Subagents

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

On this page

- Why use subagents?
- Configuration
- SubAgent (Dictionary-based)
- CompiledSubAgent
- Using SubAgent
- Using CompiledSubAgent
- The general-purpose subagent
- When to use it
- Best practices
- Write clear descriptions
- Keep system prompts detailed
- Minimize tool sets
- Choose models by task
- Return concise results
- Common patterns
- Multiple specialized subagents
- Troubleshooting
- Subagent not being called
- Context still getting bloated
- Wrong subagent being selected

Deep agents can create subagents to delegate work. You can specify custom subagents in the `subagents` parameter. Subagents are useful for context quarantine (keeping the main agent’s context clean) and for providing specialized instructions.

task tool

isolated work

Main Agent

Subagent

Research

Code

General

Final Result

## ​ Why use subagents?

Subagents solve the **context bloat problem**. When agents use tools with large outputs (web search, file reads, database queries), the context window fills up quickly with intermediate results. Subagents isolate this detailed work—the main agent receives only the final result, not the dozens of tool calls that produced it.**When to use subagents:**

- ✅ Multi-step tasks that would clutter the main agent’s context
- ✅ Specialized domains that need custom instructions or tools
- ✅ Tasks requiring different model capabilities
- ✅ When you want to keep the main agent focused on high-level coordination

**When NOT to use subagents:**

- ❌ Simple, single-step tasks
- ❌ When you need to maintain intermediate context
- ❌ When the overhead outweighs benefits

## ​ Configuration

`subagents` should be a list of dictionaries or `CompiledSubAgent` objects. There are two types:

### ​ SubAgent (Dictionary-based)

For most use cases, define subagents as dictionaries:**Required fields:**

- **name** (`str`): Unique identifier for the subagent. The main agent uses this name when calling the `task()` tool.
- **description** (`str`): What this subagent does. Be specific and action-oriented. The main agent uses this to decide when to delegate.
- **system\_prompt** (`str`): Instructions for the subagent. Include tool usage guidance and output format requirements.
- **tools** (`List[Callable]`): Tools the subagent can use. Keep this minimal and include only what’s needed.

**Optional fields:**

- **model** (`str | BaseChatModel`): Override the main agent’s model. Use the format `"provider:model-name"` (for example, `"openai:gpt-4o"`).
- **middleware** (`List[Middleware]`): Additional middleware for custom behavior, logging, or rate limiting.
- **interrupt\_on** (`Dict[str, bool]`): Configure human-in-the-loop for specific tools. Requires a checkpointer.

### ​ CompiledSubAgent

For complex workflows, use a pre-built LangGraph graph:**Fields:**

- **name** (`str`): Unique identifier
- **description** (`str`): What this subagent does
- **runnable** (`Runnable`): A compiled LangGraph graph (must call `.compile()` first)

## ​ Using SubAgent

Copy

import os
from typing import Literal
from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

def internet_search(
query: str,
max_results: int = 5,
topic: Literal["general", "news", "finance"] = "general",
include_raw_content: bool = False,
):
"""Run a web search"""
return tavily_client.search(
query,
max_results=max_results,
include_raw_content=include_raw_content,
topic=topic,
)

research_subagent = {
"name": "research-agent",
"description": "Used to research more in depth questions",
"system_prompt": "You are a great researcher",
"tools": [internet_search],
"model": "openai:gpt-4o", # Optional override, defaults to main agent model
}
subagents = [research_subagent]

agent = create_deep_agent(
model="claude-sonnet-4-5-20250929",
subagents=subagents
)

## ​ Using CompiledSubAgent

For more complex use cases, you can provide your own pre-built LangGraph graph as a subagent:

from deepagents import create_deep_agent, CompiledSubAgent
from langchain.agents import create_agent

# Create a custom agent graph
custom_graph = create_agent(
model=your_model,
tools=specialized_tools,
prompt="You are a specialized agent for data analysis..."
)

# Use it as a custom subagent
custom_subagent = CompiledSubAgent(
name="data-analyzer",
description="Specialized agent for complex data analysis tasks",
runnable=custom_graph
)

subagents = [custom_subagent]

agent = create_deep_agent(
model="claude-sonnet-4-5-20250929",
tools=[internet_search],
system_prompt=research_instructions,
subagents=subagents
)

## ​ The general-purpose subagent

In addition to any user-defined subagents, deep agents have access to a `general-purpose` subagent at all times. This subagent:

- Has the same system prompt as the main agent
- Has access to all the same tools
- Uses the same model (unless overridden)

### ​ When to use it

The general-purpose subagent is ideal for context isolation without specialized behavior. The main agent can delegate a complex multi-step task to this subagent and get a concise result back without bloat from intermediate tool calls.

## Example

Instead of the main agent making 10 web searches and filling its context with results, it delegates to the general-purpose subagent: `task(name="general-purpose", task="Research quantum computing trends")`. The subagent performs all the searches internally and returns only a summary.

## ​ Best practices

### ​ Write clear descriptions

The main agent uses descriptions to decide which subagent to call. Be specific:✅ **Good:**`"Analyzes financial data and generates investment insights with confidence scores"`❌ **Bad:**`"Does finance stuff"`

### ​ Keep system prompts detailed

Include specific guidance on how to use tools and format outputs:

research_subagent = {
"name": "research-agent",
"description": "Conducts in-depth research using web search and synthesizes findings",
"system_prompt": """You are a thorough researcher. Your job is to:

1. Break down the research question into searchable queries
2. Use internet_search to find relevant information
3. Synthesize findings into a comprehensive but concise summary
4. Cite sources when making claims

Output format:
- Summary (2-3 paragraphs)
- Key findings (bullet points)
- Sources (with URLs)

Keep your response under 500 words to maintain clean context.""",
"tools": [internet_search],
}

### ​ Minimize tool sets

Only give subagents the tools they need. This improves focus and security:

# ✅ Good: Focused tool set
email_agent = {
"name": "email-sender",
"tools": [send_email, validate_email], # Only email-related
}

# ❌ Bad: Too many tools
email_agent = {
"name": "email-sender",
"tools": [send_email, web_search, database_query, file_upload], # Unfocused
}

### ​ Choose models by task

Different models excel at different tasks:

subagents = [\
{\
"name": "contract-reviewer",\
"description": "Reviews legal documents and contracts",\
"system_prompt": "You are an expert legal reviewer...",\
"tools": [read_document, analyze_contract],\
"model": "claude-sonnet-4-5-20250929", # Large context for long documents\
},\
{\
"name": "financial-analyst",\
"description": "Analyzes financial data and market trends",\
"system_prompt": "You are an expert financial analyst...",\
"tools": [get_stock_price, analyze_fundamentals],\
"model": "openai:gpt-5", # Better for numerical analysis\
},\
]

### ​ Return concise results

Instruct subagents to return summaries, not raw data:

data_analyst = {
"system_prompt": """Analyze the data and return:
1. Key insights (3-5 bullet points)
2. Overall confidence score
3. Recommended next actions

Do NOT include:
- Raw data
- Intermediate calculations
- Detailed tool outputs

Keep response under 300 words."""
}

## ​ Common patterns

### ​ Multiple specialized subagents

Create specialized subagents for different domains:

from deepagents import create_deep_agent

subagents = [\
{\
"name": "data-collector",\
"description": "Gathers raw data from various sources",\
"system_prompt": "Collect comprehensive data on the topic",\
"tools": [web_search, api_call, database_query],\
},\
{\
"name": "data-analyzer",\
"description": "Analyzes collected data for insights",\
"system_prompt": "Analyze data and extract key insights",\
"tools": [statistical_analysis],\
},\
{\
"name": "report-writer",\
"description": "Writes polished reports from analysis",\
"system_prompt": "Create professional reports from insights",\
"tools": [format_document],\
},\
]

agent = create_deep_agent(
model="claude-sonnet-4-5-20250929",
system_prompt="You coordinate data analysis and reporting. Use subagents for specialized tasks.",
subagents=subagents
)

**Workflow:**

1. Main agent creates high-level plan
2. Delegates data collection to data-collector
3. Passes results to data-analyzer
4. Sends insights to report-writer
5. Compiles final output

Each subagent works with clean context focused only on its task.

## ​ Troubleshooting

### ​ Subagent not being called

**Problem**: Main agent tries to do work itself instead of delegating.**Solutions**:

1. **Make descriptions more specific:**

# ✅ Good
{"name": "research-specialist", "description": "Conducts in-depth research on specific topics using web search. Use when you need detailed information that requires multiple searches."}

# ❌ Bad
{"name": "helper", "description": "helps with stuff"}

2. **Instruct main agent to delegate:**

agent = create_deep_agent(
system_prompt="""...your instructions...

IMPORTANT: For complex tasks, delegate to your subagents using the task() tool.
This keeps your context clean and improves results.""",
subagents=[...]
)

### ​ Context still getting bloated

**Problem**: Context fills up despite using subagents.**Solutions**:

1. **Instruct subagent to return concise results:**

system_prompt="""...

IMPORTANT: Return only the essential summary.
Do NOT include raw data, intermediate search results, or detailed tool outputs.
Your response should be under 500 words."""

2. **Use filesystem for large data:**

system_prompt="""When you gather large amounts of data:
1. Save raw data to /data/raw_results.txt
2. Process and analyze the data
3. Return only the analysis summary

This keeps context clean."""

### ​ Wrong subagent being selected

**Problem**: Main agent calls inappropriate subagent for the task.**Solution**: Differentiate subagents clearly in descriptions:

subagents = [\
{\
"name": "quick-researcher",\
"description": "For simple, quick research questions that need 1-2 searches. Use when you need basic facts or definitions.",\
},\
{\
"name": "deep-researcher",\
"description": "For complex, in-depth research requiring multiple searches, synthesis, and analysis. Use for comprehensive reports.",\
}\
]

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Backends\\
\\
Previous Human-in-the-loop\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/human-in-the-loop

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Human-in-the-loop

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

On this page

- Basic configuration
- Decision types
- Handle interrupts
- Multiple tool calls
- Edit tool arguments
- Subagent interrupts
- Best practices
- Always use a checkpointer
- Use the same thread ID
- Match decision order to actions
- Tailor configurations by risk

Some tool operations may be sensitive and require human approval before execution. Deep agents support human-in-the-loop workflows through LangGraph’s interrupt capabilities. You can configure which tools require approval using the `interrupt_on` parameter.

no

yes

approve

edit

reject

Agent

Interrupt?

Execute

Human

Cancel

## ​ Basic configuration

The `interrupt_on` parameter accepts a dictionary mapping tool names to interrupt configurations. Each tool can be configured with:

- **`True`**: Enable interrupts with default behavior (approve, edit, reject allowed)
- **`False`**: Disable interrupts for this tool
- **`{"allowed_decisions": [...]}`**: Custom configuration with specific allowed decisions

Copy

from langchain.tools import tool
from deepagents import create_deep_agent
from langgraph.checkpoint.memory import MemorySaver

@tool

"""Delete a file from the filesystem."""
return f"Deleted {path}"

"""Read a file from the filesystem."""
return f"Contents of {path}"

"""Send an email."""
return f"Sent email to {to}"

# Checkpointer is REQUIRED for human-in-the-loop
checkpointer = MemorySaver()

agent = create_deep_agent(
model="claude-sonnet-4-5-20250929",
tools=[delete_file, read_file, send_email],
interrupt_on={
"delete_file": True, # Default: approve, edit, reject
"read_file": False, # No interrupts needed
"send_email": {"allowed_decisions": ["approve", "reject"]}, # No editing
},
checkpointer=checkpointer # Required!
)

## ​ Decision types

The `allowed_decisions` list controls what actions a human can take when reviewing a tool call:

- **`"approve"`**: Execute the tool with the original arguments as proposed by the agent
- **`"edit"`**: Modify the tool arguments before execution
- **`"reject"`**: Skip executing this tool call entirely

You can customize which decisions are available for each tool:

interrupt_on = {
# Sensitive operations: allow all options
"delete_file": {"allowed_decisions": ["approve", "edit", "reject"]},

# Moderate risk: approval or rejection only
"write_file": {"allowed_decisions": ["approve", "reject"]},

# Must approve (no rejection allowed)
"critical_operation": {"allowed_decisions": ["approve"]},
}

## ​ Handle interrupts

When an interrupt is triggered, the agent pauses execution and returns control. Check for interrupts in the result and handle them accordingly.

import uuid
from langgraph.types import Command

# Create config with thread_id for state persistence
config = {"configurable": {"thread_id": str(uuid.uuid4())}}

# Invoke the agent
result = agent.invoke({
"messages": [{"role": "user", "content": "Delete the file temp.txt"}]
}, config=config)

# Check if execution was interrupted
if result.get("__interrupt__"):
# Extract interrupt information
interrupts = result["__interrupt__"][0].value
action_requests = interrupts["action_requests"]
review_configs = interrupts["review_configs"]

# Create a lookup map from tool name to review config
config_map = {cfg["action_name"]: cfg for cfg in review_configs}

# Display the pending actions to the user
for action in action_requests:
review_config = config_map[action["name"]]
print(f"Tool: {action['name']}")
print(f"Arguments: {action['args']}")
print(f"Allowed decisions: {review_config['allowed_decisions']}")

# Get user decisions (one per action_request, in order)
decisions = [\
{"type": "approve"} # User approved the deletion\
]

# Resume execution with decisions
result = agent.invoke(
Command(resume={"decisions": decisions}),
config=config # Must use the same config!
)

# Process final result
print(result["messages"][-1].content)

## ​ Multiple tool calls

When the agent calls multiple tools that require approval, all interrupts are batched together in a single interrupt. You must provide decisions for each one in order.

result = agent.invoke({
"messages": [{\
"role": "user",\
"content": "Delete temp.txt and send an email to admin@example.com"\
}]
}, config=config)

if result.get("__interrupt__"):
interrupts = result["__interrupt__"][0].value
action_requests = interrupts["action_requests"]

# Two tools need approval
assert len(action_requests) == 2

# Provide decisions in the same order as action_requests
decisions = [\
{"type": "approve"}, # First tool: delete_file\
{"type": "reject"} # Second tool: send_email\
]

result = agent.invoke(
Command(resume={"decisions": decisions}),
config=config
)

## ​ Edit tool arguments

When `"edit"` is in the allowed decisions, you can modify the tool arguments before execution:

if result.get("__interrupt__"):
interrupts = result["__interrupt__"][0].value
action_request = interrupts["action_requests"][0]

# Original args from the agent
print(action_request["args"]) # {"to": "everyone@company.com", ...}

# User decides to edit the recipient
decisions = [{\
"type": "edit",\
"edited_action": {\
"name": action_request["name"], # Must include the tool name\
"args": {"to": "team@company.com", "subject": "...", "body": "..."}\
}\
}]

## ​ Subagent interrupts

Each subagent can have its own `interrupt_on` configuration that overrides the main agent’s settings:

agent = create_deep_agent(
tools=[delete_file, read_file],
interrupt_on={
"delete_file": True,
"read_file": False,
},
subagents=[{\
"name": "file-manager",\
"description": "Manages file operations",\
"system_prompt": "You are a file management assistant.",\
"tools": [delete_file, read_file],\
"interrupt_on": {\
# Override: require approval for reads in this subagent\
"delete_file": True,\
"read_file": True, # Different from main agent!\
}\
}],
checkpointer=checkpointer
)

When a subagent triggers an interrupt, the handling is the same – check for `__interrupt__` and resume with `Command`.

## ​ Best practices

### ​ Always use a checkpointer

Human-in-the-loop requires a checkpointer to persist agent state between the interrupt and resume:

from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
agent = create_deep_agent(
tools=[...],
interrupt_on={...},
checkpointer=checkpointer # Required for HITL
)

### ​ Use the same thread ID

When resuming, you must use the same config with the same `thread_id`:

# First call
config = {"configurable": {"thread_id": "my-thread"}}
result = agent.invoke(input, config=config)

# Resume (use same config)
result = agent.invoke(Command(resume={...}), config=config)

### ​ Match decision order to actions

The decisions list must match the order of `action_requests`:

# Create one decision per action, in order
decisions = []
for action in action_requests:
decision = get_user_decision(action) # Your logic
decisions.append(decision)

### ​ Tailor configurations by risk

Configure different tools based on their risk level:

# High risk: full control (approve, edit, reject)
interrupt_on = {
"delete_file": {"allowed_decisions": ["approve", "edit", "reject"]},
"send_email": {"allowed_decisions": ["approve", "edit", "reject"]},

# Medium risk: no editing allowed

# Low risk: no interrupts
"read_file": False,
"list_files": False,
}

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Subagents\\
\\
Previous Long-term memory\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/long-term-memory

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Long-term memory

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

On this page

- Setup
- How it works
- 1\. Short-term (transient) filesystem
- 2\. Long-term (persistent) filesystem
- Path routing
- Cross-thread persistence
- Use cases
- User preferences
- Self-improving instructions
- Knowledge base
- Research projects
- Store implementations
- InMemoryStore (development)
- PostgresStore (production)
- Best practices
- Use descriptive paths
- Document the memory structure
- Prune old data
- Choose the right storage

Deep agents come with a local filesystem to offload memory. By default, this filesystem is stored in agent state and is **transient to a single thread**—files are lost when the conversation ends.You can extend deep agents with **long-term memory** by using a **CompositeBackend** that routes specific paths to persistent storage. This enables hybrid storage where some files persist across threads while others remain ephemeral.

/memories/\*

other

Deep Agent

Path Router

Store Backend

State Backend

Persistent

across threads

Ephemeral

single thread

## ​ Setup

Configure long-term memory by using a `CompositeBackend` that routes the `/memories/` path to a `StoreBackend`:

Copy

from deepagents import create_deep_agent
from deepagents.backends import CompositeBackend, StateBackend, StoreBackend
from langgraph.store.memory import InMemoryStore
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()

def make_backend(runtime):
return CompositeBackend(
default=StateBackend(runtime), # Ephemeral storage
routes={
"/memories/": StoreBackend(runtime) # Persistent storage
}
)

agent = create_deep_agent(
store=InMemoryStore(), # Required for StoreBackend
backend=make_backend,
checkpointer=checkpointer
)

## ​ How it works

When using `CompositeBackend`, deep agents maintain **two separate filesystems**:

### ​ 1\. Short-term (transient) filesystem

- Stored in the agent’s state (via `StateBackend`)
- Persists only within a single thread
- Files are lost when the thread ends
- Accessed through standard paths: `/notes.txt`, `/workspace/draft.md`

### ​ 2\. Long-term (persistent) filesystem

- Stored in a LangGraph Store (via `StoreBackend`)
- Persists across all threads and conversations
- Survives agent restarts
- Accessed through paths prefixed with `/memories/`: `/memories/preferences.txt`

### ​ Path routing

The `CompositeBackend` routes file operations based on path prefixes:

- Files with paths starting with `/memories/` are stored in the Store (persistent)
- Files without this prefix remain in transient state
- All filesystem tools (`ls`, `read_file`, `write_file`, `edit_file`) work with both

# Transient file (lost after thread ends)
agent.invoke({
"messages": [{"role": "user", "content": "Write draft to /draft.txt"}]
})

# Persistent file (survives across threads)
agent.invoke({
"messages": [{"role": "user", "content": "Save final report to /memories/report.txt"}]
})

## ​ Cross-thread persistence

Files in `/memories/` can be accessed from any thread:

import uuid

# Thread 1: Write to long-term memory
config1 = {"configurable": {"thread_id": str(uuid.uuid4())}}
agent.invoke({
"messages": [{"role": "user", "content": "Save my preferences to /memories/preferences.txt"}]
}, config=config1)

# Thread 2: Read from long-term memory (different conversation!)
config2 = {"configurable": {"thread_id": str(uuid.uuid4())}}
agent.invoke({
"messages": [{"role": "user", "content": "What are my preferences?"}]
}, config=config2)
# Agent can read /memories/preferences.txt from the first thread

## ​ Use cases

### ​ User preferences

Store user preferences that persist across sessions:

agent = create_deep_agent(
store=InMemoryStore(),
backend=lambda rt: CompositeBackend(
default=StateBackend(rt),
routes={"/memories/": StoreBackend(rt)}
),
system_prompt="""When users tell you their preferences, save them to
/memories/user_preferences.txt so you remember them in future conversations."""
)

### ​ Self-improving instructions

An agent can update its own instructions based on feedback:

agent = create_deep_agent(
store=InMemoryStore(),
backend=lambda rt: CompositeBackend(
default=StateBackend(rt),
routes={"/memories/": StoreBackend(rt)}
),
system_prompt="""You have a file at /memories/instructions.txt with additional
instructions and preferences.

Read this file at the start of conversations to understand user preferences.

When users provide feedback like "please always do X" or "I prefer Y",
update /memories/instructions.txt using the edit_file tool."""
)

Over time, the instructions file accumulates user preferences, helping the agent improve.

### ​ Knowledge base

Build up knowledge over multiple conversations:

# Conversation 1: Learn about a project
agent.invoke({
"messages": [{"role": "user", "content": "We're building a web app with React. Save project notes."}]
})

# Conversation 2: Use that knowledge
agent.invoke({
"messages": [{"role": "user", "content": "What framework are we using?"}]
})
# Agent reads /memories/project_notes.txt from previous conversation

### ​ Research projects

Maintain research state across sessions:

research_agent = create_deep_agent(
store=InMemoryStore(),
backend=lambda rt: CompositeBackend(
default=StateBackend(rt),
routes={"/memories/": StoreBackend(rt)}
),
system_prompt="""You are a research assistant.

Save your research progress to /memories/research/:
- /memories/research/sources.txt - List of sources found
- /memories/research/notes.txt - Key findings and notes
- /memories/research/report.md - Final report draft

This allows research to continue across multiple sessions."""
)

## ​ Store implementations

Any LangGraph `BaseStore` implementation works:

### ​ InMemoryStore (development)

Good for testing and development, but data is lost on restart:

from langgraph.store.memory import InMemoryStore

store = InMemoryStore()
agent = create_deep_agent(
store=store,
backend=lambda rt: CompositeBackend(
default=StateBackend(rt),
routes={"/memories/": StoreBackend(rt)}
)
)

### ​ PostgresStore (production)

For production, use a persistent store:

from langgraph.store.postgres import PostgresStore
import os

store = PostgresStore(connection_string=os.environ["DATABASE_URL"])
agent = create_deep_agent(
store=store,
backend=lambda rt: CompositeBackend(
default=StateBackend(rt),
routes={"/memories/": StoreBackend(rt)}
)
)

## ​ Best practices

### ​ Use descriptive paths

Organize persistent files with clear paths:

/memories/user_preferences.txt
/memories/research/topic_a/sources.txt
/memories/research/topic_a/notes.txt
/memories/project/requirements.md

### ​ Document the memory structure

Tell the agent what’s stored where in your system prompt:

Your persistent memory structure:
- /memories/preferences.txt: User preferences and settings
- /memories/context/: Long-term context about the user
- /memories/knowledge/: Facts and information learned over time

### ​ Prune old data

Implement periodic cleanup of outdated persistent files to keep storage manageable.

### ​ Choose the right storage

- **Development**: Use `InMemoryStore` for quick iteration
- **Production**: Use `PostgresStore` or other persistent stores
- **Multi-tenant**: Consider using assistant\_id-based namespacing in your store

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Human-in-the-loop\\
\\
Previous Deep Agents Middleware\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/middleware

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Core capabilities

Deep Agents Middleware

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

On this page

- To-do list middleware
- Filesystem middleware
- Short-term vs. long-term filesystem
- Subagent middleware

Deep agents are built with a modular middleware architecture. Deep agents have access to:

1. A planning tool
2. A filesystem for storing context and long-term memories
3. The ability to spawn subagents

Each feature is implemented as separate middleware. When you create a deep agent with `create_deep_agent`, we automatically attach `TodoListMiddleware`, `FilesystemMiddleware`, and `SubAgentMiddleware` to your agent.

create\_deep\_agent

TodoList

Filesystem

SubAgent

Agent Tools

Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently.The following sections explain what each middleware provides.

## ​ To-do list middleware

Planning is integral to solving complex problems. If you’ve used Claude Code recently, you’ll notice how it writes out a to-do list before tackling complex, multi-part tasks. You’ll also notice how it can adapt and update this to-do list on the fly as more information comes in.`TodoListMiddleware` provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the `write_todos` tool to keep track of what it’s doing and what still needs to be done.

Copy

from langchain.agents import create_agent
from langchain.agents.middleware import TodoListMiddleware

# TodoListMiddleware is included by default in create_deep_agent
# You can customize it if building a custom agent
agent = create_agent(
model="claude-sonnet-4-5-20250929",
# Custom planning instructions can be added via middleware
middleware=[\
TodoListMiddleware(\
system_prompt="Use the write_todos tool to..." # Optional: Custom addition to the system prompt\
),\
],
)

## ​ Filesystem middleware

Context engineering is a main challenge in building effective agents. This is particularly difficult when using tools that return variable-length results (for example, web\_search and rag), as long tool results can quickly fill your context window.`FilesystemMiddleware` provides four tools for interacting with both short-term and long-term memory:

- **ls**: List the files in the filesystem
- **read\_file**: Read an entire file or a certain number of lines from a file
- **write\_file**: Write a new file to the filesystem
- **edit\_file**: Edit an existing file in the filesystem

from langchain.agents import create_agent
from deepagents.middleware.filesystem import FilesystemMiddleware

# FilesystemMiddleware is included by default in create_deep_agent
agent = create_agent(
model="claude-sonnet-4-5-20250929",
middleware=[\
FilesystemMiddleware(\
backend=None, # Optional: custom backend (defaults to StateBackend)\
system_prompt="Write to the filesystem when...", # Optional custom addition to the system prompt\
custom_tool_descriptions={\
"ls": "Use the ls tool when...",\
"read_file": "Use the read_file tool to..."\
} # Optional: Custom descriptions for filesystem tools\
),\
],
)

### ​ Short-term vs. long-term filesystem

By default, these tools write to a local “filesystem” in your graph state. To enable persistent storage across threads, configure a `CompositeBackend` that routes specific paths (like `/memories/`) to a `StoreBackend`.

from langchain.agents import create_agent
from deepagents.middleware import FilesystemMiddleware
from deepagents.backends import CompositeBackend, StateBackend, StoreBackend
from langgraph.store.memory import InMemoryStore

store = InMemoryStore()

agent = create_agent(
model="claude-sonnet-4-5-20250929",
store=store,
middleware=[\
FilesystemMiddleware(\
backend=lambda rt: CompositeBackend(\
default=StateBackend(rt),\
routes={"/memories/": StoreBackend(rt)}\
),\
custom_tool_descriptions={\
"ls": "Use the ls tool when...",\
"read_file": "Use the read_file tool to..."\
} # Optional: Custom descriptions for filesystem tools\
),\
],
)

When you configure a `CompositeBackend` with a `StoreBackend` for `/memories/`, any files prefixed with **/memories/** are saved to persistent storage and survive across different threads. Files without this prefix remain in ephemeral state storage.

## ​ Subagent middleware

Handing off tasks to subagents isolates context, keeping the main (supervisor) agent’s context window clean while still going deep on a task.The subagents middleware allows you to supply subagents through a `task` tool.

from langchain.tools import tool
from langchain.agents import create_agent
from deepagents.middleware.subagents import SubAgentMiddleware

@tool

"""Get the weather in a city."""
return f"The weather in {city} is sunny."

agent = create_agent(
model="claude-sonnet-4-5-20250929",
middleware=[\
SubAgentMiddleware(\
default_model="claude-sonnet-4-5-20250929",\
default_tools=[],\
subagents=[\
{\
"name": "weather",\
"description": "This subagent can get weather in cities.",\
"system_prompt": "Use the get_weather tool to get the weather in a city.",\
"tools": [get_weather],\
"model": "gpt-4o",\
"middleware": [],\
}\
],\
)\
],
)

A subagent is defined with a **name**, **description**, **system prompt**, and **tools**. You can also provide a subagent with a custom **model**, or with additional **middleware**. This can be particularly useful when you want to give the subagent an additional state key to share with the main agent.For more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent.

from langchain.agents import create_agent
from deepagents.middleware.subagents import SubAgentMiddleware
from deepagents import CompiledSubAgent
from langgraph.graph import StateGraph

# Create a custom LangGraph graph
def create_weather_graph():
workflow = StateGraph(...)
# Build your custom graph
return workflow.compile()

weather_graph = create_weather_graph()

# Wrap it in a CompiledSubAgent
weather_subagent = CompiledSubAgent(
name="weather",
description="This subagent can get weather in cities.",
runnable=weather_graph
)

agent = create_agent(
model="claude-sonnet-4-5-20250929",
middleware=[\
SubAgentMiddleware(\
default_model="claude-sonnet-4-5-20250929",\
default_tools=[],\
subagents=[weather_subagent],\
)\
],
)

In addition to any user-defined subagents, the main agent has access to a `general-purpose` subagent at all times. This subagent has the same instructions as the main agent and all the tools it has access to. The primary purpose of the `general-purpose` subagent is context isolation—the main agent can delegate a complex task to this subagent and get a concise answer back without bloat from intermediate tool calls.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Long-term memory\\
\\
Previous Deep Agents CLI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/cli

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Command line interface

Deep Agents CLI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

On this page

- Quick start
- Configuration
- Interactive mode
- Set project conventions with memories
- Use remote sandboxes

A terminal interface for building agents with persistent memory. Agents maintain context across sessions, learn project conventions, and execute code with approval controls.The Deep Agents CLI has the following built-in capabilities:

- **File operations** \- read, write, and edit files in your project with tools that enable agents to manage and modify code and documentation.
- **Shell command execution** \- execute shell commands to run tests, build projects, manage dependencies, and interact with version control systems.
- **Web search** \- search the web for up-to-date information and documentation (requires Tavily API key).
- **HTTP requests** \- make HTTP requests to APIs and external services for data fetching and integration tasks.
- **Task planning and tracking** \- break down complex tasks into discrete steps and track progress through the built-in todo system.
- **Memory storage and retrieval** \- store and retrieve information across sessions, enabling agents to remember project conventions and learned patterns.
- **Human-in-the-loop** \- require human approval for sensitive tool operations.

Watch the demo video to see how the Deep Agents CLI works.

## ​ Quick start

Set your API key

Export as an environment variable:

Copy

export ANTHROPIC_API_KEY="your-api-key"

Or create a `.env` file in your project root:

ANTHROPIC_API_KEY=your-api-key

Run the CLI

uvx deepagents-cli

Give the agent a task

The agent proposes changes with diffs for your approval before modifying files.

Additional installation and configuration options

Install locally if needed:

pip

uv

pip install deepagents-cli

The CLI uses Anthropic Claude Sonnet 4 by default. To use OpenAI:

export OPENAI_API_KEY="your-key"

Enable web search (optional):

export TAVILY_API_KEY="your-key"

API keys can be set as environment variables or in a `.env` file.

## ​ Configuration

Command-line options

| Option | Description |
| --- | --- |
| `--agent NAME` | Use named agent with separate memory |
| `--auto-approve` | Skip tool confirmation prompts (toggle with `Ctrl+T`) |
| `--sandbox TYPE` | Execute in remote sandbox: `modal`, `daytona`, or `runloop` |
| `--sandbox-id ID` | Reuse existing sandbox |
| `--sandbox-setup PATH` | Run setup script in sandbox |

CLI commands

| Command | Description |
| --- | --- |
| `deepagents list` | List all agents |
| `deepagents help` | Show help |
| `deepagents reset --agent NAME` | Clear agent memory and reset to default |
| `deepagents reset --agent NAME --target SOURCE` | Copy memory from another agent |

## ​ Interactive mode

Slash commands

Use these commands within the CLI session:

- `/tokens` \- Display token usage
- `/clear` \- Clear conversation history
- `/exit` \- Exit the CLI

Bash commands

Execute shell commands directly by prefixing with `!`:

Keyboard shortcuts

| Shortcut | Action |
| --- | --- |
| `Enter` | Submit |
| `Alt+Enter` | Newline |
| `Ctrl+E` | External editor |
| `Ctrl+T` | Toggle auto-approve |
| `Ctrl+C` | Interrupt |
| `Ctrl+D` | Exit |

## ​ Set project conventions with memories

Agents store information in `~/.deepagents/AGENT_NAME/memories/` as markdown files using a memory-first protocol:

1. **Research**: Searches memory for relevant context before starting tasks
2. **Response**: Checks memory when uncertain during execution
3. **Learning**: Automatically saves new information for future sessions

Organize memories by topic with descriptive filenames:

~/.deepagents/backend-dev/memories/
├── api-conventions.md
├── database-schema.md
└── deployment-process.md

Teach the agent conventions once:

It remembers for future sessions:

# Applies conventions without prompting

## ​ Use remote sandboxes

Execute code in isolated remote environments for safety and flexibility. Remote sandboxes provide the following benefits:

- **Safety**: Protect your local machine from potentially harmful code execution
- **Clean environments**: Use specific dependencies or OS configurations without local setup
- **Parallel execution**: Run multiple agents simultaneously in isolated environments
- **Long-running tasks**: Execute time-intensive operations without blocking your machine
- **Reproducibility**: Ensure consistent execution environments across teams

To use a remote sandbox, follow these steps:

1. Configure your sandbox provider ( Runloop, Daytona, or Modal):

# Runloop
export RUNLOOP_API_KEY="your-key"

# Daytona
export DAYTONA_API_KEY="your-key"

# Modal
modal setup

2. Run the CLI with a sandbox:

uvx deepagents-cli --sandbox runloop --sandbox-setup ./setup.sh

The agent runs locally but executes all code operations in the remote sandbox. Optional setup scripts can configure environment variables, clone repositories, and prepare dependencies.
3. (Optional) Create a `setup.sh` file to configure your sandbox environment:

#!/bin/bash
set -e

# Clone repository using GitHub token
git clone $HOME/workspace
cd $HOME/workspace

# Make environment variables persistent
cat >> ~/.bashrc <<'EOF'
export GITHUB_TOKEN="${GITHUB_TOKEN}"
export OPENAI_API_KEY="${OPENAI_API_KEY}"
cd $HOME/workspace
EOF

source ~/.bashrc

Store secrets in a local `.env` file for the setup script to access.

Sandboxes isolate code execution, but agents remain vulnerable to prompt injection with untrusted inputs. Use human-in-the-loop approval, short-lived secrets, and trusted setup scripts only. Note that sandbox APIs are evolving rapidly, and we expect more providers to support proxies that help mitigate prompt injection and secrets management concerns.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Deep Agents Middleware\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/quickstart)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Quickstart Tracing quickstart Evaluation quickstart

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/customization)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Customize Deep Agents Deep Agents overview Contributing to documentation

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/harness)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Agent harness capabilities Deep Agents overview Quickstart

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/backends)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Backends Deep Agents overview Quickstart

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/subagents)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Deep Agents overview Subagents Deep Agents Middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/human-in-the-loop)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Human-in-the-loop Human-in-the-loop using server API

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/long-term-memory)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Long-term memory Short-term memory

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/middleware)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Deep Agents Middleware How to add custom middleware Deep Agents overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/deepagents/cli)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Quickstart
- Customization

##### Core capabilities

- Agent harness
- Backends
- Subagents
- Human-in-the-loop
- Long-term memory
- Middleware

##### Command line interface

- Use the CLI

404

# Page not found

We couldn’t find the page you were looking for.

Deep Agents CLI Deep Agents overview Quickstart

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/local-server

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Run a LangGraph app locally

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Prerequisites
- 1\. Install the LangGraph CLI
- 2\. Create a LangGraph app
- 3\. Install dependencies
- 4\. Create a .env file
- 5\. Launch Agent Server
- 6\. Test the API
- Next steps

This quickstart shows you how to set up a LangGraph application locally for testing and development.

## ​ Prerequisites

Before you begin, ensure you have an API key for LangSmith (free to sign up).

## ​ 1\. Install the LangGraph CLI

- Python server

- Node server

Copy

pip install -U "langgraph-cli[inmem]"

npx @langchain/langgraph-cli

## ​ 2\. Create a LangGraph app

Create a new app from the `new-langgraph-project-python` template or `new-langgraph-project-js` template. This template demonstrates a single-node application you can extend with your own logic.

langgraph new path/to/your/app --template new-langgraph-project-python

langgraph new path/to/your/app --template new-langgraph-project-js

**Additional templates**

If you use `langgraph new` without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.

## ​ 3\. Install dependencies

In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:

cd path/to/your/app
pip install -e .

cd path/to/your/app
yarn install

## ​ 4\. Create a `.env` file

You will find a `.env.example` in the root of your new LangGraph app. Create a `.env` file in the root of your new LangGraph app and copy the contents of the `.env.example` file into it, filling in the necessary API keys:

LANGSMITH_API_KEY=lsv2...

## ​ 5\. Launch Agent Server

Start the Agent Server locally:

langgraph dev

npx @langchain/langgraph-cli dev

Sample output:

>
> - API:
>
> - Docs:
>
> - Studio Web UI:

The `langgraph dev` command starts Agent Server in an in-memory mode. This mode is suitable for development and testing purposes.

For production use, deploy Agent Server with a persistent storage backend. For more information, refer to the LangSmith platform options.

## ​ 6\. Test the API

- Python SDK (async)

- Python SDK (sync)

- Javascript SDK

- Rest API

1. Install the LangGraph Python SDK:

pip install langgraph-sdk

2. Send a message to the assistant (threadless run):

from langgraph_sdk import get_client
import asyncio

client = get_client(url="http://localhost:2024")

async def main():
async for chunk in client.runs.stream(
None, # Threadless run
"agent", # Name of assistant. Defined in langgraph.json.
input={
"messages": [{\
"role": "human",\
"content": "What is LangGraph?",\
}],
},
):
print(f"Receiving new event of type: {chunk.event}...")
print(chunk.data)
print("\n\n")

asyncio.run(main())

from langgraph_sdk import get_sync_client

client = get_sync_client(url="http://localhost:2024")

for chunk in client.runs.stream(
None, # Threadless run
"agent", # Name of assistant. Defined in langgraph.json.
input={
"messages": [{\
"role": "human",\
"content": "What is LangGraph?",\
}],
},
stream_mode="messages-tuple",
):
print(f"Receiving new event of type: {chunk.event}...")
print(chunk.data)
print("\n\n")

1. Install the LangGraph JS SDK:

npm install @langchain/langgraph-sdk

const { Client } = await import("@langchain/langgraph-sdk");

// only set the apiUrl if you changed the default port when calling langgraph dev
const client = new Client({ apiUrl: "http://localhost:2024"});

const streamResponse = client.runs.stream(
null, // Threadless run
"agent", // Assistant ID
{
input: {
"messages": [\
{ "role": "user", "content": "What is LangGraph?"}\
]
},
streamMode: "messages-tuple",
}
);

for await (const chunk of streamResponse) {
console.log(`Receiving new event of type: ${chunk.event}...`);
console.log(JSON.stringify(chunk.data));
console.log("\n\n");
}

curl -s --request POST \
--url "http://localhost:2024/runs/stream" \
--header 'Content-Type: application/json' \
--data "{
\"assistant_id\": \"agent\",
\"input\": {
\"messages\": [\
{\
\"role\": \"human\",\
\"content\": \"What is LangGraph?\"\
}\
]
},
\"stream_mode\": \"messages-tuple\"
}"

## ​ Next steps

Now that you have a LangGraph app running locally, you’re ready to deploy it:**Choose a hosting option for LangSmith:**

- **Cloud**: Fastest setup, fully managed (recommended).
- **Hybrid**: Data plane in your cloud, control plane managed by LangChain.
- **Self-hosted**: Full control in your infrastructure.

For more details, refer to the Platform setup comparison.**Then deploy your app:**

- Deploy to Cloud quickstart: Quick setup guide.
- Full Cloud setup guide: Comprehensive deployment documentation.

**Explore features:**

- **Studio**: Visualize, interact with, and debug your application with the Studio UI. Try the Studio quickstart.
- **API References**: LangSmith Deployment API, Python SDK, JS/TS SDK

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Deployment\\
\\
Previous App development in LangSmith Deployment\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/app-development

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

App development in LangSmith Deployment

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

**LangSmith Deployment** builds on the open-source LangGraph framework for developing stateful, multi-agent applications.
LangGraph provides the core abstractions and execution model, while LangSmith adds managed infrastructure, observability, deployment options, assistants, and concurrency controls—supporting the full lifecycle from development to production.

LangSmith Deployment is framework-agnostic: you can deploy agents built with LangGraph or other frameworks. To get started with LangGraph itself, refer to the LangGraph quickstart.

**Assistants** \\
\\
Manage agent configurations, connect to threads, and build interactive assistants.\\
\\
Explore assistants **Runs** \\
\\
Execute background jobs, stateless runs, cron jobs, and manage configurable headers.\\
\\
Learn about runs **Core capabilities** \\
\\
Streaming, human-in-the-loop, webhooks, and concurrency controls like double-texting.\\
\\
See core features **Tutorials** \\
\\
Step-by-step examples: AutoGen integration, streaming UI, and generative UI in React.\\
\\
View tutorials

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Run a LangGraph app locally\\
\\
Previous Deploy your app to Cloud\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deployment-quickstart

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Deploy your app to Cloud

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Prerequisites
- 1\. Create a repository on GitHub
- 2\. Deploy to LangSmith
- 3\. Test your application in Studio
- 4\. Get the API URL for your deployment
- 5\. Test the API
- Next steps

This is a quickstart guide for deploying your first application to LangSmith Cloud.

For a comprehensive Cloud deployment guide with all configuration options, refer to the Cloud deployment setup guide.

## ​ Prerequisites

Before you begin, ensure you have the following:

- A GitHub account
- A LangSmith account (free to sign up)

## ​ 1\. Create a repository on GitHub

To deploy an application to **LangSmith**, your application code must reside in a GitHub repository. Both public and private repositories are supported. For this quickstart, use the `new-langgraph-project` template for your application:

1. Go to the `new-langgraph-project` repository or `new-langgraphjs-project` template.
2. Click the `Fork` button in the top right corner to fork the repository to your GitHub account.
3. Click **Create fork**.

## ​ 2\. Deploy to LangSmith

1. Log in to LangSmith.
2. In the left sidebar, select **Deployments**.
3. Click the **\+ New Deployment** button. A pane will open where you can fill in the required fields.
4. If you are a first time user or adding a private repository that has not been previously connected, click the **Import from GitHub** button and follow the instructions to connect your GitHub account.
5. Select your New LangGraph Project repository.
6. Click **Submit** to deploy.
This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.

## ​ 3\. Test your application in Studio

Once your application is deployed:

1. Select the deployment you just created to view more details.
2. Click the **Studio** button in the top right corner. Studio will open to display your graph.

## ​ 4\. Get the API URL for your deployment

1. In the **Deployment details** view, click the **API URL** to copy it to your clipboard.
2. Click the `URL` to copy it to the clipboard.

## ​ 5\. Test the API

You can now test the API:

- Python SDK (Async)

- Python SDK (Sync)

- JavaScript SDK

- Rest API

1. Install the LangGraph Python SDK:

Copy

pip install langgraph-sdk

2. Send a message to the assistant (threadless run):

from langgraph_sdk import get_client

client = get_client(url="your-deployment-url", api_key="your-langsmith-api-key")

async for chunk in client.runs.stream(
None, # Threadless run
"agent", # Name of assistant. Defined in langgraph.json.
input={
"messages": [{\
"role": "human",\
"content": "What is LangGraph?",\
}],
},
stream_mode="updates",
):
print(f"Receiving new event of type: {chunk.event}...")
print(chunk.data)
print("\n\n")

from langgraph_sdk import get_sync_client

client = get_sync_client(url="your-deployment-url", api_key="your-langsmith-api-key")

for chunk in client.runs.stream(
None, # Threadless run
"agent", # Name of assistant. Defined in langgraph.json.
input={
"messages": [{\
"role": "human",\
"content": "What is LangGraph?",\
}],
},
stream_mode="updates",
):
print(f"Receiving new event of type: {chunk.event}...")
print(chunk.data)
print("\n\n")

1. Install the LangGraph JS SDK

npm install @langchain/langgraph-sdk

const { Client } = await import("@langchain/langgraph-sdk");

const client = new Client({ apiUrl: "your-deployment-url", apiKey: "your-langsmith-api-key" });

const streamResponse = client.runs.stream(
null, // Threadless run
"agent", // Assistant ID
{
input: {
"messages": [\
{ "role": "user", "content": "What is LangGraph?"}\
]
},
streamMode: "messages",
}
);

for await (const chunk of streamResponse) {
console.log(`Receiving new event of type: ${chunk.event}...`);
console.log(JSON.stringify(chunk.data));
console.log("\n\n");
}

curl -s --request POST \

--header 'Content-Type: application/json' \

--data "{
\"assistant_id\": \"agent\",
\"input\": {
\"messages\": [\
{\
\"role\": \"human\",\
\"content\": \"What is LangGraph?\"\
}\
]
},
\"stream_mode\": \"updates\"
}"

## ​ Next steps

You’ve successfully deployed your application to LangSmith Cloud. Here are some next steps:

- **Explore Studio**: Use Studio to visualize and debug your graph interactively.
- **Monitor your app**: Set up observability with traces, dashboards, and alerts.
- **Learn more about Cloud**: See the complete Cloud setup guide for all configuration options.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

App development in LangSmith Deployment\\
\\
Previous Application structure\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/application-structure

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Configure app for deployment

Application structure

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Key Concepts
- File structure
- Configuration file
- Examples
- Dependencies
- Graphs
- Use any framework with LangSmith Deployment
- Environment variables

To deploy on LangSmith, an application must consist of one or more graphs, a configuration file (`langgraph.json`), a file that specifies dependencies, and an optional `.env` file that specifies environment variables.This page explains how a LangSmith application is organized and how to provide the configuration details required for deployment.

## ​ Key Concepts

To deploy using LangSmith, provide the following information:

1. A configuration file (`langgraph.json`) that specifies the dependencies, graphs, and environment variables to use for the application.
2. The graphs that implement the logic of the application.
3. A file that specifies dependencies required to run the application.
4. Environment variables that are required for the application to run.

**Framework agnostic**LangSmith Deployment supports deploying a LangGraph _graph_. However, the implementation of a _node_ of a graph can contain arbitrary code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you implement your core application logic without using additional LangGraph OSS APIs while still using LangSmith for deployment, scaling, and observability. For more details, refer to Use any framework with LangSmith Deployment.

## ​ File structure

The following are examples of directory structures for Python and JavaScript applications:

- Python (requirements.txt)

- Python (pyproject.toml)

- JS (package.json)

Copy

my-app/
├── my_agent # all project code lies within here
│ ├── utils # utilities for your graph
│ │ ├── __init__.py
│ │ ├── tools.py # tools for your graph
│ │ ├── nodes.py # node functions for your graph
│ │ └── state.py # state definition of your graph
│ ├── __init__.py
│ └── agent.py # code for constructing your graph
├── .env # environment variables
├── requirements.txt # package dependencies
└── langgraph.json # configuration file for LangGraph

my-app/
├── my_agent # all project code lies within here
│ ├── utils # utilities for your graph
│ │ ├── __init__.py
│ │ ├── tools.py # tools for your graph
│ │ ├── nodes.py # node functions for your graph
│ │ └── state.py # state definition of your graph
│ ├── __init__.py
│ └── agent.py # code for constructing your graph
├── .env # environment variables
├── langgraph.json # configuration file for LangGraph
└── pyproject.toml # dependencies for your project

my-app/
├── src # all project code lies within here
│ ├── utils # optional utilities for your graph
│ │ ├── tools.ts # tools for your graph
│ │ ├── nodes.ts # node functions for your graph
│ │ └── state.ts # state definition of your graph
│ └── agent.ts # code for constructing your graph
├── package.json # package dependencies
├── .env # environment variables
└── langgraph.json # configuration file for LangGraph

The directory structure of an application can vary depending on the programming language and the package manager used.

## ​ Configuration file

The `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy an application.For details on all supported keys in the JSON file, refer to the LangGraph configuration file reference.

The LangGraph CLI defaults to using the configuration file `langgraph.json` in the current directory.

### ​ Examples

- Python

- JavaScript

- The dependencies involve a custom local package and the `langchain_openai` package.
- A single graph will be loaded from the file `./your_package/your_file.py` with the variable `agent`.
- The environment variables are loaded from the `.env` file.

{
"dependencies": [\
"langchain_openai",\
"./your_package"\
],
"graphs": {
"my_agent": "./your_package/your_file.py:agent"
},
"env": "./.env"
}

- The dependencies will be loaded from a dependency file in the local directory (e.g., `package.json`).
- A single graph will be loaded from the file `./your_package/your_file.js` with the function `agent`.
- The environment variable `OPENAI_API_KEY` is set inline.

{
"dependencies": [\
"."\
],
"graphs": {
"my_agent": "./your_package/your_file.js:agent"
},
"env": {
"OPENAI_API_KEY": "secret-key"
}
}

## ​ Dependencies

An application may depend on other Python packages or JavaScript libraries (depending on the programming language in which the application is written).You will generally need to specify the following information for dependencies to be set up correctly:

1. A file in the directory that specifies the dependencies (e.g., `requirements.txt`, `pyproject.toml`, or `package.json`).
2. A `dependencies` key in the configuration file that specifies the dependencies required to run the application.
3. Any additional binaries or system libraries can be specified using `dockerfile_lines` key in the LangGraph configuration file.

## ​ Graphs

Use the `graphs` key in the configuration file to specify which graphs will be available in the deployed application.You can specify one or more graphs in the configuration file. Each graph is identified by a unique name and a path to either (1) a compiled graph or (2) a function that defines a graph.

### ​ Use any framework with LangSmith Deployment

While LangSmith Deployment requires applications to be structured as a LangGraph graph, individual nodes within that graph can contain arbitrary code. This means you can use any framework or library within your nodes while still benefiting from LangSmith’s deployment infrastructure.The graph structure serves as a deployment interface, but your core application logic can use whichever tools and frameworks best suit your needs.To deploy with LangSmith, you need:

1. **A LangGraph graph structure**: Define a graph using `StateGraph` with `add_node` and `add_edge`.
2. **Node functions with arbitrary logic**: Your node functions can call any framework or library.
3. **A compiled graph**: Compile the graph to create a deployable application.

The following example shows how to wrap your existing application logic within a minimal LangGraph structure:

from langgraph.graph import StateGraph, START, END
from typing import TypedDict

# Your existing application logic using any framework
from app_logic import process_data
from app_logic import fetch_data

class State(TypedDict):
input: str
result: str

"""Node containing arbitrary framework code."""
# Use any framework or library here
raw_data = fetch_data(state["input"])
processed = process_data(raw_data)
return {"result": processed}

# Define the graph structure
graph = StateGraph(State)
graph.add_node("process", my_app_node) # Add node with your logic
graph.add_edge(START, "process") # Connect start to your node
graph.add_edge("process", END) # Connect your node to end

# Compile for deployment
app = graph.compile()

1. **A LangGraph graph structure**: Define a graph using `StateGraph` with `addNode` and `addEdge`.
2. **Node functions with arbitrary logic**: Your node functions can call any framework or library.
3. **A compiled graph**: Compile the graph to create a deployable application.

import { StateGraph, START, END } from "@langchain/langgraph";
import { Annotation } from "@langchain/langgraph";

// Your existing application logic using any framework
import { processData } from "./app-logic";
import { fetchData } from "./app-logic";

const State = Annotation.Root({

async function myAppNode(state: typeof State.State) {
// Use any framework or library here
const rawData = await fetchData(state.input);
const processed = await processData(rawData);
return { result: processed };
}

// Define the graph structure
const graph = new StateGraph(State)
.addNode("process", myAppNode) // Add node with your logic
.addEdge(START, "process") // Connect start to your node
.addEdge("process", END); // Connect your node to end

// Compile for deployment
export const app = graph.compile();

In this example, the node functions (`my_app_node` for Python and `myAppNode` for JavaScript) can contain calls to any framework or library. The LangGraph structure simply provides the deployment interface and orchestration layer.

## ​ Environment variables

If you’re working with a deployed LangGraph application locally, you can configure environment variables in the `env` key of the configuration file.For a production deployment, you will typically want to configure the environment variables in the deployment environment.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Deploy your app to Cloud\\
\\
Previous How to set up an application with requirements.txt\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/graph-rebuild

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Configure app for deployment

Rebuild graph at runtime

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Prerequisites
- Define graphs
- No rebuild
- Rebuild

You might need to rebuild your graph with a different configuration for a new run. For example, you might need to use a different graph state or graph structure depending on the config. This guide shows how you can do this.

**Note**
In most cases, customizing behavior based on the config should be handled by a single graph where each node can read a config and change its behavior based on it

## ​ Prerequisites

Make sure to check out this how-to guide on setting up your app for deployment first.

## ​ Define graphs

Let’s say you have an app with a simple graph that calls an LLM and returns the response to the user. The app file directory looks like the following:

Copy

my-app/
|-- requirements.txt
|-- .env
|-- openai_agent.py # code for your graph

where the graph is defined in `openai_agent.py`.

### ​ No rebuild

In the standard LangGraph API configuration, the server uses the compiled graph instance that’s defined at the top level of `openai_agent.py`, which looks like the following:

from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, MessageGraph

model = ChatOpenAI(temperature=0)

graph_workflow = MessageGraph()

graph_workflow.add_node("agent", model)
graph_workflow.add_edge("agent", END)
graph_workflow.add_edge(START, "agent")

agent = graph_workflow.compile()

To make the server aware of your graph, you need to specify a path to the variable that contains the `CompiledStateGraph` instance in your LangGraph API configuration (`langgraph.json`), e.g.:

{
"dependencies": ["."],
"graphs": {
"openai_agent": "./openai_agent.py:agent",
},
"env": "./.env"
}

### ​ Rebuild

To make your graph rebuild on each new run with custom configuration, you need to rewrite `openai_agent.py` to instead provide a _function_ that takes a config and returns a graph (or compiled graph) instance. Let’s say we want to return our existing graph for user ID ‘1’, and a tool-calling agent for other users. We can modify `openai_agent.py` as follows:

from typing import Annotated
from typing_extensions import TypedDict
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, MessageGraph
from langgraph.graph.state import StateGraph
from langgraph.graph.message import add_messages
from langchain.tools import tool
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnableConfig

class State(TypedDict):
messages: Annotated[list[BaseMessage], add_messages]

def make_default_graph():
"""Make a simple LLM agent"""
graph_workflow = StateGraph(State)
def call_model(state):
return {"messages": [model.invoke(state["messages"])]}

graph_workflow.add_node("agent", call_model)
graph_workflow.add_edge("agent", END)
graph_workflow.add_edge(START, "agent")

agent = graph_workflow.compile()
return agent

def make_alternative_graph():
"""Make a tool-calling agent"""

@tool
def add(a: float, b: float):
"""Adds two numbers."""
return a + b

tool_node = ToolNode([add])
model_with_tools = model.bind_tools([add])
def call_model(state):
return {"messages": [model_with_tools.invoke(state["messages"])]}

def should_continue(state: State):
if state["messages"][-1].tool_calls:
return "tools"
else:
return END

graph_workflow = StateGraph(State)

graph_workflow.add_node("agent", call_model)
graph_workflow.add_node("tools", tool_node)
graph_workflow.add_edge("tools", "agent")
graph_workflow.add_edge(START, "agent")
graph_workflow.add_conditional_edges("agent", should_continue)

# this is the graph making function that will decide which graph to
# build based on the provided config
def make_graph(config: RunnableConfig):
user_id = config.get("configurable", {}).get("user_id")
# route to different graph state / structure based on the user ID
if user_id == "1":
return make_default_graph()
else:
return make_alternative_graph()

Finally, you need to specify the path to your graph-making function (`make_graph`) in `langgraph.json`:

{
"dependencies": ["."],
"graphs": {
"openai_agent": "./openai_agent.py:make_graph",
},
"env": "./.env"
}

See more info on LangGraph API configuration file here

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith control plane\\
\\
Previous How to interact with a deployment using RemoteGraph\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/use-remote-graph

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Configure app for deployment

How to interact with a deployment using RemoteGraph

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Prerequisites
- Initialize the graph
- Use a URL
- Use a client
- Invoke the graph
- Asynchronously
- Synchronously
- Persist state at the thread level
- Use as a subgraph

`RemoteGraph` is a client-side interface that allows you to interact with your deployment as if it were a local graph. It provides API parity with `CompiledGraph`, which means that you can use the same methods (`invoke()`, `stream()`, `get_state()`, etc.) in your development and production environments. This page describes how to initialize a `RemoteGraph` and interact with it.`RemoteGraph` is useful for the following:

- Separation of development and deployment: Build and test a graph locally with `CompiledGraph`, deploy it to LangSmith, and then use `RemoteGraph` to call it in production while working with the same API interface.
- Thread-level persistence: Persist and fetch the state of a conversation across calls with a thread ID.
- Subgraph embedding: Compose modular graphs for a multi-agent workflow by embedding a `RemoteGraph` as a subgraph within another graph.
- Reusable workflows: Use deployed graphs as nodes or tools, so that you can reuse and expose complex logic.

**Important: Avoid calling the same deployment**`RemoteGraph` is designed to call graphs on other deployments. Do not use `RemoteGraph` to call itself or another graph on the same deployment, as this can lead to deadlocks and resource exhaustion. Instead, use local graph composition or subgraphs for graphs within the same deployment.

## ​ Prerequisites

Before getting started with `RemoteGraph`, make sure you have:

- Access to LangSmith, where your graphs are developed and managed.
- A running Agent Server, which hosts your deployed graphs for remote interaction.

## ​ Initialize the graph

When initializing a `RemoteGraph`, you must always specify:

- `name`: The name of the graph you want to interact with **or** an assistant ID. If you specify a graph name, the default assistant will be used. If you specify an assistant ID, that specific assistant will be used. The graph name is the same name you use in the `langgraph.json` configuration file for your deployment.
- `api_key`: A valid LangSmith API key. You can set as an environment variable (`LANGSMITH_API_KEY`) or pass directly in the `api_key` argument. You can also provide the API key in the `client` / `sync_client` arguments, if `LangGraphClient` / `SyncLangGraphClient` was initialized with the `api_key` argument.

Additionally, you have to provide one of the following:

- `url`: The URL of the deployment you want to interact with. If you pass the `url` argument, both sync and async clients will be created using the provided URL, headers (if provided), and default configuration values (e.g., timeout).
- `client`: A `LangGraphClient` instance for interacting with the deployment asynchronously (e.g., using `.astream()`, `.ainvoke()`, `.aget_state()`, `.aupdate_state()`).
- `sync_client`: A `SyncLangGraphClient` instance for interacting with the deployment synchronously (e.g., using `.stream()`, `.invoke()`, `.get_state()`, `.update_state()`).

If you pass both `client` or `sync_client` as well as the `url` argument, they will take precedence over the `url` argument. If none of the `client` / `sync_client` / `url` arguments are provided, `RemoteGraph` will raise a `ValueError` at runtime.

### ​ Use a URL

Python

JavaScript

Copy

from langgraph.pregel.remote import RemoteGraph

# Using graph name (uses default assistant)
graph_name = "agent"
remote_graph = RemoteGraph(graph_name, url=url)

# Using assistant ID

remote_graph = RemoteGraph(assistant_id, url=url)

### ​ Use a client

from langgraph_sdk import get_client, get_sync_client
from langgraph.pregel.remote import RemoteGraph

client = get_client(url=url)
sync_client = get_sync_client(url=url)

graph_name = "agent"
remote_graph = RemoteGraph(graph_name, client=client, sync_client=sync_client)

remote_graph = RemoteGraph(assistant_id, client=client, sync_client=sync_client)

## ​ Invoke the graph

`RemoteGraph` implements the same Runnable interface as `CompiledGraph`, so you can use it in the same way as a compiled graph. It supports the full set of standard methods, including `.invoke()`, `.stream()`, `.get_state()`, and `.update_state()`, as well as their asynchronous variants.

### ​ Asynchronously

To use the graph asynchronously, you must provide either the `url` or `client` when initializing the `RemoteGraph`.

# invoke the graph
result = await remote_graph.ainvoke({
"messages": [{"role": "user", "content": "what's the weather in sf"}]
})

# stream outputs from the graph
async for chunk in remote_graph.astream({
"messages": [{"role": "user", "content": "what's the weather in la"}]
}):
print(chunk)

### ​ Synchronously

To use the graph synchronously, you must provide either the `url` or `sync_client` when initializing the `RemoteGraph`.

result = remote_graph.invoke({
"messages": [{"role": "user", "content": "what's the weather in sf"}]
})

for chunk in remote_graph.stream({
"messages": [{"role": "user", "content": "what's the weather in la"}]
}):
print(chunk)

## ​ Persist state at the thread level

By default, graph runs (for example, calls made with `.invoke()` or `.stream()`) are stateless, which means that intermediate checkpoints and the final state are not persisted after a run.If you want to preserve the outputs of a run—for example, to support human-in-the-loop workflows—you can create a thread and pass its ID through the `config` argument. This works the same way as with a regular compiled graph:

from langgraph_sdk import get_sync_client

graph_name = "agent"
sync_client = get_sync_client(url=url)
remote_graph = RemoteGraph(graph_name, url=url)

# create a thread (or use an existing thread instead)
thread = sync_client.threads.create()

# invoke the graph with the thread config
config = {"configurable": {"thread_id": thread["thread_id"]}}
result = remote_graph.invoke({
"messages": [{"role": "user", "content": "what's the weather in sf"}]
}, config=config)

# verify that the state was persisted to the thread
thread_state = remote_graph.get_state(config)
print(thread_state)

## ​ Use as a subgraph

If you need to use a `checkpointer` with a graph that has a `RemoteGraph` subgraph node, make sure to use UUIDs as thread IDs.

A graph can also call out to multiple `RemoteGraph` instances as _subgraph_ nodes. This allows for modular, scalable workflows where different responsibilities are split across separate graphs.`RemoteGraph` exposes the same interface as a regular `CompiledGraph`, so you can use it directly as a subgraph inside another graph. For example:

from langgraph_sdk import get_sync_client
from langgraph.graph import StateGraph, MessagesState, START
from typing import TypedDict

# define parent graph
builder = StateGraph(MessagesState)
# add remote graph directly as a node
builder.add_node("child", remote_graph)
builder.add_edge(START, "child")
graph = builder.compile()

# invoke the parent graph
result = graph.invoke({
"messages": [{"role": "user", "content": "what's the weather in sf"}]
})
print(result)

# stream outputs from both the parent graph and subgraph
for chunk in graph.stream({
"messages": [{"role": "user", "content": "what's the weather in sf"}]
}, subgraphs=True):
print(chunk)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Rebuild graph at runtime\\
\\
Previous How to add semantic search to your agent deployment\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/semantic-search

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Configure app for deployment

How to add semantic search to your agent deployment

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Prerequisites
- Steps
- Usage
- Custom Embeddings
- Querying via the API

This guide explains how to add semantic search to your deployment’s cross-thread store, so that your agent can search for memories and other documents by semantic similarity.

## ​ Prerequisites

- A deployment (refer to how to set up an application for deployment) and details on hosting options.
- API keys for your embedding provider (in this case, OpenAI).

## ​ Steps

1. Update your `langgraph.json` configuration file to include the store configuration:

Copy

{
...
"store": {
"index": {
"embed": "openai:text-embedding-3-small",
"dims": 1536,
"fields": ["$"]
}
}
}

This configuration:

- Uses OpenAI’s text-embedding-3-small model for generating embeddings
- Sets the embedding dimension to 1536 (matching the model’s output)
- Indexes all fields in your stored data (`["$"]` means index everything, or specify specific fields like `["text", "metadata.title"]`)

# In pyproject.toml
[project]
dependencies = [\

]

Or if using requirements.txt:

## ​ Usage

Once configured, you can use semantic search in your nodes. The store requires a namespace tuple to organize memories:

def search_memory(state: State, *, store: BaseStore):
# Search the store using semantic similarity
# The namespace tuple helps organize different types of memories
# e.g., ("user_facts", "preferences") or ("conversation", "summaries")
results = store.search(
namespace=("memory", "facts"), # Organize memories by type
query="your search query",
limit=3 # number of results to return
)
return results

## ​ Custom Embeddings

If you want to use custom embeddings, you can pass a path to a custom embedding function:

{
...
"store": {
"index": {
"embed": "path/to/embedding_function.py:embed",
"dims": 1536,
"fields": ["$"]
}
}
}

The deployment will look for the function in the specified path. The function must be async and accept a list of strings:

# path/to/embedding_function.py
from openai import AsyncOpenAI

client = AsyncOpenAI()

"""Custom embedding function that must:
1. Be async
2. Accept a list of strings
3. Return a list of float arrays (embeddings)
"""
response = await client.embeddings.create(
model="text-embedding-3-small",
input=texts
)
return [e.embedding for e in response.data]

## ​ Querying via the API

You can also query the store using the LangGraph SDK. Since the SDK uses async operations:

from langgraph_sdk import get_client

async def search_store():
client = get_client()
results = await client.store.search_items(
("memory", "facts"),
query="your search query",
limit=3 # number of results to return
)
return results

# Use in an async context
results = await search_store()

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to interact with a deployment using RemoteGraph\\
\\
Previous How to add TTLs to your application\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/configure-ttl

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Configure app for deployment

How to add TTLs to your application

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Configuring checkpoint TTL
- Configuring store item TTL
- Combining TTL configurations
- Configure per-thread TTL
- Runtime overrides
- Deployment process

**Prerequisites**
This guide assumes familiarity with LangSmith, Persistence, and Cross-thread persistence concepts.

LangSmith persists both checkpoints (thread state) and cross-thread memories (store items). Configure Time-to-Live (TTL) policies in `langgraph.json` to automatically manage the lifecycle of this data, preventing indefinite accumulation.

## ​ Configuring checkpoint TTL

Checkpoints capture the state of conversation threads. Setting a TTL ensures old checkpoints and threads are automatically deleted.Add a `checkpointer.ttl` configuration to your `langgraph.json` file:

Copy

{
"dependencies": ["."],
"graphs": {
"agent": "./agent.py:graph"
},
"checkpointer": {
"ttl": {
"strategy": "delete",
"sweep_interval_minutes": 60,
"default_ttl": 43200
}
}
}

- `strategy`: Specifies the action taken on expiration. Currently, only `"delete"` is supported, which deletes all checkpoints in the thread upon expiration.
- `sweep_interval_minutes`: Defines how often, in minutes, the system checks for expired checkpoints.
- `default_ttl`: Sets the default lifespan of threads (and corresponding checkpoints) in minutes (e.g., 43200 minutes = 30 days). Applies only to checkpoints created after this configuration is deployed; existing checkpoints/threads are not changed. To clear older data, delete it explicitly.

## ​ Configuring store item TTL

Store items allow cross-thread data persistence. Configuring TTL for store items helps manage memory by removing stale data.Add a `store.ttl` configuration to your `langgraph.json` file:

{
"dependencies": ["."],
"graphs": {
"agent": "./agent.py:graph"
},
"store": {
"ttl": {
"refresh_on_read": true,
"sweep_interval_minutes": 120,
"default_ttl": 10080
}
}
}

- `refresh_on_read`: (Optional, default `true`) If `true`, accessing an item via `get` or `search` resets its expiration timer. If `false`, TTL only refreshes on `put`.
- `sweep_interval_minutes`: (Optional) Defines how often, in minutes, the system checks for expired items. If omitted, no sweeping occurs.
- `default_ttl`: (Optional) Sets the default lifespan of store items in minutes (e.g., 10080 minutes = 7 days). Applies only to items created after this configuration is deployed; existing items are not changed. If you need to clear older items, delete them manually. If omitted, items do not expire by default.

## ​ Combining TTL configurations

You can configure TTLs for both checkpoints and store items in the same `langgraph.json` file to set different policies for each data type. Here is an example:

{
"dependencies": ["."],
"graphs": {
"agent": "./agent.py:graph"
},
"checkpointer": {
"ttl": {
"strategy": "delete",
"sweep_interval_minutes": 60,
"default_ttl": 43200
}
},
"store": {
"ttl": {
"refresh_on_read": true,
"sweep_interval_minutes": 120,
"default_ttl": 10080
}
}
}

## ​ Configure per-thread TTL

You can apply TTL configurations per-thread.

thread = await client.threads.create(
ttl={
"strategy": "delete",
"ttl": 43200 # 30 days in minutes
}
)

## ​ Runtime overrides

The default `store.ttl` settings from `langgraph.json` can be overridden at runtime by providing specific TTL values in SDK method calls like `get`, `put`, and `search`.

## ​ Deployment process

After configuring TTLs in `langgraph.json`, deploy or restart your LangGraph application for the changes to take effect. Use `langgraph dev` for local development or `langgraph up` for Docker deployment.See the langgraph.json CLI reference for more details on the other configurable options.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to add semantic search to your agent deployment\\
\\
Previous Configure LangSmith Agent Server for scale\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-server-scale

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Configure app for deployment

Configure LangSmith Agent Server for scale

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Scaling for write load
- Best practices for scaling the write path
- Change N\_JOBS\_PER\_WORKER based on assistant characteristics
- Avoid synchronous blocking operations
- Minimize redundant checkpointing
- Self-hosted
- Scaling for read load
- Best practices for scaling the read path
- Use filtering to reduce the number of resources returned per request
- Set a TTLs to automatically delete old data
- Avoid polling and use /join to monitor the state of a run
- Self-hosted
- Example self-hosted Agent Server configurations
- Low reads, low writes
- Low reads, high writes
- High reads, low writes
- Medium reads, medium writes
- High reads, high writes
- Autoscaling

The default configuration for LangSmith Agent Server is designed to handle substantial read and write load across a variety of different workloads. By following the best practices outlined below, you can tune your Agent Server to perform optimally for your specific workload. This page describes scaling considerations for the Agent Server and provides examples to help configure your deployment.For some example self-hosted configurations, refer to the Example Agent Server configurations for scale section.

## ​ Scaling for write load

Write load is primarily driven by the following factors:

- Creation of new runs
- Creation of new checkpoints during run execution
- Writing to long term memory
- Creation of new threads
- Creation of new assistants
- Deletion of runs, checkpoints, threads, assistants and cron jobs

The following components are primarily responsible for handling write load:

- API server: Handles initial request and persistence of data to the database.
- Queue worker: Handles the execution of runs.
- Redis: Handles the storage of ephemeral data about on-going runs.
- Postgres: Handles the storage of all data, including run, thread, assistant, cron job, checkpointing and long term memory.

#### ​ Change `N_JOBS_PER_WORKER` based on assistant characteristics

The default value of `N_JOBS_PER_WORKER` is 10. You can change this value to scale the maximum number of runs that can be executed at a time by a single queue worker based on the characteristics of your assistant.Some general guidelines for changing `N_JOBS_PER_WORKER`:

- If your assistant is CPU bounded, the default value of 10 is likely sufficient. You might lower `N_JOBS_PER_WORKER` if you notice excessive CPU usage on queue workers or delays in run execution.
- If your assistant is IO bounded, increase `N_JOBS_PER_WORKER` to handle more concurrent runs per worker.

There is no upper limit to `N_JOBS_PER_WORKER`. However, queue workers are greedy when fetching new runs, which means they will try to pick up as many runs as they have available jobs and begin executing them immediately. Setting `N_JOBS_PER_WORKER` too high in environments with bursty traffic can lead to uneven worker utilization and increased run execution times.

#### ​ Avoid synchronous blocking operations

Avoid synchronous blocking operations in your code and prefer asynchronous operations. Long synchronous operations can block the main event loop, causing longer request and run execution times and potential timeouts.For example, consider an application that needs to sleep for 1 second. Instead of using synchronous code like this:

Copy

import time

def my_function():
time.sleep(1)

Prefer asynchronous code like this:

import asyncio

async def my_function():
await asyncio.sleep(1)

If an assistant requires synchronous blocking operations, set `BG_JOB_ISOLATED_LOOPS` to `True` to execute each run in a separate event loop.

#### ​ Minimize redundant checkpointing

Minimize redundant checkpointing by setting `durability` to the minimum value necessary to ensure your data is durable.The default durability mode is `"async", meaning checkpoints are written after each step asynchronously. If an assistant needs to persist only the final state of the run,`durability`can be set to`”exit”\`, storing only the final state of the run. This can be set when creating the run:

from langgraph_sdk import get_client

thread = await client.threads.create()
run = await client.runs.create(
thread_id=thread["thread_id"],
assistant_id="agent",
durability="exit"
)

#### ​ Self-hosted

These settings are only required for self-hosted deployments. By default, cloud deployments already have these best practices enabled.

##### Enable the use of queue workers

By default, the API server manages the queue and does not use queue workers. You can enable the use of queue workers by setting the `queue.enabled` configuration to `true`.

queue:
enabled: true

This will allow the API server to offload the queue management to the queue workers, significantly reducing the load on the API server and allowing it to focus on handling requests.

##### Support a number of jobs equal to expected throughput

The more runs you execute in parallel, the more jobs you will need to handle the load. There are two main parameters to scale the available jobs:

- `number_of_queue_workers`: The number of queue workers provisioned.
- `N_JOBS_PER_WORKER`: The number of runs that a single queue work can execute at a time. Defaults to 10.

You can calculate the available jobs with the following equation:

available_jobs = number_of_queue_workers * `N_JOBS_PER_WORKER`

Throughput is then the number of runs that can be executed per second by the available jobs:

throughput_per_second = available_jobs / average_run_execution_time_seconds

Therefore, the minimum number of queue workers you should provision to support your expected steady state throughput is:

number_of_queue_workers = throughput_per_second * average_run_execution_time_seconds / `N_JOBS_PER_WORKER`

##### Configure autoscaling for bursty workloads

Autoscaling is disabled by default, but should be configured for bursty workloads. Using the same calculations as the previous section, you can determine the maximum number of queue workers you should allow the autoscaler to scale to based on maximum expected throughput.

## ​ Scaling for read load

Read load is primarily driven by the following factors:

- Getting the results of a run
- Getting the state of a thread
- Searching for runs, threads, cron jobs and assistants
- Retrieving checkpoints and long term memory

The following components are primarily responsible for handling read load:

- API server: Handles the request and direct retrieval of data from the database.
- Postgres: Handles the storage of all data, including run, thread, assistant, cron job, checkpointing and long term memory.
- Redis: Handles the storage of ephemeral data about on-going runs, including streaming messages from queue workers to api servers.

#### ​ Use filtering to reduce the number of resources returned per request

Agent Server provides a search API for each resource type. These APIs implement pagination by default and offer many filtering options. Use filtering to reduce the number of resources returned per request and improve performance.

#### ​ Set a TTLs to automatically delete old data

Set a TTL on threads to automatically clean up old data. Runs and checkpoints are automatically deleted when the associated thread is deleted.

#### ​ Avoid polling and use /join to monitor the state of a run

Avoid polling the state of a run by using the `/join` API endpoint. This method returns the final state of the run once the run is complete.If you need to monitor the output of a run in real-time, use the `/stream` API endpoint. This method streams the run output including the final state of the run.

#### ​ Self-hosted

##### Configure autoscaling for bursty workloads

Autoscaling is disabled by default, but should be configured for bursty workloads. You can determine the maximum number of api servers you should allow the autoscaler to scale to based on maximum expected throughput. The default for cloud deployments is a maximum of 10 API servers.

## ​ Example self-hosted Agent Server configurations

The exact optimal configuration depends on your application complexity, request patterns, and data requirements. Use the following examples in combination with the information in the previous sections and your specific usage to update your deployment configuration as needed. If you have any questions, contact support via support.langchain.com.

The following table provides an overview comparing different LangSmith Agent Server configurations for various load patterns (read requests per second / write requests per second) and standard assistant characteristics (average run execution time of 1 second, moderate CPU and memory usage):

| | **Low / low** | **Low / high** | **High / low** | Medium / medium | High / high |
| --- | --- | --- | --- | --- | --- |
| Write requests per second | 5 | 5 | 500 | 50 | 500 |
| Read requests per second | 5 | 500 | 5 | 50 | 500 |

| **`N_JOBS_PER_WORKER`** | 10 (default) | 50 | 10 | 10 | 50 |
| **Redis resources** | 2 Gi (default) | 2 Gi (default) | 2 Gi (default) | 2 Gi (default) | 2 Gi (default) |

The following sample configurations enable each of these setups. Load levels are defined as:

- Low means approximately 5 requests per second
- Medium means approximately 50 requests per second
- High means approximately 500 requests per second

### ​ Low reads, low writes

The default LangSmith Deployment configuration will handle this load. No custom resource configuration is needed here.

### ​ Low reads, high writes

You have a high volume of write requests (500 per second) being processed by your deployment, but relatively few read requests (5 per second).For this, we recommend a configuration like this:

# Example configuration for low reads, high writes (5 read/500 write requests per second)
api:
replicas: 6
resources:
requests:
cpu: "1"
memory: "2Gi"
limits:
cpu: "2"
memory: "4Gi"

queue:
replicas: 10
resources:
requests:
cpu: "1"
memory: "2Gi"
limits:
cpu: "2"
memory: "4Gi"

config:
numberOfJobsPerWorker: 50

redis:
resources:
requests:
memory: "2Gi"
limits:
memory: "2Gi"

postgres:
resources:
requests:
cpu: "4"
memory: "16Gi"
limits:
cpu: "8"
memory: "32Gi"

### ​ High reads, low writes

You have a high volume of read requests (500 per second) but relatively few write requests (5 per second).For this, we recommend a configuration like this:

# Example configuration for high reads, low writes (500 read/5 write requests per second)
api:
replicas: 10
resources:
requests:
cpu: "1"
memory: "2Gi"
limits:
cpu: "2"
memory: "4Gi"

queue:
replicas: 1 # Default, minimal write load
resources:
requests:
cpu: "1"
memory: "2Gi"
limits:
cpu: "2"
memory: "4Gi"

# Consider read replicas for high read scenarios
postgres:
resources:
requests:
cpu: "4"
memory: "16Gi"
limits:
cpu: "8"
memory: "32Gi"
readReplicas: 2

### ​ Medium reads, medium writes

This is a balanced configuration that should handle moderate read and write loads (50 read/50 write requests per second).For this, we recommend a configuration like this:

# Example configuration for medium reads, medium writes (50 read/50 write requests per second)
api:
replicas: 3
resources:
requests:
cpu: "1"
memory: "2Gi"
limits:
cpu: "2"
memory: "4Gi"

queue:
replicas: 5
resources:
requests:
cpu: "1"
memory: "2Gi"
limits:
cpu: "2"
memory: "4Gi"

### ​ High reads, high writes

You have high volumes of both read and write requests (500 read/500 write requests per second).For this, we recommend a configuration like this:

# Example configuration for high reads, high writes (500 read/500 write requests per second)
api:
replicas: 15
resources:
requests:
cpu: "1"
memory: "2Gi"
limits:
cpu: "2"
memory: "4Gi"

postgres:
resources:
requests:
cpu: "8"
memory: "32Gi"
limits:
cpu: "16"
memory: "64Gi"

### ​ Autoscaling

If your deployment experiences bursty traffic, you can enable autoscaling to scale the number of API servers and queue workers to handle the load.Here is a sample configuration for autoscaling for high reads and high writes:

api:
autoscaling:
enabled: true
minReplicas: 15
maxReplicas: 25

queue:
autoscaling:
enabled: true
minReplicas: 10
maxReplicas: 20

Ensure that your deployment environment has sufficient resources to scale to the recommended size. Monitor your applications and infrastructure to ensure optimal performance. Consider implementing monitoring and alerting to track resource usage and application performance.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to add TTLs to your application\\
\\
Previous Implement a CI/CD pipeline using LangSmith Deployment and Evaluation\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/cicd-pipeline-example

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Configure app for deployment

Implement a CI/CD pipeline using LangSmith Deployment and Evaluation

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Overview
- Pipeline architecture
- Trigger sources
- Testing layers
- GitHub Actions workflow
- Prerequisites
- Deployment options
- Prerequisites for manual deployment
- Local development and testing
- Method 1: LangSmith Deployment UI
- Method 2: Control Plane API
- Connect to Your Deployed Agent
- Environment configuration
- Database & cache configuration
- Troubleshooting
- Wrong API endpoints
- LangSmith API (Traces, Ingestion, etc.)
- LangSmith Deployment API (Deployments)

This guide demonstrates how to implement a comprehensive CI/CD pipeline for AI agent applications deployed in LangSmith Deployment. In this example, you’ll use the LangGraph open source framework for orchestrating and building the agent, LangSmith for observability and evaluations. This pipeline is based on the cicd-pipeline-example repository.

## ​ Overview

The CI/CD pipeline provides:

- **Automated testing**: Unit, integration, and end-to-end tests.
- **Offline evaluations**: Performance assessment using AgentEvals, OpenEvals and LangSmith.
- **Preview and production deployments**: Automated staging and quality-gated production releases using the Control Plane API.
- **Monitoring**: Continuous evaluation and alerting.

## ​ Pipeline architecture

The CI/CD pipeline consists of several key components that work together to ensure code quality and reliable deployments:

Manual Review

Testing

Code or Graph Change

Trigger CI Pipeline

Prompt Commit in PromptHub

Online Evaluation Alert

PR Opened

Run Unit Tests

Run Integration Tests

Run End to End Tests

Run Offline Evaluations

Evaluate with OpenEvals or AgentEvals

Assertions: Hard and Soft

Run LangGraph Dev Server Test

Push to Staging Deployment - Deploy to LangSmith as Development Type

Run Online Evaluations on Live Data

Attach Scores to Traces

If Quality Below Threshold

Send to Annotation Queue

Trigger Alert via Webhook

Push Trace to Golden Dataset

Promote to Production if All Pass - Deploy to LangSmith Production

Slack or PagerDuty Notification

Human Labeling

### ​ Trigger sources

There are multiple ways you can trigger this pipeline, either during development or if your application is already live. The pipeline can be triggered by:

- **Code changes**: Pushes to main/development branches where you can modify the LangGraph architecture, try different models, update agent logic, or make any code improvements.
- **PromptHub updates**: Changes to prompt templates stored in LangSmith PromptHub—whenever there’s a new prompt commit, the system triggers a webhook to run the pipeline.
- **Online evaluation alerts**: Performance degradation notifications from live deployments
- **LangSmith traces webhooks**: Automated triggers based on trace analysis and performance metrics.
- **Manual trigger**: Manual initiation of the pipeline for testing or emergency deployments.

### ​ Testing layers

Compared to traditional software, testing AI agent applications also requires assessing response quality, so it is important to test each part of the workflow. The pipeline implements multiple testing layers:

1. **Unit tests**: Individual node and utility function testing.
2. **Integration tests**: Component interaction testing.
3. **End-to-end tests**: Full graph execution testing.
4. **Offline evaluations**: Performance assessment with real-world scenarios including end-to-end evaluations, single-step evaluations, agent trajectory analysis, and multi-turn simulations.
5. **LangGraph dev server tests**: Use the langgraph-cli tool for spinning up (inside the GitHub Action) a local server to run the LangGraph agent. This polls the `/ok` server API endpoint until it is available and for 30 seconds, after that it throws an error.

## ​ GitHub Actions workflow

The CI/CD pipeline uses GitHub Actions with the Control Plane API and LangSmith API to automate deployment. A helper script manages API interactions and deployments: workflow includes:

- **New agent deployment**: When a new PR is opened and tests pass, a new preview deployment is created in LangSmith Deployment using the Control Plane API. This allows you to test the agent in a staging environment before promoting to production.
- **Agent deployment revision**: A revision happens when an existing deployment with the same ID is found, or when the PR is merged into main. In the case of merging to main, the preview deployment is deleted and a production deployment is created. This ensures that any updates to the agent are properly deployed and integrated into the production infrastructure.!Agent Deployment Revision Workflow
- **Testing and evaluation workflow**: In addition to the more traditional testing phases (unit tests, integration tests, end-to-end tests, etc.), the pipeline includes offline evaluations and Agent dev server testing because you want to test the quality of your agent. These evaluations provide comprehensive assessment of the agent’s performance using real-world scenarios and data.!Test with Results Workflow

Final Response Evaluation

Evaluates the final output of your agent against expected results. This is the most common type of evaluation that checks if the agent’s final response meets quality standards and answers the user’s question correctly.

Single Step Evaluation

Tests individual steps or nodes within your LangGraph workflow. This allows you to validate specific components of your agent’s logic in isolation, ensuring each step functions correctly before testing the full pipeline.

Agent Trajectory Evaluation

Analyzes the complete path your agent takes through the graph, including all intermediate steps and decision points. This helps identify bottlenecks, unnecessary steps, or suboptimal routing in your agent’s workflow. It also evaluates whether your agent invoked the right tools in the right order or at the right time.

Multi-Turn Evaluation

Tests conversational flows where the agent maintains context across multiple interactions. This is crucial for agents that handle follow-up questions, clarifications, or extended dialogues with users.

See the LangGraph testing documentation for specific testing approaches and the evaluation approaches guide for a comprehensive overview of offline evaluations.

### ​ Prerequisites

Before setting up the CI/CD pipeline, ensure you have:

- An AI agent application (in this case built using LangGraph)
- A LangSmith account
- A LangSmith API key needed to deploy agents and retrieve experiment results
- Project-specific environment variables configured in your repository secrets (e.g., LLM model API keys, vector store credentials, database connections)

While this example uses GitHub, the CI/CD pipeline works with other Git hosting platforms including GitLab, Bitbucket, and others.

## ​ Deployment options

LangSmith supports multiple deployment methods, depending on how your LangSmith instance is hosted:

- **Cloud LangSmith**: Direct GitHub integration.
- **Self-Hosted/Hybrid**: Container registry-based deployments.

The deployment flow starts by modifying your agent implementation. At minimum, you must have a `langgraph.json` and dependency file in your project (`requirements.txt` or `pyproject.toml`). Use the `langgraph dev` CLI tool to check for errors—fix any errors; otherwise, the deployment will succeed when deployed to LangSmith Deployment.

Self-Hosted/Hybrid LangSmith

Cloud LangSmith

Yes

No

UI

API

LangGraph SDK

RemoteGraph

REST API

LangGraph Studio UI

Agent Implementation

langgraph.json + dependencies

Test Locally with langgraph dev

Errors?

Fix Issues

Choose LangSmith Instance

Method 1: Connect GitHub Repo in UI

Method 2: Control Plane API with GitHub Repo

Deploy via LangSmith UI

Deploy via Control Plane API

Build Docker Image langgraph build

Push to Container Registry

Deploy via?

Specify Image URI in UI

Use Control Plane API

Agent Ready for Use

Connect via?

Use LangGraph SDK

Use RemoteGraph

Use REST API

Use LangGraph Studio UI

### ​ Prerequisites for manual deployment

Before deploying your agent, ensure you have:

1. **LangGraph graph**: Your agent implementation (e.g., `./agents/simple_text2sql.py:agent`).
2. **Dependencies**: Either `requirements.txt` or `pyproject.toml` with all required packages.
3. **Configuration**: `langgraph.json` file specifying:

- Path to your agent graph
- Dependencies location
- Environment variables
- Python version

Example `langgraph.json`:

Copy

{
"graphs": {
"simple_text2sql": "./agents/simple_text2sql.py:agent"
},
"env": ".env",
"python_version": "3.11",
"dependencies": ["."],
"image_distro": "wolfi"
}

### ​ Local development and testing

# Start local development server with Studio
langgraph dev

This will:

- Spin up a local server with Studio.
- Allow you to visualize and interact with your graph.
- Validate that your agent works correctly before deployment.

If your agent runs locally without any errors, it means that deployment to LangSmith will likely succeed. This local testing helps catch configuration issues, dependency problems, and agent logic errors before attempting deployment.

See the LangGraph CLI documentation for more details.

### ​ Method 1: LangSmith Deployment UI

Deploy your agent using the LangSmith deployment interface:

1. Go to your LangSmith dashboard.
2. Navigate to the **Deployments** section.
3. Click the **\+ New Deployment** button in the top right.
4. Select your GitHub repository containing your LangGraph agent from the dropdown menu.

**Supported deployments:**

- **Cloud LangSmith**: Direct GitHub integration with dropdown menu
- **Self-Hosted/Hybrid LangSmith**: Specify your image URI in the Image Path field (e.g., `docker.io/username/my-agent:latest`)

**Benefits:**

- Simple UI-based deployment
- Direct integration with your GitHub repository (cloud)
- No manual Docker image management required (cloud)

### ​ Method 2: Control Plane API

Deploy using the Control Plane API with different approaches for each deployment type:**For Cloud LangSmith:**

- Use the Control Plane API to create deployments by pointing to your GitHub repository
- No Docker image building required for cloud deployments

**For Self-Hosted/Hybrid LangSmith:**

# Build Docker image
langgraph build -t my-agent:latest

# Push to your container registry
docker push my-agent:latest

You can push to any container registry (Docker Hub, AWS ECR, Azure ACR, Google GCR, etc.) that your deployment environment has access to.**Supported deployments:**

- **Cloud LangSmith**: Use the Control Plane API to create deployments from your GitHub repository
- **Self-Hosted/Hybrid LangSmith**: Use the Control Plane API to create deployments from your container registry

See the LangGraph CLI build documentation for more details.

### ​ Connect to Your Deployed Agent

- **LangGraph SDK**: Use the LangGraph SDK for programmatic integration.
- **RemoteGraph**: Connect using RemoteGraph for remote graph connections (to use your graph in other graphs).
- **REST API**: Use HTTP-based interactions with your deployed agent.
- **Studio**: Access the visual interface for testing and debugging.

#### ​ Database & cache configuration

By default, LangSmith Deployment create PostgreSQL and Redis instances for you. To use external services, set the following environment variables in your new deployment or revision:

# Set environment variables for external services
export POSTGRES_URI_CUSTOM="postgresql://user:pass@host:5432/db"
export REDIS_URI_CUSTOM="redis://host:6379/0"

See the environment variables documentation for more details.

## ​ Troubleshooting

### ​ Wrong API endpoints

If you’re experiencing connection issues, verify you’re using the correct endpoint format for your LangSmith instance. There are two different APIs with different endpoints:

#### ​ LangSmith API (Traces, Ingestion, etc.)

For LangSmith API operations (traces, evaluations, datasets):

| Region | Endpoint |
| --- | --- |
| US | `https://api.smith.langchain.com` |
| EU | `https://eu.api.smith.langchain.com` |

#### ​ LangSmith Deployment API (Deployments)

For LangSmith Deployment operations (deployments, revisions):

| Region | Endpoint |
| --- | --- |
| US | `https://api.host.langchain.com` |
| EU | `https://eu.api.host.langchain.com` |

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Configure LangSmith Agent Server for scale\\
\\
Previous Deploy on Cloud\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deploy-to-cloud

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Deployment guides

Deploy on Cloud

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Prerequisites
- Create new deployment
- Create new revision
- View build and server logs
- View deployment metrics
- Interrupt revision
- Delete deployment
- Deployment settings
- Add or remove GitHub repositories
- Allowlist IP addresses

This is the comprehensive setup and management guide for deploying applications to LangSmith Cloud.

**If you’re looking for a quick setup**, try the quickstart guide first.

Before setting up, review the Cloud overview page to understand the Cloud hosting model.

## ​ Prerequisites

- Applications are deployed from GitHub repositories. Configure and upload an application to a GitHub repository.
- Verify that the LangGraph API runs locally. If the API does not run successfully (i.e., `langgraph dev`), deploying to LangSmith will fail as well.

**One-Time Setup Required**: A GitHub organization owner or admin must complete the OAuth flow in the LangSmith UI to authorize the `hosted-langserve` GitHub app. This only needs to be done once per workspace. After the initial OAuth authorization, all developers with deployment permissions can create and manage deployments without requiring GitHub admin access.

## ​ Create new deployment

Starting from the LangSmith UI, select **Deployments** in the left-hand navigation panel, **Deployments**. In the top-right corner, select **\+ New Deployment** to create a new deployment:

1. In the **Create New Deployment** panel, fill out the required fields. For **Deployment details**:

1. Select **Import from GitHub** and follow the GitHub OAuth workflow to install and authorize LangChain’s `hosted-langserve` GitHub app to access the selected repositories. After installation is complete, of the organization or account. This authorization only needs to be completed once per LangSmith workspace—subsequent deployments can be created by any user with deployment permissions.

2. Specify a name for the deployment.
3. Specify the desired **Git Branch**. A deployment is linked to a branch. When a new revision is created, code for the linked branch will be deployed. The branch can be updated later in the Deployment Settings.
4. Specify the full path to the LangGraph API config file including the file name. For example, if the file `langgraph.json` is in the root of the repository, specify `langgraph.json`.
5. Use the checkbox to **Automatically update deployment on push to branch**. If checked, the deployment will automatically be updated when changes are pushed to the specified **Git Branch**. You can enable or disable this setting on the Deployment Settings in the UI.
For **Deployment Type**:

- Development deployments are meant for non-production use cases and are provisioned with minimal resources.
- Production deployments can serve up to 500 requests/second and are provisioned with highly available storage with automatic backups.
6. Determine if the deployment should be **Shareable through Studio**.

1. If unchecked, the deployment will only be accessible with a valid LangSmith API key for the workspace.
2. If checked, the deployment will be accessible through Studio to any LangSmith user. A direct URL to Studio for the deployment will be provided to share with other LangSmith users.
7. Specify **Environment Variables** and secrets. To configure additional variables for the deployment, refer to the Environment Variables reference.

1. Sensitive values such as API keys (e.g., `OPENAI_API_KEY`) should be specified as secrets.
2. Additional non-secret environment variables can be specified as well.
8. A new LangSmith tracing project is automatically created with the same name as the deployment.
2. In the top-right corner, select **Submit**. After a few seconds, the **Deployment** view appears and the new deployment will be queued for provisioning.

## ​ Create new revision

When creating a new deployment, a new revision is created by default. You can create subsequent revisions to deploy new code changes.Starting from the LangSmith UI, select **Deployments** in the left-hand navigation panel. Select an existing deployment to create a new revision for.

1. In the **Deployment** view, in the top-right corner, select **\+ New Revision**.
2. In the **New Revision** modal, fill out the required fields.

1. Specify the full path to the API config file including the file name. For example, if the file `langgraph.json` is in the root of the repository, specify `langgraph.json`.
2. Determine if the deployment should be **Shareable through Studio**.

- If unchecked, the deployment will only be accessible with a valid LangSmith API key for the workspace.
- If checked, the deployment will be accessible through Studio to any LangSmith user. A direct URL to Studio for the deployment will be provided to share with other LangSmith users.
3. Specify **Environment Variables** and secrets. Existing secrets and environment variables are prepopulated. To configure additional variables for the revision, refer to the Environment Variables reference.

1. Add new secrets or environment variables.
2. Remove existing secrets or environment variables.
3. Update the value of existing secrets or environment variables.
3. Select **Submit\`**. After a few seconds, the **New Revision** modal will close and the new revision will be queued for deployment.

## ​ View build and server logs

Build and server logs are available for each revision.Starting from the **Deployments** view:

1. Select the desired revision from the **Revisions** table. A panel slides open from the right-hand side and the **Build** tab is selected by default, which displays build logs for the revision.
2. In the panel, select the **Server** tab to view server logs for the revision. Server logs are only available after a revision has been deployed.
3. Within the **Server** tab, adjust the date/time range picker as needed. By default, the date/time range picker is set to the **Last 7 days**.

## ​ View deployment metrics

Starting from the LangSmith UI:

1. In the left-hand navigation panel, select **Deployments**.
2. Select an existing deployment to monitor.
3. Select the **Monitoring** tab to view the deployment metrics. Refer to a list of all available metrics.
4. Within the **Monitoring** tab, use the date/time range picker as needed. By default, the date/time range picker is set to the **Last 15 minutes**.

## ​ Interrupt revision

Interrupting a revision will stop deployment of the revision.

**Undefined Behavior**
Interrupted revisions have undefined behavior. This is only useful if you need to deploy a new revision and you already have a revision “stuck” in progress. In the future, this feature may be removed.

Starting from the **Deployments** view:

1. Select the menu icon (three dots) on the right-hand side of the row for the desired revision from the **Revisions** table.
2. Select **Interrupt** from the menu.
3. A modal will appear. Review the confirmation message. Select **Interrupt revision**.

## ​ Delete deployment

1. In the left-hand navigation panel, select **Deployments**, which contains a list of existing deployments.
2. Select the menu icon (three dots) on the right-hand side of the row for the desired deployment and select **Delete**.
3. A **Confirmation** modal will appear. Select **Delete**.

## ​ Deployment settings

1. In the top-right corner, select the gear icon ( **Deployment Settings**).
2. Update the `Git Branch` to the desired branch.
3. Check/uncheck checkbox to **Automatically update deployment on push to branch**.

1. Branch creation/deletion and tag creation/deletion events will not trigger an update. Only pushes to an existing branch will trigger an update.
2. Pushes in quick succession to a branch will queue subsequent updates. Once a build completes, the most recent commit will begin building and the other queued builds will be skipped.

## ​ Add or remove GitHub repositories

After installing and authorizing LangChain’s `hosted-langserve` GitHub app, repository access for the app can be modified to add new repositories or remove existing repositories. If a new repository is created, it may need to be added explicitly.

2. Under **Repository access**, select **All repositories** or **Only select repositories**. If **Only select repositories** is selected, new repositories must be explicitly added.
3. Click **Save**.
4. When creating a new deployment, the list of GitHub repositories in the dropdown menu will be updated to reflect the repository access changes.

## ​ Allowlist IP addresses

All traffic from LangSmith deployments created after January 6th 2025 will come through a NAT gateway.
This NAT gateway will have several static ip addresses depending on the region you are deploying in. Refer to the table below for the list of IP addresses to allowlist:

| US | EU |
| --- | --- |
| 35.197.29.146 | 34.90.213.236 |
| 34.145.102.123 | 34.13.244.114 |
| 34.169.45.153 | 34.32.180.189 |
| 34.82.222.17 | 34.34.69.108 |
| 35.227.171.135 | 34.32.145.240 |
| 34.169.88.30 | 34.90.157.44 |
| 34.19.93.202 | 34.141.242.180 |
| 34.19.34.50 | 34.32.141.108 |
| 34.59.244.194 | |
| 34.9.99.224 | |
| 34.68.27.146 | |
| 34.41.178.137 | |
| 34.123.151.210 | |
| 34.135.61.140 | |
| 34.121.166.52 | |
| 34.31.121.70 | |

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Implement a CI/CD pipeline using LangSmith Deployment and Evaluation\\
\\
Previous Deploy with control plane\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deploy-with-control-plane

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Deployment guides

Deploy with control plane

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Overview
- Prerequisites
- Step 1. Test locally
- Step 2. Build Docker image
- Step 3. Push to container registry
- Step 4. Deploy with the control plane UI
- Update deployment
- Private registry authentication
- Next steps

This guide shows you how to deploy your applications to hybrid or self-hosted instances with a control plane. With a control plane, you build Docker images locally, push them to a registry that your Kubernetes cluster has access to, and deploy them with the LangSmith UI.

**This guide is for deploying applications, not setting up infrastructure.**Before using this guide, you must have already completed infrastructure setup:

- **Hybrid setup**: For hybrid hosting.
- **Enable LangSmith Deployment**: For self-hosted with control plane.

If you haven’t set up your infrastructure yet, start with the Platform setup section.

## ​ Overview

Applications deployed to hybrid or self-hosted LangSmith instances with control plane use Docker images. In this guide, the application deployment workflow is:

1. Test your application locally using `langgraph dev` or Studio.
2. Build a Docker image using the `langgraph build` command.
3. Push the image to a container registry accessible by your infrastructure.
4. Deploy from the control plane UI by specifying the image URL.

## ​ Prerequisites

Before completing this guide, you’ll need the following:

- Completed infrastructure setup to enable your data plane to receive application deployments:

- Hybrid setup: Installs data plane components (listener, operator, CRDs) in your Kubernetes cluster that connect to LangChain’s managed control plane.
- Enable LangSmith Deployment: Enables LangSmith Deployment on your self-hosted LangSmith instance.
- Access to the LangSmith UI with LangSmith Deployment enabled.
- A container registry accessible by your Kubernetes cluster. If using a private registry that requires authentication, you must configure image pull secrets as part of your infrastructure setup. Refer to Private registry authentication.

## ​ Step 1. Test locally

Before deploying, test your application locally. You can use the LangGraph CLI to run an Agent server in development mode:

Copy

langgraph dev

For a full guide local testing, refer to the Local server quickstart.

## ​ Step 2. Build Docker image

Build a Docker image of your application using the `langgraph build` command:

langgraph build -t my-image

Build command options include:

| Option | Default | Description |
| --- | --- | --- |
| `-t, --tag TEXT` | Required | Tag for the Docker image |
| `--platform TEXT` | | Target platform(s) to build for (e.g., `linux/amd64,linux/arm64`) |
| `--pull / --no-pull` | `--pull` | Build with latest remote Docker image |
| `-c, --config FILE` | `langgraph.json` | Path to configuration file |

Example with platform specification:

langgraph build --platform linux/amd64 -t my-image:v1.0.0

For full details, see the CLI reference.

## ​ Step 3. Push to container registry

Push your image to a container registry accessible by your Kubernetes cluster. The specific commands depend on your registry provider.

Tag your images with version information (e.g., `my-registry.com/my-app:v1.0.0`) to make rollbacks easier.

## ​ Step 4. Deploy with the control plane UI

The control plane UI allows you to create and manage deployments, view logs and metrics, and update configurations. To create a new deployment in the LangSmith UI:

1. In the left-hand navigation panel, select **Deployments**.
2. In the top-right corner, select **\+ New Deployment**.
3. In the deployment configuration panel, provide:
- **Image URL**: The full image URL you pushed in Step 3.
- **Listener/Compute ID**: Select the listener configured for your infrastructure.
- **Namespace**: The Kubernetes namespace to deploy to.
- **Environment variables**: Any required configuration (API keys, etc.).
- Other deployment settings as needed.
4. Select **Submit**.

The control plane will coordinate with your data plane listener to deploy your application.After creating a deployment, the infrastructure is provisioned asynchronously. Deployment can take up to several minutes, with initial deployments taking longer due to database creation.From the control plane UI, you can view build logs, server logs, and deployment metrics including CPU/memory usage, replicas, and API performance. For more details, refer to the control plane monitoring documentation.

A LangSmith Observability tracing project is automatically created for each deployment with the same name as the deployment. Tracing environment variables are set automatically by the control plane.

## ​ Update deployment

To deploy a new version of your application, create a new revision:Starting from the LangSmith UI:

1. In the left-hand navigation panel, select **Deployments**.
2. Select an existing deployment.
3. In the Deployment view, select **\+ New Revision** in the top-right corner.
4. Update the configuration:
- Update the **Image URL** to your new image version.
- Update environment variables if needed.
- Adjust other settings as needed.
5. Select **Submit**.

## ​ Private registry authentication

If your container registry requires authentication (e.g., AWS ECR, Azure ACR, GCP Artifact Registry, private Docker registry), you must configure Kubernetes image pull secrets before deploying applications. This is a one-time infrastructure configuration.

**This configuration is done at the infrastructure level, not per-deployment.** Once configured, all deployments automatically inherit the registry credentials.

The configuration steps depend on your deployment type:

- **Self-hosted with control plane**: Configure `imagePullSecrets` in your LangSmith Helm chart’s `values.yaml` file. See the detailed steps in the Enable LangSmith Deployment guide.
- **Hybrid**: Configure `imagePullSecrets` in your `langgraph-dataplane-values.yaml` file using the same format.

For detailed steps on creating image pull secrets for different registry providers, refer to the Kubernetes documentation on pulling images from private registries.

## ​ Next steps

- **Control plane**: Learn more about control plane features.
- **Data plane**: Understand data plane architecture.
- **Observability**: Monitor your deployments with automatic tracing.
- **Studio**: Test and debug deployed applications.
- **LangGraph CLI**: Full CLI reference documentation.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Deploy on Cloud\\
\\
Previous Self-host standalone servers\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deploy-standalone-server

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Deployment guides

Self-host standalone servers

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Prerequisites
- Kubernetes
- Docker
- Docker Compose

This guide shows you how to deploy **standalone Agent Servers** without the LangSmith UI or control plane. This is the most lightweight self-hosting option for running one or a few agents as independent services.

This deployment option provides flexibility but requires you to manage your own infrastructure and configuration.For production workloads, consider self-hosting the full LangSmith platform or deploying with the control plane, which offer standardized deployment patterns and UI-based management.

**This is the setup page for deploying Agent Servers directly without the LangSmith platform.**Review the self-hosted options to understand:

- Standalone Server: What this guide covers (no UI, just servers).
- LangSmith: For the full LangSmith platform with UI.
- LangSmith Deployment: For UI-based deployment management.

Before continuing, review the standalone server overview.

## ​ Prerequisites

1. Use the LangGraph CLI to test your application locally.
2. Use the LangGraph CLI to build a Docker image (i.e. `langgraph build`).
3. The following environment variables are needed for a data plane deployment.
4. `REDIS_URI`: Connection details to a Redis instance. Redis will be used as a pub-sub broker to enable streaming real time output from background runs. The value of `REDIS_URI` must be a valid Redis connection URI.

**Shared Redis Instance**

5. `DATABASE_URI`: Postgres connection details. Postgres will be used to store assistants, threads, runs, persist thread state and long term memory, and to manage the state of the background task queue with ‘exactly once’ semantics. The value of `DATABASE_URI` must be a valid Postgres connection URI.

**Shared Postgres Instance**

6. `LANGSMITH_API_KEY`: LangSmith API key.
7. `LANGGRAPH_CLOUD_LICENSE_KEY`: LangSmith license key. This will be used to authenticate ONCE at server start up.
8. `LANGSMITH_ENDPOINT`: To send traces to a self-hosted LangSmith instance, set `LANGSMITH_ENDPOINT` to the hostname of the self-hosted LangSmith instance.
9. Egress to `https://beacon.langchain.com` from your network. This is required for license verification and usage reporting if not running in air-gapped mode. See the Egress documentation for more details.

## ​ Kubernetes

Use this Helm chart to deploy an Agent Server to a Kubernetes cluster.

## ​ Docker

Run the following `docker` command:

Copy

docker run \
--env-file .env \
-p 8123:8000 \
-e REDIS_URI="foo" \
-e DATABASE_URI="bar" \
-e LANGSMITH_API_KEY="baz" \
my-image

- You need to replace `my-image` with the name of the image you built in the prerequisite steps (from `langgraph build`)

and you should provide appropriate values for `REDIS_URI`, `DATABASE_URI`, and `LANGSMITH_API_KEY`.

- If your application requires additional environment variables, you can pass them in a similar way.

## ​ Docker Compose

Docker Compose YAML file:

volumes:
langgraph-data:
driver: local
services:
langgraph-redis:
image: redis:6
healthcheck:
test: redis-cli ping
interval: 5s
timeout: 1s
retries: 5
langgraph-postgres:
image: postgres:16
ports:
- "5432:5432"
environment:
POSTGRES_DB: postgres
POSTGRES_USER: postgres
POSTGRES_PASSWORD: postgres
volumes:
- langgraph-data:/var/lib/postgresql/data
healthcheck:
test: pg_isready -U postgres
start_period: 10s
timeout: 1s
retries: 5
interval: 5s
langgraph-api:
image: ${IMAGE_NAME}
ports:
- "8123:8000"
depends_on:
langgraph-redis:
condition: service_healthy
langgraph-postgres:
condition: service_healthy
env_file:
- .env
environment:
REDIS_URI: redis://langgraph-redis:6379
LANGSMITH_API_KEY: ${LANGSMITH_API_KEY}
DATABASE_URI: postgres://postgres:postgres@langgraph-postgres:5432/postgres?sslmode=disable

You can run the command `docker compose up` with this Docker Compose file in the same folder.This will launch an Agent Server on port `8123` (if you want to change this, you can change this by changing the ports in the `langgraph-api` volume). You can test if the application is healthy by running:

curl --request GET --url 0.0.0.0:8123/ok

Assuming everything is running correctly, you should see a response like:

{"ok":true}

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Deploy with control plane\\
\\
Previous Assistants\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/studio

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Studio

LangSmith Studio

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Features
- Graph mode
- Chat mode
- Learn more
- Video guide

**Prerequisites**

- LangSmith
- Agent Server
- LangGraph CLI

Studio is a specialized agent IDE that enables visualization, interaction, and debugging of agentic systems that implement the Agent Server API protocol. Studio also integrates with tracing, evaluation, and prompt engineering.

## ​ Features

Key features of Studio:

- Visualize your graph architecture
- Run and interact with your agent
- Manage assistants
- Manage threads
- Iterate on prompts
- Run experiments over a dataset
- Manage long term memory
- Debug agent state via time travel

LangSmith Deployment

creates

LangGraph CLI

Agent Server deployment

SDKs

RemoteGraph

Studio works for graphs that are deployed on LangSmith or for graphs that are running locally via the Agent Server.Studio supports two modes:

### ​ Graph mode

Graph mode exposes the full feature-set and is useful when you would like as many details about the execution of your agent, including the nodes traversed, intermediate states, and LangSmith integrations (such as adding to datasets and playground).

### ​ Chat mode

Chat mode is a simpler UI for iterating on and testing chat-specific agents. It is useful for business users and those who want to test overall agent behavior. Chat mode is only supported for graph’s whose state includes or extends `MessagesState`.

## ​ Learn more

- See this guide on how to get started with Studio.

## ​ Video guide

LangSmith Studio v2: The Ultimate Agent Development Environment - YouTube

Photo image of LangChain

LangChain

165K subscribers

LangSmith Studio v2: The Ultimate Agent Development Environment

Search

Watch later

Share

Copy link

Info

Shopping

Tap to unmute

If playback doesn't begin shortly, try restarting your device.

More videos

## More videos

You're signed out

Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.

CancelConfirm

Include playlist

An error occurred while retrieving sharing information. Please try again later.

Watch on

0:00

0:00 / 8:09

•Live

•

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to implement generative user interfaces with LangGraph\\
\\
Previous Get started with Studio\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/use-studio

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Studio

How to use Studio

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Run application
- Specify input
- Run settings
- Assistant
- Streaming
- Breakpoints
- Submit run
- Manage assistants
- Manage threads
- View threads
- Edit thread history
- Next steps

This page describes the core workflows you’ll use in Studio. It explains how to run your application, manage assistant configurations, and work with conversation threads. Each section includes steps in both graph mode (full-featured view of your graph’s execution) and chat mode (lightweight conversational interface):

- Run application: Execute your application or agent and observe its behavior.
- Manage assistants: Create, edit, and select the assistant configuration used by your application.
- Manage threads: View and organize the threads, including forking or editing past runs for debugging.

## ​ Run application

- Graph

- Chat

### ​ Specify input

1. Define the input to your graph in the **Input** section on the left side of the page, below the graph interface. Studio will attempt to render a form for your input based on the graph’s defined state schema. To disable this, click the **View Raw** button, which will present you with a JSON editor.
2. Click the up or down arrows at the top of the **Input** section to toggle through and use previously submitted inputs.

#### ​ Assistant

To specify the assistant that is used for the run:

1. Click the **Settings** button in the bottom left corner. If an assistant is currently selected the button will also list the assistant name. If no assistant is selected it will say **Manage Assistants**.
2. Select the assistant to run.
3. Click the **Active** toggle at the top of the modal to activate it.

For more information, refer to Manage assistants.

#### ​ Streaming

Click the dropdown next to **Submit** and click the toggle to enable or disable streaming.

#### ​ Breakpoints

To run your graph with breakpoints:

1. Click **Interrupt**.
2. Select a node and whether to pause before or after that node has executed.
3. Click **Continue** in the thread log to resume execution.

For more information on breakpoints, refer to Human-in-the-loop.

### ​ Submit run

To submit the run with the specified input and run settings:

1. Click the **Submit** button. This will add a run to the existing selected thread. If no thread is currently selected, a new one will be created.
2. To cancel the ongoing run, click the **Cancel** button.

Specify the input to your chat application in the bottom of the conversation panel.

1. Click the **Send message** button to submit the input as a Human message and have the response streamed back.

To cancel the ongoing run:

1. Click **Cancel**.
2. Click the **Show tool calls** toggle to hide or show tool calls in the conversation.

## ​ Manage assistants

Studio lets you view, edit, and update your assistants, and allows you to run your graph using these assistant configurations.For more conceptual details, refer to the Assistants overview.

To view your assistants:

1. Click **Manage Assistants** in the bottom left corner. This opens a modal for you to view all the assistants for the selected graph.
2. Specify the assistant and its version you would like to mark as **Active**. LangSmith will use this assistant when runs are submitted.

The **Default configuration** option will be active, which reflects the default configuration defined in your graph. Edits made to this configuration will be used to update the run-time configuration, but will not update or create a new assistant unless you click **Create new assistant**.

Chat mode enables you to switch through the different assistants in your graph via the dropdown selector at the top of the page. To create, edit, or delete assistants, use Graph mode.

## ​ Manage threads

Studio provides tools to view all threads saved on the server and edit their state. You can create new threads, switch between threads, and modify past states both in graph mode and chat mode.

### ​ View threads

1. In the top of the right-hand pane, select the dropdown menu to view existing threads.
2. Select the desired thread, and the thread history will populate in the right-hand side of the page.
3. To create a new thread, click **\+ New Thread** and submit a run.
4. To view more granular information in the thread, drag the slider at the top of the page to the right. To view less information, drag the slider to the left. Additionally, collapse or expand individual turns, nodes, and keys of the state.
5. Switch between `Pretty` and `JSON` mode for different rendering formats.

### ​ Edit thread history

To edit the state of the thread:

1. Select **Edit node state** next to the desired node.
2. Edit the node’s output as desired and click **Fork** to confirm. This will create a new forked run from the checkpoint of the selected node.

If you instead want to re-run the thread from a given checkpoint without editing the state, click **Re-run from here**. This will again create a new forked run from the selected checkpoint. This is useful for re-running with changes that are not specific to the state, such as the selected assistant.

1. View all threads in the right-hand pane of the page.
2. Select the desired thread and the thread history will populate in the center panel.
3. To create a new thread, click **+** and submit a run.

To edit a human message in the thread:

1. Click **Edit node state** below the human message.
2. Edit the message as desired and submit. This will create a new fork of the conversation history.
3. To re-generate an AI message, click the retry icon below the AI message.

## ​ Next steps

Refer to the following guides for more detail on tasks you can complete in Studio:

- Iterate on prompts
- Run experiments over datasets

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Get started with Studio\\
\\
Previous Observability in Studio\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability-studio

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Studio

Observability in Studio

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Iterate on prompts
- Direct node editing
- Graph configuration
- langgraph\_nodes
- langgraph\_type
- Editing prompts in the UI
- Playground
- Run experiments over a dataset
- Prerequisites
- Experiment setup
- Debug LangSmith traces
- Open deployed threads
- Testing local agents with remote traces
- Prerequisites
- Clone thread
- Add node to dataset

LangSmith Studio provides tools to inspect, debug, and improve your app beyond execution. By working with traces, datasets, and prompts, you can see how your application behaves in detail, measure its performance, and refine its outputs:

- Iterate on prompts: Modify prompts inside graph nodes directly or with the LangSmith playground.
- Run experiments over a dataset: Execute your assistant over a LangSmith dataset to score and compare results.
- Debug LangSmith traces: Import traced runs into Studio and optionally clone them into your local agent.
- Add a node to a dataset: Turn parts of thread history into dataset examples for evaluation or further analysis.

## ​ Iterate on prompts

Studio supports the following methods for modifying prompts in your graph:

- Direct node editing
- Playground interface

### ​ Direct node editing

Studio allows you to edit prompts used inside individual nodes, directly from the graph interface.

### ​ Graph configuration

Define your configuration to specify prompt fields and their associated nodes using `langgraph_nodes` and `langgraph_type` keys.

#### ​ `langgraph_nodes`

- **Description**: Specifies which nodes of the graph a configuration field is associated with.
- **Value Type**: Array of strings, where each string is the name of a node in your graph.
- **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata["json_schema_extra"]` dictionary for dataclasses.
- **Example**:

Copy

system_prompt: str = Field(
default="You are a helpful AI assistant.",
json_schema_extra={"langgraph_nodes": ["call_model", "other_node"]},
)

#### ​ `langgraph_type`

- **Description**: Specifies the type of configuration field, which determines how it’s handled in the UI.
- **Value Type**: String
- **Supported Values**:

- `"prompt"`: Indicates the field contains prompt text that should be treated specially in the UI.
- **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata["json_schema_extra"]` dictionary for dataclasses.
- **Example**:

system_prompt: str = Field(
default="You are a helpful AI assistant.",
json_schema_extra={
"langgraph_nodes": ["call_model"],
"langgraph_type": "prompt",
},
)

Full example configuration

## Using Pydantic
from pydantic import BaseModel, Field
from typing import Annotated, Literal

class Configuration(BaseModel):
"""The configuration for the agent."""

system_prompt: str = Field(
default="You are a helpful AI assistant.",
description="The system prompt to use for the agent's interactions. "
"This prompt sets the context and behavior for the agent.",
json_schema_extra={
"langgraph_nodes": ["call_model"],
"langgraph_type": "prompt",
},
)

model: Annotated[\
Literal[\
"anthropic/claude-sonnet-4-5-20250929",\
"anthropic/claude-haiku-4-5-20251001",\
"openai/o1",\
"openai/gpt-4o-mini",\
"openai/o1-mini",\
"openai/o3-mini",\
],\
{"__template_metadata__": {"kind": "llm"}},\
] = Field(
default="openai/gpt-4o-mini",
description="The name of the language model to use for the agent's main interactions. "
"Should be in the form: provider/model-name.",
json_schema_extra={"langgraph_nodes": ["call_model"]},
)

## Using Dataclasses
from dataclasses import dataclass, field

@dataclass(kw_only=True)
class Configuration:
"""The configuration for the agent."""

system_prompt: str = field(
default="You are a helpful AI assistant.",
metadata={
"description": "The system prompt to use for the agent's interactions. "
"This prompt sets the context and behavior for the agent.",
"json_schema_extra": {"langgraph_nodes": ["call_model"]},
},
)

model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = field(
default="anthropic/claude-3-5-sonnet-20240620",
metadata={
"description": "The name of the language model to use for the agent's main interactions. "
"Should be in the form: provider/model-name.",
"json_schema_extra": {"langgraph_nodes": ["call_model"]},
},
)

#### ​ Editing prompts in the UI

1. Locate the gear icon on nodes with associated configuration fields.
2. Click to open the configuration modal.
3. Edit the values.
4. Save to update the current assistant version or create a new one.

### ​ Playground

The playground interface allows testing individual LLM calls without running the full graph:

1. Select a thread.
2. Click **View LLM Runs** on a node. This lists all the LLM calls (if any) made inside the node.
3. Select an LLM run to open in the playground.
4. Modify prompts and test different model and tool settings.
5. Copy updated prompts Run experiments over a dataset

Studio lets you run evaluations by executing your assistant against a predefined LangSmith dataset. This allows you to test performance across a variety of inputs, compare outputs to reference answers, and score results with configured evaluators.This guide shows you how to run a full end-to-end experiment directly from Studio.

### ​ Prerequisites

Before running an experiment, ensure you have the following:

- **A LangSmith dataset**: Your dataset should contain the inputs you want to test and optionally, reference outputs for comparison. The schema for the inputs must match the required input schema for the assistant. For more information on schemas, see here. For more on creating datasets, refer to How to Manage Datasets.
- **(Optional) Evaluators**: You can attach evaluators (e.g., LLM-as-a-Judge, heuristics, or custom functions) to your dataset in LangSmith. These will run automatically after the graph has processed all inputs.
- **A running application**: The experiment can be run against:

- An application deployed on LangSmith.
- A locally running application started via the langgraph-cli.

Studio experiments follow the same data retention rules as other experiments. By default, traces have base tier retention (14 days). However, traces will automatically upgrade to extended tier retention (400 days) if feedback is added to them. Feedback can be added in one of two ways:

- The dataset has evaluators configured.
- Feedback is manually added to a trace.

This auto-upgrade increases both the retention period and the cost of the trace. For more details, refer to Data retention auto-upgrades.

### ​ Experiment setup

1. Launch the experiment. Click the **Run experiment** button in the top right corner of the Studio page.
2. Select your dataset. In the modal that appears, select the dataset (or a specific dataset split) to use for the experiment and click **Start**.
3. Monitor the progress. All of the inputs in the dataset will now be run against the active assistant. Monitor the experiment’s progress via the badge in the top right corner.
4. You can continue to work in Studio while the experiment runs in the background. Click the arrow icon button at any time to navigate to LangSmith and view the detailed experiment results.

## ​ Debug LangSmith traces

This guide explains how to open LangSmith traces in Studio for interactive investigation and debugging.

### ​ Open deployed threads

1. Open the LangSmith trace, selecting the root run.
2. Click **Run in Studio**.

This will open Studio connected to the associated deployment with the trace’s parent thread selected.

### ​ Testing local agents with remote traces

This section explains how to test a local agent against remote traces from LangSmith. This enables you to use production traces as input for local testing, allowing you to debug and verify agent modifications in your development environment.

#### ​ Prerequisites

- A LangSmith traced thread
- A locally running agent.

**Local agent requirements**

- Contains the same set of nodes present in the remote trace

#### ​ Clone thread

1. Open the LangSmith trace, selecting the root run.
2. Click the dropdown next to **Run in Studio**.
3. Enter your local agent’s URL.
4. Select **Clone thread locally**.
5. If multiple graphs exist, select the target graph.

A new thread will be created in your local agent with the thread history inferred and copied from the remote thread, and you will be navigated to Studio for your locally running application.

## ​ Add node to dataset

Add examples to LangSmith datasets from nodes in the thread log. This is useful to evaluate individual steps of the agent.

1. Select a thread.
2. Click **Add to Dataset**.
3. Select nodes whose input/output you want to add to a dataset.
4. For each selected node, select the target dataset to create the example in. By default a dataset for the specific assistant and node will be selected. If this dataset does not yet exist, it will be created.
5. Edit the example’s input/output as needed before adding it to the dataset.
6. Select **Add to dataset** at the bottom of the page to add all selected nodes to their respective datasets.

For more details, refer to How to evaluate an application’s intermediate steps.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to use Studio\\
\\
Previous Studio troubleshooting\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/troubleshooting-studio

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Studio

Studio troubleshooting

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Safari Connection Issues
- Solution 1: Use Cloudflare Tunnel
- Solution 2: Use Chromium browser
- Chrome connection issues
- Symptoms
- Solution: Allow local network access in Chrome
- Additional troubleshooting
- Brave Connection Issues
- Solution 1: Disable Brave Shields
- Solution 2: Use Cloudflare Tunnel
- Graph Edge Issues
- Solution 1: Path map
- Solution 2: Router type definition
- Experiment troubleshooting in Studio
- Run experiment button is disabled
- Evaluator results are missing

## ​ Safari Connection Issues

Safari blocks plain-HTTP traffic on localhost. When running Studio with `langgraph dev`, you may see “Failed to load assistants” errors.

### ​ Solution 1: Use Cloudflare Tunnel

- Python

- JS

Copy

langgraph dev --tunnel

npx @langchain/langgraph-cli dev --tunnel

The command outputs a URL in this format:

Use this URL in Safari to load Studio. Here, the `baseUrl` parameter specifies your agent server endpoint.

### ​ Solution 2: Use Chromium browser

Chrome and other Chromium browsers allow HTTP on localhost. Use `langgraph dev` without additional configuration.

## ​ Chrome connection issues

Starting with Chrome version 142, you may experience “Failed to initialize Studio” errors with “TypeError: Failed to fetch” when trying to connect LangSmith Studio to your local development server via `langgraph dev`. This occurs even when the API server at `http://127.0.0.1:2024/docs` loads successfully.**Root Cause:** Chrome 142 fully enforces the Private Network Access (PNA) specification with no fallback, which blocks HTTPS sites (like `https://smith.langchain.com`) from accessing HTTP localhost servers by default.

### ​ Symptoms

- Running `langgraph dev` starts the server successfully.
- Navigating to `http://127.0.0.1:2024/docs` shows the API documentation correctly.
- LangSmith Studio at `https://smith.langchain.com` shows: “Failed to initialize Studio - Please verify if the API server is running or accessible from the browser. TypeError: Failed to fetch”.
- Browser console shows errors like: `Permission was denied for this request to access the 'unknown' address space`.

### ​ Solution: Allow local network access in Chrome

1. Open LangSmith Studio at `https://smith.langchain.com` in Chrome.
2. Click the **lock icon** (or site information icon) to the left of the address bar.
3. Look for the **“Local network access”** option in the dropdown.
4. Change the setting from **“Ask (default)”** or **“Block”** to **“Allow”**.
5. Reload the page.

Studio should now connect to your local development server successfully.

### ​ Additional troubleshooting

**Check for browser extension conflicts**Browser extensions (especially Ollama Chrome extension or AI model extensions) can interfere with localhost connections:

1. Disable all browser extensions temporarily.
2. Restart Chrome.
3. Try connecting to Studio again.
4. If it works, re-enable extensions one by one to identify the culprit.

**Verify dependencies are up to date**

pip install -U "langgraph-cli[inmem]"

**Clear browser cache and site data**

2. Find `https://smith.langchain.com` in the list.
3. Click **Clear data**.
4. Restart Chrome and try again.

## ​ Brave Connection Issues

Brave blocks plain-HTTP traffic on localhost when Brave Shields are enabled. When running Studio with `langgraph dev`, you may see “Failed to load assistants” errors.

### ​ Solution 1: Disable Brave Shields

Disable Brave Shields for LangSmith using the Brave icon in the URL bar.

### ​ Solution 2: Use Cloudflare Tunnel

Use this URL in Brave to load Studio. Here, the `baseUrl` parameter specifies your agent server endpoint.

## ​ Graph Edge Issues

Undefined conditional edges may show unexpected connections in your graph. This is
because without proper definition, Studio assumes the conditional edge could access all other nodes. To address this, explicitly define the routing paths using one of these methods:

### ​ Solution 1: Path map

Define a mapping between router outputs and target nodes:

- Javascript

graph.add_conditional_edges("node_a", routing_function, {True: "node_b", False: "node_c"})

graph.addConditionalEdges("node_a", routingFunction, { true: "node_b", false: "node_c" });

### ​ Solution 2: Router type definition

Specify possible routing destinations using Python’s `Literal` type:

if state['some_condition'] == True:
return "node_b"
else:
return "node_c"

## ​ Experiment troubleshooting in Studio

### ​ **Run experiment** button is disabled

Check the following:

- **Deployed application**: If your application is deployed on LangSmith, you may need to create a new revision to enable this feature.
- **Local development server**: If you are running your application locally, make sure you have upgraded to the latest version of the `langgraph-cli` (`pip install -U langgraph-cli`). Additionally, ensure you have tracing enabled by setting the `LANGSMITH_API_KEY` in your project’s `.env` file.

### ​ Evaluator results are missing

When you run an experiment, any attached evaluators are scheduled for execution in a queue. If you don’t see results immediately, it likely means they are still pending.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Observability in Studio\\
\\
Previous Authentication & access control\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/auth

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Auth & access control

Authentication & access control

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Core Concepts
- Authentication vs authorization
- Default security models
- LangSmith
- Self-hosted
- System architecture
- Authentication
- Agent authentication
- Agent authentication with MCP
- Authorization
- Resource-specific handlers
- Filter operations
- Common access patterns
- Single-owner resources
- Permission-based access
- Supported resources
- Supported actions and types
- Next steps

LangSmith provides a flexible authentication and authorization system that can integrate with most authentication schemes.

## ​ Core Concepts

### ​ Authentication vs authorization

While often used interchangeably, these terms represent distinct security concepts:

- **Authentication** (“AuthN”) verifies _who_ you are. This runs as middleware for every request.
- **Authorization** (“AuthZ”) determines _what you can do_. This validates the user’s privileges and roles on a per-resource basis.

In LangSmith, authentication is handled by your `@auth.authenticate` handler, and authorization is handled by your `@auth.on` handlers.

## ​ Default security models

LangSmith provides different security defaults:

### ​ LangSmith

- Uses LangSmith API keys by default
- Requires valid API key in `x-api-key` header
- Can be customized with your auth handler

**Custom auth**
Custom auth **is supported** for all plans in LangSmith.

### ​ Self-hosted

- No default authentication
- Complete flexibility to implement your security model
- You control all aspects of authentication and authorization

## ​ System architecture

A typical authentication setup involves three main components:

1. **Authentication Provider** (Identity Provider/IdP)

- A dedicated service that manages user identities and credentials
- Handles user registration, login, password resets, etc.
- Issues tokens (JWT, session tokens, etc.) after successful authentication
- Examples: Auth0, Supabase Auth, Okta, or your own auth server

2. **Agent Server** (Resource Server)

- Your agent or LangGraph application, which contains business logic and protected resources
- Validates tokens with the auth provider
- Enforces access control based on user identity and permissions
- Doesn’t store user credentials directly

3. **Client Application** (Frontend)

- Web app, mobile app, or API client
- Collects time-sensitive user credentials and sends to auth provider
- Receives tokens from auth provider
- Includes these tokens in requests to the Agent Server

Here’s how these components typically interact:

Agent ServerAuth ProviderClient AppAgent ServerAuth ProviderClient App4\. Validate token (@auth.authenticate)7\. Apply access control (@auth.on.\*)1\. Login (username/password)2\. Return token3\. Request with token5\. Fetch user info6\. Confirm validity8\. Return resources

Your `@auth.authenticate` handler in LangGraph handles steps 4-6, while your `@auth.on` handlers implement step 7.

## ​ Authentication

Authentication in LangGraph runs as middleware on every request. Your `@auth.authenticate` handler receives request information and should:

1. Validate the credentials
2. Return user info containing the user’s identity and user information if valid
3. Raise an HTTP exception or AssertionError if invalid

Copy

from langgraph_sdk import Auth

auth = Auth()

@auth.authenticate

# Validate credentials (e.g., API key, JWT token)
api_key = headers.get(b"x-api-key")
if not api_key or not is_valid_key(api_key):
raise Auth.exceptions.HTTPException(
status_code=401,
detail="Invalid API key"
)

# Return user info - only identity and is_authenticated are required
# Add any additional fields you need for authorization
return {
"identity": "user-123", # Required: unique user identifier
"is_authenticated": True, # Optional: assumed True by default
"permissions": ["read", "write"], # Optional: for permission-based auth
# You can add more custom fields if you want to implement other auth patterns
"role": "admin",
"org_id": "org-456"

}

The returned user information is available:

- To your authorization handlers via `ctx.user`
- In your application via `config["configuration"]["langgraph_auth_user"]`

Supported Parameters

The `@auth.authenticate` handler can accept any of the following parameters by name:

- request (Request): The raw ASGI request object
- path (str): The request path, e.g., `"/threads/abcd-1234-abcd-1234/runs/abcd-1234-abcd-1234/stream"`
- method (str): The HTTP method, e.g., `"GET"`
- path\_params (dict\[str, str\]): URL path parameters, e.g., `{"thread_id": "abcd-1234-abcd-1234", "run_id": "abcd-1234-abcd-1234"}`
- query\_params (dict\[str, str\]): URL query parameters, e.g., `{"stream": "true"}`
- headers (dict\[bytes, bytes\]): Request headers

In many of our tutorials, we will just show the “authorization” parameter to be concise, but you can opt to accept more information as needed
to implement your custom authentication scheme.

### ​ Agent authentication

Custom authentication permits delegated access. The values you return in `@auth.authenticate` are added to the run context, giving agents user-scoped credentials lets them access resources on the user’s behalf.

External ServiceSecret StoreAgent ServerAuth ProviderClientExternal ServiceSecret StoreAgent ServerAuth ProviderClient4\. Validate token (@auth.authenticate)7\. Apply access control (@auth.on.\*)9\. External service validates header and executes action1\. Login (username / password)2\. Return token3\. Request with token5\. Fetch user info6\. Confirm validity6a. Fetch user tokens6b. Return tokens8\. Call external service (with header)10\. Service response11\. Return resources

After authentication, the platform creates a special configuration object that is passed to your graph and all nodes via the configurable context.
This object contains information about the current user, including any custom fields you return from your `@auth.authenticate` handler.To enable an agent to act on behalf of the user, use custom authentication middleware. This will allow the agent to interact with external systems like MCP servers, external databases, and even other agents on behalf of the user.For more information, see the Use custom auth guide.

### ​ Agent authentication with MCP

For information on how to authenticate an agent to an MCP server, see the MCP conceptual guide.

## ​ Authorization

After authentication, LangGraph calls your `@auth.on` handlers to control access to specific resources (e.g., threads, assistants, crons). These handlers can:

1. Add metadata to be saved during resource creation by mutating the `value["metadata"]` dictionary directly. See the supported actions table for the list of types the value can take for each action.
2. Filter resources by metadata during search/list or read operations by returning a filter dictionary.
3. Raise an HTTP exception if access is denied.

If you want to just implement simple user-scoped access control, you can use a single `@auth.on` handler for all resources and actions. If you want to have different control depending on the resource and action, you can use resource-specific handlers. See the Supported Resources section for a full list of the resources that support access control.

@auth.on
async def add_owner(
ctx: Auth.types.AuthContext,
value: dict # The payload being sent to this access method

"""Authorize all access to threads, runs, crons, and assistants.

This handler does two things:
- Adds a value to resource metadata (to persist with the resource so it can be filtered later)
- Returns a filter (to restrict access to existing resources)

Args:
ctx: Authentication context containing user info, permissions, the path, and
value: The request payload sent to the endpoint. For creation
operations, this contains the resource parameters. For read
operations, this contains the resource being accessed.

Returns:
A filter dictionary that LangGraph uses to restrict access to resources.
See Filter Operations for supported operators.
"""
# Create filter to restrict access to just this user's resources
filters = {"owner": ctx.user.identity}

# Get or create the metadata dictionary in the payload
# This is where we store persistent info about the resource
metadata = value.setdefault("metadata", {})

# Add owner to metadata - if this is a create or update operation,
# this information will be saved with the resource
# So we can filter by it later in read operations
metadata.update(filters)

# Return filters to restrict access
# These filters are applied to ALL operations (create, read, update, search, etc.)
# to ensure users can only access their own resources
return filters

### ​ Resource-specific handlers

You can register handlers for specific resources and actions by chaining the resource and action names together with the `@auth.on` decorator.
When a request is made, the most specific handler that matches that resource and action is called. Below is an example of how to register handlers for specific resources and actions. For the following setup:

1. Authenticated users are able to create threads, read threads, and create runs on threads
2. Only users with the “assistants:create” permission are allowed to create new assistants
3. All other endpoints (e.g., e.g., delete assistant, crons, store) are disabled for all users.

**Supported Handlers**
For a full list of supported resources and actions, see the Supported Resources section below.

# Generic / global handler catches calls that aren't handled by more specific handlers
@auth.on

print(f"Request to {ctx.path} by {ctx.user.identity}")
raise Auth.exceptions.HTTPException(
status_code=403,
detail="Forbidden"
)

# Matches the "thread" resource and all actions - create, read, update, delete, search
# Since this is **more specific** than the generic @auth.on handler, it will take precedence
# over the generic handler for all actions on the "threads" resource
@auth.on.threads
async def on_thread(
ctx: Auth.types.AuthContext,
value: Auth.types.threads.create.value
):
# Setting metadata on the thread being created
# will ensure that the resource contains an "owner" field
# Then any time a user tries to access this thread or runs within the thread,
# we can filter by owner
metadata = value.setdefault("metadata", {})
metadata["owner"] = ctx.user.identity
return {"owner": ctx.user.identity}

# Thread creation. This will match only on thread create actions
# Since this is **more specific** than both the generic @auth.on handler and the @auth.on.threads handler,
# it will take precedence for any "create" actions on the "threads" resources
@auth.on.threads.create
async def on_thread_create(
ctx: Auth.types.AuthContext,
value: Auth.types.threads.create.value
):
# Reject if the user does not have write access
if "write" not in ctx.permissions:
raise Auth.exceptions.HTTPException(
status_code=403,
detail="User lacks the required permissions."
)

# Reading a thread. Since this is also more specific than the generic @auth.on handler, and the @auth.on.threads handler,
# it will take precedence for any "read" actions on the "threads" resource
@auth.on.threads.read
async def on_thread_read(
ctx: Auth.types.AuthContext,
value: Auth.types.threads.read.value
):
# Since we are reading (and not creating) a thread,
# we don't need to set metadata. We just need to
# return a filter to ensure users can only see their own threads
return {"owner": ctx.user.identity}

# Run creation, streaming, updates, etc.
# This takes precedenceover the generic @auth.on handler and the @auth.on.threads handler
@auth.on.threads.create_run
async def on_run_create(
ctx: Auth.types.AuthContext,
value: Auth.types.threads.create_run.value
):
metadata = value.setdefault("metadata", {})
metadata["owner"] = ctx.user.identity
# Inherit thread's access control

# Assistant creation
@auth.on.assistants.create
async def on_assistant_create(
ctx: Auth.types.AuthContext,
value: Auth.types.assistants.create.value
):
if "assistants:create" not in ctx.permissions:
raise Auth.exceptions.HTTPException(
status_code=403,
detail="User lacks the required permissions."
)

Notice that we are mixing global and resource-specific handlers in the above example. Since each request is handled by the most specific handler, a request to create a `thread` would match the `on_thread_create` handler but NOT the `reject_unhandled_requests` handler. A request to `update` a thread, however would be handled by the global handler, since we don’t have a more specific handler for that resource and action.

### ​ Filter operations

Authorization handlers can return `None`, a boolean, or a filter dictionary.

- `None` and `True` mean “authorize access to all underling resources”
- `False` means “deny access to all underling resources (raises a 403 exception)”
- A metadata filter dictionary will restrict access to resources

A filter dictionary is a dictionary with keys that match the resource metadata. It supports three operators:

- The default value is a shorthand for exact match, or “$eq”, below. For example, `{"owner": user_id}` will include only resources with metadata containing `{"owner": user_id}`
- `$eq`: Exact match (e.g., `{"owner": {"$eq": user_id}}`) \- this is equivalent to the shorthand above, `{"owner": user_id}`
- `$contains`: List membership (e.g., `{"allowed_users": {"$contains": user_id}}`) or list containment (e.g., `{"allowed_users": {"$contains": [user_id_1, user_id_2]}}`). The value here must be an element of the list or a subset of the elements of the list, respectively. The metadata in the stored resource must be a list/container type.

A dictionary with multiple keys is treated using a logical `AND` filter. For example, `{"owner": org_id, "allowed_users": {"$contains": user_id}}` will only match resources with metadata whose “owner” is `org_id` and whose “allowed\_users” list contains `user_id`.
See the reference `Auth`(Auth) for more information.

## ​ Common access patterns

Here are some typical authorization patterns:

### ​ Single-owner resources

This common pattern lets you scope all threads, assistants, crons, and runs to a single user. It’s useful for common single-user use cases like regular chatbot-style apps.

@auth.on
async def owner_only(ctx: Auth.types.AuthContext, value: dict):
metadata = value.setdefault("metadata", {})
metadata["owner"] = ctx.user.identity
return {"owner": ctx.user.identity}

### ​ Permission-based access

This pattern lets you control access based on **permissions**. It’s useful if you want certain roles to have broader or more restricted access to resources.

# In your auth handler:

...
return {
"identity": "user-123",
"is_authenticated": True,
"permissions": ["threads:write", "threads:read"] # Define permissions in auth
}

def _default(ctx: Auth.types.AuthContext, value: dict):
metadata = value.setdefault("metadata", {})
metadata["owner"] = ctx.user.identity
return {"owner": ctx.user.identity}

@auth.on.threads.create
async def create_thread(ctx: Auth.types.AuthContext, value: dict):
if "threads:write" not in ctx.permissions:
raise Auth.exceptions.HTTPException(
status_code=403,
detail="Unauthorized"
)
return _default(ctx, value)

@auth.on.threads.read
async def rbac_create(ctx: Auth.types.AuthContext, value: dict):
if "threads:read" not in ctx.permissions and "threads:write" not in ctx.permissions:
raise Auth.exceptions.HTTPException(
status_code=403,
detail="Unauthorized"
)
return _default(ctx, value)

## ​ Supported resources

LangGraph provides three levels of authorization handlers, from most general to most specific:

1. **Global Handler** (`@auth.on`): Matches all resources and actions
2. **Resource Handler** (e.g., `@auth.on.threads`, `@auth.on.assistants`, `@auth.on.crons`): Matches all actions for a specific resource
3. **Action Handler** (e.g., `@auth.on.threads.create`, `@auth.on.threads.read`): Matches a specific action on a specific resource

The most specific matching handler will be used. For example, `@auth.on.threads.create` takes precedence over `@auth.on.threads` for thread creation.
If a more specific handler is registered, the more general handler will not be called for that resource and action.

“Type Safety”

@auth.on.threads.create
async def on_thread_create(
ctx: Auth.types.AuthContext,
value: Auth.types.on.threads.create.value # Specific type for thread creation
):
...

@auth.on.threads
async def on_threads(
ctx: Auth.types.AuthContext,
value: Auth.types.on.threads.value # Union type of all thread actions
):
...

@auth.on
async def on_all(
ctx: Auth.types.AuthContext,
value: dict # Union type of all possible actions
):
...

More specific handlers provide better type hints since they handle fewer action types.

#### ​ Supported actions and types

Here are all the supported action handlers:

| Resource | Handler | Description | Value Type |
| --- | --- | --- | --- |
| **Threads** | `@auth.on.threads.create` | Thread creation | `ThreadsCreate` |
| | `@auth.on.threads.read` | Thread retrieval | `ThreadsRead` |
| | `@auth.on.threads.update` | Thread updates | `ThreadsUpdate` |
| | `@auth.on.threads.delete` | Thread deletion | `ThreadsDelete` |
| | `@auth.on.threads.search` | Listing threads | `ThreadsSearch` |
| | `@auth.on.threads.create_run` | Creating or updating a run | `RunsCreate` |
| **Assistants** | `@auth.on.assistants.create` | Assistant creation | `AssistantsCreate` |
| | `@auth.on.assistants.read` | Assistant retrieval | `AssistantsRead` |
| | `@auth.on.assistants.update` | Assistant updates | `AssistantsUpdate` |
| | `@auth.on.assistants.delete` | Assistant deletion | `AssistantsDelete` |
| | `@auth.on.assistants.search` | Listing assistants | `AssistantsSearch` |
| **Crons** | `@auth.on.crons.create` | Cron job creation | `CronsCreate` |
| | `@auth.on.crons.read` | Cron job retrieval | `CronsRead` |
| | `@auth.on.crons.update` | Cron job updates | `CronsUpdate` |
| | `@auth.on.crons.delete` | Cron job deletion | `CronsDelete` |
| | `@auth.on.crons.search` | Listing cron jobs | `CronsSearch` |

“About Runs”Runs are scoped to their parent thread for access control. This means permissions are typically inherited from the thread, reflecting the conversational nature of the data model. All run operations (reading, listing) except creation are controlled by the thread’s handlers.
There is a specific `create_run` handler for creating new runs because it had more arguments that you can view in the handler.

## ​ Next steps

For implementation details:

- Check out the introductory tutorial on setting up authentication
- See the how-to guide on implementing a custom auth handlers

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Studio troubleshooting\\
\\
Previous Add custom authentication\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-auth

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Auth & access control

Add custom authentication

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Add custom authentication to your deployment
- Enable agent authentication
- Authorizing a user for Studio
- Learn more

This guide shows you how to add custom authentication to your LangSmith application. The steps on this page apply to both cloud and self-hosted deployments. It does not apply to isolated usage of the LangGraph open source library in your own custom server.

## ​ Add custom authentication to your deployment

To leverage custom authentication and access user-level metadata in your deployments, set up custom authentication to automatically populate the `config["configurable"]["langgraph_auth_user"]` object through a custom authentication handler. You can then access this object in your graph with the `langgraph_auth_user` key to allow an agent to perform authenticated actions on behalf of the user.

1. Implement authentication:

Without a custom `@auth.authenticate` handler, LangGraph sees only the API-key owner (usually the developer), so requests aren’t scoped to individual end-users. To propagate custom tokens, you must implement your own handler.

Copy

from langgraph_sdk import Auth
import requests

auth = Auth()

is_valid = # your API key validation logic
return is_valid

@auth.authenticate # (1)!

api_key = headers.get(b"x-api-key")
if not api_key or not is_valid_key(api_key):
raise Auth.exceptions.HTTPException(status_code=401, detail="Invalid API key")

# Fetch user-specific tokens from your secret store
user_tokens = await fetch_user_tokens(api_key)

return { # (2)!
"identity": api_key, # fetch user ID from LangSmith
"github_token" : user_tokens.github_token
"jira_token" : user_tokens.jira_token
# ... custom fields/secrets here
}

- This handler receives the request (headers, etc.), validates the user, and returns a dictionary with at least an identity field.
- You can add any custom fields you want (e.g., OAuth tokens, roles, org IDs, etc.).

2. In your `langgraph.json`, add the path to your auth file:

{
"dependencies": ["."],
"graphs": {
"agent": "./agent.py:graph"
},
"env": ".env",
"auth": {
"path": "./auth.py:my_auth"
}
}

3. Once you’ve set up authentication in your server, requests must include the required authorization information based on your chosen scheme. Assuming you are using JWT token authentication, you could access your deployments using any of the following methods:

- Python Client

- Python RemoteGraph

- JavaScript Client

- JavaScript RemoteGraph

- CURL

from langgraph_sdk import get_client

my_token = "your-token" # In practice, you would generate a signed token with your auth provider
client = get_client(
url="http://localhost:2024",
headers={"Authorization": f"Bearer {my_token}"}
)
threads = await client.threads.search()

from langgraph.pregel.remote import RemoteGraph

my_token = "your-token" # In practice, you would generate a signed token with your auth provider
remote-graph = RemoteGraph(
"agent",
url="http://localhost:2024",
headers={"Authorization": f"Bearer {my_token}"}
)
threads = await remote-graph.ainvoke(...)

import { Client } from "@langchain/langgraph-sdk";

const my_token = "your-token"; // In practice, you would generate a signed token with your auth provider
const client = new Client({
apiUrl: "http://localhost:2024",
defaultHeaders: { Authorization: `Bearer ${my_token}` },
});
const threads = await client.threads.search();

import { RemoteGraph } from "@langchain/langgraph/remote";

const my_token = "your-token"; // In practice, you would generate a signed token with your auth provider
const remoteGraph = new RemoteGraph({
graphId: "agent",
url: "http://localhost:2024",
headers: { Authorization: `Bearer ${my_token}` },
});
const threads = await remoteGraph.invoke(...);

curl -H "Authorization: Bearer ${your-token}"

For more details on RemoteGraph, refer to the Use RemoteGraph guide.

## ​ Enable agent authentication

After authentication, the platform creates a special configuration object (`config`) that is passed to LangSmith deployment. This object contains information about the current user, including any custom fields you return from your `@auth.authenticate` handler.To allow an agent to perform authenticated actions on behalf of the user, access this object in your graph with the `langgraph_auth_user` key:

def my_node(state, config):
user_config = config["configurable"].get("langgraph_auth_user")
# token was resolved during the @auth.authenticate function
token = user_config.get("github_token","")
...

Fetch user credentials from a secure secret store. Storing secrets in graph state is not recommended.

### ​ Authorizing a user for Studio

By default, if you add custom authorization on your resources, this will also apply to interactions made from Studio. If you want, you can handle logged-in Studio users differently by checking is\_studio\_user().

`is_studio_user` was added in version 0.1.73 of the langgraph-sdk. If you’re on an older version, you can still check whether `isinstance(ctx.user, StudioUser)`.

from langgraph_sdk.auth import is_studio_user, Auth
auth = Auth()

# ... Setup authenticate, etc.

@auth.on
async def add_owner(
ctx: Auth.types.AuthContext,
value: dict # The payload being sent to this access method

if is_studio_user(ctx.user):
return {}

filters = {"owner": ctx.user.identity}
metadata = value.setdefault("metadata", {})
metadata.update(filters)
return filters

Only use this if you want to permit developer access to a graph deployed on the managed LangSmith SaaS.

## ​ Learn more

- Authentication & Access Control
- Setting up custom authentication tutorial

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Authentication & access control\\
\\
Previous Set up custom authentication\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/set-up-custom-auth

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Auth & access control

Set up custom authentication

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- 1\. Create your app
- 2\. Add authentication
- 3\. Test your bot
- 4\. Chat with your bot
- Next steps

In this tutorial, we will build a chatbot that only lets specific users access it. We’ll start with the LangGraph template and add token-based security step by step. By the end, you’ll have a working chatbot that checks for valid tokens before allowing access.This is part 1 of our authentication series:

1. Set up custom authentication (you are here) - Control who can access your bot
2. Make conversations private \- Let users have private conversations
3. Connect an authentication provider \- Add real user accounts and validate using OAuth2 for production

This guide assumes basic familiarity with the following concepts:

- **Authentication & Access Control**
- **LangSmith**

Custom auth is only available for LangSmith SaaS deployments or Enterprise Self-Hosted deployments.

## ​ 1\. Create your app

Create a new chatbot using the LangGraph starter template:

pip

uv

Copy

pip install -U "langgraph-cli[inmem]"
langgraph new --template=new-langgraph-project-python custom-auth
cd custom-auth

The template gives us a placeholder LangGraph app. Try it out by installing the local dependencies and running the development server:

npm

pip install -e .
langgraph dev

The server will start and open Studio in your browser:

>
> This in-memory server is designed for development and testing.
> For production use, please use LangSmith.

If you were to self-host this on the public internet, anyone could access it.!No authentication: the dev server is publicly reachable, anyone can access the bot if exposed to the internet.

## ​ 2\. Add authentication

Now that you have a base LangGraph app, add authentication to it.

In this tutorial, you will start with a hard-coded token for example purposes. You will get to a “production-ready” authentication scheme in the third tutorial.

The Auth object lets you register an authentication function that the LangSmith deployment will run on every request. This function receives each request and decides whether to accept or reject.Create a new file `src/security/auth.py`. This is where your code will live to check if users are allowed to access your bot:

src/security/auth.py

from langgraph_sdk import Auth

# This is our toy user database. Do not do this in production
VALID_TOKENS = {
"user1-token": {"id": "user1", "name": "Alice"},
"user2-token": {"id": "user2", "name": "Bob"},
}

# The "Auth" object is a container that LangGraph will use to mark our authentication function
auth = Auth()

# The `authenticate` decorator tells LangGraph to call this function as middleware
# for every request. This will determine whether the request is allowed or not
@auth.authenticate

"""Check if the user's token is valid."""
assert authorization
scheme, token = authorization.split()
assert scheme.lower() == "bearer"
# Check if token is valid
if token not in VALID_TOKENS:
raise Auth.exceptions.HTTPException(status_code=401, detail="Invalid token")

# Return user info if valid
user_data = VALID_TOKENS[token]
return {
"identity": user_data["id"],
}

Notice that your Auth.authenticate handler does two important things:

1. Checks if a valid token is provided in the request’s Authorization header
2. Returns the user’s MinimalUserDict

Now tell LangGraph to use authentication by adding the following to the langgraph.json configuration:

langgraph.json

{
"dependencies": ["."],
"graphs": {
"agent": "./src/agent/graph.py:graph"
},
"env": ".env",
"auth": {
"path": "src/security/auth.py:auth"
}
}

## ​ 3\. Test your bot

Start the server again to test everything out:

langgraph dev --no-browser

If you didn’t add the `--no-browser`, the Studio UI will open in the browser. By default, we also permit access from Studio, even when using custom auth. This makes it easier to develop and test your bot in Studio. You can remove this alternative authentication option by setting `disable_studio_auth: true` in your auth configuration:

{
"auth": {
"path": "src/security/auth.py:auth",
"disable_studio_auth": true
}
}

## ​ 4\. Chat with your bot

You should now only be able to access the bot if you provide a valid token in the request header. Users will still, however, be able to access each other’s resources until you add resource authorization handlers in the next section of the tutorial.!Auth gate passes requests with a valid token, but no per-resource filters are applied yet—so users share visibility until authorization handlers are added in the next step.Run the following code in a file or notebook:

from langgraph_sdk import get_client

# Try without a token (should fail)
client = get_client(url="http://localhost:2024")
try:
thread = await client.threads.create()
print("❌ Should have failed without token!")
except Exception as e:
print("✅ Correctly blocked access:", e)

# Try with a valid token
client = get_client(
url="http://localhost:2024", headers={"Authorization": "Bearer user1-token"}
)

# Create a thread and chat
thread = await client.threads.create()
print(f"✅ Created thread as Alice: {thread['thread_id']}")

response = await client.runs.create(
thread_id=thread["thread_id"],
assistant_id="agent",
input={"messages": [{"role": "user", "content": "Hello!"}]},
)
print("✅ Bot responded:")
print(response)

You should see that:

1. Without a valid token, we can’t access the bot
2. With a valid token, we can create threads and chat

Congratulations! You’ve built a chatbot that only lets “authenticated” users access it. While this system doesn’t (yet) implement a production-ready security scheme, we’ve learned the basic mechanics of how to control access to our bot. In the next tutorial, we’ll learn how to give each user their own private conversations.

## ​ Next steps

Now that you can control who accesses your bot, you might want to:

1. Continue the tutorial by going to Make conversations private to learn about resource authorization.
2. Read more about authentication concepts.
3. Check out the API reference for Auth, Auth.authenticate, and MinimalUserDict for more authentication details.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Add custom authentication\\
\\
Previous Make conversations private\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/resource-auth

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Auth & access control

Make conversations private

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Prerequisites
- 1\. Add resource authorization
- 2\. Test private conversations
- 3\. Add scoped authorization handlers
- Next steps

In this tutorial, you will extend the chatbot created in the last tutorial to give each user their own private conversations. You’ll add resource-level access control so users can only see their own threads.!Authorization flow: after authentication, an authorization handler tags each resource with owner=user id and returns a filter so users only see their own threads.

## ​ Prerequisites

Before you start this tutorial, ensure you have the bot from the first tutorial running without errors.

## ​ 1\. Add resource authorization

Recall that in the last tutorial, the `Auth` object lets you register an authentication function, which LangSmith uses to validate the bearer tokens in incoming requests. Now you’ll use it to register an **authorization** handler.Authorization handlers are functions that run **after** authentication succeeds. These handlers can add metadata to resources (like who owns them) and filter what each user can see.Update your `src/security/auth.py` and add one authorization handler to run on every request:

src/security/auth.py

Copy

from langgraph_sdk import Auth

# Keep our test users from the previous tutorial
VALID_TOKENS = {
"user1-token": {"id": "user1", "name": "Alice"},
"user2-token": {"id": "user2", "name": "Bob"},
}

auth = Auth()

@auth.authenticate

"""Our authentication handler from the previous tutorial."""
assert authorization
scheme, token = authorization.split()
assert scheme.lower() == "bearer"

if token not in VALID_TOKENS:
raise Auth.exceptions.HTTPException(status_code=401, detail="Invalid token")

user_data = VALID_TOKENS[token]
return {
"identity": user_data["id"],
}

@auth.on
async def add_owner(
ctx: Auth.types.AuthContext, # Contains info about the current user
value: dict, # The resource being created/accessed
):
"""Make resources private to their creator."""
# Examples:
# ctx: AuthContext(
# permissions=[],
# user=ProxyUser(
# identity='user1',
# is_authenticated=True,
# display_name='user1'
# ),
# resource='threads',
# action='create_run'
# )
# value:
# {
# 'thread_id': UUID('1e1b2733-303f-4dcd-9620-02d370287d72'),
# 'assistant_id': UUID('fe096781-5601-53d2-b2f6-0d3403f7e9ca'),
# 'run_id': UUID('1efbe268-1627-66d4-aa8d-b956b0f02a41'),
# 'status': 'pending',
# 'metadata': {},
# 'prevent_insert_if_inflight': True,
# 'multitask_strategy': 'reject',
# 'if_not_exists': 'reject',
# 'after_seconds': 0,
# 'kwargs': {
# 'input': {'messages': [{'role': 'user', 'content': 'Hello!'}]},
# 'command': None,
# 'config': {
# 'configurable': {
# 'langgraph_auth_user': ... Your user object...
# 'langgraph_auth_user_id': 'user1'
# }
# },
# 'stream_mode': ['values'],
# 'interrupt_before': None,
# 'interrupt_after': None,
# 'webhook': None,
# 'feedback_keys': None,
# 'temporary': False,
# 'subgraphs': False

# Does 2 things:
# 1. Add the user's ID to the resource's metadata. Each LangGraph resource has a `metadata` dict that persists with the resource.
# this metadata is useful for filtering in read and update operations
# 2. Return a filter that lets users only see their own resources
filters = {"owner": ctx.user.identity}
metadata = value.setdefault("metadata", {})
metadata.update(filters)

# Only let users see their own resources
return filters

The handler receives two parameters:

1. `ctx` ( AuthContext): contains info about the current `user`, the user’s `permissions`, the `resource` (“threads”, “crons”, “assistants”), and the `action` being taken (“create”, “read”, “update”, “delete”, “search”, “create\_run”)
2. `value` (`dict`): data that is being created or accessed. The contents of this dict depend on the resource and action being accessed. See adding scoped authorization handlers below for information on how to get more tightly scoped access control.

Notice that the simple handler does two things:

1. Adds the user’s ID to the resource’s metadata.
2. Returns a metadata filter so users only see resources they own.

## ​ 2\. Test private conversations

Test your authorization. If you have set things up correctly, you will see all ✅ messages. Be sure to have your development server running (run `langgraph dev`):

from langgraph_sdk import get_client

# Create clients for both users
alice = get_client(
url="http://localhost:2024",
headers={"Authorization": "Bearer user1-token"}
)

bob = get_client(
url="http://localhost:2024",
headers={"Authorization": "Bearer user2-token"}
)

# Alice creates an assistant
alice_assistant = await alice.assistants.create()
print(f"✅ Alice created assistant: {alice_assistant['assistant_id']}")

# Alice creates a thread and chats
alice_thread = await alice.threads.create()
print(f"✅ Alice created thread: {alice_thread['thread_id']}")

await alice.runs.create(
thread_id=alice_thread["thread_id"],
assistant_id="agent",
input={"messages": [{"role": "user", "content": "Hi, this is Alice's private chat"}]}
)

# Bob tries to access Alice's thread
try:
await bob.threads.get(alice_thread["thread_id"])
print("❌ Bob shouldn't see Alice's thread!")
except Exception as e:
print("✅ Bob correctly denied access:", e)

# Bob creates his own thread
bob_thread = await bob.threads.create()
await bob.runs.create(
thread_id=bob_thread["thread_id"],
assistant_id="agent",
input={"messages": [{"role": "user", "content": "Hi, this is Bob's private chat"}]}
)
print(f"✅ Bob created his own thread: {bob_thread['thread_id']}")

# List threads - each user only sees their own
alice_threads = await alice.threads.search()
bob_threads = await bob.threads.search()
print(f"✅ Alice sees {len(alice_threads)} thread")
print(f"✅ Bob sees {len(bob_threads)} thread")

Output:

✅ Alice created assistant: fc50fb08-78da-45a9-93cc-1d3928a3fc37
✅ Alice created thread: 533179b7-05bc-4d48-b47a-a83cbdb5781d
✅ Bob correctly denied access: Client error '404 Not Found' for url 'http://localhost:2024/threads/533179b7-05bc-4d48-b47a-a83cbdb5781d'
For more information check:
✅ Bob created his own thread: 437c36ed-dd45-4a1e-b484-28ba6eca8819
✅ Alice sees 1 thread
✅ Bob sees 1 thread

This means:

1. Each user can create and chat in their own threads
2. Users can’t see each other’s threads
3. Listing threads only shows your own

## ​ 3\. Add scoped authorization handlers

The broad `@auth.on` handler matches on all authorization events. This is concise, but it means the contents of the `value` dict are not well-scoped, and the same user-level access control is applied to every resource. If you want to be more fine-grained, you can also control specific actions on resources.Update `src/security/auth.py` to add handlers for specific resource types:

# Keep our previous handlers...

@auth.on.threads.create
async def on_thread_create(
ctx: Auth.types.AuthContext,
value: Auth.types.on.threads.create.value,
):
"""Add owner when creating threads.

This handler runs when creating new threads and does two things:
1. Sets metadata on the thread being created to track ownership
2. Returns a filter that ensures only the creator can access it
"""
# Example value:
# {'thread_id': UUID('99b045bc-b90b-41a8-b882-dabc541cf740'), 'metadata': {}, 'if_exists': 'raise'}

# Add owner metadata to the thread being created
# This metadata is stored with the thread and persists
metadata = value.setdefault("metadata", {})
metadata["owner"] = ctx.user.identity

# Return filter to restrict access to just the creator
return {"owner": ctx.user.identity}

@auth.on.threads.read
async def on_thread_read(
ctx: Auth.types.AuthContext,
value: Auth.types.on.threads.read.value,
):
"""Only let users read their own threads.

This handler runs on read operations. We don't need to set
metadata since the thread already exists - we just need to
return a filter to ensure users can only see their own threads.
"""
return {"owner": ctx.user.identity}

@auth.on.assistants
async def on_assistants(
ctx: Auth.types.AuthContext,
value: Auth.types.on.assistants.value,
):
# For illustration purposes, we will deny all requests
# that touch the assistants resource
# 'assistant_id': UUID('63ba56c3-b074-4212-96e2-cc333bbc4eb4'),
# 'graph_id': 'agent',
# 'config': {},
# 'name': 'Untitled'
raise Auth.exceptions.HTTPException(
status_code=403,
detail="User lacks the required permissions.",
)

# Assumes you organize information in store like (user_id, resource_type, resource_id)
@auth.on.store()
async def authorize_store(ctx: Auth.types.AuthContext, value: dict):
# The "namespace" field for each store item is a tuple you can think of as the directory of an item.
namespace: tuple = value["namespace"]
assert namespace[0] == ctx.user.identity, "Not authorized"

Notice that instead of one global handler, you now have specific handlers for:

1. Creating threads
2. Reading threads
3. Accessing assistants

The first three of these match specific **actions** on each resource (see resource actions), while the last one (`@auth.on.assistants`) matches _any_ action on the `assistants` resource. For each request, LangGraph will run the most specific handler that matches the resource and action being accessed. This means that the four handlers above will run rather than the broadly scoped “`@auth.on`” handler.Try adding the following test code to your test file:

# ... Same as before
# Try creating an assistant. This should fail
try:
await alice.assistants.create("agent")
print("❌ Alice shouldn't be able to create assistants!")
except Exception as e:
print("✅ Alice correctly denied access:", e)

# Try searching for assistants. This also should fail
try:
await alice.assistants.search()
print("❌ Alice shouldn't be able to search assistants!")
except Exception as e:
print("✅ Alice correctly denied access to searching assistants:", e)

# Alice can still create threads

✅ Alice created thread: dcea5cd8-eb70-4a01-a4b6-643b14e8f754
✅ Bob correctly denied access: Client error '404 Not Found' for url 'http://localhost:2024/threads/dcea5cd8-eb70-4a01-a4b6-643b14e8f754'
For more information check:
✅ Bob created his own thread: 400f8d41-e946-429f-8f93-4fe395bc3eed
✅ Alice sees 1 thread
✅ Bob sees 1 thread
✅ Alice correctly denied access:
For more information check:
✅ Alice correctly denied access to searching assistants:

Congratulations! You’ve built a chatbot where each user has their own private conversations. While this system uses simple token-based authentication, these authorization patterns will work with implementing any real authentication system. In the next tutorial, you’ll replace your test users with real user accounts using OAuth2.

## ​ Next steps

Now that you can control access to resources, you might want to:

1. Move on to Connect an authentication provider to add real user accounts.
2. Read more about authorization patterns.
3. Check out the API reference for details about the interfaces and methods used in this tutorial.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Set up custom authentication\\
\\
Previous Connect an authentication provider\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/add-auth-server

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Auth & access control

Connect an authentication provider

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Background
- Prerequisites
- 1\. Install dependencies
- 2\. Set up the authentication provider
- 3\. Implement token validation
- 4\. Test authentication flow
- Next steps

In the last tutorial, you added resource authorization to give users private conversations. However, you are still using hard-coded tokens for authentication, which is not secure. Now you’ll replace those tokens with real user accounts using OAuth2.You’ll keep the same `Auth` object and resource-level access control, but upgrade authentication to use Supabase as your identity provider. While Supabase is used in this tutorial, the concepts apply to any OAuth2 provider. You’ll learn how to:

1. Replace test tokens with real JWT tokens
2. Integrate with OAuth2 providers for secure user authentication
3. Handle user sessions and metadata while maintaining our existing authorization logic

## ​ Background

OAuth2 involves three main roles:

1. **Authorization server**: The identity provider (e.g., Supabase, Auth0, Google) that handles user authentication and issues tokens
2. **Application backend**: Your LangGraph application. This validates tokens and serves protected resources (conversation data)
3. **Client application**: The web or mobile app where users interact with your service

A standard OAuth2 flow works something like this:

Agent ServerAuthServerClientUserAgent ServerAuthServerClientUserInitiate loginEnter credentialsSend tokensRequest with tokenValidate tokenToken validServe request (e.g., run agent or graph)

## ​ Prerequisites

Before you start this tutorial, ensure you have:

- The bot from the second tutorial running without errors.
- A Supabase project to use its authentication server.

## ​ 1\. Install dependencies

Install the required dependencies. Start in your `custom-auth` directory and ensure you have the `langgraph-cli` installed:

pip

uv

Copy

cd custom-auth
pip install -U "langgraph-cli[inmem]"

## ​ 2\. Set up the authentication provider

Next, fetch the URL of your auth server and the private key for authentication.
Since you’re using Supabase for this, you can do this in the Supabase dashboard:

1. In the left sidebar, click on t️⚙ Project Settings” and then click “API”
2. Copy your project URL and add it to your `.env` file

echo "SUPABASE_URL=your-project-url" >> .env

3. Copy your service role secret key and add it to your `.env` file:

echo "SUPABASE_SERVICE_KEY=your-service-role-key" >> .env

4. Copy your “anon public” key and note it down. This will be used later when you set up our client code.

SUPABASE_URL=your-project-url
SUPABASE_SERVICE_KEY=your-service-role-key

## ​ 3\. Implement token validation

In the previous tutorials, you used the `Auth` object to validate hard-coded tokens and add resource ownership.Now you’ll upgrade your authentication to validate real JWT tokens from Supabase. The main changes will all be in the `@auth.authenticate` decorated function:

- Instead of checking against a hard-coded list of tokens, you’ll make an HTTP request to Supabase to validate the token.
- You’ll extract real user information (ID, email) from the validated token.
- The existing resource authorization logic remains unchanged.

Update `src/security/auth.py` to implement this:

src/security/auth.py

import os
import httpx
from langgraph_sdk import Auth

auth = Auth()

# This is loaded from the `.env` file you created above
SUPABASE_URL = os.environ["SUPABASE_URL"]
SUPABASE_SERVICE_KEY = os.environ["SUPABASE_SERVICE_KEY"]

@auth.authenticate
async def get_current_user(authorization: str | None):
"""Validate JWT tokens and extract user information."""
assert authorization
scheme, token = authorization.split()
assert scheme.lower() == "bearer"

try:
# Verify token with auth provider
async with httpx.AsyncClient() as client:
response = await client.get(
f"{SUPABASE_URL}/auth/v1/user",
headers={
"Authorization": authorization,
"apiKey": SUPABASE_SERVICE_KEY,
},
)
assert response.status_code == 200
user = response.json()
return {
"identity": user["id"], # Unique user identifier
"email": user["email"],
"is_authenticated": True,
}
except Exception as e:
raise Auth.exceptions.HTTPException(status_code=401, detail=str(e))

# ... the rest is the same as before

# Keep our resource authorization from the previous tutorial
@auth.on
async def add_owner(ctx, value):
"""Make resources private to their creator using resource metadata."""
filters = {"owner": ctx.user.identity}
metadata = value.setdefault("metadata", {})
metadata.update(filters)
return filters

The most important change is that we’re now validating tokens with a real authentication server. Our authentication handler has the private key for our Supabase project, which we can use to validate the user’s token and extract their information.

## ​ 4\. Test authentication flow

Let’s test out the new authentication flow. You can run the following code in a file or notebook. You will need to provide:

- A valid email address
- A Supabase project URL (from above)
- A Supabase anon **public key** (also from above)

import os
import httpx
from getpass import getpass
from langgraph_sdk import get_client

# Get email from command line
email = getpass("Enter your email: ")
base_email = email.split("@")
password = "secure-password" # CHANGEME
email1 = f"{base_email[0]}+1@{base_email[1]}"
email2 = f"{base_email[0]}+2@{base_email[1]}"

SUPABASE_URL = os.environ.get("SUPABASE_URL")
if not SUPABASE_URL:
SUPABASE_URL = getpass("Enter your Supabase project URL: ")

# This is your PUBLIC anon key (which is safe to use client-side)
# Do NOT mistake this for the secret service role key
SUPABASE_ANON_KEY = os.environ.get("SUPABASE_ANON_KEY")
if not SUPABASE_ANON_KEY:
SUPABASE_ANON_KEY = getpass("Enter your public Supabase anon key: ")

async def sign_up(email: str, password: str):
"""Create a new user account."""
async with httpx.AsyncClient() as client:
response = await client.post(
f"{SUPABASE_URL}/auth/v1/signup",
json={"email": email, "password": password},
headers={"apiKey": SUPABASE_ANON_KEY},
)
assert response.status_code == 200
return response.json()

# Create two test users
print(f"Creating test users: {email1} and {email2}")
await sign_up(email1, password)
await sign_up(email2, password)

⚠️ Before continuing: Check your email and click both confirmation links. Supabase will reject `/login` requests until after you have confirmed your users’ email.Now test that users can only see their own data. Make sure the server is running (run `langgraph dev`) before proceeding. The following snippet requires the “anon public” key that you copied from the Supabase dashboard while setting up the auth provider previously.

async def login(email: str, password: str):
"""Get an access token for an existing user."""
async with httpx.AsyncClient() as client:
response = await client.post(
f"{SUPABASE_URL}/auth/v1/token?grant_type=password",
json={
"email": email,
"password": password
},
headers={
"apikey": SUPABASE_ANON_KEY,
"Content-Type": "application/json"
},
)
assert response.status_code == 200
return response.json()["access_token"]

# Log in as user 1
user1_token = await login(email1, password)
user1_client = get_client(
url="http://localhost:2024", headers={"Authorization": f"Bearer {user1_token}"}
)

# Create a thread as user 1
thread = await user1_client.threads.create()
print(f"✅ User 1 created thread: {thread['thread_id']}")

# Try to access without a token
unauthenticated_client = get_client(url="http://localhost:2024")
try:
await unauthenticated_client.threads.create()
print("❌ Unauthenticated access should fail!")
except Exception as e:
print("✅ Unauthenticated access blocked:", e)

# Try to access user 1's thread as user 2
user2_token = await login(email2, password)
user2_client = get_client(
url="http://localhost:2024", headers={"Authorization": f"Bearer {user2_token}"}
)

try:
await user2_client.threads.get(thread["thread_id"])
print("❌ User 2 shouldn't see User 1's thread!")
except Exception as e:
print("✅ User 2 blocked from User 1's thread:", e)

The output should look like this:

✅ User 1 created thread: d6af3754-95df-4176-aa10-dbd8dca40f1a
✅ Unauthenticated access blocked: Client error '403 Forbidden' for url 'http://localhost:2024/threads'
✅ User 2 blocked from User 1's thread: Client error '404 Not Found' for url 'http://localhost:2024/threads/d6af3754-95df-4176-aa10-dbd8dca40f1a'

Your authentication and authorization are working together:

1. Users must log in to access the bot
2. Each user can only see their own threads

All users are managed by the Supabase auth provider, so you don’t need to implement any additional user management logic.

## ​ Next steps

You’ve successfully built a production-ready authentication system for your LangGraph application! Let’s review what you’ve accomplished:

1. Set up an authentication provider (Supabase in this case)
2. Added real user accounts with email/password authentication
3. Integrated JWT token validation into your Agent Server
4. Implemented proper authorization to ensure users can only access their own data
5. Created a foundation that’s ready to handle your next authentication challenge

Now that you have production authentication, consider:

1. Building a web UI with your preferred framework (see the Custom Auth template for an example)
2. Learn more about the other aspects of authentication and authorization in the conceptual guide on authentication.
3. Customize your handlers and setup further after reading the reference docs.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Make conversations private\\
\\
Previous Document API authentication in OpenAPI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/openapi-security

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Auth & access control

Document API authentication in OpenAPI

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Default Schema
- Custom Security Schema
- Testing

This guide shows how to customize the OpenAPI security schema for your LangSmith API documentation. A well-documented security schema helps API consumers understand how to authenticate with your API and even enables automatic client generation. See the Authentication & Access Control conceptual guide for more details about LangGraph’s authentication system.

**Implementation vs Documentation**
This guide only covers how to document your security requirements in OpenAPI. To implement the actual authentication logic, see How to add custom authentication.

This guide applies to all LangSmith deployments (Cloud and self-hosted). It does not apply to usage of the LangGraph open source library if you are not using LangSmith.

## ​ Default Schema

The default security scheme varies by deployment type:

- LangSmith

By default, LangSmith requires a LangSmith API key in the `x-api-key` header:

Copy

components:
securitySchemes:
apiKeyAuth:
type: apiKey
in: header
name: x-api-key
security:
- apiKeyAuth: []

When using one of the LangGraph SDK’s, this can be inferred from environment variables.

- Self-hosted

By default, self-hosted deployments have no security scheme. This means they are to be deployed only on a secured network or with authentication. To add custom authentication, see How to add custom authentication.

## ​ Custom Security Schema

To customize the security schema in your OpenAPI documentation, add an `openapi` field to your `auth` configuration in `langgraph.json`. Remember that this only updates the API documentation - you must also implement the corresponding authentication logic as shown in How to add custom authentication.Note that LangSmith does not provide authentication endpoints - you’ll need to handle user authentication in your client application and pass the resulting credentials to the LangGraph API.

- OAuth2 with Bearer Token

- API Key

{
"auth": {
"path": "./auth.py:my_auth", // Implement auth logic here
"openapi": {
"securitySchemes": {
"OAuth2": {
"type": "oauth2",
"flows": {
"implicit": {
"authorizationUrl": "https://your-auth-server.com/oauth/authorize",
"scopes": {
"me": "Read information about the current user",
"threads": "Access to create and manage threads"
}
}
}
}
},
"security": [\
{"OAuth2": ["me", "threads"]}\
]
}
}
}

{
"auth": {
"path": "./auth.py:my_auth", // Implement auth logic here
"openapi": {
"securitySchemes": {
"apiKeyAuth": {
"type": "apiKey",
"in": "header",
"name": "X-API-Key"
}
},
"security": [\
{"apiKeyAuth": []}\
]
}
}
}

## ​ Testing

After updating your configuration:

1. Deploy your application
2. Visit `/docs` to see the updated OpenAPI documentation
3. Try out the endpoints using credentials from your authentication server (make sure you’ve implemented the authentication logic first)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Connect an authentication provider\\
\\
Previous Set up Agent Auth (Beta)\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-auth

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Auth & access control

Set up Agent Auth (Beta)

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Installation
- Quickstart
- 1\. Initialize the client
- 2\. Set up OAuth providers
- 3\. Authenticate from an agent
- In LangGraph context
- Outside LangGraph context

Agent Auth is in **Beta** and under active development. To provide feedback or use this feature, reach out to the LangChain team.

## ​ Installation

Install the Agent Auth client library from PyPI:

pip

uv

Copy

pip install langchain-auth

## ​ Quickstart

### ​ 1\. Initialize the client

from langchain_auth import Client

client = Client(api_key="your-langsmith-api-key")

### ​ 2\. Set up OAuth providers

Before agents can authenticate, you need to configure an OAuth provider using the following process:

1. Select a unique identifier for your OAuth provider to use in LangChain’s platform (e.g., “github-local-dev”, “google-workspace-prod”).
2. Go to your OAuth provider’s developer console and create a new OAuth application.
3. Set the callback URL in your OAuth provider using this structure:

For example, if your provider\_id is “github-local-dev”, use:

4. Use `client.create_oauth_provider()` with the credentials from your OAuth app:

new_provider = await client.create_oauth_provider(
provider_id="{provider_id}", # Provide any unique ID. Not formally tied to the provider.
name="{provider_display_name}", # Provide any display name
client_id="{your_client_id}",
client_secret="{your_client_secret}",
auth_url="{auth_url_of_your_provider}",
token_url="{token_url_of_your_provider}",
)

### ​ 3\. Authenticate from an agent

The client `authenticate()` API is used to get OAuth tokens from pre-configured providers. On the first call, it takes the caller through an OAuth 2.0 auth flow.

#### ​ In LangGraph context

By default, tokens are scoped to the calling agent using the Assistant ID parameter.

auth_result = await client.authenticate(
provider="{provider_id}",
scopes=["scopeA"],
user_id="your_user_id" # Any unique identifier to scope this token to the human caller
)

# Or if you'd like a token that can be used by any agent, set agent_scoped=False
auth_result = await client.authenticate(
provider="{provider_id}",
scopes=["scopeA"],
user_id="your_user_id",
agent_scoped=False
)

During execution, if authentication is required, the SDK will throw an interrupt. The agent execution pauses and presents the OAuth URL to the user:!Studio interrupt showing OAuth URLAfter the user completes OAuth authentication and we receive the callback from the provider, they will see the auth success page.!GitHub OAuth success pageThe agent then resumes execution from the point it left off at, and the token can be used for any API calls. We store and refresh OAuth tokens so that future uses of the service by either the user or agent do not require an OAuth flow.

token = auth_result.token

#### ​ Outside LangGraph context

Provide the `auth_url` to the user for out-of-band OAuth flows.

# Default: user-scoped token (works for any agent under this user)
auth_result = await client.authenticate(
provider="{provider_id}",
scopes=["scopeA"],
user_id="your_user_id"
)

if auth_result.needs_auth:
print(f"Complete OAuth at: {auth_result.auth_url}")
# Wait for completion
completed_auth = await client.wait_for_completion(auth_result.auth_id)
token = completed_auth.token
else:
token = auth_result.token

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Document API authentication in OpenAPI\\
\\
Previous How to add custom lifespan events\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-lifespan

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Server customization

How to add custom lifespan events

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Create app
- Configure langgraph.json
- Start server
- Deploying
- Next steps

When deploying agents to LangSmith, you often need to initialize resources like database connections when your server starts up, and ensure they’re properly closed when it shuts down. Lifespan events let you hook into your server’s startup and shutdown sequence to handle these critical setup and teardown tasks.This works the same way as adding custom routes. You just need to provide your own `Starlette` app (including `FastAPI`, `FastHTML` and other compatible apps).Below is an example using FastAPI.

“Python only”

## ​ Create app

Starting from an **existing** LangSmith application, add the following lifespan code to your `webapp.py` file. If you are starting from scratch, you can create a new app from a template using the CLI.

Copy

langgraph new --template=new-langgraph-project-python my_new_project

Once you have a LangGraph project, add the following app code:

# ./src/agent/webapp.py
from contextlib import asynccontextmanager
from fastapi import FastAPI
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

@asynccontextmanager
async def lifespan(app: FastAPI):
# for example...
engine = create_async_engine("postgresql+asyncpg://user:pass@localhost/db")
# Create reusable session factory
async_session = sessionmaker(engine, class_=AsyncSession)
# Store in app state
app.state.db_session = async_session
yield
# Clean up connections
await engine.dispose()

app = FastAPI(lifespan=lifespan)

# ... can add custom routes if needed.

## ​ Configure `langgraph.json`

Add the following to your `langgraph.json` configuration file. Make sure the path points to the `webapp.py` file you created above.

{
"dependencies": ["."],
"graphs": {
"agent": "./src/agent/graph.py:graph"
},
"env": ".env",
"http": {
"app": "./src/agent/webapp.py:app"
}
// Other configuration options like auth, store, etc.
}

## ​ Start server

Test the server out locally:

langgraph dev --no-browser

You should see your startup message printed when the server starts, and your cleanup message when you stop it with `Ctrl+C`.

## ​ Deploying

You can deploy your app as-is to cloud or to your self-hosted platform.

## ​ Next steps

Now that you’ve added lifespan events to your deployment, you can use similar techniques to add custom routes or custom middleware to further customize your server’s behavior.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Set up Agent Auth (Beta)\\
\\
Previous How to add custom middleware\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-middleware

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Server customization

How to add custom middleware

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Create app
- Configure langgraph.json
- Customize middleware ordering
- Start server
- Deploying
- Next steps

When deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes. You just need to provide your own `Starlette` app (including `FastAPI`, `FastHTML` and other compatible apps).Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they’re hitting your custom endpoints or the built-in LangSmith APIs.Below is an example using FastAPI.

“Python only”

## ​ Create app

Starting from an **existing** LangSmith application, add the following middleware code to your `webapp.py` file. If you are starting from scratch, you can create a new app from a template using the CLI.

Copy

langgraph new --template=new-langgraph-project-python my_new_project

Once you have a LangGraph project, add the following app code:

# ./src/agent/webapp.py
from fastapi import FastAPI, Request
from starlette.middleware.base import BaseHTTPMiddleware

app = FastAPI()

class CustomHeaderMiddleware(BaseHTTPMiddleware):
async def dispatch(self, request: Request, call_next):
response = await call_next(request)
response.headers['X-Custom-Header'] = 'Hello from middleware!'
return response

# Add the middleware to the app
app.add_middleware(CustomHeaderMiddleware)

## ​ Configure `langgraph.json`

Add the following to your `langgraph.json` configuration file. Make sure the path points to the `webapp.py` file you created above.

{
"dependencies": ["."],
"graphs": {
"agent": "./src/agent/graph.py:graph"
},
"env": ".env",
"http": {
"app": "./src/agent/webapp.py:app"
}
// Other configuration options like auth, store, etc.
}

### ​ Customize middleware ordering

By default, custom middleware runs before authentication logic. To run custom middleware _after_ authentication, set `middleware_order` to `auth_first` in your `http` configuration. (This customization is supported starting with API server v0.4.35 and later.)

{
"dependencies": ["."],
"graphs": {
"agent": "./src/agent/graph.py:graph"
},
"env": ".env",
"http": {
"app": "./src/agent/webapp.py:app",
"middleware_order": "auth_first"
},
"auth": {
"path": "./auth.py:my_auth"
}
}

## ​ Start server

Test the server out locally:

langgraph dev --no-browser

Now any request to your server will include the custom header `X-Custom-Header` in its response.

## ​ Deploying

You can deploy this app as-is to cloud or to your self-hosted platform.

## ​ Next steps

Now that you’ve added custom middleware to your deployment, you can use similar techniques to add custom routes or define custom lifespan events to further customize your server’s behavior.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to add custom lifespan events\\
\\
Previous How to add custom routes\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-routes

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Server customization

How to add custom routes

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Create app
- Configure langgraph.json
- Start server
- Deploying
- Next steps

When deploying agents to LangSmith Deployment, your server automatically exposes routes for creating runs and threads, interacting with the long-term memory store, managing configurable assistants, and other core functionality ( see all default API endpoints).You can add custom routes by providing your own `Starlette` app (including `FastAPI`, `FastHTML` and other compatible apps). You make LangSmith aware of this by providing a path to the app in your `langgraph.json` configuration file.Defining a custom app object lets you add any routes you’d like, so you can do anything from adding a `/login` endpoint to writing an entire full-stack web-app, all deployed in a single Agent Server.Below is an example using FastAPI.

## ​ Create app

Starting from an **existing** LangSmith application, add the following custom route code to your `webapp.py` file. If you are starting from scratch, you can create a new app from a template using the CLI.

Copy

langgraph new --template=new-langgraph-project-python my_new_project

Once you have a LangGraph project, add the following app code:

# ./src/agent/webapp.py
from fastapi import FastAPI

app = FastAPI()

@app.get("/hello")
def read_root():
return {"Hello": "World"}

## ​ Configure `langgraph.json`

Add the following to your `langgraph.json` configuration file. Make sure the path points to the FastAPI application instance `app` in the `webapp.py` file you created above.

{
"dependencies": ["."],
"graphs": {
"agent": "./src/agent/graph.py:graph"
},
"env": ".env",
"http": {
"app": "./src/agent/webapp.py:app"
}
// Other configuration options like auth, store, etc.
}

## ​ Start server

Test the server out locally:

langgraph dev --no-browser

If you navigate to `localhost:2024/hello` in your browser (`2024` is the default development port), you should see the `/hello` endpoint returning `{"Hello": "World"}`.

**Shadowing default endpoints**
The routes you create in the app are given priority over the system defaults, meaning you can shadow and redefine the behavior of any default endpoint.

## ​ Deploying

You can deploy this app as-is to LangSmith or to your self-hosted platform.

## ​ Next steps

Now that you’ve added a custom route to your deployment, you can use this same technique to further customize how your server behaves, such as defining custom custom middleware and custom lifespan events.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to add custom middleware\\
\\
Previous Use HTTP headers for runtime configuration\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/configurable-headers

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Server customization

Use HTTP headers for runtime configuration

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Using within your graph
- Opt-out of configurable headers

LangGraph allows runtime configuration to modify agent behavior and permissions dynamically. When using LangSmith Deployment, you can pass this configuration in the request body (`config`) or specific request headers. This enables adjustments based on user identity or other requests.For privacy, control which headers are passed to the runtime configuration via the `http.configurable_headers` section in your `langgraph.json` file.Here’s how to customize the included and excluded headers:

Copy

{
"http": {
"configurable_headers": {
"includes": ["x-user-id", "x-organization-id", "my-prefix-*"],
"excludes": ["authorization", "x-api-key"]
}
}
}

The `includes` and `excludes` lists accept exact header names or patterns using `*` to match any number of characters. For your security, no other regex patterns are supported.

## ​ Using within your graph

You can access the included headers in your graph using the `config` argument of any node.

def my_node(state, config):
organization_id = config["configurable"].get("x-organization-id")
...

Or by fetching from context (useful in tools and or within other nested functions).

from langgraph.config import get_config

def search_everything(query: str):
organization_id = get_config()["configurable"].get("x-organization-id")
...

You can even use this to dynamically compile the graph.

# my_graph.py.
import contextlib

@contextlib.asynccontextmanager
async def generate_agent(config):
organization_id = config["configurable"].get("x-organization-id")
if organization_id == "org1":
graph = ...
yield graph
else:
graph = ...
yield graph

{
"graphs": {"agent": "my_grph.py:generate_agent"}
}

### ​ Opt-out of configurable headers

If you’d like to opt-out of configurable headers, you can simply set a wildcard pattern in the `s` list:

{
"http": {
"configurable_headers": {
"excludes": ["*"]
}
}
}

This will exclude all headers from being added to your run’s configuration.Note that exclusions take precedence over inclusions.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to add custom routes\\
\\
Previous Include HTTP headers in server logs\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/configurable-logs

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Server customization

Include HTTP headers in server logs

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

By default, the Agent Server omits HTTP headers from server logs for privacy reasons. However, logging request and correlation IDs can help you debug issues and trace requests across distributed systems. You can opt-in to logging headers for all API calls by modifying the `logging_headers` section in your `langgraph.json` file.

Copy

{
"$schema": "https://langgra.ph/schema.json",
"http": {
"logging_headers": {
"includes": ["request-id", "x-purchase-id", "*-trace-*"],
"excludes": ["authorization", "x-api-key", "x-organization-id", "x-user-id"]
}
}
}

The `includes` and `excludes` lists accept exact header names or glob patterns using `*` as a wildcard to match any number of characters (case-insensitive). For your security, no other pattern types are supported.Note that exclusions take precedence over inclusions. For example, if you include `*-id` but exclude `x-user-id`, the `x-user-id` header will not be logged.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Use HTTP headers for runtime configuration\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/setup-app-requirements-txt

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Setup

How to set up an application with requirements.txt

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- With requirements.txt
- With pyproject.toml
- Set up a JavaScript application
- Monorepo support
- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Specify dependencies
- Specify environment variables
- Define graphs
- Create the configuration file
- Next

An application must be configured with a configuration file in order to be deployed to LangSmith (or to be self-hosted). This how-to guide discusses the basic steps to set up an application for deployment using `requirements.txt` to specify project dependencies.This example is based on this repository, which uses the LangGraph framework.The final repository structure will look something like this:

Copy

my-app/
├── my_agent # all project code lies within here
│ ├── utils # utilities for your graph
│ │ ├── __init__.py
│ │ ├── tools.py # tools for your graph
│ │ ├── nodes.py # node functions for your graph
│ │ └── state.py # state definition of your graph
│ ├── requirements.txt # package dependencies
│ ├── __init__.py
│ └── agent.py # code for constructing your graph
├── .env # environment variables
└── langgraph.json # configuration file for LangGraph

LangSmith Deployment supports deploying a LangGraph _graph_. However, the implementation of a _node_ of a graph can contain arbitrary code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you implement your core application logic without using additional LangGraph OSS APIs while still using LangSmith for deployment, scaling, and observability. For more details, refer to Use any framework with LangSmith Deployment.

You can also set up with:

- `pyproject.toml`: If you prefer using poetry for dependency management, check out this how-to guide on using `pyproject.toml` for LangSmith.
- a monorepo: If you are interested in deploying a graph located inside a monorepo, take a look at this repository for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## ​ Specify dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the configuration file.The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:

Example `requirements.txt` file:

langgraph
langchain_anthropic
tavily-python
langchain_community
langchain_openai

Example file directory:

my-app/
├── my_agent # all project code lies within here
│ └── requirements.txt # package dependencies

## ​ Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the Environment Variables reference to configure additional variables for a deployment.Example `.env` file:

MY_ENV_VAR_1=foo
MY_ENV_VAR_2=bar
OPENAI_API_KEY=key

my-app/
├── my_agent # all project code lies within here
│ └── requirements.txt # package dependencies
└── .env # environment variables

By default, LangSmith follows the `uv`/`pip` behavior of **not** installing prerelease versions unless explicitly allowed. If want to use prereleases, you have the following options:

- With `pyproject.toml`: add `allow-prereleases = true` to your `[tool.uv]` section.
- With `requirements.txt` or `setup.py`: you must explicitly specify every prerelease dependency, including transitive ones. For example, if you declare `a==0.0.1a1` and `a` depends on `b==0.0.1a1`, then you must also explicitly include `b==0.0.1a1` in your dependencies.

## ​ Define graphs

Implement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each CompiledStateGraph to be included in the application. The variable names will be used later when creating the LangGraph configuration file.Example `agent.py` file, which shows how to import from other modules you define (code for the modules is not shown here, please see this repository to see their implementation):

# my_agent/agent.py
from typing import Literal
from typing_extensions import TypedDict

from langgraph.graph import StateGraph, END, START
from my_agent.utils.nodes import call_model, should_continue, tool_node # import nodes
from my_agent.utils.state import AgentState # import state

# Define the runtime context
class GraphContext(TypedDict):
model_name: Literal["anthropic", "openai"]

workflow = StateGraph(AgentState, context_schema=GraphContext)
workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node)
workflow.add_edge(START, "agent")
workflow.add_conditional_edges(
"agent",
should_continue,
{
"continue": "action",
"end": END,
},
)
workflow.add_edge("action", "agent")

graph = workflow.compile()

my-app/
├── my_agent # all project code lies within here
│ ├── utils # utilities for your graph
│ │ ├── __init__.py
│ │ ├── tools.py # tools for your graph
│ │ ├── nodes.py # node functions for your graph
│ │ └── state.py # state definition of your graph
│   ├── requirements.txt # package dependencies
│   ├── __init__.py
│   └── agent.py # code for constructing your graph
└── .env # environment variables

## ​ Create the configuration file

Create a configuration file called `langgraph.json`. See the configuration file reference for detailed explanations of each key in the JSON object of the configuration file.Example `langgraph.json` file:

{
"dependencies": ["./my_agent"],
"graphs": {
"agent": "./my_agent/agent.py:graph"
},
"env": ".env"
}

**Configuration file location**
The configuration file must be placed in a directory that is at the same level or higher than the Python files that contain compiled graphs and associated dependencies.

my-app/
├── my_agent # all project code lies within here
│ ├── utils # utilities for your graph
│ │ ├── __init__.py
│ │ ├── tools.py # tools for your graph
│ │ ├── nodes.py # node functions for your graph
│ │ └── state.py # state definition of your graph
│   ├── requirements.txt # package dependencies
│   ├── __init__.py
│   └── agent.py # code for constructing your graph
├── .env # environment variables
└── langgraph.json # configuration file for LangGraph

## ​ Next

After you set up your project and place it in a GitHub repository, it’s time to deploy your app.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Application structure\\
\\
Previous How to set up an application with pyproject.toml\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/monorepo-support

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Setup

Monorepo support

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- With requirements.txt
- With pyproject.toml
- Set up a JavaScript application
- Monorepo support
- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Repository Structure
- LangGraph.json configuration
- Building the application
- Tips and best practices

LangSmith supports deploying agents from monorepo setups where your agent code may depend on shared packages located elsewhere in the repository. This guide shows how to structure your monorepo and configure your `langgraph.json` file to work with shared dependencies.

## ​ Repository Structure

For complete working examples, see:

- Python monorepo example
- JS monorepo example

Python

JS

Copy

my-monorepo/
├── shared-utils/ # Shared Python package
│ ├── __init__.py
│ ├── common.py
│ └── pyproject.toml # Or setup.py
├── agents/
│ └── customer-support/ # Agent directory
│ ├── agent/
│ │ ├── __init__.py
│ │ └── graph.py
│ ├── langgraph.json # Config file in agent directory
│ ├── .env
│ └── pyproject.toml # Agent dependencies
└── other-service/
└── ...

## ​ LangGraph.json configuration

Place the langgraph.json file in your agent’s directory (not in the monorepo root). Ensure the file follows the required structure:

{
"dependencies": [\
".", # Current agent package\
"../../shared-utils" # Relative path to shared package\
],
"graphs": {
"customer_support": "./agent/graph.py:graph"
},
"env": ".env"
}

The Python implementation automatically handles packages in parent directories by:

- Detecting relative paths that start with `"."`.
- Adding parent directories to the Docker build context as needed.
- Supporting both real packages (with `pyproject.toml`/`setup.py`) and simple Python modules.

For JavaScript monorepos:

- Shared workspace dependencies are resolved automatically by your package manager.
- Your `package.json` should reference shared packages using workspace syntax.

Example `package.json` in the agent directory:

{
"name": "customer-support-agent",
"dependencies": {
"@company/shared-utils": "workspace:*",
"@langchain/langgraph": "^0.2.0"
}
}

## ​ Building the application

Run `langgraph build`:

cd agents/customer-support
langgraph build -t my-customer-support-agent

The Python build process:

1. Automatically detects relative dependency paths.
2. Copies shared packages into the Docker build context.
3. Installs all dependencies in the correct order.
4. No special flags or commands required.

The JavaScript build process:

1. Uses the directory you called `langgraph build` from (the monorepo root in this case) as the build context.
2. Automatically detects your package manager (yarn, npm, pnpm, bun)
3. Runs the appropriate install command.
- If you have one or both of a custom build/install command it will run from the directory you called `langgraph build` from.
- Otherwise, it will run from the directory where the `langgraph.json` file is located.
4. Optionally runs a custom build command from the directory where the `langgraph.json` file is located (only if you pass the `--build-command` flag).

## ​ Tips and best practices

1. **Keep agent configs in agent directories**: Place `langgraph.json` files in the specific agent directories, not at the monorepo root. This allows you to support multiple agents in the same monorepo, without having to deploy them all in the same LangSmith deployment.
2. **Use relative paths for Python**: For Python monorepos, use relative paths like `"../../shared-package"` in the `dependencies` array.
3. **Leverage workspace features for JS**: For JavaScript/TypeScript, use your package manager’s workspace features to manage dependencies between packages.
4. **Test locally first**: Always test your build locally before deploying to ensure all dependencies are correctly resolved.
5. **Environment variables**: Keep environment files (`.env`) in your agent directories for environment-specific configuration.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to set up a JavaScript application\\
\\
Previous LangSmith Deployment components\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/local-server)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server LangSmith Deployment components Self-hosted LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/app-development)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

App development in LangSmith Deployment LangSmith docs LangSmith Agent Builder App for Slack

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deployment-quickstart)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

App development in LangSmith Deployment Prompt engineering quickstart Tracing quickstart

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/application-structure)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Application structure LangSmith docs LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/graph-rebuild)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment components Self-hosted LangSmith LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/use-remote-graph)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment components LangSmith Tool Server How to interact with a deployment using RemoteGraph

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/semantic-search)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Build a semantic search engine with LangChain Trace with Semantic Kernel LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/configure-ttl)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Configure LangSmith for scale Configure LangSmith Agent Server for scale How to add TTLs to your application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-server-scale)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Configure LangSmith Agent Server for scale LangSmith Tool Server Agent Server API reference for LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/cicd-pipeline-example)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Implement a CI/CD pipeline using LangSmith Deployment and Evaluation How to evaluate an application's intermediate steps LangSmith Fetch

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deploy-to-cloud)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deploy-with-control-plane)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith control plane Control plane API reference for LangSmith Deployment Deploy with control plane

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deploy-standalone-server)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server LangSmith Deployment LangSmith Deployment components

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/studio)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Studio

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/use-studio)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Studio

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability-studio)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability Observability in Studio

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/troubleshooting-studio)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Studio

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/auth)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server LangSmith Deployment Control plane API reference for LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-auth)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server Control plane API reference for LangSmith Deployment Set up custom authentication

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/set-up-custom-auth)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server Set up custom authentication Set up Agent Auth (Beta)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/resource-auth)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server Control plane API reference for LangSmith Deployment LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/add-auth-server)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server Self-hosted LangSmith Control plane API reference for LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/openapi-security)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Document API authentication in OpenAPI LangSmith docs LangSmith Fetch

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-auth)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server Set up Agent Auth (Beta) Configure LangSmith Agent Server for scale

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-lifespan)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

How to add custom lifespan events LangSmith Tool Server LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-middleware)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

How to add custom middleware LangSmith Tool Server Custom middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-routes)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

How to add custom routes LangSmith Tool Server LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/configurable-headers)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Use HTTP headers for runtime configuration Configure LangSmith for scale Configure LangSmith Agent Server for scale

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/configurable-logs)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Configure your collector for LangSmith telemetry Configure LangSmith for scale Configure LangSmith Agent Server for scale

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-server).A

Skip to main content.A#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deploy-to-cloud):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deploy-with-control-plane):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith control plane Control plane API reference for LangSmith Deployment Deploy with control plane

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deploy-standalone-server):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server LangSmith Deployment LangSmith Deployment components

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/setup-app-requirements-txt),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

How to set up an application with requirements.txt LangSmith Deployment App development in LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/monorepo-support)).

Skip to main content).#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Monorepo support Self-hosted LangSmith LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-server).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Configure LangSmith Agent Server for scale LangSmith Tool Server Agent Server API reference for LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/auth).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server LangSmith Deployment Control plane API reference for LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-lifespan),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

How to add custom lifespan events LangSmith Tool Server LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-middleware),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

How to add custom middleware LangSmith Tool Server Custom middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/custom-routes)).

Skip to main content).#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

How to add custom routes LangSmith Tool Server LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/studio).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Studio

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-essentials

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Get started

Essentials

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

On this page

- Tools
- Triggers
- Memory and updates
- Custom models
- Sub-agents
- Human in the loop
- Setting up approval steps
- What you can do when your agent pauses
- Next steps

## ​ Tools

Tools let your agents interact with your apps and services. Your agents can send emails, create calendar events, post messages, search the web, and more. Choose from built-in tools for Gmail, Slack, Google Calendar, GitHub, and many others.See Supported tools for a complete list.

## ​ Triggers

Triggers define when your agent should start running. You can connect your agent to external tools or time-based schedules, letting it respond automatically to messages, emails, or recurring events.Here are some popular ways to trigger your agent:

## Slack

Activate your agent when messages are received in specific Slack channels.

## Gmail

Trigger your agent when emails are received.

## Cron schedules

Run your agent on a time-based schedule for recurring tasks.

## ​ Memory and updates

Your agents get smarter over time. They remember important information from previous conversations and can update themselves to work better.

- Memory: Agents remember relevant details from past interactions, so they can make better decisions in future conversations.
- Self-updates: Agents can add new tools, remove ones they don’t need, or adjust their instructions to improve how they work.
- What stays the same: Agents can’t change their name, description, or the triggers that start them.

## ​ Custom models

Agent Builder supports custom models. You can override the default Anthropic or OpenAI model for a specific agent by editing the agent’s settings:

1. In the LangSmith UI, navigate to the agent you want to edit.
2. Click on the settings icon in the top right corner.
3. In the **Model** section, select **\+ Add custom model**.
4. Enter the model ID, display name, base URL, and API key name and value.
5. Click **Save**.

Custom models may not perform as well as built-in models. Test your custom model before using it in production.

## ​ Sub-agents

Build complex agents by breaking big tasks into smaller, specialized helpers. Think of sub-agents as a team of specialists—each one handles a specific part of the job while working together with your main agent.This approach makes it easier to build sophisticated systems. Instead of one agent trying to do everything, you can have specialized helpers that each excel at their part of the task.Here are some ways you might use sub-agents:

- Split into sub-tasks: Have one agent fetch data, another summarize it, and a third format the results.
- Specialized tools: Give different agents access to different tools based on what they need to do.
- Independent work: Let sub-agents work on their own, then bring their results Human in the loop

Stay in control of important decisions. You can set up your agent to pause and ask for your approval before taking certain actions. This ensures your agent handles most tasks automatically, while you retain oversight.

### ​ Setting up approval steps

1

Select a tool

When setting up your agent, choose the tool or action you want to review before it runs.

2

Turn on approval

Find the approval option for that tool and switch it on.

3

Agent waits for you

When your agent reaches that step, it will pause and wait for your approval before continuing.

### ​ What you can do when your agent pauses

When your agent stops to ask for approval, you have three options:

## Accept

Give the green light and let your agent proceed with its plan.

## Edit

Modify the agent’s message or parameters before allowing it to continue.

## Send feedback

Share feed Next steps

- Set up your workspace
- Connect apps and services
- Use remote connections
- Choose between workspace and private agents
- Call agents from your app

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Agent Builder\\
\\
Previous Templates\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-templates

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Get started

Templates

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

Agent Builder includes starter templates to help you create agents quickly. Templates include predefined system prompts, tools, and triggers (if applicable) for common use cases. You can use templates as-is, or as a baseline to customize.

## ​ How to use templates

Pick a template

In Agent Builder, choose a template that matches your use case (e.g., Gmail assistant, Linear Slack bot, etc.).

Review tools and prompts

Each template comes with an initial system prompt and a set of tools. Review the tools and prompt to ensure they align with your needs (you can always edit them later, or have the agent edit it for you).

Clone and authenticate

Click `Clone Template` in the top right to start the cloning process. If you haven’t already authenticated with OAuth for the tools in the template, you’ll be prompted to do so.

Add triggers (optional)

If a template includes a trigger, you’ll be prompted to:

- Authenticate and setup the trigger if you haven’t done this before
or
- Select an existing trigger from the dropdown list

Test and iterate

Run the agent, review outputs, and refine prompts or tools. To edit your cloned agent, either make the changes manually, or ask the agent to make the changes for you!

Templates are starting points. You can customize prompts, add or remove tools, attach triggers, and switch models at any time.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Essentials\\
\\
Previous Agent Builder setup\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-setup

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Get started

Agent Builder setup

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

On this page

- How to add workspace secrets
- Required model key
- Agent Builder specific secrets
- Optional tool keys
- MCP server configuration

This page lists the workspace secrets you need to add before using Agent Builder. Add these in your LangSmith workspace settings under Secrets. Keep values scoped to your workspace and avoid placing credentials in prompts or code.

## ​ How to add workspace secrets

In the LangSmith UI, ensure that you have an LLM API key set as a workspace secret (either Anthropic or OpenAI API key).

1. Navigate to **Settings** and then move to the **Secrets** tab.
2. Select **Add secret** and enter either `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` as the **name**, and your API key as the **value**.
3. Select **Save secret**.

When adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.

## ​ Required model key

For Agent Builder to make API calls to LLMs, you need to set an OpenAI or Anthropic API key as a workspace secret. The agent graphs load this key from workspace secrets for inference.

Agent Builder supports custom models per agent. See Custom models for more information.

## ​ Agent Builder specific secrets

Secrets prefixed with `AGENT_BUILDER_` are prioritized over workspace secrets within Agent Builder. This way, you can better track the usage of Agent Builder vs other parts of LangSmith which use the same secrets.If you have both `OPENAI_API_KEY` and `AGENT_BUILDER_OPENAI_API_KEY`, the `AGENT_BUILDER_OPENAI_API_KEY` secret will be used.

## ​ Optional tool keys

Add keys for any tools you enable. These are read from workspace secrets at runtime.

- `EXA_API_KEY`: Required for Exa search tools (general web and LinkedIn profile search).
- `TAVILY_API_KEY`: Required for Tavily web search.
- `TWITTER_API_KEY` and `TWITTER_API_KEY_SECRET`: Required for Twitter/X read operations (app‑only bearer). Posting/media upload is not enabled.

## ​ MCP server configuration

Agent Builder can pull tools from one or more remote Model Context Protocol (MCP) servers. Configure MCP servers and headers in your workspace settings. Agent Builder automatically discovers tools and applies the configured headers when calling them.For more details on using the remote MCP servers, refer to the the MCP Framework page.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Templates\\
\\
Previous Workspace vs. private agents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-workspace-vs-private

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Get started

Workspace vs. private agents

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

Agent Builder supports two visibility modes:

- Private agents: private to the creator. Useful for personal workflows and experiments.
- Workspace agents: shared within the workspace. Good for team workflows, or agents you want to share with others.

## ​ Differences

- Ownership and access: private agents are only visible to you; workspace agents are visible to anyone else within the same LangSmith workspace.
- Tool Authentication:
- **OAuth**: Both modes support OAuth and secret-based tools. OAuth credentials are always scoped to a user, so workspace agents can not share OAuth tokens, and new users cloning workspace agents must re-authenticate with the selected tools.
- **Secrets**: Since secrets are scoped to a workspace, workspace agents & private agents will both use the same LangSmith secret.

## ​ What’s public vs. private

### ​ Threads/chat history

Threads are always user scoped, so even if an agent is workspace scoped, the chat history created within that agent will always be private, and only accessible to the specific user who created them.

### ​ System prompt, tools, sub-agents

The system prompt, selected tools, and sub-agents will be public on workspace scoped agents. Users will not be able to modify these fields on the original workspace scoped agent, but can make changes once they’ve cloned the agent.

### ​ Triggers

The trigger type on workspace scoped agents is public (e.g., Slack message received), but the specific connection with the trigger (e.g. the Slack channel, or Gmail address) is not shared. This way, users know what trigger to use when cloning an agent, but can’t gain unauthorized access to any connections the original user has set up.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Agent Builder setup\\
\\
Previous Tools\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-tools

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Tools and integrations

Tools

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

On this page

- Built-in tools
- Remote MCP server tools
- How it works
- Configuration

You can access a variety of tools in Agent Builder, including built-in tools and tools from remote MCP servers.

## ​ Built-in tools

Use these built-in tools to give your agents access to email, calendars, chat, project management, code hosting, spreadsheets/BI, search, social, and general web utilities.

Google, Slack, Linear, GitHub, and LinkedIn use OAuth. Exa, Tavily, Pylon, and Twitter/X use workspace secrets (API keys).

## Gmail

Read and send email

- Read emails (optionally include body, filter with search)
- Send email or reply to an existing message
- Create draft emails
- Mark messages as read
- Get a conversation thread
- Apply or create labels
- List mailbox labels

## Google Calendar

Manage events

- List events for a date
- Get event details
- Create new events

## Google Sheets

Spreadsheets

- Create spreadsheets
- Read ranges

## BigQuery

Analytics

- Execute SQL queries

## Slack

Send and read messages

- Send a direct message to a user
- Post a message to a channel
- Reply in a thread
- Read channel history
- Read thread messages

## LinkedIn

Post to profile

- Publish a post with optional image or link

## Twitter/X

- Read a tweet by ID
- Read recent posts from a list

## GitHub

PRs, issues, and content

- List pull requests
- Get pull request details
- Create issues and pull requests
- Comment on issues and pull requests
- Read repository files and list directories

## Linear

Manage issues and teams

- List teams and team members
- List issues with filters
- Get issue details
- Create, update, or delete issues

## Pylon

Issue management

- List issues
- Get issue details
- Update issues

## Search

- Exa web search (optionally fetch page contents)
- Exa LinkedIn profile search
- Tavily web search

## Web utilities

- Read webpage text content
- Extract image URLs and metadata
- Notify user (for confirmations/updates)

## ​ Remote MCP server tools

Agent Builder can discover and use tools from remote Model Context Protocol (MCP) servers. This lets you connect to external MCP servers and use their tools in your agents.

### ​ How it works

- Agent Builder discovers tools from remote MCP servers via the standard MCP protocol.
- Headers configured in your workspace are automatically attached when fetching tools or calling them. Headers can be used for authentication.
- Tools from remote servers are available alongside built-in tools in Agent Builder.

### ​ Configuration

Configure remote MCP servers in your LangSmith workspace:

1. Navigate to your workspace settings in the LangSmith UI.
2. Add your MCP server URL and any required headers (for example, `Authorization: Bearer {{MCP_TOKEN}}`).
3. Agent Builder automatically discovers tools from the server and applies the configured headers when calling tools.

Use workspace secret placeholders like `{{MCP_TOKEN}}` in headers. The platform resolves these from your workspace secrets at runtime.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Workspace vs. private agents\\
\\
Previous LangSmith Agent Builder App for Slack\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-slack-app

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Tools and integrations

LangSmith Agent Builder App for Slack

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

On this page

- How to install
- Permissions
- [](https://docs.langchain.com/langsmith/agent-builder-slack-app#privacy-policy)
- AI components and disclaimers
- What you should know
- Technical details
- Pricing

The LangSmith Agent Builder App for Slack integrates your agents with Slack for secure, context-aware communication inside your Slack workspace.After installation, your agents will be able to:

- Send direct messages.
- Post to channels.
- Read thread messages.
- Reply in threads.
- Read conversation history.

## ​ How to install

To install the LangSmith Agent Builder for Slack:

1. Navigate to Agent Builder in your LangSmith workspace.
2. Create or edit an agent.
3. Add Slack as a trigger or enable Slack tools.
4. When prompted, authorize the Slack connection.
5. Follow the OAuth flow to grant permissions to your Slack workspace.

The app will be installed automatically when you complete the authorization, _but_ you will still need to invite the app into the specific channels you want to use it in.To invite the Slack bot, you can send the following message:

Copy

/invite @LangSmith Agent Builder

## ​ Permissions

The LangSmith Agent Builder requires the following permissions to your Slack workspace:

- **Send messages** \- Send direct messages and post to channels
- **Read messages** \- Read channel history and thread messages
- **View channels** \- Access basic channel information
- **View users** \- Look up user information for messaging

These permissions enable agents to communicate effectively within your Slack workspace.

## ​

The LangSmith Agent Builder App for Slack collects, manages, and stores third-party data in accordance with our . For full details on how your data is handled, please see our .

## ​ AI components and disclaimers

The LangSmith Agent Builder uses Large Language Models (LLMs) to power AI agents that interact with users in Slack. While these models are powerful, they have the potential to generate inaccurate responses, summaries, or other outputs.

### ​ What you should know

- **AI-generated content**: All responses from agents are generated by AI and may contain errors or inaccuracies. Always verify important information.
- **Data usage**: Slack data is not used to train LLMs. Your workspace data remains private and is only used to provide agent functionality.
- **Transparency**: The Agent Builder is transparent about the actions it will take once added to your workspace, as outlined in the permissions section above.

### ​ Technical details

The Agent Builder uses the following approach to AI:

- **Model**: Uses LLMs provided through the LangSmith platform
- **Data retention**: User data is retained according to LangSmith’s data retention policies
- **Data tenancy**: Data is handled according to your LangSmith organization settings
- **Data residency**: Data residency follows your LangSmith configuration

For more information about AI safety and best practices, see the Agent Builder documentation.

## ​ Pricing

The LangSmith Agent Builder App for Slack itself does not have any direct pricing. However, agent runs and traces are billed through the LangSmith platform according to your organization’s plan.For current pricing information, see the LangSmith pricing page.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Tools\\
\\
Previous Auth-aware tool responses\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-auth-format

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Advanced

Auth-aware tool responses

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

Some tools require user authorization (for example, Google, Slack, GitHub). Agent Builder includes middleware to detect when a tool needs authorization and to pause the run with a clear prompt to the user. After the user completes auth, the same tool call is retried automatically.

## ​ Return shape to request auth

If a tool detects missing authorization, return a JSON string containing the following fields:

Copy

{
"auth_required": true,
"auth_url": "https://auth.example.com/start",
"auth_id": "opaque-tracking-id"
}

- `auth_required`: set to `true` to signal an interrupt is needed.
- `auth_url`: where the user should be redirected to authorize.
- `auth_id`: optional correlation ID to track the auth session.

When Agent Builder detects this response, it interrupts the run, displays the authentication UI to the user, and automatically retries the tool call once authorization completes.If you want your custom tools to reuse the same authentication required interrupt + UI, ensure your tools return the same shape of JSON.

Return only this JSON as the tool’s output. Avoid including additional text or content. Agent Builder parses the response to trigger the authentication flow.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Agent Builder App for Slack\\
\\
Previous Call agents from code\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-code

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Advanced

Call agents from code

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

On this page

- Authentication
- Example

You can invoke Agent Builder agents from your applications using the LangGraph SDK. You can use all the same API methods as you would with any other LangGraph deployment.

## ​ Authentication

To authenticate with the deployment your Agent Builder agents are running on, you must provide a personal access token (PAT) API key tied to your user to the `api_key` arg when instantiating the LangGraph SDK client, or via the `X-API-Key` header. Then, set the `X-Auth-Scheme` header to `langsmith-api-key`.If the PAT you pass is not tied to the owner of the agent, your request will be rejected with a 404 not found error.If the agent you’re trying to invoke is a workspace agent, and you’re not the owner, you’ll be able to preform all the same operations as you would in the UI (read-only).

## ​ Example

To invoke the agent, you can copy the code below, and replace the `agent_id` and `api_url` with the correct values.Alternatively, you can copy the same code shown below, but pre-populated with the proper agent ID and API URL, via the Agent Builder UI. To do this, navigate to the agent you want to invoke, visit the editor page, then click on the settings icon in the top right corner, and click `View code snippets`. You’ll still need to manually set your `LANGGRAPH_API_KEY` environment variable.

- Python

- TypeScript

Copy

import os
from dotenv import load_dotenv
from langgraph_sdk.client import get_client

load_dotenv()

agent_id = "your-agent-id"

# This must be a PAT API key tied to your user
api_key = os.getenv("LANGGRAPH_API_KEY")
api_url = "https://prod-agent-builder-5ccea139413e5ef289ab8e5d04688e11.us.langgraph.app"

client = get_client(
url=api_url,
api_key=api_key,
headers={
"X-Auth-Scheme": "langsmith-api-key",
},
)

async def get_assistant(agent_id: str):
agent = await client.assistants.get(agent_id)
print(agent)

if __name__ == "__main__":
import asyncio
asyncio.run(get_assistant(agent_id))

import "dotenv/config";
import { Client } from "@langchain/langgraph-sdk";

const agentId = "your-agent-id";

// This must be a PAT API key tied to your user
const apiKey = process.env.LANGGRAPH_API_KEY;
const apiUrl = "https://prod-agent-builder-5ccea139413e5ef289ab8e5d04688e11.us.langgraph.app";

const client = new Client({
apiUrl,
apiKey,
defaultHeaders: {
"X-Auth-Scheme": "langsmith-api-key",
},
});

async function main(agentId: string) {
const agent = await client.assistants.get(agentId);
console.log(agent);
}

main(agentId).catch(console.error);

Use a PAT (Personal Access Token) API key tied to your user account. Set the `X-Auth-Scheme` header to `langsmith-api-key` for authentication. If you implemented custom authentication, pass your user’s token in headers so the agent can use user‑scoped tools. See “Add custom authentication”.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Auth-aware tool responses\\
\\
Previous LangSmith Tool Server\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-mcp-framework

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Advanced

LangSmith Tool Server

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

On this page

- Create a custom toolkit
- Call tools via MCP protocol
- Use as an MCP gateway
- Authenticate
- OAuth for third-party APIs
- Custom request authentication

The LangSmith Tool Server is a standalone MCP framework for building and deploying tools with built-in authentication and authorization. Use the Tool Server when you want to:

- Create custom tools that integrate with LangSmith’s Agent Auth for OAuth authentication
- Build an MCP gateway for agents you’re building yourself (outside of Agent Builder)

If you’re using Agent Builder, you don’t need to interact with the Tool Server directly. Agent Builder provides built-in tools and supports remote MCP servers without requiring Tool Server setup.However, you can configure the associated tool server instance as an MCP server, which will allow you to use your custom MCP servers in your agent.

Download the PyPi package to get started.

## ​ Create a custom toolkit

Install the LangSmith Tool Server and LangChain CLI:

Copy

pip install langsmith-tool-server
pip install langchain-cli-v2

Create a new toolkit:

langchain tools new my-toolkit
cd my-toolkit

This creates a toolkit with the following structure:

my-toolkit/
├── pyproject.toml
├── toolkit.toml
└── my_toolkit/
├── __init__.py
├── auth.py
└── tools/
├── __init__.py
└── ...

Define your tools using the `@tool` decorator:

from langsmith_tool_server import tool

@tool

"""Greet someone by name."""
return f"Hello, {name}!"

"""Add two numbers."""
return x + y

TOOLS = [hello, add]

Run the server:

langchain tools serve

Your tool server will start on `http://localhost:8000`.

## ​ Call tools via MCP protocol

Below is an example that lists available tools and calls the `add` tool:

import asyncio
import aiohttp

async def mcp_request(url: str, method: str, params: dict = None):
async with aiohttp.ClientSession() as session:
payload = {"jsonrpc": "2.0", "method": method, "params": params or {}, "id": 1}
async with session.post(f"{url}/mcp", json=payload) as response:
return await response.json()

async def main():
url = "http://localhost:8000"

tools = await mcp_request(url, "tools/list")
print(f"Tools: {tools}")

result = await mcp_request(url, "tools/call", {"name": "add", "arguments": {"a": 5, "b": 3}})
print(f"Result: {result}")

asyncio.run(main())

## ​ Use as an MCP gateway

The LangSmith Tool Server can act as an MCP gateway, aggregating tools from multiple MCP servers into a single endpoint. Configure MCP servers in your `toolkit.toml`:

[toolkit]
name = "my-toolkit"
tools = "./my_toolkit/__init__.py:TOOLS"

[[mcp_servers]]
name = "weather"
transport = "streamable_http"
url = "http://localhost:8001/mcp/"

[[mcp_servers]]
name = "math"
transport = "stdio"
command = "python"
args = ["-m", "mcp_server_math"]

All tools from connected MCP servers are exposed through your server’s `/mcp` endpoint. MCP tools are prefixed with their server name to avoid conflicts (e.g., `weather_get_forecast`, `math_add`).

## ​ Authenticate

### ​ OAuth for third-party APIs

For tools that need to access third-party APIs (like Google, GitHub, Slack, etc.), you can use OAuth authentication with Agent Auth.Before using OAuth in your tools, you’ll need to configure an OAuth provider in your LangSmith workspace settings. See the Agent Auth documentation for setup instructions.Once configured, specify the `auth_provider` in your tool decorator:

from langsmith_tool_server import tool, Context
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build

@tool(
auth_provider="google",
scopes=["https://www.googleapis.com/auth/gmail.readonly"],
integration="gmail"
)

"""Read recent emails from Gmail."""
credentials = Credentials(token=context.token)
service = build('gmail', 'v1', credentials=credentials)
# ... Gmail API calls
return f"Retrieved {max_results} emails"

Tools with `auth_provider` must:

- Have `context: Context` as the first parameter
- Specify at least one scope
- Use `context.token` to make authenticated API calls

### ​ Custom request authentication

Custom authentication allows you to validate requests and integrate with your identity provider. Define an authentication handler in your `auth.py` file:

from langsmith_tool_server import Auth

auth = Auth()

@auth.authenticate

"""Validate requests and return user identity."""
if not authorization or not authorization.startswith("Bearer "):
raise auth.exceptions.HTTPException(
status_code=401,
detail="Unauthorized"
)

token = authorization.replace("Bearer ", "")
# Validate token with your identity provider
user = await verify_token_with_idp(token)

return {"identity": user.id}

The handler runs on every request and must return a dict with `identity` (and optionally `permissions`).

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Call agents from code\\
\\
Previous Updating MCP servers\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-update-mcp-servers

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Advanced

Updating MCP servers

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

On this page

- How to update an MCP server URL

Changing the URL of a custom MCP server will break any agents that use tools from that server.

Agent Builder stores tool references by MCP server URL. If you update the URL of a custom MCP server, existing agents will fail when attempting to call those tools because the stored URL no longer matches.

## ​ How to update an MCP server URL

1. Update your MCP server URL in the workspace settings
2. For each agent using tools from that server:
- Remove the affected tools from the agent configuration
- Re-add the tools (they will now reference the new URL)
3. Test the agent to confirm tools work correctly

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith Tool Server\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-essentials)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Agent Builder App for Slack LangSmith Tool Server Configure LangSmith Agent Server for scale

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-templates)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Agent Builder App for Slack LangSmith Tool Server Agent Builder setup

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-setup)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Agent Builder App for Slack Agent Builder setup Configure LangSmith Agent Server for scale

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-workspace-vs-private)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

Workspace vs. private agents LangSmith Agent Builder App for Slack Agent Builder setup

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-tools)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server LangSmith Agent Builder App for Slack Test a ReAct agent with Pytest/Vitest and LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-slack-app)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Agent Builder App for Slack LangSmith Tool Server App development in LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-auth-format)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server LangSmith Agent Builder App for Slack Set up Agent Auth (Beta)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-code)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Agent Builder App for Slack LangSmith Tool Server Agent Server API reference for LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-mcp-framework)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server MCP endpoint in Agent Server LangSmith Agent Builder App for Slack

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-update-mcp-servers)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Tool Server MCP endpoint in Agent Server Agent Server API reference for LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-builder-templates).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

##### Get started

- Essentials
- Templates
- Setup
- Agent visibility

##### Tools and integrations

- Tools
- Slack app

##### Advanced

- Auth responses
- Call from code
- MCP framework
- Updating MCP servers

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Agent Builder App for Slack LangSmith Tool Server Agent Builder setup

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/trace-with-langchain

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Integrations

Trace with LangChain (Python and JS/TS)

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Overview
- LangChain
- LangGraph
- Anthropic (Python only)
- OpenAI
- AutoGen
- Claude Agent SDK
- Claude Code
- CrewAI
- Google ADK
- Instructor (Python only)
- OpenAI Agents SDK
- OpenTelemetry
- PydanticAI
- Semantic Kernel
- Vercel AI SDK
- LiveKit
- Pipecat
- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Installation
- Quick start
- 1\. Configure your environment
- 2\. Log a trace
- 3\. View your trace
- Trace selectively
- Log to a specific project
- Statically
- Dynamically
- Add metadata and tags to traces
- Customize run name
- Customize run ID
- Access run (span) ID for LangChain invocations
- Ensure all traces are submitted before exiting
- Trace without setting environment variables
- Distributed tracing with LangChain (Python)
- Interoperability between LangChain (Python) and LangSmith SDK
- Interoperability between LangChain.JS and LangSmith SDK
- Tracing LangChain objects inside traceable (JS only)
- Tracing LangChain child runs via traceable / RunTree API (JS only)

LangSmith integrates seamlessly with LangChain (Python and JavaScript), the popular open-source framework for building LLM applications.

## ​ Installation

Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).For a full list of packages available, see the LangChain docs.

pip

yarn

npm

pnpm

Copy

pip install langchain_openai langchain_core

## ​ Quick start

### ​ 1\. Configure your environment

export LANGSMITH_TRACING=true

If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:`export LANGCHAIN_CALLBACKS_BACKGROUND=true`If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:`export LANGCHAIN_CALLBACKS_BACKGROUND=false`

### ​ 2\. Log a trace

No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would.

Python

TypeScript

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_messages([\
("system", "You are a helpful assistant. Please respond to the user's request only based on the given context."),\
("user", "Question: {question}\nContext: {context}")\
])

model = ChatOpenAI(model="gpt-4o-mini")
output_parser = StrOutputParser()
chain = prompt | model | output_parser

question = "Can you summarize this morning's meetings?"
context = "During this morning's meeting, we solved all world conflict."

chain.invoke({"question": question, "context": context})

### ​ 3\. View your trace

By default, the trace will be logged to the project with the name `default`. An example of a trace logged using the above code is made public and can be viewed here.!Langchain trace

## ​ Trace selectively

The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application.There are two ways to do this in Python: by manually passing in a `LangChainTracer` instance as a callback, or by using the `tracing_context` context manager.In JS/TS, you can pass a `LangChainTracer` instance as a callback.

# You can opt-in to specific invocations..
import langsmith as ls

with ls.tracing_context(enabled=True):
chain.invoke({"question": "Am I using a callback?", "context": "I'm using a callback"})

# This will NOT be traced (assuming LANGSMITH_TRACING is not set)
chain.invoke({"question": "Am I being traced?", "context": "I'm not being traced"})

# This would not be traced, even if LANGSMITH_TRACING=true
with ls.tracing_context(enabled=False):
chain.invoke({"question": "Am I being traced?", "context": "I'm not being traced"})

## ​ Log to a specific project

### ​ Statically

As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set the `LANGSMITH_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your application.

export LANGSMITH_PROJECT=my-project

### ​ Dynamically

This largely builds off of the previous section and allows you to set the project name for a specific `LangChainTracer` instance or as parameters to the `tracing_context` context manager in Python.

# You can set the project name using the project_name parameter.

with ls.tracing_context(project_name="My Project", enabled=True):
chain.invoke({"question": "Am I using a context manager?", "context": "I'm using a context manager"})

## ​ Add metadata and tags to traces

You can annotate your traces with arbitrary metadata and tags by providing them in the `RunnableConfig`. This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it. For information on how to query traces and runs by metadata and tags, see this guide

When you attach metadata or tags to a runnable (either through the `RunnableConfig` or at runtime with invocation params), they are inherited by all child runnables of that runnable.

prompt = ChatPromptTemplate.from_messages([\
("system", "You are a helpful AI."),\
("user", "{input}")\
])

# The tag "model-tag" and metadata {"model-key": "model-value"} will be attached to the ChatOpenAI run only
chat_model = ChatOpenAI().with_config({"tags": ["model-tag"], "metadata": {"model-key": "model-value"}})
output_parser = StrOutputParser()

# Tags and metadata can be configured with RunnableConfig
chain = (prompt | chat_model | output_parser).with_config({"tags": ["config-tag"], "metadata": {"config-key": "config-value"}})

# Tags and metadata can also be passed at runtime
chain.invoke({"input": "What is the meaning of life?"}, {"tags": ["invoke-tag"], "metadata": {"invoke-key": "invoke-value"}})

## ​ Customize run name

You can customize the name of a given run when invoking or streaming your LangChain code by providing it in the Config. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI. This can be done by setting a `run_name` in the `RunnableConfig` object at construction or by passing a `run_name` in the invocation parameters in JS/TS.

# When tracing within LangChain, run names default to the class name of the traced object (e.g., 'ChatOpenAI').
configured_chain = chain.with_config({"run_name": "MyCustomChain"})
configured_chain.invoke({"input": "What is the meaning of life?"})

# You can also configure the run name at invocation time, like below
chain.invoke({"input": "What is the meaning of life?"}, {"run_name": "MyCustomChain"})

The `run_name` parameter only changes the name of the runnable you invoke (e.g., a chain, function). It does not rename the nested run automatically created when you invoke an LLM object like `ChatOpenAI` (`gpt-4o-mini`). In the example, the enclosing run will appear in LangSmith as `MyCustomChain`, while the nested LLM run still shows the model’s default name.To give the LLM run a more meaningful name, you can either:

- Wrap the model in another runnable and assign a `run_name` to that step.
- Use a tracing decorator or helper (e.g., `@traceable` in Python, or `traceable` from `langsmith` in JS/TS) to create a custom run around the model call.

## ​ Customize run ID

You can customize the ID of a given run when invoking or streaming your LangChain code by providing it in the Config. This ID is used to uniquely identify the run in LangSmith and can be used to query specific runs. The ID can be useful for linking runs across different systems or for implementing custom tracking logic. This can be done by setting a `run_id` in the `RunnableConfig` object at construction or by passing a `run_id` in the invocation parameters.

This feature is not currently supported directly for LLM objects.

import uuid

my_uuid = uuid.uuid4()

# You can configure the run ID at invocation time:
chain.invoke({"input": "What is the meaning of life?"}, {"run_id": my_uuid})

Note that if you do this at the **root** of a trace (i.e., the top-level run, that run ID will be used as the `trace_id`).

## ​ Access run (span) ID for LangChain invocations

When you invoke a LangChain object, you can manually specify the run ID of the invocation. This run ID can be used to query the run in LangSmith.In JS/TS, you can use a `RunCollectorCallbackHandler` instance to access the run ID.

prompt = ChatPromptTemplate.from_messages([\
("system", "You are a helpful assistant. Please respond to the user's request only based on the given context."),\
("user", "Question: {question}\n\nContext: {context}")\
])
model = ChatOpenAI(model="gpt-4o-mini")
output_parser = StrOutputParser()

chain = prompt | model | output_parser

question = "Can you summarize this morning's meetings?"
context = "During this morning's meeting, we solved all world conflict."
my_uuid = uuid.uuid4()
result = chain.invoke({"question": question, "context": context}, {"run_id": my_uuid})
print(my_uuid)

## ​ Ensure all traces are submitted before exiting

In LangChain Python, LangSmith’s tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes.You can make callbacks synchronous by setting the `LANGCHAIN_CALLBACKS_BACKGROUND` environment variable to `"false"`.For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application. Below is an example:

from langchain_openai import ChatOpenAI
from langchain_core.tracers.langchain import wait_for_all_tracers

llm = ChatOpenAI()

try:
llm.invoke("Hello, World!")
finally:
wait_for_all_tracers()

## ​ Trace without setting environment variables

As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:

- `LANGSMITH_TRACING`
- `LANGSMITH_API_KEY`
- `LANGSMITH_ENDPOINT`
- `LANGSMITH_PROJECT`

However, in some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.This largely builds off of the previous section.

# You can create a client instance with an api key and api url
client = ls.Client(
api_key="YOUR_API_KEY", # This can be retrieved from a secrets manager
api_url="https://api.smith.langchain.com", # Update appropriately for self-hosted installations or the EU region
)

# You can pass the client and project_name to the tracing_context
with ls.tracing_context(client=client, project_name="test-no-env", enabled=True):
chain.invoke({"question": "Am I using a callback?", "context": "I'm using a callback"})

## ​ Distributed tracing with LangChain (Python)

LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications. The principles are similar to the distributed tracing guide for the LangSmith SDK.

import langsmith
from langchain_core.runnables import chain
from langsmith.run_helpers import get_current_run_tree

# -- This code should be in a separate file or service --
@chain
def child_chain(inputs):
return inputs["test"] + 1

def child_wrapper(x, headers):
with langsmith.tracing_context(parent=headers):
child_chain.invoke({"test": x})

@chain
def parent_chain(inputs):
rt = get_current_run_tree()
headers = rt.to_headers()
# ... make a request to another service with the headers
# The headers should be passed to the other service, eventually to the child_wrapper function

parent_chain.invoke({"test": 1})

## ​ Interoperability between LangChain (Python) and LangSmith SDK

If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly.LangChain objects will be traced when invoked within a `traceable` function and be bound as a child run of the `traceable` function.

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langsmith import traceable

# The above chain will be traced as a child run of the traceable function
@traceable(
tags=["openai", "chat"],
metadata={"foo": "bar"}
)
def invoke_runnnable(question, context):
result = chain.invoke({"question": question, "context": context})
return "The response is: " + result

invoke_runnnable("Can you summarize this morning's meetings?", "During this morning's meeting, we solved all world conflict.")

This will produce the following trace tree: !Trace tree python interop

## ​ Interoperability between LangChain.JS and LangSmith SDK

### ​ Tracing LangChain objects inside `traceable` (JS only)

Starting with `langchain@0.2.x`, LangChain objects are traced automatically when used inside `@traceable` functions, inheriting the client, tags, metadata and project name of the traceable function.For older versions of LangChain below `0.2.x`, you will need to manually pass an instance `LangChainTracer` created from the tracing context found in `@traceable`.

import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { getLangchainCallbacks } from "langsmith/langchain";

const prompt = ChatPromptTemplate.fromMessages([\
[\
"system",\
"You are a helpful assistant. Please respond to the user's request only based on the given context.",\
],\
["user", "Question: {question}\nContext: {context}"],\
]);

const model = new ChatOpenAI({ modelName: "gpt-4o-mini" });
const outputParser = new StringOutputParser();
const chain = prompt.pipe(model).pipe(outputParser);

const main = traceable(

const callbacks = await getLangchainCallbacks();
const response = await chain.invoke(input, { callbacks });
return response;
},
{ name: "main" }
);

### ​ Tracing LangChain child runs via `traceable` / RunTree API (JS only)

We’re working on improving the interoperability between `traceable` and LangChain. The following limitations are present when using combining LangChain with `traceable`:

1. Mutating RunTree obtained from `getCurrentRunTree()` of the RunnableLambda context will result in a no-op.
2. It’s discouraged to traverse the RunTree obtained from RunnableLambda via `getCurrentRunTree()` as it may not contain all the RunTree nodes.
3. Different child runs may have the same `execution_order` and `child_execution_order` value. Thus in extreme circumstances, some runs may end up in a different order, depending on the `start_time`.

In some uses cases, you might want to run `traceable` functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the `RunTree` API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke `traceable`-wrapped functions within RunnableLambda.

import { traceable } from "langsmith/traceable";
import { RunnableLambda } from "@langchain/core/runnables";
import { RunnableConfig } from "@langchain/core/runnables";

name: "Child Run",
});

const parrot = new RunnableLambda({

return await tracedChild(input.text);
},
});

Traceable

Run Tree

// Pass the config to existing traceable function
await tracedChild(config, input.text);
return input.text;
},
});

If you prefer a video tutorial, check out the Alternative Ways to Trace video from the Introduction to LangSmith Course.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Integrations\\
\\
Previous Trace with LangGraph\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/changelog-py

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Releases

Changelog

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

FiltersClear

langchainintegrationslanggraph

**Subscribe**: Our changelog includes an RSS feed that can integrate with Slack, email, Discord bots like Readybot or RSS Feeds to Discord Bot, and other subscription tools.

​

Dec 15, 2025

langchainintegrations

## ​ `langchain` v1.2.0

- `create_agent`: Simplified support for provider-specific tool parameters and definitions via a new `extras` attribute on tools. Examples:

- Provider-specific configuration such as Anthropic’s programmatic tool calling and tool search.
- Built-in tools that are executed client-side, as supported by Anthropic, OpenAI, and other providers.
- Support for strict schema-adherence in agent `response_format` (see `ProviderStrategy` docs).

Dec 8, 2025

## ​ `langchain-google-genai` v4.0.0

We’ve re-written the Google GenAI integration to use Google’s consolidated Generative AI SDK, which provides access to the Gemini API and Vertex AI Platform under the same interface. This includes minimal breaking changes as well as deprecated packages in `langchain-google-vertexai`.See the full release notes and migration guide for details.

Nov 25, 2025

langchain

## ​ `langchain` v1.1.0

- Model profiles: Chat models now expose supported features and capabilities through a `.profile` attribute. These data are derived from models.dev, an open source project providing model capability data.
- Summarization middleware: Updated to support flexible trigger points using model profiles for context-aware summarization.
- Structured output: `ProviderStrategy` support (native structured output) can now be inferred from model profiles.
- `SystemMessage` for `create_agent`: Support for passing `SystemMessage` instances directly to `create_agent`’s `system_prompt` parameter, enabling advanced features like cache control and structured content blocks.
- Model retry middleware: New middleware for automatically retrying failed model calls with configurable exponential backoff.
- Content moderation middleware: OpenAI content moderation middleware for detecting and handling unsafe content in agent interactions. Supports checking user input, model output, and tool results.

Oct 20, 2025

langchainlanggraph

## ​ v1.0.0

### ​ `langchain`

- Release notes
- Migration guide

### ​ `langgraph`

If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content and API reference.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Versioning\\
\\
Previous What's new in LangChain v1\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/trace-with-langchain).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Trace with LangGraph LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/short-term-memory).Notice

Skip to main content.Notice#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/changelog-py)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/graph-api

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Graph API

Graph API overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Choosing APIs
- Graph API
- Use the graph API
- Functional API

- Runtime

On this page

- Graphs
- StateGraph
- Compiling your graph
- State
- Schema
- Multiple schemas
- Reducers
- Default Reducer
- Overwrite
- Working with Messages in Graph State
- Why use messages?
- Using Messages in your Graph
- Serialization
- MessagesState
- Nodes
- START Node
- END Node
- Node Caching
- Edges
- Normal Edges
- Conditional Edges
- Entry point
- Conditional entry point
- Send
- Command
- When should I use Command instead of conditional edges?
- Navigating to a node in a parent graph
- Using inside tools
- Human-in-the-loop
- Graph migrations
- Runtime context
- Recursion limit
- Accessing and handling the recursion counter
- How it works
- Accessing the current step counter
- Proactive recursion handling
- Proactive vs reactive approaches
- Other available metadata
- Visualization

## ​ Graphs

At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:

1. `State`: A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.
2. `Nodes`: Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.
3. `Edges`: Functions that determine which `Node` to execute next based on the current state. They can be conditional branches or fixed transitions.

By composing `Nodes` and `Edges`, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state.To emphasize: `Nodes` and `Edges` are nothing more than functions – they can contain an LLM or just good ol’ code.In short: _nodes do the work, edges tell what to do next_.LangGraph’s underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google’s Pregel system, the program proceeds in discrete “super-steps.”A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an `inactive` state. A node becomes `active` when it receives a new message (state) on any of its incoming edges (or “channels”). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to `halt` by marking themselves as `inactive`. The graph execution terminates when all nodes are `inactive` and no messages are in transit.

### ​ StateGraph

The `StateGraph` class is the main graph class to use. This is parameterized by a user defined `State` object.

### ​ Compiling your graph

To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed?Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the `.compile` method:

Copy

graph = graph_builder.compile(...)

You **MUST** compile your graph before you can use it.

## ​ State

The first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the schema of the graph as well as `reducer` functions which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and can be either a `TypedDict` or a `Pydantic` model. All `Nodes` will emit updates to the `State` which are then applied using the specified `reducer` function.

### ​ Schema

The main documented way to specify the schema of a graph is by using a `TypedDict`. If you want to provide default values in your state, use a `dataclass`. We also support using a Pydantic `BaseModel` as your graph state if you want recursive data validation (though note that Pydantic is less performant than a `TypedDict` or `dataclass`).By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the guide for more information.

#### ​ Multiple schemas

Typically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:

- Internal nodes can pass information that is not required in the graph’s input / output.
- We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.

It is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, `PrivateState`.It is also possible to define explicit input and output schemas for a graph. In these cases, we define an “internal” schema that contains _all_ keys relevant to graph operations. But, we also define `input` and `output` schemas that are sub-sets of the “internal” schema to constrain the input and output of the graph. See this guide for more detail.Let’s look at an example:

class InputState(TypedDict):
user_input: str

class OutputState(TypedDict):
graph_output: str

class OverallState(TypedDict):
foo: str
user_input: str
graph_output: str

class PrivateState(TypedDict):
bar: str

# Write to OverallState
return {"foo": state["user_input"] + " name"}

# Read from OverallState, write to PrivateState
return {"bar": state["foo"] + " is"}

# Read from PrivateState, write to OutputState
return {"graph_output": state["bar"] + " Lance"}

builder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)
builder.add_node("node_1", node_1)
builder.add_node("node_2", node_2)
builder.add_node("node_3", node_3)
builder.add_edge(START, "node_1")
builder.add_edge("node_1", "node_2")
builder.add_edge("node_2", "node_3")
builder.add_edge("node_3", END)

graph = builder.compile()
graph.invoke({"user_input":"My"})
# {'graph_output': 'My name is Lance'}

There are two subtle and important points to note here:

1. We pass `state: InputState` as the input schema to `node_1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node _can write to any state channel in the graph state._ The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`.
2. We initialize the graph with:

StateGraph(
OverallState,
input_schema=InputState,
output_schema=OutputState
)

So, how can we write to `PrivateState` in `node_2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization?We can do this because `_nodes` can also declare additional state `channels_` as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it.

### ​ Reducers

Reducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:

#### ​ Default Reducer

These two examples show how to use the default reducer:

Example A

from typing_extensions import TypedDict

class State(TypedDict):
foo: int
bar: list[str]

In this example, no reducer functions are specified for any key. Let’s assume the input to the graph is:`{"foo": 1, "bar": ["hi"]}`. Let’s then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["bye"]}`

Example B

from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
foo: int
bar: Annotated[list[str], add]

In this example, we’ve used the `Annotated` type to specify a reducer function (`operator.add`) for the second key (`bar`). Note that the first key remains unchanged. Let’s assume the input to the graph is `{"foo": 1, "bar": ["hi"]}`. Let’s then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["hi", "bye"]}`. Notice here that the `bar` key is updated by adding the two lists together.

#### ​ Overwrite

In some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the `Overwrite` type for this purpose. Learn how to use `Overwrite` here.

#### ​ Why use messages?

Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain’s chat model interface in particular accepts a list of message objects as inputs. These messages come in a variety of forms such as `HumanMessage` (user input) or `AIMessage` (LLM response).To read more about what message objects are, please refer to the Messages conceptual guide.

#### ​ Using Messages in your Graph

In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don’t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use `operator.add` as a reducer.However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use `operator.add`, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt `add_messages` function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.

#### ​ Serialization

In addition to keeping track of message IDs, the `add_messages` function will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel.See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:

# this is supported
{"messages": [HumanMessage(content="message")]}

# and this is also supported
{"messages": [{"type": "human", "content": "message"}]}

Since the state updates are always deserialized into LangChain `Messages` when using `add_messages`, you should use dot notation to access message attributes, like `state["messages"][-1].content`.Below is an example of a graph that uses `add_messages` as its reducer function.

from langchain.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated
from typing_extensions import TypedDict

class GraphState(TypedDict):
messages: Annotated[list[AnyMessage], add_messages]

#### ​ MessagesState

Since having a list of messages in your state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages. `MessagesState` is defined with a single `messages` key which is a list of `AnyMessage` objects and uses the `add_messages` reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:

from langgraph.graph import MessagesState

class State(MessagesState):
documents: list[str]

## ​ Nodes

In LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:

1. `state` – The state of the graph
2. `config` – A `RunnableConfig` object that contains configuration information like `thread_id` and tracing information like `tags`
3. `runtime` – A `Runtime` object that contains runtime `context` and other information like `store` and `stream_writer`

Similar to `NetworkX`, you add these nodes to a graph using the `add_node` method:

from dataclasses import dataclass
from typing_extensions import TypedDict

from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph
from langgraph.runtime import Runtime

class State(TypedDict):
input: str
results: str

@dataclass
class Context:
user_id: str

builder = StateGraph(State)

def plain_node(state: State):
return state

def node_with_runtime(state: State, runtime: Runtime[Context]):
print("In node: ", runtime.context.user_id)
return {"results": f"Hello, {state['input']}!"}

def node_with_config(state: State, config: RunnableConfig):
print("In node with thread_id: ", config["configurable"]["thread_id"])
return {"results": f"Hello, {state['input']}!"}

builder.add_node("plain_node", plain_node)
builder.add_node("node_with_runtime", node_with_runtime)
builder.add_node("node_with_config", node_with_config)
...

Behind the scenes, functions are converted to `RunnableLambda`, which add batch and async support to your function, along with native tracing and debugging.If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.

builder.add_node(my_node)
# You can then create edges to/from this node by referencing it as `"my_node"`

### ​ `START` Node

The `START` Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.

from langgraph.graph import START

graph.add_edge(START, "node_a")

### ​ `END` Node

The `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.

from langgraph.graph import END

graph.add_edge("node_a", END)

### ​ Node Caching

LangGraph supports caching of tasks/nodes based on the input to the node. To use caching:

- Specify a cache when compiling a graph (or specifying an entrypoint)
- Specify a cache policy for nodes. Each cache policy supports:
- `key_func` used to generate a cache key based on the input to a node, which defaults to a `hash` of the input with pickle.
- `ttl`, the time to live for the cache in seconds. If not specified, the cache will never expire.

For example:

import time
from typing_extensions import TypedDict
from langgraph.graph import StateGraph
from langgraph.cache.memory import InMemoryCache
from langgraph.types import CachePolicy

class State(TypedDict):
x: int
result: int

# expensive computation
time.sleep(2)
return {"result": state["x"] * 2}

builder.add_node("expensive_node", expensive_node, cache_policy=CachePolicy(ttl=3))
builder.set_entry_point("expensive_node")
builder.set_finish_point("expensive_node")

graph = builder.compile(cache=InMemoryCache())

print(graph.invoke({"x": 5}, stream_mode='updates'))
# [{'expensive_node': {'result': 10}}]
# [{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}]

1. First run takes two seconds to run (due to mocked expensive computation).
2. Second run utilizes cache and returns quickly.

## ​ Edges

Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:

- Normal Edges: Go directly from one node to the next.
- Conditional Edges: Call a function to determine which node(s) to go to next.
- Entry Point: Which node to call first when user input arrives.
- Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.

A node can have multiple outgoing edges. If a node has multiple outgoing edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep.

### ​ Normal Edges

If you **always** want to go from node A to node B, you can use the `add_edge` method directly.

graph.add_edge("node_a", "node_b")

### ​ Conditional Edges

If you want to **optionally** route to one or more edges (or optionally terminate), you can use the `add_conditional_edges` method. This method accepts the name of a node and a “routing function” to call after that node is executed:

graph.add_conditional_edges("node_a", routing_function)

Similar to nodes, the `routing_function` accepts the current `state` of the graph and returns a value.By default, the return value `routing_function` is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.You can optionally provide a dictionary that maps the `routing_function`’s output to the name of the next node.

graph.add_conditional_edges("node_a", routing_function, {True: "node_b", False: "node_c"})

Use `Command` instead of conditional edges if you want to combine state updates and routing in a single function.

### ​ Entry point

The entry point is the first node(s) that are run when the graph starts. You can use the `add_edge` method from the virtual `START` node to the first node to execute to specify where to enter the graph.

### ​ Conditional entry point

A conditional entry point lets you start at different nodes depending on custom logic. You can use `add_conditional_edges` from the virtual `START` node to accomplish this.

graph.add_conditional_edges(START, routing_function)

You can optionally provide a dictionary that maps the `routing_function`’s output to the name of the next node.

graph.add_conditional_edges(START, routing_function, {True: "node_b", False: "node_c"})

## ​ `Send`

By default, `Nodes` and `Edges` are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of `State` to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input `State` to the downstream `Node` should be different (one for each generated object).To support this design pattern, LangGraph supports returning `Send` objects from conditional edges. `Send` takes two arguments: first is the name of the node, and second is the state to pass to that node.

def continue_to_jokes(state: OverallState):
return [Send("generate_joke", {"subject": s}) for s in state['subjects']]

graph.add_conditional_edges("node_a", continue_to_jokes)

## ​ `Command`

It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a `Command` object from node functions:

return Command(
# state update
update={"foo": "bar"},
# control flow
goto="my_other_node"
)

With `Command` you can also achieve dynamic control flow behavior (identical to conditional edges):

if state["foo"] == "bar":
return Command(update={"foo": "baz"}, goto="my_other_node")

When returning `Command` in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. `Command[Literal["my_other_node"]]`. This is necessary for the graph rendering and tells LangGraph that `my_node` can navigate to `my_other_node`.

Check out this how-to guide for an end-to-end example of how to use `Command`.

### ​ When should I use Command instead of conditional edges?

- Use `Command` when you need to **both** update the graph state **and** route to a different node. For example, when implementing multi-agent handoffs where it’s important to route to a different agent and pass some information to that agent.
- Use conditional edges to route between nodes conditionally without updating the state.

### ​ Navigating to a node in a parent graph

If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:

return Command(
update={"foo": "bar"},
goto="other_subgraph", # where `other_subgraph` is a node in the parent graph
graph=Command.PARENT
)

Setting `graph` to `Command.PARENT` will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key that’s shared by both parent and subgraph state schemas, you **must** define a reducer for the key you’re updating in the parent graph state. See this example.

This is particularly useful when implementing multi-agent handoffs.Check out this guide for detail.

### ​ Using inside tools

A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.Refer to this guide for detail.

### ​ Human-in-the-loop

`Command` is an important part of human-in-the-loop workflows: when using `interrupt()` to collect user input, `Command` is then used to supply the input and resume execution via `Command(resume="User input")`. Check out this conceptual guide for more information.

## ​ Graph migrations

LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.

- For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)
- For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) — if this is a blocker please reach out and we can prioritize a solution.
- For modifying state, we have full backwards and forwards compatibility for adding and removing keys
- State keys that are renamed lose their saved state in existing threads
- State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change — if this is a blocker please reach out and we can prioritize a solution.

## ​ Runtime context

When creating a graph, you can specify a `context_schema` for runtime context passed to nodes. This is useful for passing
information to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.

@dataclass
class ContextSchema:
llm_provider: str = "openai"

graph = StateGraph(State, context_schema=ContextSchema)

You can then pass this context into the graph using the `context` parameter of the `invoke` method.

graph.invoke(inputs, context={"llm_provider": "anthropic"})

You can then access and use this context inside a node or conditional edge:

from langgraph.runtime import Runtime

def node_a(state: State, runtime: Runtime[ContextSchema]):
llm = get_llm(runtime.context.llm_provider)
# ...

See this guide for a full breakdown on configuration.

### ​ Recursion limit

The recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raise `GraphRecursionError`. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to `invoke`/`stream` via the config dictionary. Importantly, `recursion_limit` is a standalone `config` key and should not be passed inside the `configurable` key as all other user-defined configuration. See the example below:

graph.invoke(inputs, config={"recursion_limit": 5}, context={"llm": "anthropic"})

Read this how-to to learn more about how the recursion limit works.

### ​ Accessing and handling the recursion counter

The current step counter is accessible in `config["metadata"]["langgraph_step"]` within any node, allowing for proactive recursion handling before hitting the recursion limit. This enables you to implement graceful degradation strategies within your graph logic.

#### ​ Accessing the current step counter

You can access the current step counter within any node to monitor execution progress.

from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

current_step = config["metadata"]["langgraph_step"]
print(f"Currently on step: {current_step}")
return state

#### ​ Proactive recursion handling

LangGraph provides a `RemainingSteps` managed value that tracks how many steps remain before hitting the recursion limit. This allows for graceful degradation within your graph.

from typing import Annotated, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.managed import RemainingSteps

class State(TypedDict):
messages: Annotated[list, lambda x, y: x + y]
remaining_steps: RemainingSteps # Managed value - tracks steps until limit

# RemainingSteps is automatically populated by LangGraph
remaining = state["remaining_steps"]

# Check if we're running low on steps
if remaining <= 2:
return {"messages": ["Approaching limit, wrapping up..."]}

# Normal processing
return {"messages": ["thinking..."]}

"""Route based on remaining steps"""
if state["remaining_steps"] <= 2:
return "fallback_node"
return "reasoning_node"

"""Handle cases where recursion limit is approaching"""
return {"messages": ["Reached complexity limit, providing best effort answer"]}

# Build graph
builder = StateGraph(State)
builder.add_node("reasoning_node", reasoning_node)
builder.add_node("fallback_node", fallback_node)
builder.add_edge(START, "reasoning_node")
builder.add_conditional_edges("reasoning_node", route_decision)
builder.add_edge("fallback_node", END)

graph = builder.compile()

# RemainingSteps works with any recursion_limit
result = graph.invoke({"messages": []}, {"recursion_limit": 10})

#### ​ Proactive vs reactive approaches

There are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).

from typing import Annotated, Literal, TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.managed import RemainingSteps
from langgraph.errors import GraphRecursionError

class State(TypedDict):
messages: Annotated[list, lambda x, y: x + y]
remaining_steps: RemainingSteps

# Proactive Approach (recommended) - using RemainingSteps

"""Proactively monitor and handle recursion within the graph"""
remaining = state["remaining_steps"]

# Early detection - route to internal handling
if remaining <= 2:
return {
"messages": ["Approaching limit, returning partial result"]
}

return {"messages": [f"Processing... ({remaining} steps remaining)"]}

if state["remaining_steps"] <= 2:
return END
return "agent"

builder = StateGraph(State)
builder.add_node("agent", agent_with_monitoring)
builder.add_edge(START, "agent")
builder.add_conditional_edges("agent", route_decision)
graph = builder.compile()

# Proactive: Graph completes gracefully

# Reactive Approach (fallback) - catching error externally
try:
result = graph.invoke({"messages": []}, {"recursion_limit": 10})
except GraphRecursionError as e:
# Handle externally after graph execution fails
result = {"messages": ["Fallback: recursion limit exceeded"]}

The key differences between these approaches are:

| Approach | Detection | Handling | Control Flow |
| --- | --- | --- | --- |
| Proactive (using `RemainingSteps`) | Before limit reached | Inside graph via conditional routing | Graph continues to completion node |
| Reactive (catching `GraphRecursionError`) | After limit exceeded | Outside graph in try/catch | Graph execution terminated |

**Proactive advantages:**

- Graceful degradation within the graph
- Can save intermediate state in checkpoints
- Better user experience with partial results
- Graph completes normally (no exception)

**Reactive advantages:**

- Simpler implementation
- No need to modify graph logic
- Centralized error handling

#### ​ Other available metadata

Along with `langgraph_step`, the following metadata is also available in `config["metadata"]`:

metadata = config["metadata"]

print(f"Step: {metadata['langgraph_step']}")
print(f"Node: {metadata['langgraph_node']}")
print(f"Triggers: {metadata['langgraph_triggers']}")
print(f"Path: {metadata['langgraph_path']}")
print(f"Checkpoint NS: {metadata['langgraph_checkpoint_ns']}")

return state

## ​ Visualization

It’s often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See this how-to guide for more info.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Choosing between Graph and Functional APIs\\
\\
Previous Use the graph API\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/functional-api

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Functional API

Functional API overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Functional API
- Use the Functional API
- Runtime

On this page

- Functional API vs. Graph API
- Example
- Entrypoint
- Definition
- Injectable parameters
- Executing
- Resuming
- Short-term memory
- entrypoint.final
- Task
- Definition
- Execution
- When to use a task
- Serialization
- Determinism
- Idempotency
- Common Pitfalls
- Handling side effects
- Non-deterministic control flow

The **Functional API** allows you to add LangGraph’s key features — persistence, memory, human-in-the-loop, and streaming — to your applications with minimal changes to your existing code.It is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as `if` statements, `for` loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.The Functional API uses two key building blocks:

- **`@entrypoint`** – Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.
- **`@task`** – Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.

This provides a minimal abstraction for building workflows with state management and streaming.

For information on how to use the functional API, see Use Functional API.

## ​ Functional API vs. Graph API

For users who prefer a more declarative approach, LangGraph’s Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.Here are some key differences:

- **Control flow**: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.
- **Short-term memory**: The **GraphAPI** requires declaring a **State** and may require defining **reducers** to manage updates to the graph state. `@entrypoint` and `@tasks` do not require explicit state management as their state is scoped to the function and is not shared across functions.
- **Checkpointing**: Both APIs generate and use checkpoints. In the **Graph API** a new checkpoint is generated after every superstep. In the **Functional API**, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.
- **Visualization**: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.

## ​ Example

Below we demonstrate a simple application that writes an essay and interrupts to request human review.

Copy

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.func import entrypoint, task
from langgraph.types import interrupt

@task

"""Write an essay about the given topic."""
time.sleep(1) # A placeholder for a long-running task.
return f"An essay about topic: {topic}"

@entrypoint(checkpointer=InMemorySaver())

"""A simple workflow that writes an essay and asks for a review."""
essay = write_essay("cat").result()
is_approved = interrupt({
# Any json-serializable payload provided to interrupt as argument.
# It will be surfaced on the client side as an Interrupt when streaming data
# from the workflow.
"essay": essay, # The essay we want reviewed.
# We can add any additional information that we need.
# For example, introduce a key called "action" with some instructions.
"action": "Please approve/reject the essay",
})

return {
"essay": essay, # The essay that was generated
"is_approved": is_approved, # Response from HIL
}

Detailed Explanation

This workflow will write an essay about the topic “cat” and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.When the workflow is resumed, it executes from the very start, but because the result of the `writeEssay` task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.

import time
import uuid
from langgraph.func import entrypoint, task
from langgraph.types import interrupt
from langgraph.checkpoint.memory import InMemorySaver

"""Write an essay about the given topic."""
time.sleep(1) # This is a placeholder for a long-running task.
return f"An essay about topic: {topic}"

"""A simple workflow that writes an essay and asks for a review."""
essay = write_essay("cat").result()
is_approved = interrupt(
{
"essay": essay, # The essay we want reviewed.
"action": "Please approve/reject the essay",
}
)
return {
"essay": essay, # The essay that was generated
"is_approved": is_approved, # Response from HIL
}

thread_id = str(uuid.uuid4())
config = {"configurable": {"thread_id": thread_id}}
for item in workflow.stream("cat", config):
print(item)

An essay has been written and is ready for review. Once the review is provided, we can resume the workflow:

from langgraph.types import Command

# Get review from a user (e.g., via a UI)
# In this case, we're using a bool, but this can be any json-serializable value.
human_review = True

for item in workflow.stream(Command(resume=human_review), config):
print(item)

{'workflow': {'essay': 'An essay about topic: cat', 'is_approved': False}}

The workflow has been completed and the review has been added to the essay.

## ​ Entrypoint

The `@entrypoint` decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling _long-running tasks_ and interrupts.

### ​ Definition

An **entrypoint** is defined by decorating a function with the `@entrypoint` decorator.The function **must accept a single positional argument**, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.Decorating a function with an `entrypoint` produces a `Pregel` instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).You will usually want to pass a **checkpointer** to the `@entrypoint` decorator to enable persistence and use features like **human-in-the-loop**.

- Sync

- Async

from langgraph.func import entrypoint

@entrypoint(checkpointer=checkpointer)

# some logic that may involve long-running tasks like API calls,
# and may be interrupted for human-in-the-loop.
...
return result

# and may be interrupted for human-in-the-loop

**Serialization**
The **inputs** and **outputs** of entrypoints must be JSON-serializable to support checkpointing. Please see the serialization section for more details.

### ​ Injectable parameters

When declaring an `entrypoint`, you can request access to additional parameters that will be injected automatically at run time. These parameters include:

| Parameter | Description |
| --- | --- |
| **previous** | Access the state associated with the previous `checkpoint` for the given thread. See short-term-memory. |
| **store** | An instance of \[BaseStore\]\[langgraph.store.base.BaseStore\]. Useful for long-term memory. |
| **writer** | Use to access the StreamWriter when working with Async Python < 3.11. See streaming with functional API for details. |
| **config** | For accessing run time configuration. See RunnableConfig for information. |

Declare the parameters with the appropriate name and type annotation.

Requesting Injectable Parameters

from langchain_core.runnables import RunnableConfig
from langgraph.func import entrypoint
from langgraph.store.base import BaseStore
from langgraph.store.memory import InMemoryStore

in_memory_store = InMemoryStore(...) # An instance of InMemoryStore for long-term memory

@entrypoint(
checkpointer=checkpointer, # Specify the checkpointer
store=in_memory_store # Specify the store
)
def my_workflow(
some_input: dict, # The input (e.g., passed via `invoke`)
*,
previous: Any = None, # For short-term memory
store: BaseStore, # For long-term memory
writer: StreamWriter, # For streaming custom data
config: RunnableConfig # For accessing the configuration passed to the entrypoint

### ​ Executing

Using the `@entrypoint` yields a `Pregel` object that can be executed using the `invoke`, `ainvoke`, `stream`, and `astream` methods.

- Invoke

- Async Invoke

- Stream

- Async Stream

config = {
"configurable": {
"thread_id": "some_thread_id"
}
}
my_workflow.invoke(some_input, config) # Wait for the result synchronously

config = {
"configurable": {
"thread_id": "some_thread_id"
}
}
await my_workflow.ainvoke(some_input, config) # Await result asynchronously

config = {
"configurable": {
"thread_id": "some_thread_id"
}
}

for chunk in my_workflow.stream(some_input, config):
print(chunk)

async for chunk in my_workflow.astream(some_input, config):
print(chunk)

### ​ Resuming

Resuming an execution after an interrupt can be done by passing a **resume** value to the `Command` primitive.

my_workflow.invoke(Command(resume=some_resume_value), config)

await my_workflow.ainvoke(Command(resume=some_resume_value), config)

for chunk in my_workflow.stream(Command(resume=some_resume_value), config):
print(chunk)

async for chunk in my_workflow.astream(Command(resume=some_resume_value), config):
print(chunk)

**Resuming after an error**To resume after an error, run the `entrypoint` with a `None` and the same **thread id** (config).This assumes that the underlying **error** has been resolved and execution can proceed successfully.

my_workflow.invoke(None, config)

await my_workflow.ainvoke(None, config)

for chunk in my_workflow.stream(None, config):
print(chunk)

async for chunk in my_workflow.astream(None, config):
print(chunk)

### ​ Short-term memory

When an `entrypoint` is defined with a `checkpointer`, it stores information between successive invocations on the same **thread id** in checkpoints.This allows accessing the state from the previous invocation using the `previous` parameter.By default, the `previous` parameter is the return value of the previous invocation.

previous = previous or 0
return number + previous

my_workflow.invoke(1, config) # 1 (previous was None)
my_workflow.invoke(2, config) # 3 (previous was 1 from the previous invocation)

#### ​ `entrypoint.final`

`entrypoint.final` is a special primitive that can be returned from an entrypoint and allows **decoupling** the value that is **saved in the checkpoint** from the **return value of the entrypoint**.The first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is `entrypoint.final[return_type, save_type]`.

previous = previous or 0
# This will return the previous value to the caller, saving
# 2 * number to the checkpoint, which will be used in the next invocation
# for the `previous` parameter.
return entrypoint.final(value=previous, save=2 * number)

config = {
"configurable": {
"thread_id": "1"
}
}

my_workflow.invoke(3, config) # 0 (previous was None)
my_workflow.invoke(1, config) # 6 (previous was 3 * 2 from the previous invocation)

## ​ Task

A **task** represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:

- **Asynchronous Execution**: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.
- **Checkpointing**: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details).

### ​ Definition

Tasks are defined using the `@task` decorator, which wraps a regular Python function.

from langgraph.func import task

@task()
def slow_computation(input_value):
# Simulate a long-running operation

**Serialization**
The **outputs** of tasks must be JSON-serializable to support checkpointing.

### ​ Execution

**Tasks** can only be called from within an **entrypoint**, another **task**, or a state graph node.Tasks _cannot_ be called directly from the main application code.When you call a **task**, it returns _immediately_ with a future object. A future is a placeholder for a result that will be available later.To obtain the result of a **task**, you can either wait for it synchronously (using `result()`) or await it asynchronously (using `await`).

- Synchronous Invocation

- Asynchronous Invocation

future = slow_computation(some_input)
return future.result() # Wait for the result synchronously

return await slow_computation(some_input) # Await result asynchronously

## ​ When to use a task

**Tasks** are useful in the following scenarios:

- **Checkpointing**: When you need to save the result of a long-running operation to a checkpoint, so you don’t need to recompute it when resuming the workflow.
- **Human-in-the-loop**: If you’re building a workflow that requires human intervention, you MUST use **tasks** to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details.
- **Parallel Execution**: For I/O-bound tasks, **tasks** enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).
- **Observability**: Wrapping operations in **tasks** provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.
- **Retryable Work**: When work needs to be retried to handle failures or inconsistencies, **tasks** provide a way to encapsulate and manage the retry logic.

## ​ Serialization

There are two key aspects to serialization in LangGraph:

1. `entrypoint` inputs and outputs must be JSON-serializable.
2. `task` outputs must be JSON-serializable.

These requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable.Serialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.Providing non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.

## ​ Determinism

To utilize features like **human-in-the-loop**, any randomness should be encapsulated inside of **tasks**. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same _sequence of steps_, even if **task** results are non-deterministic.LangGraph achieves this behavior by persisting **task** and **subgraph** results as they execute. A well-designed workflow ensures that resuming execution follows the _same sequence of steps_, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running **tasks** or **tasks** with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.While different runs of a workflow can produce different results, resuming a **specific** run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up **task** and **subgraph** results that were executed prior to the graph being interrupted and avoid recomputing them.

## ​ Idempotency

Idempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside **tasks** functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a **task** starts, but does not complete successfully. Then, if the workflow is resumed, the **task** will run again. Use idempotency keys or verify existing results to avoid duplication.

## ​ Common Pitfalls

### ​ Handling side effects

Encapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.

- Incorrect

- Correct

In this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.

# This code will be executed a second time when resuming the workflow.
# Which is likely not what you want.
with open("output.txt", "w") as f:
f.write("Side effect executed")
value = interrupt("question")
return value

In this example, the side effect is encapsulated in a task, ensuring consistent execution upon resumption.

@task
def write_to_file():
with open("output.txt", "w") as f:
f.write("Side effect executed")

# The side effect is now encapsulated in a task.
write_to_file().result()
value = interrupt("question")
return value

### ​ Non-deterministic control flow

Operations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.

- In a task: Get random number (5) → interrupt → resume → (returns 5 again) → …
- Not in a task: Get random number (5) → interrupt → resume → get new random number (7) → …

This is especially important when using **human-in-the-loop** workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it’s matched with the corresponding resume value. This matching is strictly **index-based**, so the order of the resume values should match the order of the interrupts.If order of execution is not maintained when resuming, one `interrupt` call may be matched with the wrong `resume` value, leading to incorrect results.Please read the section on determinism for more details.

In this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.

t0 = inputs["t0"]
t1 = time.time()

delta_t = t1 - t0

result = slow_task(1).result()
value = interrupt("question")
else:
result = slow_task(2).result()
value = interrupt("question")

return {
"result": result,
"value": value
}

In this example, the workflow uses the input `t0` to determine which task to execute. This is deterministic because the result of the workflow depends only on the input.

import time

return time.time()

t0 = inputs["t0"]
t1 = get_time().result()

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Use the graph API\\
\\
Previous Use the functional API\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/trace-with-langgraph

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Integrations

Trace with LangGraph

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Overview
- LangChain
- LangGraph
- Anthropic (Python only)
- OpenAI
- AutoGen
- Claude Agent SDK
- Claude Code
- CrewAI
- Google ADK
- Instructor (Python only)
- OpenAI Agents SDK
- OpenTelemetry
- PydanticAI
- Semantic Kernel
- Vercel AI SDK
- LiveKit
- Pipecat
- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- With LangChain
- 1\. Installation
- 2\. Configure your environment
- 3\. Log a trace
- Without LangChain
- 1\. Installation
- 2\. Configure your environment
- 3\. Log a trace

LangSmith smoothly integrates with LangGraph (Python and JS) to help you trace agents, whether you’re using LangChain modules or other SDKs.

## ​ With LangChain

If you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing.This guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide.

### ​ 1\. Installation

Install the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below).For a full list of packages available, see the LangChain Python docs and LangChain JS docs.

pip

yarn

npm

pnpm

Copy

pip install langchain_openai langgraph

### ​ 2\. Configure your environment

export LANGSMITH_TRACING=true

If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:`export LANGCHAIN_CALLBACKS_BACKGROUND=true`If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:`export LANGCHAIN_CALLBACKS_BACKGROUND=false`See this LangChain.js guide for more information.

### ​ 3\. Log a trace

Once you’ve set up your environment, you can call LangChain runnables as normal. LangSmith will infer the proper tracing config:

Python

TypeScript

from typing import Literal
from langchain.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langchain.tools import tool
from langgraph.prebuilt import ToolNode
from langgraph.graph import StateGraph, MessagesState

@tool
def search(query: str):
"""Call to surf the web."""
if "sf" in query.lower() or "san francisco" in query.lower():
return "It's 60 degrees and foggy."
return "It's 90 degrees and sunny."

tools = [search]
tool_node = ToolNode(tools)

model = ChatOpenAI(model="gpt-4o", temperature=0).bind_tools(tools)

messages = state['messages']
last_message = messages[-1]
if last_message.tool_calls:
return "tools"
return "__end__"

def call_model(state: MessagesState):
messages = state['messages']
# Invoking `model` will automatically infer the correct tracing context
response = model.invoke(messages)
return {"messages": [response]}

workflow = StateGraph(MessagesState)
workflow.add_node("agent", call_model)
workflow.add_node("tools", tool_node)
workflow.add_edge("__start__", "agent")
workflow.add_conditional_edges(
"agent",
should_continue,
)
workflow.add_edge("tools", 'agent')

app = workflow.compile()

final_state = app.invoke(
{"messages": [HumanMessage(content="what is the weather in sf")]},
config={"configurable": {"thread_id": 42}}
)

final_state["messages"][-1].content

An example trace from running the above code looks like this:!Trace tree for a LangGraph run with LangChain

## ​ Without LangChain

If you are using other SDKs or custom functions within LangGraph, you will need to wrap or decorate them appropriately (with the `@traceable` decorator in Python or the `traceable` function in JS, or something like e.g. `wrap_openai` for SDKs). If you do so, LangSmith will automatically nest traces from those wrapped methods.Here’s an example. You can also see this page for more information.

### ​ 1\. Installation

Install the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).

pip install openai langsmith langgraph

### ​ 2\. Configure your environment

### ​ 3\. Log a trace

Once you’ve set up your environment, wrap or decorate the custom functions/SDKs you want to trace. LangSmith will then infer the proper tracing config:

import json
import openai
import operator
from langsmith import traceable
from langsmith.wrappers import wrap_openai
from typing import Annotated, Literal, TypedDict
from langgraph.graph import StateGraph

class State(TypedDict):
messages: Annotated[list, operator.add]

tool_schema = {
"type": "function",
"function": {
"name": "search",
"description": "Call to surf the web.",
"parameters": {
"type": "object",
"properties": {"query": {"type": "string"}},
"required": ["query"],
},
},
}

# Decorating the tool function will automatically trace it with the correct context
@traceable(run_type="tool", name="Search Tool")
def search(query: str):
"""Call to surf the web."""
if "sf" in query.lower() or "san francisco" in query.lower():
return "It's 60 degrees and foggy."
return "It's 90 degrees and sunny."

tools = [search]

def call_tools(state):
function_name_to_function = {"search": search}
messages = state["messages"]
tool_call = messages[-1]["tool_calls"][0]
function_name = tool_call["function"]["name"]
function_arguments = tool_call["function"]["arguments"]
arguments = json.loads(function_arguments)
function_response = function_name_to_functionfunction_name
tool_message = {
"tool_call_id": tool_call["id"],
"role": "tool",
"name": function_name,
"content": function_response,
}
return {"messages": [tool_message]}

wrapped_client = wrap_openai(openai.Client())

messages = state["messages"]
last_message = messages[-1]
if last_message["tool_calls"]:
return "tools"
return "__end__"

def call_model(state: State):
messages = state["messages"]
# Calling the wrapped client will automatically infer the correct tracing context
response = wrapped_client.chat.completions.create(
messages=messages, model="gpt-4o-mini", tools=[tool_schema]
)
raw_tool_calls = response.choices[0].message.tool_calls
tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []
response_message = {
"role": "assistant",
"content": response.choices[0].message.content,
"tool_calls": tool_calls,
}
return {"messages": [response_message]}

workflow = StateGraph(State)
workflow.add_node("agent", call_model)
workflow.add_node("tools", call_tools)
workflow.add_edge("__start__", "agent")
workflow.add_conditional_edges(
"agent",
should_continue,
)
workflow.add_edge("tools", 'agent')

final_state = app.invoke(
{"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)

final_state["messages"][-1]["content"]

An example trace from running the above code looks like this:!Trace tree for a LangGraph run without LangChain

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Trace with LangChain (Python and JS/TS)\\
\\
Previous Trace with Anthropic\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/graph-api)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Use the graph API Choosing between Graph and Functional APIs LangSmith Deployment components

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/functional-api).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Use the functional API Functional API overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/trace-with-langgraph).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangGraph Trace with LangChain (Python and JS/TS) LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/mcp

```json
{
"server": {
"name": "Docs by LangChain",
"version": "1.0.0",
"transport": "http"
},
"capabilities": {
"tools": {
"SearchDocsByLangChain": {
"name": "SearchDocsByLangChain",
"description": "Search across the Docs by LangChain knowledge base to find relevant information, code examples, API references, and guides. Use this tool when you need to answer questions about Docs by LangChain, find specific documentation, understand how features work, or locate implementation details. The search returns contextual content with titles and direct links to the documentation pages.",
"inputSchema": {
"type": "object",
"properties": {
"query": {
"type": "string",
"description": "A query to search the content with."
}
},
"required": [
"query"
]
},
"operationId": "MintlifyDefaultSearch"
}
},
"resources": [],
"prompts": []
}
}

---

# https://docs.langchain.com/langsmith/interrupt-concurrent

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Double-texting

Interrupt concurrent

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Streaming API
- Human-in-the-loop using server API
- Time travel using the server API
- MCP endpoint in Agent Server
- A2A endpoint in Agent Server
- Distributed tracing
- Webhooks
- Double-texting

- Overview
- Interrupt
- Rollback
- Reject
- Enqueue
- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Setup
- Create runs
- View run results

This guide assumes knowledge of what double-texting is, which you can learn about in the double-texting conceptual guide.The guide covers the `interrupt` option for double texting, which interrupts the prior run of the graph and starts a new one with the double-text. This option does not delete the first run, but rather keeps it in the database but sets its status to `interrupted`. Below is a quick example of using the `interrupt` option.

## ​ Setup

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

- Javascript

- CURL

Copy

function prettyPrint(m) {
const padded = " " + m['type'] + " ";
const sepLen = Math.floor((80 - padded.length) / 2);
const sep = "=".repeat(sepLen);
const secondSep = sep + (padded.length % 2 ? "=" : "");

console.log(`${sep}${padded}${secondSep}`);
console.log("\n\n");
console.log(m.content);
}

# PLACE THIS IN A FILE CALLED pretty_print.sh
pretty_print() {
local type="$1"
local content="$2"
local padded=" $type "
local total_width=80
local sep_len=$(( (total_width - ${#padded}) / 2 ))
local sep=$(printf '=%.0s' $(eval "echo {1.."${sep_len}"}"))
local second_sep=$sep
if (( (total_width - ${#padded}) % 2 )); then
second_sep="${second_sep}="
fi

echo "${sep}${padded}${second_sep}"
echo
echo "$content"
}

Now, let’s import our required packages and instantiate our client, assistant, and thread.

- Python

import asyncio

from langchain_core.messages import convert_to_messages
from langgraph_sdk import get_client

# Using the graph deployed with the name "agent"
assistant_id = "agent"
thread = await client.threads.create()

import { Client } from "@langchain/langgraph-sdk";

// Using the graph deployed with the name "agent"
const assistantId = "agent";
const thread = await client.threads.create();

curl --request POST \

--header 'Content-Type: application/json' \
--data '{}'

## ​ Create runs

Now we can start our two runs and join the second one until it has completed:

# the first run will be interrupted
interrupted_run = await client.runs.create(
thread["thread_id"],
assistant_id,
input={"messages": [{"role": "user", "content": "what's the weather in sf?"}]},
)
# sleep a bit to get partial outputs from the first run
await asyncio.sleep(2)
run = await client.runs.create(
thread["thread_id"],
assistant_id,
input={"messages": [{"role": "user", "content": "what's the weather in nyc?"}]},
multitask_strategy="interrupt",
)
# wait until the second run completes
await client.runs.join(thread["thread_id"], run["run_id"])

// the first run will be interrupted
let interruptedRun = await client.runs.create(
thread["thread_id"],
assistantId,
{ input: { messages: [{ role: "human", content: "what's the weather in sf?" }] } }
);
// sleep a bit to get partial outputs from the first run

let run = await client.runs.create(
thread["thread_id"],
assistantId,
{
input: { messages: [{ role: "human", content: "what's the weather in nyc?" }] },
multitaskStrategy: "interrupt"
}
);

// wait until the second run completes
await client.runs.join(thread["thread_id"], run["run_id"]);

curl --request POST \
--url <DEPLOY<ENT_URL>>/threads/<THREAD_ID>/runs \
--header 'Content-Type: application/json' \
--data "{
\"assistant_id\": \"agent\",
\"input\": {\"messages\": [{\"role\": \"human\", \"content\": \"what\'s the weather in sf?\"}]},
}" && sleep 2 && curl --request POST \
--url <DEPLOY<ENT_URL>>/threads/<THREAD_ID>/runs \
--header 'Content-Type: application/json' \
--data "{
\"assistant_id\": \"agent\",
\"input\": {\"messages\": [{\"role\": \"human\", \"content\": \"what\'s the weather in nyc?\"}]},
\"multitask_strategy\": \"interrupt\"
}" && curl --request GET \

## ​ View run results

We can see that the thread has partial data from the first run + data from the second run

state = await client.threads.get_state(thread["thread_id"])

for m in convert_to_messages(state["values"]["messages"]):
m.pretty_print()

const state = await client.threads.getState(thread["thread_id"]);

for (const m of state['values']['messages']) {
prettyPrint(m);
}

source pretty_print.sh && curl --request GET \

jq -c '.values.messages[]' | while read -r element; do
type=$(echo "$element" | jq -r '.type')
content=$(echo "$element" | jq -r '.content | if type == "array" then tostring else . end')
pretty_print "$type" "$content"
done

Output:

================================ Human Message =================================

what's the weather in sf?
================================== Ai Message ==================================

[{'id': 'toolu_01MjNtVJwEcpujRGrf3x6Pih', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
tavily_search_results_json (toolu_01MjNtVJwEcpujRGrf3x6Pih)
Call ID: toolu_01MjNtVJwEcpujRGrf3x6Pih
Args:
query: weather in san francisco
================================= Tool Message =================================
Name: tavily_search_results_json

[{"url": "https://www.wunderground.com/hourly/us/ca/san-francisco/KCASANFR2002/date/2024-6-18", "content": "High 64F. Winds W at 10 to 20 mph. A few clouds from time to time. Low 49F. Winds W at 10 to 20 mph. Temp. San Francisco Weather Forecasts. Weather Underground provides local & long-range weather ..."}]
================================ Human Message =================================

what's the weather in nyc?
================================== Ai Message ==================================

[{'id': 'toolu_01KtE1m1ifPLQAx4fQLyZL9Q', 'input': {'query': 'weather in new york city'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]
Tool Calls:
tavily_search_results_json (toolu_01KtE1m1ifPLQAx4fQLyZL9Q)
Call ID: toolu_01KtE1m1ifPLQAx4fQLyZL9Q
Args:
query: weather in new york city
================================= Tool Message =================================
Name: tavily_search_results_json

[{"url": "https://www.accuweather.com/en/us/new-york/10021/june-weather/349727", "content": "Get the monthly weather forecast for New York, NY, including daily high/low, historical averages, to help you plan ahead."}]
================================== Ai Message ==================================

The search results provide weather forecasts and information for New York City. Based on the top result from AccuWeather, here are some key details about the weather in NYC:

* This is a monthly weather forecast for New York City for the month of June.
* It includes daily high and low temperatures to help plan ahead.
* Historical averages for June in NYC are also provided as a reference point.
* More detailed daily or hourly forecasts with precipitation chances, humidity, wind, etc. can be found by visiting the AccuWeather page.

So in summary, the search provides a convenient overview of the expected weather conditions in New York City over the next month to give you an idea of what to prepare for if traveling or making plans there. Let me know if you need any other details!

Verify that the original, interrupted run was interrupted

print((await client.runs.get(thread["thread_id"], interrupted_run["run_id"]))["status"])

console.log((await client.runs.get(thread['thread_id'], interruptedRun["run_id"]))["status"])

'interrupted'

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Double texting\\
\\
Previous Rollback Concurrent\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/langgraph-python-sdk

Skip to content

Edit this page

# LangGraph reference

![PyPI - Version](https://pypi.org/project/langgraph/#history)![PyPI - License](https://opensource.org/licenses/MIT)![PyPI - Downloads](https://pypistats.org/packages/langgraph)

Welcome to the LangGraph reference docs!

These pages detail the core interfaces you will use when building with LangGraph. Each section covers a different part of the ecosystem.

## `langgraph` ¶

The core APIs for the LangGraph open source library.

- Graphs: Main graph abstraction and usage.
- Functional API: Functional programming interface for graphs.
- Pregel: Pregel-inspired computation model.
- Checkpointing: Saving and restoring graph state.
- Storage: Storage backends and options.
- Caching: Caching mechanisms for performance.
- Types: Type definitions for graph components.
- Runtime: Runtime configuration and options.
- Config: Configuration options.
- Errors: Error types and handling.
- Constants: Global constants.
- Channels: Message passing and channels.

Model Context Protocol (MCP) support

To use MCP tools in your LangGraph application, check out `langchain-mcp-adapters`.

## Prebuilt components ¶

Higher-level abstractions for common workflows, agents, and other patterns.

- Agents: Built-in agent patterns.
- Supervisor: Orchestration and delegation.
- Swarm: Multi-agent collaboration.

---

# https://docs.langchain.com/langsmith/self-hosted

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Self-hosted

Self-hosted LangSmith

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

On this page

- Self-host LangSmith Observability and Evaluation
- Services
- Storage services
- Setup methods
- Setup guides
- Enable LangSmith Deployment
- Workflow
- Standalone Server
- Workflow
- Supported compute platforms
- Setup guide

**Important**

Self-hosted LangSmith is an add-on to the Enterprise plan designed for our largest, most security-conscious customers. For more details, refer to Pricing. Contact our sales team if you want to get a license key to trial LangSmith in your environment.

LangSmith supports different self-hosted configurations depending on your scale, security, and infrastructure needs.You can use LangSmith for observability and evaluation without agent deployment. Or, you can set up the **full self-hosted platform** for observability, evaluation, and agent deployment. Alternatively, you can deploy agents directly without the control plane.This page provides an overview of each self-hosted model:

**LangSmith Observability and Evaluation** \\
\\
Host an instance of LangSmith that includes observability, tracing, and evaluations in the UI and API. Best for teams who want self-hosted monitoring and evaluation without deploying agents. **LangSmith Observability, Evaluation, and Deployment** \\
\\
Enables deploying graphs to Agent Server via the control plane. The control plane and data plane provide the full LangSmith platform for running and monitoring agents. This includes observability, evaluation, and deployment. **Standalone server** \\
\\
Host an Agent Server directly without the control plane UI. A lightweight option for running one or a few agents as independent services, with full control over scaling and integration.

| Model | Includes | Best for | Methods |
| --- | --- | --- | --- |

For setup guides, refer to:

- Enable LangSmith Deployment
- Deploy Standalone Server

Supported compute platforms: Kubernetes (for LangSmith Deployment), any compute platform (for Standalone Server)

## ​ Self-host LangSmith Observability and Evaluation

Host an instance of LangSmith that includes observability, tracing, and evaluations in the UI and API, but **without** the ability to deploy agents through the control plane.This includes:**Services:**

- LangSmith frontend UI
- LangSmith backend API
- LangSmith Platform backend
- LangSmith Playground
- LangSmith queue
- LangSmith ACE (Arbitrary Code Execution) backend

**Storage services:**

- ClickHouse (traces and feedback data)
- PostgreSQL (operational data)
- Redis (queuing and caching)
- Blob storage (optional, but recommended for production)

### ​ Services

| Service | Description |
| --- | --- |
| **LangSmith frontend** | The frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users. |
| **LangSmith backend** | The backend is the main entrypoint for CRUD API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and SDK, preparing traces for ingestion, and supporting the hub API. |
| **LangSmith queue** | The queue handles incoming traces and feed Storage services

LangSmith will bundle all storage services by default. You can configure it to use external versions of all storage services. In a production setting, we **strongly recommend using external storage services**.

| Service | Description |
| --- | --- |

### ​ Setup methods

- **Docker Compose** (development/testing only)
- **Kubernetes + Helm** (recommended for production)

### ​ Setup guides

- Install on Kubernetes (production)
- Install with Docker (development only)

## ​ Enable LangSmith Deployment

**LangSmith Deployment** is an optional add-on that can be enabled on your LangSmith instance. It’s ideal for enterprise teams who want a centralized, UI-driven platform to deploy and manage multiple agents and graphs, with all infrastructure, data, and orchestration fully under their control.This includes everything from LangSmith, plus:

| Component | Responsibilities | Where it runs | Who manages it |
| --- | --- | --- | --- |

You run both the control plane and the data plane entirely within your own infrastructure. You are responsible for provisioning and managing all components.

Learn more about the control plane and data plane architecture concepts.

### ​ Workflow

If you want to self-host LangSmith for observability, evaluation, and agent deployment, follow these steps:

1

Install self-hosted LangSmith

You must already have a self-hosted LangSmith instance installed in your cloud with a Kubernetes cluster (required for control plane and data plane).

2

Test your graph locally

Use `langgraph-cli` or Studio to test your graph locally.

3

Enable LangSmith Deployment

Follow the setup guide to enable LangSmith Deployment on your LangSmith instance.

## ​ Standalone Server

The **Standalone server** option is the most lightweight and flexible way to run LangSmith. Unlike the other models, you only manage a simplified data plane made up of Agent Servers and their required backing services (PostgreSQL, Redis, etc.).This includes:

| Component | Responsibilities | Where it runs | Who manages it |
| --- | --- | --- | --- |
| **Control plane** | n/a | n/a | n/a |

This option gives you full control over scaling, deployment, and CI/CD pipelines, while still allowing optional integration with LangSmith for tracing and evaluation.

Do not run standalone servers in serverless environments. Scale-to-zero may cause task loss and scaling up will not work reliably.

### ​ Workflow

1. Define and test your graph locally using the `langgraph-cli` or Studio
2. Package your agent as a Docker image
3. Deploy the Agent Server to your compute platform of choice (Kubernetes, Docker, VM)
4. Optionally, configure LangSmith API keys and endpoints so the server reports traces and evaluations Supported compute platforms

- **Kubernetes**: Use the LangSmith Helm chart to run Agent Servers in a Kubernetes cluster. This is the recommended option for production-grade deployments.
- **Docker**: Run in any Docker-supported compute platform (local dev machine, VM, ECS, etc.). This is best suited for development or small-scale workloads.

### ​ Setup guide

To set up an Agent Server, refer to the how-to guide in the application deployment section.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Set up hybrid LangSmith\\
\\
Previous Self-host LangSmith on Kubernetes\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/cli

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

LangSmith Deployment

LangGraph CLI

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

On this page

- Installation
- Quick commands
- Configuration file
- Examples
- Basic configuration
- Using Wolfi base images
- Adding semantic search to the store
- Semantic search with a custom embedding function
- Adding custom authentication
- Configuring store item Time-to-Live
- Configuring checkpoint Time-to-Live
- Configuring checkpointer serde
- Customizing HTTP middleware and headers
- Pinning API version
- Commands
- dev
- build
- up
- dockerfile

**LangGraph CLI** is a command-line tool for building and running the Agent Server locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.

## ​ Installation

1. Ensure Docker is installed (e.g., `docker --version`).
2. Install the CLI:

\[Python (pip)\]

JavaScript

Copy

pip install langgraph-cli

3. Verify the install

langgraph --help

### ​ Quick commands

| Command | What it does |
| --- | --- |
| `langgraph dev` | Starts a lightweight local dev server (no Docker required), ideal for rapid testing. |
| `langgraph build` | Builds a Docker image of your LangGraph API server for deployment. |
| `langgraph dockerfile` | Emits a Dockerfile derived from your config for custom builds. |
| `langgraph up` | Starts the LangGraph API server locally in Docker. Requires Docker running; LangSmith API key for local dev; license for production. |

## ​ Configuration file

To build and run a valid application, the LangGraph CLI requires a JSON configuration file that follows this schema. It contains the following properties:

The LangGraph CLI defaults to using the configuration file named **langgraph.json** in the current directory.

- Python

- JS

| Key | Description |
| --- | --- |

| `auth` | _(Added in v0.0.11)_ Auth configuration containing the path to your authentication handler. Example: `./your_package/auth.py:auth`, where `auth` is an instance of `langgraph_sdk.Auth`. See authentication guide for details. |
| `base_image` | Optional. Base image to use for the LangGraph API server. Defaults to `langchain/langgraph-api` or `langchain/langgraphjs-api`. Use this to pin your builds to a particular version of the langgraph API, such as `"langchain/langgraph-server:0.2"`. See for more details. (added in `langgraph-cli==0.2.8`) |

| `env` | Path to `.env` file or a mapping from environment variable to its value. |

| `ui` | Optional. Named definitions of UI components emitted by the agent, each pointing to a JS/TS file. (added in `langgraph-cli==0.1.84`) |
| `python_version` | `3.11`, `3.12`, or `3.13`. Defaults to `3.11`. |
| `node_version` | Specify `node_version: 20` to use LangGraph.js. |
| `pip_config_file` | Path to `pip` config file. |
| `pip_installer` | _(Added in v0.3)_ Optional. Python package installer selector. It can be set to `"auto"`, `"pip"`, or `"uv"`. From version 0.3 onward the default strategy is to run `uv pip`, which typically delivers faster builds while remaining a drop-in replacement. In the uncommon situation where `uv` cannot handle your dependency graph or the structure of your `pyproject.toml`, specify `"pip"` here to revert to the earlier behaviour. |

| `dockerfile_lines` | Array of additional lines to add to Dockerfile following the import from parent image. |

| `api_version` | _(Added in v0.3.7)_ Which semantic version of the LangGraph API server to use (e.g., `"0.3"`). Defaults to latest. Check the server changelog for details on each release. |

| `node_version` | Specify `node_version: 20` to use LangGraph.js. |
| `dockerfile_lines` | Array of additional lines to add to Dockerfile following the import from parent image. |

### ​ Examples

#### ​ Basic configuration

{
"$schema": "https://langgra.ph/schema.json",
"dependencies": ["."],
"graphs": {
"chat": "chat.graph:graph"
}
}

#### ​ Using Wolfi base images

{
"$schema": "https://langgra.ph/schema.json",
"dependencies": ["."],
"graphs": {
"chat": "chat.graph:graph"
},
"image_distro": "wolfi"
}

#### ​ Adding semantic search to the store

All deployments come with a DB-backed BaseStore. Adding an “index” configuration to your `langgraph.json` will enable semantic search within the BaseStore of your deployment.The `index.fields` configuration determines which parts of your documents to embed:

- If omitted or set to `["$"]`, the entire document will be embedded
- To embed specific fields, use JSON path notation: `["metadata.title", "content.text"]`
- Documents missing specified fields will still be stored but won’t have embeddings for those fields
- You can still override which fields to embed on a specific item at `put` time using the `index` parameter

{
"dependencies": ["."],
"graphs": {
"memory_agent": "./agent/graph.py:graph"
},
"store": {
"index": {
"embed": "openai:text-embedding-3-small",
"dims": 1536,
"fields": ["$"]
}
}
}

**Common model dimensions**

- `openai:text-embedding-3-large`: 3072
- `openai:text-embedding-3-small`: 1536
- `openai:text-embedding-ada-002`: 1536
- `cohere:embed-english-v3.0`: 1024
- `cohere:embed-english-light-v3.0`: 384
- `cohere:embed-multilingual-v3.0`: 1024
- `cohere:embed-multilingual-light-v3.0`: 384

#### ​ Semantic search with a custom embedding function

If you want to use semantic search with a custom embedding function, you can pass a path to a custom embedding function:

{
"dependencies": ["."],
"graphs": {
"memory_agent": "./agent/graph.py:graph"
},
"store": {
"index": {
"embed": "./embeddings.py:embed_texts",
"dims": 768,
"fields": ["text", "summary"]
}
}
}

The `embed` field in store configuration can reference a custom function that takes a list of strings and returns a list of embeddings. Example implementation:

# embeddings.py

"""Custom embedding function for semantic search."""
# Implementation using your preferred embedding model
return [[0.1, 0.2, ...] for _ in texts] # dims-dimensional vectors

#### ​ Adding custom authentication

{
"$schema": "https://langgra.ph/schema.json",
"dependencies": ["."],
"graphs": {
"chat": "chat.graph:graph"
},
"auth": {
"path": "./auth.py:auth",
"openapi": {
"securitySchemes": {
"apiKeyAuth": {
"type": "apiKey",
"in": "header",
"name": "X-API-Key"
}
},
"security": [{ "apiKeyAuth": [] }]
},
"disable_studio_auth": false
}
}

See the authentication conceptual guide for details, and the setting up custom authentication guide for a practical walk through of the process.

#### ​ Configuring store item Time-to-Live

You can configure default data expiration for items/memories in the BaseStore using the `store.ttl` key. This determines how long items are retained after they are last accessed (with reads potentially refreshing the timer based on `refresh_on_read`). Note that these defaults can be overwritten on a per-call basis by modifying the corresponding arguments in `get`, `search`, etc.The `ttl` configuration is an object containing optional fields:

- `refresh_on_read`: If `true` (the default), accessing an item via `get` or `search` resets its expiration timer. Set to `false` to only refresh TTL on writes (`put`).
- `default_ttl`: The default lifespan of an item in **minutes**. Applies only to newly created items; existing items are not modified. If not set, items do not expire by default.
- `sweep_interval_minutes`: How frequently (in minutes) the system should run a background process to delete expired items. If not set, sweeping does not occur automatically.

Here is an example enabling a 7-day TTL (10080 minutes), refreshing on reads, and sweeping every hour:

{
"$schema": "https://langgra.ph/schema.json",
"dependencies": ["."],
"graphs": {
"memory_agent": "./agent/graph.py:graph"
},
"store": {
"ttl": {
"refresh_on_read": true,
"sweep_interval_minutes": 60,
"default_ttl": 10080
}
}
}

#### ​ Configuring checkpoint Time-to-Live

You can configure the time-to-live (TTL) for checkpoints using the `checkpointer` key. This determines how long checkpoint data is retained before being automatically handled according to the specified strategy (e.g., deletion). Two optional sub-objects are supported:

- `ttl`: Includes `strategy`, `sweep_interval_minutes`, and `default_ttl`, which collectively set how checkpoints expire.
- `serde` _(Agent server 0.5+)_ : Lets you control deserialization behavior for checkpoint payloads.

Here’s an example setting a default TTL of 30 days (43200 minutes):

{
"$schema": "https://langgra.ph/schema.json",
"dependencies": ["."],
"graphs": {
"chat": "chat.graph:graph"
},
"checkpointer": {
"ttl": {
"strategy": "delete",
"sweep_interval_minutes": 10,
"default_ttl": 43200
}
}
}

In this example, checkpoints older than 30 days will be deleted, and the check runs every 10 minutes.

#### ​ Configuring checkpointer serde

The `checkpointer.serde` object shapes deserialization:

- `allowed_json_modules` defines an allow list for custom Python objects you want the server to be able to deserialize from payloads saved in “json” mode. This is a list of `[path, to, module, file, symbol]` sequences. If omitted, only LangChain-safe defaults are allowed. You can unsafely set to `true` to allow any module to be deserialized.
- `pickle_fallback`: Whether to fall

{
"checkpointer": {
"serde": {
"allowed_json_modules": [\
["my_agent", "auth", "SessionState"]\
]
}
}
}

#### ​ Customizing HTTP middleware and headers

The `http` block lets you fine-tune request handling:

- `middleware_order`: Choose `"auth_first"` to run authentication before your middleware, or `"middleware_first"` (default) to invert that order.
- `enable_custom_route_auth`: Extend authentication to routes you mount through `http.app`.
- `configurable_headers` / `logging_headers`: Each accepts an object with optional `includes` and `excludes` arrays; wildcards are supported and exclusions run before inclusions.
- `cors`: In addition to `allow_origins`, `allow_methods`, and `allow_headers`, you can set `allow_credentials`, `allow_origin_regex`, `expose_headers`, and `max_age` for detailed browser control.

#### ​ Pinning API version

_(Added in v0.3.7)_You can pin the API version of the Agent Server by using the `api_version` key. This is useful if you want to ensure that your server uses a specific version of the API.
By default, builds in Cloud deployments use the latest stable version of the server. This can be pinned by setting the `api_version` key to a specific version.

{
"$schema": "https://langgra.ph/schema.json",
"dependencies": ["."],
"graphs": {
"chat": "chat.graph:graph"
},
"api_version": "0.2"
}

#### ​ Basic configuration

{
"$schema": "https://langgra.ph/schema.json",
"graphs": {
"chat": "./src/graph.ts:graph"
}
}

#### ​ Pinning API version

{
"$schema": "https://langgra.ph/schema.json",
"dependencies": ["."],
"graphs": {
"chat": "./src/chat/graph.ts:graph"
},
"api_version": "0.2"
}

## ​ Commands

**Usage**

The base command for the LangGraph CLI is `langgraph`.

langgraph [OPTIONS] COMMAND [ARGS]

The base command for the LangGraph.js CLI is `langgraphjs`.

npx @langchain/langgraph-cli [OPTIONS] COMMAND [ARGS]

We recommend using `npx` to always use the latest version of the CLI.

### ​ `dev`

Run LangGraph API server in development mode with hot reloading and debugging capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory.

**Installation**This command requires the “inmem” extra to be installed:

pip install -U "langgraph-cli[inmem]"

langgraph dev [OPTIONS]

**Options**

| Option | Default | Description |
| --- | --- | --- |
| `-c, --config FILE` | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables |
| `--host TEXT` | `127.0.0.1` | Host to bind the server to |
| `--port INTEGER` | `2024` | Port to bind the server to |
| `--no-reload` | | Disable auto-reload |
| `--n-jobs-per-worker INTEGER` | | Number of jobs per worker. Default is 10 |
| `--debug-port INTEGER` | | Port for debugger to listen on |
| `--wait-for-client` | `False` | Wait for a debugger client to connect to the debug port before starting the server |
| `--no-browser` | | Skip automatically opening the browser when the server starts |
| `--studio-url TEXT` | | URL of the Studio instance to connect to. Defaults to |
| `--allow-blocking` | `False` | Do not raise errors for synchronous I/O blocking operations in your code (added in `0.2.6`) |
| `--tunnel` | `False` | Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers like Safari or networks blocking localhost connections |
| `--help` | | Display command documentation |

Run LangGraph API server in development mode with hot reloading capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory.**Usage**

npx @langchain/langgraph-cli dev [OPTIONS]

| Option | Default | Description |
| --- | --- | --- |
| `-c, --config FILE` | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables |
| `--host TEXT` | `127.0.0.1` | Host to bind the server to |
| `--port INTEGER` | `2024` | Port to bind the server to |
| `--no-reload` | | Disable auto-reload |
| `--n-jobs-per-worker INTEGER` | | Number of jobs per worker. Default is 10 |
| `--debug-port INTEGER` | | Port for debugger to listen on |
| `--wait-for-client` | `False` | Wait for a debugger client to connect to the debug port before starting the server |
| `--no-browser` | | Skip automatically opening the browser when the server starts |
| `--studio-url TEXT` | | URL of the Studio instance to connect to. Defaults to |
| `--allow-blocking` | `False` | Do not raise errors for synchronous I/O blocking operations in your code |
| `--tunnel` | `False` | Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers or networks blocking localhost connections |
| `--help` | | Display command documentation |

### ​ `build`

Build LangSmith API server Docker image.**Usage**

langgraph build [OPTIONS]

| Option | Default | Description |
| --- | --- | --- |
| `--platform TEXT` | | Target platform(s) to build the Docker image for. Example: `langgraph build --platform linux/amd64,linux/arm64` |
| `-t, --tag TEXT` | | **Required**. Tag for the Docker image. Example: `langgraph build -t my-image` |
| `--pull / --no-pull` | `--pull` | Build with latest remote Docker image. Use `--no-pull` for running the LangSmith API server with locally built images. |
| `-c, --config FILE` | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables. |
| `--build-command TEXT`\* | | Build command to run. Runs from the directory where your `langgraph.json` file lives. Example: `langgraph build --build-command "yarn run turbo build"` |
| `--install-command TEXT`\* | | Install command to run. Runs from the directory where you call `langgraph build` from. Example: `langgraph build --install-command "yarn install"` |
| `--help` | | Display command documentation. |

\*Only supported for JS deployments, will have no impact on Python deployments.

npx @langchain/langgraph-cli build [OPTIONS]

| Option | Default | Description |
| --- | --- | --- |
| `--platform TEXT` | | Target platform(s) to build the Docker image for. Example: `langgraph build --platform linux/amd64,linux/arm64` |
| `-t, --tag TEXT` | | **Required**. Tag for the Docker image. Example: `langgraph build -t my-image` |
| `--no-pull` | | Use locally built images. Defaults to `false` to build with latest remote Docker image. |
| `-c, --config FILE` | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables. |
| `--help` | | Display command documentation. |

### ​ `up`

Start LangGraph API server. For local testing, requires a LangSmith API key with access to LangSmith. Requires a license key for production use.**Usage**

langgraph up [OPTIONS]

| Option | Default | Description |
| --- | --- | --- |
| `--wait` | | Wait for services to start before returning. Implies —detach |
| `--base-image TEXT` | `langchain/langgraph-api` | Base image to use for the LangGraph API server. Pin to specific versions using version tags. |
| `--image TEXT` | | Docker image to use for the langgraph-api service. If specified, skips building and uses this image directly. |
| `--postgres-uri TEXT` | Local database | Postgres URI to use for the database. |
| `--watch` | | Restart on file changes |
| `--debugger-base-url TEXT` | `http://127.0.0.1:[PORT]` | URL used by the debugger to access LangGraph API. |
| `--debugger-port INTEGER` | | Pull the debugger image locally and serve the UI on specified port |
| `--verbose` | | Show more output from the server logs. |
| `-c, --config FILE` | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables. |
| `-d, --docker-compose FILE` | | Path to docker-compose.yml file with additional services to launch. |
| `-p, --port INTEGER` | `8123` | Port to expose. Example: `langgraph up --port 8000` |
| `--pull / --no-pull` | `pull` | Pull latest images. Use `--no-pull` for running the server with locally-built images. Example: `langgraph up --no-pull` |
| `--recreate / --no-recreate` | `no-recreate` | Recreate containers even if their configuration and image haven’t changed |
| `--help` | | Display command documentation. |

npx @langchain/langgraph-cli up [OPTIONS]

| Option | Default | Description |
| --- | --- | --- |
| `--wait` | | Wait for services to start before returning. Implies —detach |
| `--base-image TEXT` | `langchain/langgraph-api` | Base image to use for the LangGraph API server. Pin to specific versions using version tags. |
| `--image TEXT` | | Docker image to use for the langgraph-api service. If specified, skips building and uses this image directly. |
| `--postgres-uri TEXT` | Local database | Postgres URI to use for the database. |
| `--watch` | | Restart on file changes |
| `-c, --config FILE` | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables. |
| `-d, --docker-compose FILE` | | Path to docker-compose.yml file with additional services to launch. |
| `-p, --port INTEGER` | `8123` | Port to expose. Example: `langgraph up --port 8000` |
| `--no-pull` | | Use locally built images. Defaults to `false` to build with latest remote Docker image. |
| `--recreate` | | Recreate containers even if their configuration and image haven’t changed |
| `--help` | | Display command documentation. |

### ​ `dockerfile`

Generate a Dockerfile for building a LangSmith API server Docker image.**Usage**

langgraph dockerfile [OPTIONS] SAVE_PATH

| Option | Default | Description |
| --- | --- | --- |
| `-c, --config FILE` | `langgraph.json` | Path to the configuration file declaring dependencies, graphs and environment variables. |
| `--help` | | Show this message and exit. |

Example:

langgraph dockerfile -c langgraph.json Dockerfile

This generates a Dockerfile that looks similar to:

FROM langchain/langgraph-api:3.11

ADD ./pipconf.txt /pipconfig.txt

RUN PIP_CONFIG_FILE=/pipconfig.txt PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt langchain_community langchain_anthropic langchain_openai wikipedia scikit-learn

ADD ./graphs /deps/__outer_graphs/src
RUN set -ex && \
for line in '[project]' \
'name = "graphs"' \
'version = "0.1"' \
'[tool.setuptools.package-data]' \
'"*" = ["**/*"]'; do \
echo "$line" >> /deps/__outer_graphs/pyproject.toml; \
done

RUN PIP_CONFIG_FILE=/pipconfig.txt PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*

ENV LANGSERVE_GRAPHS='{"agent": "/deps/__outer_graphs/src/agent.py:graph", "storm": "/deps/__outer_graphs/src/storm.py:graph"}'

The `langgraph dockerfile` command translates all the configuration in your `langgraph.json` file into Dockerfile commands. When using this command, you will have to re-run it whenever you update your `langgraph.json` file. Otherwise, your changes will not be reflected when you build or run the dockerfile.

npx @langchain/langgraph-cli dockerfile [OPTIONS] SAVE_PATH

npx @langchain/langgraph-cli dockerfile -c langgraph.json Dockerfile

FROM langchain/langgraphjs-api:20

ADD . /deps/agent

RUN cd /deps/agent && yarn install

ENV LANGSERVE_GRAPHS='{"agent":"./src/react_agent/graph.ts:graph"}'

WORKDIR /deps/agent

RUN (test ! -f /api/langgraph_api/js/build.mts && echo "Prebuild script not found, skipping") || tsx /api/langgraph_api/js/build.mts

The `npx @langchain/langgraph-cli dockerfile` command translates all the configuration in your `langgraph.json` file into Dockerfile commands. When using this command, you will have to re-run it whenever you update your `langgraph.json` file. Otherwise, your changes will not be reflected when you build or run the dockerfile.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Update Oauth Provider\\
\\
Previous RemoteGraph\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langgraph/agentic-rag

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangGraph

Build a custom RAG agent with LangGraph

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Router: Knowledge base
- LangGraph

- Custom RAG agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### LangChain Academy

- LangChain Academy

##### Additional resources

- Case studies
- Get help

On this page

- Overview
- Concepts
- Setup
- 1\. Preprocess documents
- 2\. Create a retriever tool
- 3\. Generate query
- 4\. Grade documents
- 5\. Rewrite question
- 6\. Generate an answer
- 7\. Assemble the graph
- 8\. Run the agentic RAG

## ​ Overview

In this tutorial we will build a retrieval agent using LangGraph.LangChain offers built-in agent implementations, implemented using LangGraph primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a retrieval agent. Retrieval agents are useful when you want an LLM to make a decision about whether to retrieve context from a vectorstore or respond to the user directly.By the end of the tutorial we will have done the following:

1. Fetch and preprocess documents that will be used for retrieval.
2. Index those documents for semantic search and create a retriever tool for the agent.
3. Build an agentic RAG system that can decide when to use the retriever tool.

### ​ Concepts

We will cover the following concepts:

- Retrieval using document loaders, text splitters, embeddings, and vector stores
- The LangGraph Graph API, including state, nodes, edges, and conditional edges.

## ​ Setup

Let’s download the required packages and set our API keys:

npm

pnpm

yarn

bun

Copy

npm install @langchain/langgraph @langchain/openai @langchain/community @langchain/textsplitters

Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.

## ​ 1\. Preprocess documents

1. Fetch documents to use in our RAG system. We will use three of the most recent pages from Lilian Weng’s excellent blog. We’ll start by fetching the content of the pages using `CheerioWebBaseLoader`:

import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";

const urls = [\
"https://lilianweng.github.io/posts/2023-06-23-agent/",\
"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",\
"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",\
];

const docs = await Promise.all(

);

2. Split the fetched documents into smaller chunks for indexing into our vectorstore:

import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const docsList = docs.flat();

const textSplitter = new RecursiveCharacterTextSplitter({
chunkSize: 500,
chunkOverlap: 50,
});
const docSplits = await textSplitter.splitDocuments(docsList);

## ​ 2\. Create a retriever tool

Now that we have our split documents, we can index them into a vector store that we’ll use for semantic search.

1. Use an in-memory vector store and OpenAI embeddings:

import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";
import { OpenAIEmbeddings } from "@langchain/openai";

const vectorStore = await MemoryVectorStore.fromDocuments(
docSplits,
new OpenAIEmbeddings(),
);

const retriever = vectorStore.asRetriever();

2. Create a retriever tool using LangChain’s prebuilt `createRetrieverTool`:

import { createRetrieverTool } from "@langchain/classic/tools/retriever";

const tool = createRetrieverTool(
retriever,
{
name: "retrieve_blog_posts",
description:
"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.",
},
);
const tools = [tool];

## ​ 3\. Generate query

Now we will start building components ( nodes and edges) for our agentic RAG graph.

1. Build a `generateQueryOrRespond` node. It will call an LLM to generate a response based on the current graph state (list of messages). Given the input messages, it will decide to retrieve using the retriever tool, or respond directly to the user. Note that we’re giving the chat model access to the `tools` we created earlier via `.bindTools`:

import { ChatOpenAI } from "@langchain/openai";

async function generateQueryOrRespond(state) {
const { messages } = state;
const model = new ChatOpenAI({
model: "gpt-4o",
temperature: 0,
}).bindTools(tools);

const response = await model.invoke(messages);
return {
messages: [response],
};
}

2. Try it on a random input:

import { HumanMessage } from "@langchain/core/messages";

const input = { messages: [new HumanMessage("hello!")] };
const result = await generateQueryOrRespond(input);
console.log(result.messages[0]);

**Output:**

AIMessage {
content: "Hello! How can I help you today?",
tool_calls: []
}

3. Ask a question that requires semantic search:

const input = {
messages: [\
new HumanMessage("What does Lilian Weng say about types of reward hacking?")\
]
};
const result = await generateQueryOrRespond(input);
console.log(result.messages[0]);

AIMessage {
content: "",
tool_calls: [\
{\
name: "retrieve_blog_posts",\
args: { query: "types of reward hacking" },\
id: "call_...",\
type: "tool_call"\
}\
]
}

## ​ 4\. Grade documents

1. Add a node — `gradeDocuments` — to determine whether the retrieved documents are relevant to the question. We will use a model with structured output using Zod for document grading. We’ll also add a conditional edge — `checkRelevance` — that checks the grading result and returns the name of the node to go to (`generate` or `rewrite`):

import * as z from "zod";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";
import { AIMessage } from "@langchain/core/messages";

const prompt = ChatPromptTemplate.fromTemplate(
`You are a grader assessing relevance of retrieved docs to a user question.
Here are the retrieved docs:
\n ------- \n
{context}
\n ------- \n
Here is the user question: {question}
If the content of the docs are relevant to the users question, score them as relevant.
Give a binary score 'yes' or 'no' score to indicate whether the docs are relevant to the question.
Yes: The docs are relevant to the question.
No: The docs are not relevant to the question.`,
);

const gradeDocumentsSchema = z.object({
binaryScore: z.string().describe("Relevance score 'yes' or 'no'"),
})

async function gradeDocuments(state) {
const { messages } = state;

const model = new ChatOpenAI({
model: "gpt-4o",
temperature: 0,
}).withStructuredOutput(gradeDocumentsSchema);

const score = await prompt.pipe(model).invoke({
question: messages.at(0)?.content,
context: messages.at(-1)?.content,
});

if (score.binaryScore === "yes") {
return "generate";
}
return "rewrite";
}

2. Run this with irrelevant documents in the tool response:

import { ToolMessage } from "@langchain/core/messages";

const input = {
messages: [\
new HumanMessage("What does Lilian Weng say about types of reward hacking?"),\
new AIMessage({\
tool_calls: [\
{\
type: "tool_call",\
name: "retrieve_blog_posts",\
args: { query: "types of reward hacking" },\
id: "1",\
}\
]\
}),\
new ToolMessage({\
content: "meow",\
tool_call_id: "1",\
})\
]
}
const result = await gradeDocuments(input);

3. Confirm that the relevant documents are classified as such:

const input = {
messages: [\
new HumanMessage("What does Lilian Weng say about types of reward hacking?"),\
new AIMessage({\
tool_calls: [\
{\
type: "tool_call",\
name: "retrieve_blog_posts",\
args: { query: "types of reward hacking" },\
id: "1",\
}\
]\
}),\
new ToolMessage({\
content: "reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering",\
tool_call_id: "1",\
})\
]
}
const result = await gradeDocuments(input);

## ​ 5\. Rewrite question

1. Build the `rewrite` node. The retriever tool can return potentially irrelevant documents, which indicates a need to improve the original user question. To do so, we will call the `rewrite` node:

import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatOpenAI } from "@langchain/openai";

const rewritePrompt = ChatPromptTemplate.fromTemplate(
`Look at the input and try to reason about the underlying semantic intent / meaning. \n
Here is the initial question:
\n ------- \n
{question}
\n ------- \n
Formulate an improved question:`,
);

async function rewrite(state) {
const { messages } = state;
const question = messages.at(0)?.content;

const model = new ChatOpenAI({
model: "gpt-4o",
temperature: 0,
});

const response = await rewritePrompt.pipe(model).invoke({ question });
return {
messages: [response],
};
}

2. Try it out:

import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";

const input = {
messages: [\
new HumanMessage("What does Lilian Weng say about types of reward hacking?"),\
new AIMessage({\
content: "",\
tool_calls: [\
{\
id: "1",\
name: "retrieve_blog_posts",\
args: { query: "types of reward hacking" },\
type: "tool_call"\
}\
]\
}),\
new ToolMessage({ content: "meow", tool_call_id: "1" })\
]
};

const response = await rewrite(input);
console.log(response.messages[0].content);

What are the different types of reward hacking described by Lilian Weng, and how does she explain them?

## ​ 6\. Generate an answer

1. Build `generate` node: if we pass the grader checks, we can generate the final answer based on the original question and the retrieved context:

async function generate(state) {
const { messages } = state;
const question = messages.at(0)?.content;
const context = messages.at(-1)?.content;

const prompt = ChatPromptTemplate.fromTemplate(
`You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.
Use three sentences maximum and keep the answer concise.
Question: {question}
Context: {context}`
);

const llm = new ChatOpenAI({
model: "gpt-4o",
temperature: 0,
});

const ragChain = prompt.pipe(llm);

const response = await ragChain.invoke({
context,
question,
});

return {
messages: [response],
};
}

2. Try it:

const input = {
messages: [\
new HumanMessage("What does Lilian Weng say about types of reward hacking?"),\
new AIMessage({\
content: "",\
tool_calls: [\
{\
id: "1",\
name: "retrieve_blog_posts",\
args: { query: "types of reward hacking" },\
type: "tool_call"\
}\
]\
}),\
new ToolMessage({\
content: "reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering",\
tool_call_id: "1"\
})\
]
};

const response = await generate(input);
console.log(response.messages[0].content);

Lilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering. She considers reward hacking as a broad concept that includes both of these categories. Reward hacking occurs when an agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended behaviors.

## ​ 7\. Assemble the graph

Now we’ll assemble all the nodes and edges into a complete graph:

- Start with a `generateQueryOrRespond` and determine if we need to call the retriever tool
- Route to next step using a conditional edge:
- If `generateQueryOrRespond` returned `tool_calls`, call the retriever tool to retrieve context
- Otherwise, respond directly to the user
- Grade retrieved document content for relevance to the question (`gradeDocuments`) and route to next step:

- If not relevant, rewrite the question using `rewrite` and then call `generateQueryOrRespond` again
- If relevant, proceed to `generate` and generate final response using the `ToolMessage` with the retrieved document context

import { StateGraph, START, END } from "@langchain/langgraph";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { AIMessage } from "langchain";

// Create a ToolNode for the retriever
const toolNode = new ToolNode(tools);

// Helper function to determine if we should retrieve
function shouldRetrieve(state) {
const { messages } = state;
const lastMessage = messages.at(-1);

if (AIMessage.isInstance(lastMessage) && lastMessage.tool_calls.length) {
return "retrieve";
}
return END;
}

// Define the graph
const builder = new StateGraph(GraphState)
.addNode("generateQueryOrRespond", generateQueryOrRespond)
.addNode("retrieve", toolNode)
.addNode("gradeDocuments", gradeDocuments)
.addNode("rewrite", rewrite)
.addNode("generate", generate)
// Add edges
.addEdge(START, "generateQueryOrRespond")
// Decide whether to retrieve
.addConditionalEdges("generateQueryOrRespond", shouldRetrieve)
.addEdge("retrieve", "gradeDocuments")
// Edges taken after grading documents
.addConditionalEdges(
"gradeDocuments",
// Route based on grading decision

// The gradeDocuments function returns either "generate" or "rewrite"
const lastMessage = state.messages.at(-1);
return lastMessage.content === "generate" ? "generate" : "rewrite";
}
)
.addEdge("generate", END)
.addEdge("rewrite", "generateQueryOrRespond");

// Compile
const graph = builder.compile();

## ​ 8\. Run the agentic RAG

Now let’s test the complete graph by running it with a question:

const inputs = {
messages: [\
new HumanMessage("What does Lilian Weng say about types of reward hacking?")\
]
};

for await (const output of await graph.stream(inputs)) {
for (const [key, value] of Object.entries(output)) {
const lastMsg = output[key].messages[output[key].messages.length - 1];
console.log(`Output from node: '${key}'`);
console.log({
type: lastMsg._getType(),
content: lastMsg.content,
tool_calls: lastMsg.tool_calls,
});
console.log("---\n");
}
}

Output from node: 'generateQueryOrRespond'
{
type: 'ai',
content: '',
tool_calls: [\
{\
name: 'retrieve_blog_posts',\
args: { query: 'types of reward hacking' },\
id: 'call_...',\
type: 'tool_call'\
}\
]
}

Output from node: 'retrieve'
{
type: 'tool',
content: '(Note: Some work defines reward tampering as a distinct category...\n' +
'At a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\n' +
'...',
tool_calls: undefined
}

Output from node: 'generate'
{
type: 'ai',
content: 'Lilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering. She considers reward hacking as a broad concept that includes both of these categories. Reward hacking occurs when an agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended behaviors.',
tool_calls: []
}

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a multi-source knowledge base with routing\\
\\
Previous Component architecture\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/rag

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangChain

Build a RAG agent with LangChain

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Overview
- Concepts
- Preview
- Setup
- Installation
- LangSmith
- Components
- 1\. Indexing
- Loading documents
- Splitting documents
- Storing documents
- 2\. Retrieval and Generation
- RAG agents
- RAG chains
- Next steps

## ​ Overview

One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.This tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:

1. A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.
2. A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.

### ​ Concepts

We will cover the following concepts:

- **Indexing**: a pipeline for ingesting data from a source and indexing it. _This usually happens in a separate process._
- **Retrieval and generation**: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.

Once we’ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.

The indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you’re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation

### ​ Preview

In this guide we’ll build an app that answers questions about the website’s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.We can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:

Expand for full code snippet

Copy

import bs4
from langchain.agents import AgentState, create_agent
from langchain_community.document_loaders import WebBaseLoader
from langchain.messages import MessageLikeRepresentation
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Load and chunk contents of the blog
loader = WebBaseLoader(
web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
bs_kwargs=dict(
parse_only=bs4.SoupStrainer(
class_=("post-content", "post-title", "post-header")
)
),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)

# Index chunks
_ = vector_store.add_documents(documents=all_splits)

# Construct a tool for retrieving context
@tool(response_format="content_and_artifact")
def retrieve_context(query: str):
"""Retrieve information to help answer a query."""
retrieved_docs = vector_store.similarity_search(query, k=2)
serialized = "\n\n".join(
(f"Source: {doc.metadata}\nContent: {doc.page_content}")
for doc in retrieved_docs
)
return serialized, retrieved_docs

tools = [retrieve_context]
# If desired, specify custom instructions
prompt = (
"You have access to a tool that retrieves context from a blog post. "
"Use the tool to help answer user queries."
)
agent = create_agent(model, tools, system_prompt=prompt)

query = "What is task decomposition?"
for step in agent.stream(
{"messages": [{"role": "user", "content": query}]},
stream_mode="values",
):
step["messages"][-1].pretty_print()

================================ Human Message =================================

What is task decomposition?
================================== Ai Message ==================================
Tool Calls:
retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)
Call ID: call_xTkJr8njRY0geNz43ZvGkX0R
Args:
query: task decomposition
================================= Tool Message =================================
Name: retrieve_context

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Task decomposition can be done by...

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Component One: Planning...
================================== Ai Message ==================================

Task decomposition refers to...

Check out the LangSmith trace.

## ​ Setup

### ​ Installation

This tutorial requires these langchain dependencies:

pip

uv

pip install langchain langchain-text-splitters langchain-community bs4

For more details, see our Installation guide.

### ​ LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.After you sign up at the link above, make sure to set your environment variables to start logging traces:

export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

Or, set them in Python:

import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()

### ​ Components

We will need to select three components from LangChain’s suite of integrations.Select a chat model:

- OpenAI

- Anthropic

- Azure

- Google Gemini

- AWS Bedrock

- HuggingFace

👉 Read the OpenAI chat model integration docs

pip install -U "langchain[openai]"

init\_chat\_model

Model Class

import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")

👉 Read the Anthropic chat model integration docs

pip install -U "langchain[anthropic]"

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")

👉 Read the Azure chat model integration docs

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
"azure_openai:gpt-4.1",
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)

👉 Read the Google GenAI chat model integration docs

pip install -U "langchain[google-genai]"

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")

👉 Read the AWS Bedrock chat model integration docs

pip install -U "langchain[aws]"

from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
#

model = init_chat_model(
"anthropic.claude-3-5-sonnet-20240620-v1:0",
model_provider="bedrock_converse",
)

👉 Read the HuggingFace chat model integration docs

pip install -U "langchain[huggingface]"

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

model = init_chat_model(
"microsoft/Phi-3-mini-4k-instruct",
model_provider="huggingface",
temperature=0.7,
max_tokens=1024,
)

Select an embeddings model:

- Google Vertex

- AWS

- Ollama

- Cohere

- MistralAI

- Nomic

- NVIDIA

- Voyage AI

- IBM watsonx

- Fake

- Isaacus

pip install -U "langchain-openai"

if not os.environ.get("OPENAI_API_KEY"):
os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

if not os.environ.get("AZURE_OPENAI_API_KEY"):
os.environ["AZURE_OPENAI_API_KEY"] = getpass.getpass("Enter API key for Azure: ")

from langchain_openai import AzureOpenAIEmbeddings

embeddings = AzureOpenAIEmbeddings(
azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
openai_api_version=os.environ["AZURE_OPENAI_API_VERSION"],
)

pip install -qU langchain-google-genai

if not os.environ.get("GOOGLE_API_KEY"):
os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter API key for Google Gemini: ")

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

pip install -qU langchain-google-vertexai

from langchain_google_vertexai import VertexAIEmbeddings

embeddings = VertexAIEmbeddings(model="text-embedding-005")

pip install -qU langchain-aws

from langchain_aws import BedrockEmbeddings

embeddings = BedrockEmbeddings(model_id="amazon.titan-embed-text-v2:0")

pip install -qU langchain-huggingface

from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

pip install -qU langchain-ollama

from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(model="llama3")

pip install -qU langchain-cohere

if not os.environ.get("COHERE_API_KEY"):
os.environ["COHERE_API_KEY"] = getpass.getpass("Enter API key for Cohere: ")

from langchain_cohere import CohereEmbeddings

embeddings = CohereEmbeddings(model="embed-english-v3.0")

pip install -qU langchain-mistralai

if not os.environ.get("MISTRALAI_API_KEY"):
os.environ["MISTRALAI_API_KEY"] = getpass.getpass("Enter API key for MistralAI: ")

from langchain_mistralai import MistralAIEmbeddings

embeddings = MistralAIEmbeddings(model="mistral-embed")

pip install -qU langchain-nomic

if not os.environ.get("NOMIC_API_KEY"):
os.environ["NOMIC_API_KEY"] = getpass.getpass("Enter API key for Nomic: ")

from langchain_nomic import NomicEmbeddings

embeddings = NomicEmbeddings(model="nomic-embed-text-v1.5")

pip install -qU langchain-nvidia-ai-endpoints

if not os.environ.get("NVIDIA_API_KEY"):
os.environ["NVIDIA_API_KEY"] = getpass.getpass("Enter API key for NVIDIA: ")

from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings

embeddings = NVIDIAEmbeddings(model="NV-Embed-QA")

pip install -qU langchain-voyageai

if not os.environ.get("VOYAGE_API_KEY"):
os.environ["VOYAGE_API_KEY"] = getpass.getpass("Enter API key for Voyage AI: ")

from langchain-voyageai import VoyageAIEmbeddings

embeddings = VoyageAIEmbeddings(model="voyage-3")

pip install -qU langchain-ibm

if not os.environ.get("WATSONX_APIKEY"):
os.environ["WATSONX_APIKEY"] = getpass.getpass("Enter API key for IBM watsonx: ")

from langchain_ibm import WatsonxEmbeddings

embeddings = WatsonxEmbeddings(
model_id="ibm/slate-125m-english-rtrvr",
url="https://us-south.ml.cloud.ibm.com",

)

pip install -qU langchain-core

from langchain_core.embeddings import DeterministicFakeEmbedding

embeddings = DeterministicFakeEmbedding(size=4096)

pip install -qU langchain-isaacus

if not os.environ.get("ISAACUS_API_KEY"):
os.environ["ISAACUS_API_KEY"] = getpass.getpass("Enter API key for Isaacus: ")

from langchain_isaacus import IsaacusEmbeddings

embeddings = IsaacusEmbeddings(model="kanon-2-embedder")

Select a vector store:

- In-memory

- Amazon OpenSearch

- AstraDB

- Chroma

- FAISS

- Milvus

- MongoDB

- PGVector

- PGVectorStore

- Pinecone

- Qdrant

pip install -U "langchain-core"

from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)

pip install -qU boto3

from opensearchpy import RequestsHttpConnection

service = "es" # must set the service as 'es'
region = "us-east-2"
credentials = boto3.Session(
aws_access_key_id="xxxxxx", aws_secret_access_key="xxxxx"
).get_credentials()
awsauth = AWS4Auth("xxxxx", "xxxxxx", region, service, session_token=credentials.token)

vector_store = OpenSearchVectorSearch.from_documents(
docs,
embeddings,
opensearch_url="host url",
http_auth=awsauth,
timeout=300,
use_ssl=True,
verify_certs=True,
connection_class=RequestsHttpConnection,
index_name="test-index",
)

pip install -U "langchain-astradb"

from langchain_astradb import AstraDBVectorStore

vector_store = AstraDBVectorStore(
embedding=embeddings,
api_endpoint=ASTRA_DB_API_ENDPOINT,
collection_name="astra_vector_langchain",
token=ASTRA_DB_APPLICATION_TOKEN,
namespace=ASTRA_DB_NAMESPACE,
)

pip install -qU langchain-chroma

from langchain_chroma import Chroma

vector_store = Chroma(
collection_name="example_collection",
embedding_function=embeddings,
persist_directory="./chroma_langchain_db", # Where to save data locally, remove if not necessary
)

pip install -qU langchain-community faiss-cpu

import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS

embedding_dim = len(embeddings.embed_query("hello world"))
index = faiss.IndexFlatL2(embedding_dim)

vector_store = FAISS(
embedding_function=embeddings,
index=index,
docstore=InMemoryDocstore(),
index_to_docstore_id={},
)

pip install -qU langchain-milvus

from langchain_milvus import Milvus

URI = "./milvus_example.db"

vector_store = Milvus(
embedding_function=embeddings,
connection_args={"uri": URI},
index_params={"index_type": "FLAT", "metric_type": "L2"},
)

pip install -qU langchain-mongodb

from langchain_mongodb import MongoDBAtlasVectorSearch

vector_store = MongoDBAtlasVectorSearch(
embedding=embeddings,
collection=MONGODB_COLLECTION,
index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,
relevance_score_fn="cosine",
)

pip install -qU langchain-postgres

from langchain_postgres import PGVector

vector_store = PGVector(
embeddings=embeddings,
collection_name="my_docs",
connection="postgresql+psycopg://...",
)

from langchain_postgres import PGEngine, PGVectorStore

pg_engine = PGEngine.from_connection_string(
url="postgresql+psycopg://..."
)

vector_store = PGVectorStore.create_sync(
engine=pg_engine,
table_name='test_table',
embedding_service=embedding
)

pip install -qU langchain-pinecone

from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

pc = Pinecone(api_key=...)
index = pc.Index(index_name)

vector_store = PineconeVectorStore(embedding=embeddings, index=index)

pip install -qU langchain-qdrant

from qdrant_client.models import Distance, VectorParams
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient

client = QdrantClient(":memory:")

vector_size = len(embeddings.embed_query("sample text"))

if not client.collection_exists("test"):
client.create_collection(
collection_name="test",
vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
)
vector_store = QdrantVectorStore(
client=client,
collection_name="test",
embedding=embeddings,
)

## ​ 1\. Indexing

**This section is an abbreviated version of the content in the semantic search tutorial.**If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you’re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.

Indexing commonly works as follows:

1. **Load**: First we need to load our data. This is done with Document Loaders.
2. **Split**: Text splitters break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won’t fit in a model’s finite context window.
3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.

### ​ Loading documents

import bs4
from langchain_community.document_loaders import WebBaseLoader

# Only keep post title, headers, and content from the full HTML.
bs4_strainer = bs4.SoupStrainer(class_=("post-title", "post-header", "post-content"))
loader = WebBaseLoader(
web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
bs_kwargs={"parse_only": bs4_strainer},
)
docs = loader.load()

assert len(docs) == 1
print(f"Total characters: {len(docs[0].page_content)}")

Total characters: 43131

print(docs[0].page_content[:500])

LLM Powered Autonomous Agents

Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng

Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.
Agent System Overview#
In

**Go deeper**`DocumentLoader`: Object that loads data from a source as list of `Documents`.

- Integrations: 160+ integrations to choose from.
- `BaseLoader`: API reference for the base interface.

### ​ Splitting documents

Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.To handle this we’ll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.As in the semantic search tutorial, we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
chunk_size=1000, # chunk size (characters)
chunk_overlap=200, # chunk overlap (characters)
add_start_index=True, # track index in original document
)
all_splits = text_splitter.split_documents(docs)

print(f"Split blog post into {len(all_splits)} sub-documents.")

Split blog post into 66 sub-documents.

**Go deeper**`TextSplitter`: Object that splits a list of `Document` objects into smaller
chunks for storage and retrieval.

- Integrations
- Interface: API reference for the base interface.

### ​ Storing documents

Now we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.

document_ids = vector_store.add_documents(documents=all_splits)

print(document_ids[:3])

['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']

**Go deeper**`Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings.

- Integrations: 30+ integrations to choose from.
- Interface: API reference for the base interface.

`VectorStore`: Wrapper around a vector database, used for storing and querying embeddings.

- Integrations: 40+ integrations to choose from.
- Interface: API reference for the base interface.

This completes the **Indexing** portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.

## ​ 2\. Retrieval and Generation

RAG applications commonly work as follows:

1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a Retriever.
2. **Generate**: A model produces an answer using a prompt that includes both the question with the retrieved data

### ​ RAG agents

One formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:

from langchain.tools import tool

Here we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.

Retrieval tools are not limited to a single string `query` argument, as in the above example. You can
force the LLM to specify additional search parameters by adding arguments— for example, a category:

from typing import Literal

def retrieve_context(query: str, section: Literal["beginning", "middle", "end"]):

Given our tool, we can construct the agent:

from langchain.agents import create_agent

tools = [retrieve_context]
prompt = (
"You have access to a tool that retrieves context from a blog post. "
"Use the tool to help answer user queries."
)
agent = create_agent(model, tools, system_prompt=prompt)

Let’s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:

query = (
"What is the standard method for Task Decomposition?\n\n"
"Once you get the answer, look up common extensions of that method."
)

for event in agent.stream(
{"messages": [{"role": "user", "content": query}]},
stream_mode="values",
):
event["messages"][-1].pretty_print()

What is the standard method for Task Decomposition?

Once you get the answer, look up common extensions of that method.
================================== Ai Message ==================================
Tool Calls:
retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)
Call ID: call_d6AVxICMPQYwAKj9lgH4E337
Args:
query: standard method for Task Decomposition
================================= Tool Message =================================
Name: retrieve_context

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Task decomposition can be done...

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Component One: Planning...
================================== Ai Message ==================================
Tool Calls:
retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)
Call ID: call_0dbMOw7266jvETbXWn4JqWpR
Args:
query: common extensions of the standard method for Task Decomposition
================================= Tool Message =================================
Name: retrieve_context

The standard method for Task Decomposition often used is the Chain of Thought (CoT)...

Note that the agent:

1. Generates a query to search for a standard method for task decomposition;
2. Receiving the answer, generates a second query to search for common extensions of it;
3. Having received all necessary context, answers the question.

We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.

You can add a deeper level of control and customization using the LangGraph framework directly— for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph’s Agentic RAG tutorial for more advanced formulations.

### ​ RAG chains

In the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:

| ✅ Benefits | ⚠️ Drawbacks |
| --- | --- |
| **Search only when needed** – The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches. | **Two inference calls** – When a search is performed, it requires one call to generate the query and another to produce the final response. |
| **Contextual search queries** – By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context. | **Reduced control** – The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary. |
| **Multiple searches allowed** – The LLM can execute several searches in support of a single user query. | |

Another common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.In this approach we no longer call the model in a loop, but instead make a single pass.We can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:

from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dynamic_prompt

"""Inject context into state messages."""
last_query = request.state["messages"][-1].text
retrieved_docs = vector_store.similarity_search(last_query)

docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)

system_message = (
"You are a helpful assistant. Use the following context in your response:"
f"\n\n{docs_content}"
)

return system_message

agent = create_agent(model, tools=[], middleware=[prompt_with_context])

Let’s try this out:

What is task decomposition?
================================== Ai Message ==================================

Task decomposition is...

In the LangSmith trace we can see the retrieved context incorporated into the model prompt.This is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.

Returning source documents

The above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:

1. Adding a key to the state to store the retrieved documents
2. Adding a new node via a pre-model hook to populate that key (as well as inject the context).

from typing import Any
from langchain_core.documents import Document
from langchain.agents.middleware import AgentMiddleware, AgentState

class State(AgentState):
context: list[Document]

class RetrieveDocumentsMiddleware(AgentMiddleware[State]):
state_schema = State

last_message = state["messages"][-1]
retrieved_docs = vector_store.similarity_search(last_message.text)

augmented_message_content = (
f"{last_message.text}\n\n"
"Use the following context to answer the query:\n"
f"{docs_content}"
)
return {
"messages": [last_message.model_copy(update={"content": augmented_message_content})],
"context": retrieved_docs,
}

agent = create_agent(
model,
tools=[],
middleware=[RetrieveDocumentsMiddleware()],
)

## ​ Next steps

Now that we’ve implemented a simple RAG application via `create_agent`, we can easily incorporate new features and go deeper:

- Stream tokens and other information for responsive user experiences
- Add conversational memory to support multi-turn interactions
- Add long-term memory to support memory across conversational threads
- Add structured responses
- Deploy your application with LangSmith Deployment

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a semantic search engine with LangChain\\
\\
Previous Build a SQL agent\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/contributing/documentation

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Contribute

Contributing to documentation

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Contribute

- Documentation
- Code
- Integrations

On this page

- Contribute
- Quick edits
- Larger edits and additions
- Set up local environment
- Edit documentation
- Create a sharable preview build
- Run quality checks
- Publish to prod
- Documentation types
- How-to guides
- Conceptual guides
- Reference
- Tutorials
- Writing standards
- Mintlify components
- Page structure
- Co-locate Python and JavaScript/TypeScript content
- Quality standards
- General guidelines
- Accessibility requirements
- Cross-referencing
- Get help

Accessible documentation is a vital part of LangChain. We welcome both documentation for new features and integrations, as well as community improvements to existing docs.

These are contribution guidelines for our open source projects, but they also apply to the LangSmith documentation.

## ​ Contribute

### ​ Quick edits

For quick changes like fixing typos or changing a link, you can edit directly on GitHub without setting up a local development environment:

**Prerequisites:**

- A GitHub account
- Basic familiarity of the fork-and-pull workflow for contributing

1. At the bottom of the page you want to edit, click the link **Edit this page on GitHub**.
2. GitHub will prompt you to fork the repository to your account. Make sure to fork into your personal account.
3. Make the changes directly in GitHub’s web editor.
4. Click **Commit changes…** and give your commit a descriptive title like `fix(docs): summary of change`. If applicable, add an extended description.
5. GitHub will redirect you to create a pull request. Give it a title (often the same as the commit) and follow the PR template checklist.

Docs PRs are typically reviewed within a few days. Keep an eye on your PR to address any feedback from maintainers.Do not bump the PR unless you have new information to provide – maintainers will address it as their availability permits.

### ​ Larger edits and additions

For larger changes, additions, or ongoing contributions, it’s necessary to set up a local development environment on your machine. Our documentation build pipeline offers local preview, important for ensuring your changes appear as intended before submitting.

#### ​ Set up local environment

Before you can work on this project, ensure you have the following installed:**Required:**

- **`uv`** \- Python package manager (used for dependency management)
- **Node.js** and **`npm`** \- For Mintlify CLI and reference documentation builds
- **Make** \- For running build commands
- **Git** \- For version control

**Optional but recommended:**

- **`markdownlint-cli`** \- For linting markdown files

Copy

npm install -g markdownlint-cli

- **`pnpm`** \- Required only if you’re working on reference documentation

npm install -g pnpm@10.14.0

- **Mintlify MDX VSCode extension**

**Setup steps:**

1. Clone the `langchain-ai/docs` repo. Follow the steps outlined in `IDE_SETUP.md`.
2. Install dependencies:

make install

This command will: - Install Python dependencies using `uv sync --all-groups`
- Install Mintlify CLI globally via npm
3. Verify your setup:

make build

This should build the documentation without errors.

After install, you’ll have access to the `docs` command:

docs --help

Common commands:

- `docs dev` \- Start development mode with file watching and hot reload
- `docs build` \- Build documentation

See Available commands for more details.

#### ​ Edit documentation

**Only edit files in `src/`** – The `build/` directory is automatically generated.

1. Ensure your environment is set up and that you have followed the steps in `IDE_SETUP.md` to configure your IDE/editor to automatically apply the correct settings.
2. Edit files in `src/` - Make changes to markdown files and the build system will automatically detect changes and rebuild affected files.
- If OSS content varies between Python and JavaScript/TypeScript, add content for both in the same file. Otherwise, content will be identical for both languages.
- Use Mintlify syntax for formatting.
3. Start development mode to preview changes locally:

docs dev

This starts a development server with hot reload at `http://localhost:3000`.
4. Iterate - Continue editing and see changes reflected immediately.
- The development server rebuilds only changed files for faster feedback.
5. Run the quality checks to ensure your changes are valid.
6. Get approval from the relevant reviewers.LangChain team members can generate a sharable preview build
7. Publish to production (team members only).

#### ​ Create a sharable preview build

Only LangChain team members can create sharable preview builds.

Instructions

Previews are useful for sharing work-in-progress changes with others.When you create or update a PR, a preview branch/ID is automatically generated for you. A comment will be left on the PR with the ID, which you can then use to generate a preview. (You can also run this workflow manually if needed.)

1. Copy the preview branch’s ID from the comment.
2. In the Mintlify dashboard, click **Create preview deployment**.
3. Enter the preview branch’s ID.
4. Click **Create deployment**.
A **Manual update** will display in the **Previews** table.
5. Select the preview and click **Visit** to view the preview build.

To redeploy the preview build with the latest changes, click **Redeploy** on the Mintlify dashboard.

#### ​ Run quality checks

Before submitting changes, ensure your code passes formatting and linting checks:

# Check broken links
make mint-broken-links

# Format code automatically
make format

# Check for linting issues
make lint

# Fix markdown issues
make lint_md_fix

# Run tests to ensure your changes don't break existing functionality
make test

For more details, see the available commands section in the `README`.

#### ​ Publish to prod

Only internal team members can publish to production.

Once your branch has been merged into `main`, you need to push the changes to `prod` for them to render on the live docs site. Use the Publish documentation GH action:

1. Go to Publish documentation.
2. Click the **Run workflow** button.
3. Select the **main** branch to deploy.
4. Click **Run workflow**.

## ​ Documentation types

All documentation falls under one of four categories:

**How-to guides** \\
\\
Task-oriented instructions for users who know what they want to accomplish. **Conceptual guides** \\
\\
Explanations that provide deeper understanding and insights. **Reference** \\
\\
Technical descriptions of APIs and implementation details. **Tutorials** \\
\\
Lessons that guide users through practical activities to build understanding.

Where applicable, all documentation must have both Python and JavaScript/TypeScript content. For more details, see the co-locate Python and JavaScript/TypeScript content section.

### ​ How-to guides

How-to guides are task-oriented instructions for users who know what they want to accomplish. Examples of how-to guides are on the LangChain and LangGraph tabs.

Characteristics

- **Task-focused**: Focus on a specific task or problem
- **Step-by-step**: Break down the task into smaller steps
- **Hands-on**: Provide concrete examples and code snippets

Tips

- Focus on the **how** rather than the **why**
- Use concrete examples and code snippets
- Break down the task into smaller steps
- Link to related conceptual guides and references

Examples

- Messages
- Tools
- Streaming

### ​ Conceptual guides

Conceptual guide cover core concepts abstractly, providing deep understanding.

- **Understanding-focused**: Explain why things work as they do
- **Broad perspective**: Higher and wider view than other types
- **Design-oriented**: Explain decisions and trade-offs
- **Context-rich**: Use analogies and comparisons

- Focus on the **“why”** rather than the “how”
- Provides supplementary information not necessarily required for feature usage
- Can use analogies and reference alternatives
- Avoid blending in too much reference content
- Link to related tutorials and how-to guides

- Memory
- Context
- Graph API
- Functional API

### ​ Reference

Reference documentation contains detailed, low-level information describing exactly what functionality exists and how to use it.

**Python reference** **JavaScript/TypeScript reference**

A good reference should:

- Describe what exists (all parameters, options, return values)
- Be comprehensive and structured for easy lookup
- Serve as the authoritative source for technical details

Contributing to references

See the contributing guide for Python reference docs.

LangChain reference best practices

- **Be consistent**; follow existing patterns for provider-specific documentation
- Include both basic usage (code snippets) and common edge cases/failure modes
- Note when features require specific versions

When to create new reference documentation

- New integrations or providers need dedicated reference pages
- Complex configuration options require detailed explanation
- API changes introduce new parameters or behavior
- Community frequently asks questions about specific functionality

### ​ Tutorials

Tutorials are longer form step-by-step guides that builds upon itself and takes users through a specific practical activity to build understanding. Tutorials are typically found on the Learn tab.

We generally do not merge new tutorials from outside contributors without an acute need. If you feel that a certain topic is missing from docs or is not sufficiently covered, please open a new issue.

- **Practical**: Focus on practical activities to build understanding.
- **Step-by-step**: Break down the activity into smaller steps.
- **Hands-on**: Provide sequential, working code snippets.
- **Supplementary**: Provide additional context and information not necessarily required for feature usage.

- Code snippets should be sequential and working if the user follows the steps in order.
- Provide some context for the activity, but link to related conceptual guides and references for more detailed information.

- Semantic search
- RAG agent

## ​ Writing standards

Reference documentation has different standards - see the reference docs contributing guide for details.

### ​ Mintlify components

Use Mintlify components to enhance readability:

- Callouts

- Structure

- Code

- Always specify language tags on code blocks (e.g., ``` ```python```, ``` ```javascript```).
- Titles for code blocks (e.g. `Success`, `Error Response`)

### ​ Page structure

Every documentation page must begin with YAML frontmatter:

title: "Clear, specific title"
sidebarTitle: "Short title for the sidebar (optional)"

### ​ Co-locate Python and JavaScript/TypeScript content

All documentation must be written in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages:

:::python
Python-specific content. In real docs, the preceding backslash (before `python`) is omitted.
:::

:::js
JavaScript/TypeScript-specific content. In real docs, the preceding backslash (before `js`) is omitted.
:::

Content for both languages (not wrapped)

This will generate two outputs (one for each language) at `/oss/python/concepts/foo.mdx` and `/oss/javascript/concepts/foo.mdx`. Each outputted page will need to be added to the `/src/docs.json` file to be included in the navigation.

We don’t want a lack of parity to block contributions. If a feature is only available in one language, it’s okay to have documentation only in that language until the other language catches up. In such cases, please include a note indicating that the feature is not yet available in the other language.If you need help translating content between Python and JavaScript/TypeScript, please ask in the community slack or tag a maintainer in your PR.

## ​ Quality standards

### ​ General guidelines

Avoid duplication

Multiple pages covering the same material are difficult to maintain and cause confusion. There should be only one canonical page for each concept or feature. Link to other guides instead of re-explaining.

Link frequently

Documentation sections don’t exist in a vacuum. Link to other sections frequently to allow users to learn about unfamiliar topics. This includes linking to API references and conceptual sections.

Be concise

Take a less-is-more approach. If another section with a good explanation exists, link to it rather than re-explain, unless your content presents a new angle.

### ​ Accessibility requirements

Ensure documentation is accessible to all users:

- Structure content for easy scanning with headers and lists
- Use specific, actionable link text instead of “click here”
- Include descriptive alt text for all images and diagrams

### ​ Cross-referencing

Use consistent cross-references to connect docs with API reference documentation.**From docs to API reference:**Use the `@[]` syntax to link to API reference pages:

See @[`ChatAnthropic`] for all configuration options.

The @[`bind_tools`][ChatAnthropic.bind_tools] method accepts...

The build pipeline transforms these into proper markdown links based on the current language scope (Python or JavaScript). For example, `@[ChatAnthropic]` becomes a link to the Python or JS API reference page depending on which version of the docs is being built, **but only if an entry exists in the `link_map.py` file!** See below for details.

How autolinks work

The `@[]` syntax is processed by `handle_auto_links.py`. It looks up link keys in `link_map.py`, which contains dictionary mappings for both Python and JavaScript scopes.**Supported formats:**

| Syntax | Result |
| --- | --- |
| `@[ChatAnthropic]` | Link with “ChatAnthropic” as the displayed text |
| ``@[`ChatAnthropic`]`` | Link with ```ChatAnthropic``` (code formatted) as text |
| `@[text][ChatAnthropic]` | Link with “text” as text and `ChatAnthropic` as the key in the link map |
| `\@[ChatAnthropic]` | Escaped: renders as literal `@[ChatAnthropic]` (no link – what’s being used on this page!) |

**Adding new links:**If a link isn’t found in the map, it will be left unchanged in the output. To add a new autolink:

1. Open `pipeline/preprocessors/link_map.py`
2. Add an entry to the appropriate scope (`python` or `js`) in `LINK_MAPS`
3. The key is the link name used in `@[key]` or `@[text][key]`, the value is the path relative to the reference host

**From API reference stubs to OSS docs:**See the `README` for more information on linking from API reference stubs to Python OSS docs. Specifically see the `mkdocstrings` cross-reference linking syntax.

## ​ Get help

Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please ask in the community slack or open a forum post. Internal team members can reach out in the #documentation Slack channel.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Contributing\\
\\
Previous Contributing to code\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/contributing/code

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Contribute

Contributing to code

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Contribute

- Documentation
- Code
- Integrations

On this page

- Getting started
- Quick fix: submit a bugfix
- Full development setup
- Contribution guidelines
- Backwards compatibility
- New features
- Security guidelines
- Development environment
- Repository structure
- Development workflow
- Testing requirements
- Unit tests
- Integration tests
- Code quality standards
- Testing and validation
- Running tests locally
- Test writing guidelines
- Getting help

Code contributions are always welcome! Whether you’re fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.

## ​ Getting started

Before submitting large **new features or refactors**, please first discuss your ideas in the forum. This ensures alignment with project goals and prevents duplicate work.This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. See the quickstart guide below.

### ​ Quick fix: submit a bugfix

For simple bugfixes, you can get started immediately:

1

Reproduce the issue

Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification

2

Fork the repository

Fork the LangChain or LangGraph repo to your personal GitHub account

3

Clone and setup

Copy

git clone

# For instance, for LangChain:

# For LangGraph:

# Inside your repo, install dependencies
uv sync --all-groups

You will need to install `uv` if you haven’t previously

4

Create a branch

Create a new branch for your fix. This helps keep your changes organized and makes it easier to submit a pull request later.

git checkout -b your-username/short-bugfix-name

5

Write failing tests

Add unit tests that will fail without your fix. This allows us to verify the bug is resolved and prevents regressions

6

Make your changes

Fix the bug while following our code quality standards. Make the **minimal change necessary** to resolve the issue. We strongly encourage contributors to comment on the issue before they start coding. For example:_“I’d like to work on this. My intended approach would be to \[…brief description…\]. Does this align with maintainer expectations?”_A 30-second comment often prevents wasted effort if your initial approach is wrong.

7

Verify the fix

Ensure that tests pass and no regressions are introduced. Ensure all tests pass locally before submitting your PR

make lint
make test

# For bugfixes involving integrations, also run:
make integration_tests
# (You may need to set up API testing credentials)

8

Document the change

Update docstrings if behavior changes, add comments for complex logic

9

Submit a pull request

Follow the PR template provided. If applicable, reference the issue you’re fixing using a closing keyword (e.g. `Fixes #ISSUE_NUMBER`) so that the issue is automatically closed when your PR is merged.

### ​ Full development setup

For ongoing development or larger contributions:

1. Review our contribution guidelines for features, bugfixes, and integrations
2. Set up your environment following our setup guide below
3. Understand the repository structure and package organization
4. Learn our development workflow including testing and linting

* * *

## ​ Contribution guidelines

Before you start contributing to LangChain, take a moment to think about why you want to. If your only goal is to add a “first contribution” to your resume (or if you’re just looking for a quick win) you might be better off doing a boot-camp or an online tutorial.Contributing to open source projects takes time and effort, but it can also help you become a better developer and learn new skills. However, it’s important to know that it might be harder and slower than following a training course. That said, contributing to open source is worth it if you’re willing to take the time to do things well.

### ​ Backwards compatibility

Breaking changes to public APIs are not allowed except for critical security fixes.See our versioning policy for details on major version releases.

Maintain compatibility via:

Stable interfaces

**Always preserve**:

- Function signatures and parameter names
- Class interfaces and method names
- Return value structure and types
- Import paths for public APIs

Safe changes

**Acceptable modifications**:

- Adding new optional parameters
- Adding new methods to classes
- Improving performance without changing behavior
- Adding new modules or functions

Before making changes

- **Would this break existing user code?**
- Check if your target is public
- If needed, is it exported in `__init__.py`?
- Are there existing usage patterns in tests?

### ​ New features

We aim to keep the bar high for new features. We generally don’t accept new core abstractions from outside contributors without an existing issue that demonstrates an acute need for them. This also applies to changes to infra and dependencies.In general, feature contribution requirements include:

Design discussion

Open an issue describing:

- The problem you’re solving
- Proposed API design
- Expected usage patterns

Implementation

- Follow existing code patterns
- Include comprehensive tests and documentation
- Consider security implications

Integration considerations

- How does this interact with existing features?
- Are there performance implications?
- Does this introduce new dependencies?

We will reject features that are likely to lead to security vulnerabilities or reports.

### ​ Security guidelines

Security is paramount. Never introduce vulnerabilities or unsafe patterns.

Security checklist:

Input validation

- Validate and sanitize all user inputs
- Properly escape data in templates and queries
- Never use `eval()`, `exec()`, or `pickle` on user data, as this can lead to arbitrary code execution vulnerabilities

Error handling

- Use specific exception types
- Don’t expose sensitive information in error messages
- Implement proper resource cleanup

Dependencies

- Avoid adding hard dependencies
- Keep optional dependencies minimal
- Review third-party packages for security issues

## ​ Development environment

Our Python projects use `uv` for dependency management. Make sure you have the latest version installed.

Once you’ve reviewed the contribution guidelines, set up a development environment for the package(s) you’re working on.

- LangChain

- LangGraph

Core abstractions

For changes to `langchain-core`:

cd libs/core
uv sync --all-groups
make test # Ensure tests pass before starting development

Main package

For changes to `langchain`:

cd libs/langchain
uv sync --all-groups
make test # Ensure tests pass before starting development

Partner packages

For changes to partner integrations:

cd libs/partners/langchain-{partner}
uv sync --all-groups
make test # Ensure tests pass before starting development

Community packages

For changes to community integrations (located in a separate repo):

cd libs/community/langchain_community/path/to/integration
uv sync --all-groups
make test # Ensure tests pass before starting development

WIP - coming soon! In the meantime, follow instructions for LangChain.

## ​ Repository structure

LangChain is organized as a monorepo with multiple packages:

Core packages

- **`langchain`** (located in `libs/langchain/`): Main package with chains, agents, and retrieval logic
- **`langchain-core`** (located in `libs/core/`): Base interfaces and core abstractions

Located in `libs/partners/`, these are independently versioned packages for specific integrations. For example:

- **`langchain-openai`**: OpenAI integrations
- **`langchain-anthropic`**: Anthropic integrations
- **`langchain-google-genai`**: Google Generative AI integrations

Many partner packages are in external repositories. Please check the list of integrations for details.

Supporting packages

- **`langchain-text-splitters`**: Text splitting utilities
- **`langchain-standard-tests`**: Standard test suites for integrations
- **`langchain-cli`**: Command line interface
- **`langchain-community`**: Community maintained integrations (located in a separate repo)

## ​ Development workflow

### ​ Testing requirements

Directories are relative to the package you’re working in.

Every code change must include comprehensive tests.

#### ​ Unit tests

**Location**: `tests/unit_tests/`**Requirements**:

- No network calls allowed
- Test all code paths including edge cases
- Use mocks for external dependencies

make test

# Or directly:
uv run --group test pytest tests/unit_tests

#### ​ Integration tests

**Location**: `tests/integration_tests/`Integration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.Not every code change will require an integration test, but keep in mind that we’ll require/ run integration tests separately as apart of our review process.**Requirements**:

- Test real integrations with external services
- Use environment variables for API keys
- Skip gracefully if credentials unavailable

### ​ Code quality standards

Contributions must adhere to the following quality requirements:

- Type hints

- Documentation

- Code style

**Required**: Complete type annotations for all functions

def process_documents(
docs: list[Document],
processor: DocumentProcessor,
*,
batch_size: int = 100

"""Process documents in batches.

Args:
docs: List of documents to process.
processor: Document processing instance.
batch_size: Number of documents per batch.

Returns:
Processing results with success/failure counts.
"""

**Required**: Google-style docstrings for all public functions.**Guiding principle**: Docstrings describe “what”; docs on this site explain the “how” and “why.”

| Content type | Location | Purpose |
| --- | --- | --- |
| Parameter descriptions | Docstrings | Auto-generates into API reference |
| Return types and exceptions | Docstrings | API reference |
| Minimal usage example | Docstrings | Show basic instantiation pattern |
| Feature tutorials | This site | In-depth walkthroughs |
| End-to-end examples | This site | Real-world usage patterns |
| Conceptual explanations | This site | Understanding and context |

**Docstrings should contain:**

1. One-line summary of what the class/function does
2. Link to this site for tutorials, guides, and usage patterns
3. Parameter documentation with types and descriptions
4. Return value description
5. Exceptions that may be raised
6. Single minimal example showing basic instantiation/usage as necessary

Good docstring example

class ChatAnthropic(BaseChatModel):
"""Interface to Claude chat models.

See the usage guide
for tutorials, feature walkthroughs, and examples.

Args:
model: Model identifier (e.g., `'claude-sonnet-4-5-20250929'`).
temperature: Sampling temperature between `0` and `1`.
max_tokens: Maximum number of tokens to generate.
api_key: Anthropic API key.

If not provided, reads from the `ANTHROPIC_API_KEY`
environment variable.
timeout: Request timeout in seconds.
max_retries: Maximum number of retries for failed requests.

Returns:
A chat model instance that can be invoked with messages.

Raises:
ValueError: If the model identifier is not recognized.
AuthenticationError: If the API key is invalid.

Example:
```python
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
response = model.invoke("Hello!")

"""

What does NOT belong in docstrings

Avoid duplicating content that belongs in OSS docs:

- **Feature tutorials**: Don’t include extended walkthroughs. Instead, link to this site:

"""
...

See the extended thinking guide
for configuration options.

...
"""

- **Multiple example variations**: Include one minimal example, then link to comprehensive guides:

"""
Example:
\`\`\`python
message = HumanMessage(content=[\
{"type": "image", "url": "https://example.com/image.jpg"}\
])
\`\`\`

See the multimodal guide
for all supported input formats.
"""

- **Conceptual explanations**: Keep to factual parameter descriptions. Link to docs for deeper context.
- **MkDocs-specific syntax**: Avoid `???+`, accordions, or tabs in docstrings. They don’t render in IDEs.

**Automated**: Formatting and linting via `ruff`

make format # Apply formatting
make lint # Check style and types

**Standards**:

- Descriptive variable names
- Break up complex functions (aim for fewer than 20 lines)
- Follow existing patterns in the codebase

## ​ Testing and validation

### ​ Running tests locally

Before submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.

Unit tests

All unit tests must pass

Integration tests

(Run if your changes affect integrations)

Formatting

make format
make lint

Code must pass all style checks

Type checking

make type_check

All type hints must be valid

PR submission

Push your branch and open a pull request. Follow the provided form template. Note related issues using a closing keyword. After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.

### ​ Test writing guidelines

In order to write effective tests, there’s a few good practices to follow:

- Use natural language to describe the test in docstrings
- Use descriptive variable names
- Be exhaustive with assertions

- Unit tests

- Integration tests

- Mock usage

def test_document_processor_handles_empty_input():
"""Test processor gracefully handles empty document list."""
processor = DocumentProcessor()

result = processor.process([])

assert result.success
assert result.processed_count == 0
assert len(result.errors) == 0

@pytest.mark.requires("openai")
def test_openai_chat_integration():
"""Test OpenAI chat integration with real API."""

chat = ChatOpenAI()
response = chat.invoke("Hello")

assert isinstance(response.content, str)

def test_retry_mechanism(mocker):
"""Test retry mechanism handles transient failures."""
mock_client = mocker.Mock()
mock_client.call.side_effect = [\
ConnectionError("Temporary failure"),\
{"result": "success"}\
]

service = APIService(client=mock_client)
result = service.call_with_retry()

assert result["result"] == "success"
assert mock_client.call.call_count == 2

## ​ Getting help

Our goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the community slack or open a forum post.

You’re now ready to contribute high-quality code to LangChain!

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Contributing to documentation\\
\\
Previous Contributing integrations\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/contributing/integrations-langchain

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integrations

Contributing integrations

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Contribute

- Documentation
- Code
- Integrations

- Guide
- Implement
- Standard tests
- Publish
- Co-marketing

On this page

- Why contribute an integration to LangChain?
- Components to integrate
- How to contribute an integration

**Integrations are a core component of LangChain.**LangChain provides standard interfaces for several different components (language models, vector stores, etc) that are crucial when building LLM applications. Contributing an integration helps expand LangChain’s ecosystem and makes your service discoverable to millions of developers.

## ​ Why contribute an integration to LangChain?

## Discoverability

LangChain is the most used framework for building LLM applications, with over 20 million monthly downloads.

## Interoperability

LangChain components expose a standard interface, allowing developers to easily swap them for each other. If you implement a LangChain integration, any developer using a different component will easily be able to swap yours in.

## Best Practices

Through their standard interface, LangChain components encourage and facilitate best practices (streaming, async, etc.) that improve developer experience and application performance.

## ​ Components to integrate

While any component can be integrated into LangChain, there are specific types of integrations we encourage more:**Integrate these ✅**:

- **Chat Models**: Most actively used component type
- **Tools/Toolkits**: Enable agent capabilities
- **Retrievers**: Core to RAG applications
- **Embedding Models**: Foundation for vector operations
- **Vector Stores**: Essential for semantic search

**Not these ❌**:

- **LLMs (Text-Completion Models)**: Deprecated in favor of Chat Models
- **Document Loaders**: High maintenance burden
- **Key-Value Stores**: Limited usage
- **Document Transformers**: Niche use cases
- **Model Caches**: Infrastructure concerns
- **Graphs**: Complex abstractions
- **Message Histories**: Storage abstractions
- **Callbacks**: System-level components
- **Chat Loaders**: Limited demand
- **Adapters**: Edge case utilities

## ​ How to contribute an integration

1

Confirm eligibility

Verify that your integration is in the list of encouraged components we are currently accepting.

2

Implement your package

**How to implement a LangChain integration**

3

Pass standard tests

If applicable, implement support for LangChain’s standard test suite for your integration and successfully run them.

4

Publish integration

**How to publish an integration**

5

Add documentation

Open a PR to add documentation for your integration to the official LangChain docs.

Integration documentation guide

- Chat models
- Tools and toolkits
- Retrievers
- Text splitters - Coming soon
- Embedding models - Coming soon
- Vector stores
- Document loaders - Coming soon
- Key-value stores - Coming soon

For reference docs, please open an issue on the repo so that a maintainer can add them.

Co-marketing

(Optional) Engage with the LangChain team for joint co-marketing.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Contributing to code\\
\\
Previous Implement a LangChain integration\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/contributing/documentation)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Contribute

- Documentation
- Code
- Integrations

404

# Page not found

We couldn’t find the page you were looking for.

Contributing to documentation Contributing integrations LangSmith reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/contributing/code)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Contribute

- Documentation
- Code
- Integrations

404

# Page not found

We couldn’t find the page you were looking for.

Contributing to code Contributing to documentation Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/contributing/integrations-langchain)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Contribute

- Documentation
- Code
- Integrations

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Contributing integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/quickstart).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Quickstart Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/common-errors

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Errors

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

This page contains guides around resolving common errors you may find while building with LangChain and LangGraph.Errors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.

| Error code |
| --- |
| GRAPH\_RECURSION\_LIMIT |
| INVALID\_CHAT\_HISTORY |
| INVALID\_CONCURRENT\_GRAPH\_UPDATE |
| INVALID\_GRAPH\_NODE\_RETURN\_VALUE |
| INVALID\_PROMPT\_INPUT |
| INVALID\_TOOL\_RESULTS |
| MESSAGE\_COERCION\_FAILURE |
| MISSING\_CHECKPOINTER |
| MODEL\_AUTHENTICATION |
| MODEL\_NOT\_FOUND |
| MODEL\_RATE\_LIMIT |
| MULTIPLE\_SUBGRAPHS |
| OUTPUT\_PARSING\_FAILURE |

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Deep Agents\\
\\
Previous Versioning\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/versioning

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Releases

Versioning

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

On this page

- Version numbering
- API stability
- Stable APIs
- Beta APIs
- Alpha APIs
- Deprecated APIs
- Internal APIs
- Release cycles
- Version support policy
- Long-term support (LTS) releases
- Check your version
- Upgrade
- Pre-release versions
- See also

Each LangChain and LangGraph version number follows the format: `MAJOR.MINOR.PATCH`

- **Major**: Breaking API updates that require code changes.
- **Minor**: New features and improvements that maintain backward compatibility.
- **Patch**: Bug fixes and minor improvements.

## ​ Version numbering

LangChain and LangGraph follow Semantic Versioning principles:

- `1.0.0`: First stable release with production-ready APIs
- `1.1.0`: New features added in a backward-compatible manner
- `1.0.1`: Backward-compatible bug fixes

## ​ API stability

We communicate the stability of our APIs as follows:

### ​ Stable APIs

All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features and only introduce breaking changes in major releases.

### ​ Beta APIs

APIs marked as `beta` are feature-complete but may undergo minor changes based on user feedback. They are safe for production use but may require small adjustments in future releases.

### ​ Alpha APIs

APIs marked as `alpha` are experimental and subject to significant changes. Use these with caution in production environments.

### ​ Deprecated APIs

APIs marked as `deprecated` will be removed in future major releases. When possible, we specify the intended version of removal. To handle deprecations:

1. Switch to the recommended alternative API
2. Follow the migration guide (released alongside major releases)
3. Use automated migration tools when available

### ​ Internal APIs

Certain APIs are explicitly marked as “internal” in a couple of ways:

- Some documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.
- Functions, methods, and other objects prefixed by a leading underscore ( **`_`**). This is the standard Python convention of indicating that something is private; if any method starts with a single **`_`**, it’s an internal API.

- **Exception:** Certain methods are prefixed with `_` , but do not contain an implementation. These methods are _meant_ to be overridden by sub-classes that provide the implementation. Such methods are generally part of the **Public API** of LangChain.

## ​ Release cycles

Major releases

Major releases (e.g., `1.0.0` → `2.0.0`) may include:

- Breaking API changes
- Removal of deprecated features
- Significant architectural improvements

We provide:

- Detailed migration guides
- Automated migration tools when possible
- Extended support period for the previous major version

Minor releases

Minor releases (e.g., `1.0.0` → `1.1.0`) include:

- New features and capabilities
- Performance improvements
- New optional parameters
- Backward-compatible enhancements

Patch releases

Patch releases (e.g., `1.0.0` → `1.0.1`) include:

- Bug fixes
- Security updates
- Documentation improvements
- Performance optimizations without API changes

## ​ Version support policy

- **Latest major version**: Full support with active development (ACTIVE status)
- **Previous major version**: Security updates and critical bug fixes for 12 months after the next major release (MAINTENANCE status)
- **Older versions**: Community support only

### ​ Long-term support (LTS) releases

Both LangChain and LangGraph 1.0 are designated as LTS releases:

- Version 1.0 will remain in ACTIVE status until version 2.0 is released
- After version 2.0 is released, version 1.0 will enter MAINTENANCE mode for at least 1 year
- LTS releases follow semantic versioning (semver), allowing safe upgrades between minor versions
- Legacy versions (LangChain 0.3 and LangGraph 0.4) are in MAINTENANCE mode until December 2026

For detailed information about release status and support timelines, see the Release policy.

## ​ Check your version

To check your installed version:

LangChain

LangGraph

Copy

import langchain_core
print(langchain_core.__version__)

## ​ Upgrade

# Upgrade to the latest version
pip install -U langchain-core langchain

# Upgrade to a specific version
pip install langchain-core==1.0.0

## ​ Pre-release versions

We occasionally release alpha and beta versions for early testing:

- **Alpha** (e.g., `1.0.0a1`): Early preview, significant changes expected
- **Beta** (e.g., `1.0.0b1`): Feature-complete, minor changes possible
- **Release Candidate** (e.g., `1.0.0rc1`): Final testing before stable release

## ​ See also

- Release policy \- Detailed release and deprecation policies
- Releases \- Version-specific release notes and migration guides

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Errors\\
\\
Previous Changelog\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/release-policy

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Policies

Release policy

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

On this page

- Release cadence
- API stability
- Stability of other packages
- Deprecation policy
- Long-term support (LTS)
- Release status definitions
- Current LTS releases
- Legacy version support
- Special considerations

This page explains the LangChain and LangGraph release policies. Click on the tabs below to view the release policies for each:

- LangChain

- LangGraph

The LangChain ecosystem is composed of different component packages (e.g., `langchain-core`, `langchain`, `langchain-community`, partner packages, etc.)

## ​ Release cadence

With the release of LangChain 1.0, **minor** releases (e.g., from `1.0.x` to `1.1.0`) of `langchain` and `langchain-core` follow semantic versioning and may be released frequently. Minor releases contain new features and improvements but do not include breaking changes.Patch versions are released frequently, up to a few times per week, as they contain bug fixes and minor improvements.

## ​ API stability

The development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in `langchain` and `langchain-core` will continue to evolve to better serve the needs of our users.With LangChain 1.0’s adoption of semantic versioning:

- Breaking changes to the public API will only occur in major version releases (e.g., `2.0.0`)
- Minor version bumps (e.g., `1.0.0` to `1.1.0`) add new features without breaking changes
- Patch version bumps (e.g., `1.0.0` to `1.0.1`) contain bug fixes and minor improvements

We will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed.

### ​ Stability of other packages

The stability of other packages in the LangChain ecosystem may vary:

- **Partner packages maintained by LangChain** (such as `langchain-openai` and `langchain-anthropic`) follow semantic versioning and are expected to be stable post 1.0. Other partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information.
- **`langchain-community`** is a community maintained package that contains 3rd party integrations. Due to the number of integrations there, `langchain-community` does not follow the same strict semantic versioning policy as `langchain` and `langchain-core`. See the “Special considerations” section under Long-term support for more details.

## ​ Deprecation policy

We will generally avoid deprecating features until a better alternative is available.With LangChain 1.0’s semantic versioning approach, deprecated features will continue to work throughout the entire 1.x release series. Breaking changes, including the removal of deprecated features, will only occur in major version releases (e.g., 2.0).When a feature is deprecated in `langchain` or `langchain-core`, we will:

- Clearly mark it as deprecated in the code and documentation
- Provide migration guidance to the recommended alternative
- Provide security updates for the deprecated feature through all 1.x minor releases

In some situations, we may allow deprecated features to remain in the code base even longer if they are not causing maintenance issues, to further reduce the burden on users.

## ​ Long-term support (LTS)

LangChain follows a long-term support (LTS) policy to provide stability for production applications:

### ​ Release status definitions

Packages are marked with one of the following statuses:

- **ACTIVE**: Current active development, includes bug fixes, security patches, and new features
- **MAINTENANCE**: Receives all security patches and critical bug fixes, but no new features

### ​ Current LTS releases

**LangChain 1.0** is designated as an LTS release:

- **Status**: ACTIVE until the release of 2.0
- **Support period**: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year
- **Semver compliance**: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes

### ​ Legacy version support

**LangChain 0.3**:

- **Status**: MAINTENANCE mode
- **Support period**: Until December 2026
- **Support includes**: Security patches and critical bug fixes

### ​ Special considerations

**langchain-community 0.4**: Due to the nature of community contributions and third-party integrations, `langchain-community` may have breaking changes on minor releases. It has been released as version 0.4 to reflect this different stability policy.

LangGraph follows a structured release policy to ensure stability and predictability for users building production applications.

We expect to space out **major** releases by at least 6-12 months to provide stability for production applications.**Minor** releases are typically released every 1-2 months with new features and improvements.**Patch** releases are released as needed, often weekly, to address bugs and security issues.

### ​ Stable APIs

All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features within a major version.

### ​ Beta features

Features marked as `beta` in the documentation are:

- Feature-complete and tested
- Safe for production use with the understanding they may change
- Subject to minor API adjustments based on user feedback

### ​ Experimental features

Features marked as `experimental` or `alpha`:

- Are under active development
- May change significantly or be removed
- Should be used with caution in production

### ​ Internal APIs

APIs prefixed with underscore (`_`) or explicitly marked as internal:

- Are not part of the public API
- May change without notice
- Should not be used directly

When deprecating features:

1. **Deprecation Notice**: Features are marked as deprecated with clear migration guidance
2. **Grace Period**: Deprecated features remain functional for at least one minor version
3. **Removal**: Features are removed only in major version releases
4. **Migration Support**: We provide migration guides and, when possible, automated tools

## ​ Platform compatibility

### ​ Python support

- We support Python versions that are actively maintained by the Python Software Foundation
- Python version requirements may change only in major releases
- Currently requires Python 3.10 or later

## ​ Breaking changes

Breaking changes are only introduced in major versions and include:

- Removal of deprecated APIs
- Changes to required parameters
- Changes to default behavior that affect existing applications
- Minimum Python/Node.js version updates

## ​ Migration support

For major version upgrades, we provide:

- Comprehensive migration guides
- Automated migration scripts when feasible
- Extended support period for the previous major version
- Clear documentation of all breaking changes

LangGraph follows a long-term support (LTS) policy to provide stability for production applications:

### ​ Release status definitions

### ​ Current LTS releases

**LangGraph 1.0** is designated as an LTS release:

### ​ Legacy version support

**LangGraph 0.4**:

- **Status**: MAINTENANCE mode
- **Support period**: Until December 2026
- **Support includes**: All security patches and critical bug fixes

## ​ See also

- Versioning \- Version numbering and support details
- Releases \- Version-specific release notes and migration guides

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangGraph v1 migration guide\\
\\
Previous Security policy\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/security-policy

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Policies

Security policy

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

On this page

- Best practices
- Reporting OSS vulnerabilities
- Bug bounty eligibility
- Out-of-scope targets
- Reporting LangSmith Vulnerabilities
- Other Security Concerns

LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.

## ​ Best practices

When building such applications developers should remember to follow good security practices:

- **Limit permissions**: Scope permissions specifically to the application’s need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), specifying proxy configurations to control external requests, etc. as appropriate for your application.
- **Anticipate potential misuse**: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it’s safest to assume that any LLM able to use those credentials may in fact delete data.
- **Defense in depth**: No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It’s best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.

Risks of not doing so include, but are not limited to:

- Data corruption or loss.
- Unauthorized access to confidential information.
- Compromised performance or availability of critical resources.

Example scenarios with mitigation strategies:

- A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container.
- A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse.
- A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.

If you’re building applications that access external resources like file systems, APIs
or databases, consider speaking with your company’s security team to determine how to best
design and secure your applications.

## ​ Reporting OSS vulnerabilities

Please report security vulnerabilities associated with the LangChain open source projects using the following process:

1. **Submit a security advisory** on the Security tab in the GitHubrepository where the vulnerability exists.
2. **Send an email** to `security@langchain.dev` notifying us that you’ve filed a security issue and which repository it was filed in.

Before reporting a vulnerability, please review the Best Practices above to understand what we consider to be a security vulnerability vs. developer responsibility.

### ​ Bug bounty eligibility

We welcome security vulnerability reports for all LangChain libraries. However, we may offer ad hoc bug bounties only for vulnerabilities in the following packages:

- Core libraries owned and maintained by the LangChain team: `langchain-core`, `langchain` (v1), `langgraph`, and related checkpointer packages (or their JavaScript equivalents)
- Popular integrations maintained by the LangChain team (e.g., `langchain-openai`, `langchain-anthropic`, etc., or their JavaScript equivalents)

The vulnerability must be in the library code itself, not in example code or example applications.We welcome reports for all other LangChain packages and will address valid security concerns, but bug bounties will not be awarded for packages outside this scope. This includes `langchain-community`, which due to its community-driven nature is not eligible for bug bounties, though we will accept and address reports.

### ​ Out-of-scope targets

The following are out-of-scope for security vulnerability reports:

- **langchain-experimental**: This repository is for experimental code and is not in scope for security reports (see package warning).
- **Examples and example applications**: Example code and demo applications are not in scope for security reports.
- **Code documented with security notices**: This will be decided on a case-by-case basis, but likely will not be in scope as the code is already documented with guidelines for developers that should be followed for making their application secure.
- **LangSmith related repositories or APIs**: See Reporting LangSmith Vulnerabilities below.

## ​ Reporting LangSmith Vulnerabilities

Please report security vulnerabilities associated with LangSmith by email to `security@langchain.dev`.

- LangSmith site:
- SDK client:

### ​ Other Security Concerns

For any other security concerns, please contact us at `security@langchain.dev`.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Release policy\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/releases/changelog/rss.xml

15 Dec 2025 16:33:34 GMT`langchain` v1.2.0

- `create_agent`: Simplified support for provider-specific tool parameters and definitions via a new `extras` attribute on tools. Examples:

- Provider-specific configuration such as Anthropic's programmatic tool calling and tool search.
- Built-in tools that are executed client-side, as supported by Anthropic, OpenAI, and other providers.
- Support for strict schema-adherence in agent `response_format` (see `ProviderStrategy` docs).

08 Dec 2025 19:35:47 GMT`langchain-google-genai` v4.0.0

We've re-written the Google GenAI integration to use Google's consolidated Generative AI SDK, which provides access to the Gemini API and Vertex AI Platform under the same interface. This includes minimal breaking changes as well as deprecated packages in `langchain-google-vertexai`.

See the full release notes and migration guide for details.

24 Nov 2025 15:43:09 GMT`langchain` v1.1.0

- Model profiles: Chat models now expose supported features and capabilities through a `.profile` attribute. These data are derived from models.dev, an open source project providing model capability data.
- Summarization middleware: Updated to support flexible trigger points using model profiles for context-aware summarization.
- Structured output: `ProviderStrategy` support (native structured output) can now be inferred from model profiles.
- `SystemMessage` for `create_agent`: Support for passing `SystemMessage` instances directly to `create_agent`'s `system_prompt` parameter, enabling advanced features like cache control and structured content blocks.
- Model retry middleware: New middleware for automatically retrying failed model calls with configurable exponential backoff.
- Content moderation middleware: OpenAI content moderation middleware for detecting and handling unsafe content in agent interactions. Supports checking user input, model output, and tool results.

01 Dec 2025 21:38:10 GMTv1.0.0

### `langchain`

- Release notes
- Migration guide

### `langgraph`

\]\]>

---

# https://docs.langchain.com/oss/python/integrations/chat/anthropic

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

ChatAnthropic

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Integration details
- Model features
- Setup
- Installation
- Credentials
- Instantiation
- Invocation
- Content blocks
- Tools
- Strict tool use
- Input examples
- Token-efficient tool use
- Fine-grained tool streaming
- Programmatic tool calling
- Multimodal
- Supported input methods
- Image input
- PDF input
- Extended thinking
- Effort
- Citations
- Simple example
- In tool results (agentic RAG)
- Using with text splitters
- Prompt caching
- Messages
- Caching tools
- Incremental caching in conversational applications
- Token counting
- Context management
- Extended context window
- Structured output
- Built-in tools
- Bash tool
- Code execution
- Computer use
- Remote MCP
- Text editor
- Web fetching
- Web search
- Memory tool
- Tool search
- Response metadata
- Token usage metadata
- API reference

You can find information about Anthropic’s latest models, their costs, context windows, and supported input types in the Claude docs.

**API Reference**For detailed documentation of all features and configuration options, head to the `ChatAnthropic` API reference.

**AWS Bedrock and Google VertexAI**Note that certain Anthropic models can also be accessed via AWS Bedrock and Google VertexAI. See the `ChatBedrock` and `ChatVertexAI` integrations to use Anthropic models via these services.

## ​ Overview

### ​ Integration details

| Class | Package | Local | Serializable | JS/TS Support | Downloads | Latest Version |
| --- | --- | --- | --- | --- | --- | --- |
| `ChatAnthropic` | `langchain-anthropic` | ❌ | beta | ✅ (npm) | ![Downloads per month](https://pypi.org/project/langchain-anthropic/) | ![PyPI - Latest version](https://pypi.org/project/langchain-anthropic/) |

### ​ Model features

| Tool calling | Structured output | JSON mode | Image input | Audio input | Video input | Token-level streaming | Native async | Token usage | Logprobs |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | ❌ |

## ​ Setup

To access Anthropic (Claude) models you’ll need to install the `langchain-anthropic` integration package and acquire a Claude API key.

### ​ Installation

pip

uv

Copy

pip install -U langchain-anthropic

### ​ Credentials

Head to the Claude console to sign up and generate a Claude API key. Once you’ve done this set the `ANTHROPIC_API_KEY` environment variable:

import getpass
import os

if "ANTHROPIC_API_KEY" not in os.environ:
os.environ["ANTHROPIC_API_KEY"] = getpass.getpass("Enter your Anthropic API key: ")

To enable automated tracing of your model calls, set your LangSmith API key:

os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
os.environ["LANGSMITH_TRACING"] = "true"

## ​ Instantiation

Now we can instantiate our model object and generate chat completions:

from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
model="claude-haiku-4-5-20251001",
# temperature=,
# max_tokens=,
# timeout=,
# max_retries=,
# ...
)

See the `ChatAnthropic` API reference for details on all available instantiation parameters.

## ​ Invocation

Invoke

messages = [\
(\
"system",\
"You are a helpful translator. Translate the user sentence to French.",\
),\
(\
"human",\
"I love programming.",\
),\
]
model.invoke(messages)

print(ai_msg.text)

J'adore la programmation.

Stream

for chunk in model.stream(messages):
print(chunk.text, end="")

AIMessageChunk(content="J", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
AIMessageChunk(content="'", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
AIMessageChunk(content="a", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
AIMessageChunk(content="ime", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
AIMessageChunk(content=" la", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
AIMessageChunk(content=" programm", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
AIMessageChunk(content="ation", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")
AIMessageChunk(content=".", id="run-272ff5f9-8485-402c-b90d-eac8babc5b25")

To aggregate the full message from the stream:

stream = model.stream(messages)
full = next(stream)
for chunk in stream:
full += chunk
full

AIMessageChunk(content="J'aime la programmation.", id="run-b34faef0-882f-4869-a19c-ed2b856e6361")

Async

await model.ainvoke(messages)

# stream
async for chunk in (await model.astream(messages))

# batch
await model.abatch([messages])

AIMessage(
content="J'aime la programmation.",
response_metadata={
"id": "msg_01Trik66aiQ9Z1higrD5XFx3",
"model": "claude-sonnet-4-5-20250929",
"stop_reason": "end_turn",
"stop_sequence": None,
"usage": {"input_tokens": 25, "output_tokens": 11},
},
id="run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0",
usage_metadata={
"input_tokens": 25,
"output_tokens": 11,
"total_tokens": 36,
},
)

Learn more about supported invocation methods in our models guide.

## ​ Content blocks

When using tools, extended thinking, and other features, content from a single Anthropic `AIMessage` can either be a single string or a list of Anthropic content blocks.For example, when an Anthropic model invokes a tool, the tool invocation is part of the message content (as well as being exposed in the standardized `AIMessage.tool_calls`):

from langchain_anthropic import ChatAnthropic
from typing_extensions import Annotated

model = ChatAnthropic(model="claude-haiku-4-5-20251001")

def get_weather(
location: Annotated[str, ..., "Location as city and state."]

"""Get the weather at a location."""
return "It's sunny."

model_with_tools = model.bind_tools([get_weather])
response = model_with_tools.invoke("Which city is hotter today: LA or NY?")
response.content

[{'text': "I'll help you compare the temperatures of Los Angeles and New York by checking their current weather. I'll retrieve the weather for both cities.",\
'type': 'text'},\
{'id': 'toolu_01CkMaXrgmsNjTso7so94RJq',\
'input': {'location': 'Los Angeles, CA'},\
'name': 'get_weather',\
'type': 'tool_use'},\
{'id': 'toolu_01SKaTBk9wHjsBTw5mrPVSQf',\
'input': {'location': 'New York, NY'},\
'name': 'get_weather',\
'type': 'tool_use'}]

Using `content_blocks` will render the content in LangChain’s standard format that is consistent across other model providers. Read more about content blocks.

response.content_blocks

You can also access tool calls specifically in a standard format using the
`tool_calls` attribute:

response.tool_calls

[{'name': 'GetWeather',\
'args': {'location': 'Los Angeles, CA'},\
'id': 'toolu_01Ddzj5PkuZkrjF4tafzu54A'},\
{'name': 'GetWeather',\
'args': {'location': 'New York, NY'},\
'id': 'toolu_012kz4qHZQqD4qg8sFPeKqpP'}]

## ​ Tools

Anthropic’s tool use features allow you to define external functions that Claude can call during a conversation. This enables dynamic information retrieval, computations, and interactions with external systems.See `ChatAnthropic.bind_tools` for details on how to bind tools to your model instance.

For information about Claude’s built-in tools (code execution, web browsing, files API, etc), see the Built-in tools.

from pydantic import BaseModel, Field

class GetWeather(BaseModel):
'''Get the current weather in a given location'''

location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

class GetPopulation(BaseModel):
'''Get the current population in a given location'''

model_with_tools = model.bind_tools([GetWeather, GetPopulation])
ai_msg = model_with_tools.invoke("Which city is hotter today and which is bigger: LA or NY?")
ai_msg.tool_calls

[\
{\
"name": "GetWeather",\
"args": {"location": "Los Angeles, CA"},\
"id": "toolu_01KzpPEAgzura7hpBqwHbWdo",\
},\
{\
"name": "GetWeather",\
"args": {"location": "New York, NY"},\
"id": "toolu_01JtgbVGVJbiSwtZk3Uycezx",\
},\
{\
"name": "GetPopulation",\
"args": {"location": "Los Angeles, CA"},\
"id": "toolu_01429aygngesudV9nTbCKGuw",\
},\
{\
"name": "GetPopulation",\
"args": {"location": "New York, NY"},\
"id": "toolu_01JPktyd44tVMeBcPPnFSEJG",\
},\
]

### ​ Strict tool use

Strict tool use requires:

- Claude Sonnet 4.5 or Opus 4.1.

Anthropic supports opt-in strict schema adherence to tool calls. This guarantees that tool names and arguments are validated and correctly typed through constrained decoding.Without strict mode, Claude can occasionally generate invalid tool inputs that break your applications:

- **Type mismatches**: `passengers: "2"` instead of `passengers: 2`
- **Missing required fields**: Omitting fields your function expects
- **Invalid enum values**: Values outside the allowed set
- **Schema violations**: Nested objects not matching expected structure

Strict tool use guarantees schema-compliant tool calls:

- Tool inputs strictly follow your `input_schema`
- Guaranteed field types and required fields
- Eliminate error handling for malformed inputs
- Tool `name` used is always from provided tools

| Use strict tool use | Use standard tool calling |
| --- | --- |
| Building agentic workflows where reliability is critical | Simple, single-turn tool calls |
| Tools with many parameters or nested objects | Prototyping and experimentation |
| Functions that require specific types (e.g., `int` vs `str`) | |

To enable strict tool use, specify `strict=True` when calling `bind_tools`.

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")

model_with_tools = model.bind_tools([get_weather], strict=True)

Example: Type-safe booking system

Consider a booking system where `passengers` must be an integer:

from langchain_anthropic import ChatAnthropic
from typing import Literal

def book_flight(
destination: str,
departure_date: str,
passengers: int,
cabin_class: Literal["economy", "business", "first"]

"""Book a flight to a destination.

Args:
destination: The destination city
departure_date: Date in YYYY-MM-DD format
passengers: Number of passengers (must be an integer)
cabin_class: The cabin class for the flight
"""
return f"Booked {passengers} passengers to {destination}"

model_with_tools = model.bind_tools(
[book_flight],
strict=True,
tool_choice="any",
)
response = model_with_tools.invoke("Book 2 passengers to Tokyo, business class, 2025-01-15")

# With strict=True, passengers is guaranteed to be int, not "2" or "two"
print(response.tool_calls[0]["args"]["passengers"])

2

Strict tool use has some JSON schema limitations to be aware of. See the Claude docs for more details.If your tool schema uses unsupported features, you’ll receive a 400 error. In these cases, simplify the schema or use standard (non-strict) tool calling.

### ​ Input examples

For complex tools, you can provide usage examples to help Claude understand how to use them correctly. This is done by setting `input_examples` in the tool’s `extras` parameter.

from langchain_anthropic import ChatAnthropic
from langchain.tools import tool

@tool(
extras={
"input_examples": [\
{\
"query": "weather report",\
"location": "San Francisco",\
"format": "detailed"\
},\
{\
"query": "temperature",\
"location": "New York",\
"format": "brief"\
}\
]
}
)

"""Search weather database with specific query and format preferences.

Args:
query: The type of weather information to retrieve
location: City or region to search
format: Output format, either 'brief' or 'detailed'
"""
return f"{format.title()} {query} for {location}: Data found"

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
model_with_tools = model.bind_tools([search_weather_data])

response = model_with_tools.invoke(
"Get me a detailed weather report for Seattle"
)

The `extras` parameter also supports:

- `defer_loading` (bool): Load tool on-demand for tool search
- `cache_control` (dict): Enable prompt caching for the tool

### ​ Token-efficient tool use

Anthropic supports a token-efficient tool use feature. It is supported by default on all Claude 4 models and above.

Enabling token-efficient tool use with Claude 3.7

To use token-efficient tool use with Claude 3.7, specify the `token-efficient-tools-2025-02-19` beta-header when instantiating the model:

model = ChatAnthropic(
model="claude-3-7-sonnet-20250219",
betas=["token-efficient-tools-2025-02-19"],
)

@tool

model_with_tools = model.bind_tools([get_weather])
response = model_with_tools.invoke("What's the weather in San Francisco?")
print(response.tool_calls)
print(f"\nTotal tokens: {response.usage_metadata['total_tokens']}")

[{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'toolu_01EoeE1qYaePcmNbUvMsWtmA', 'type': 'tool_call'}]

Total tokens: 408

Anthropic automatically caches tool descriptions to reduce token usage on subsequent calls. See Caching tools for details.

### ​ Fine-grained tool streaming

Anthropic supports fine-grained tool streaming, a beta feature that reduces latency when streaming tool calls with large parameters.Rather than buffering entire parameter values before transmission, fine-grained streaming sends parameter data as it becomes available. This can reduce the initial delay from 15 seconds to around 3 seconds for large tool parameters.

Fine-grained streaming may return invalid or partial JSON inputs, especially if the response reaches `max_tokens` before completing. Implement appropriate error handling for incomplete JSON data.

To enable fine-grained tool streaming, specify the `fine-grained-tool-streaming-2025-05-14` beta header when initializing the model:

model = ChatAnthropic(
model="claude-sonnet-4-5-20250929",
betas=["fine-grained-tool-streaming-2025-05-14"],
)

"""Write a document with the given title and content."""
return f"Document '{title}' written successfully"

model_with_tools = model.bind_tools([write_document])

# Stream tool calls with reduced latency
for chunk in model_with_tools.stream(
"Write a detailed technical document about the benefits of streaming APIs"
):
print(chunk.content)

The streaming data arrives as `input_json_delta` blocks in `chunk.content`. You can accumulate these to build the complete tool arguments:

import json

accumulated_json = ""

for chunk in model_with_tools.stream("Write a document about AI"):
for block in chunk.content:
if isinstance(block, dict) and block.get("type") == "input_json_delta":
accumulated_json += block.get("partial_json", "")
try:
# Try to parse accumulated JSON
parsed = json.loads(accumulated_json)
print(f"Complete args: {parsed}")
except json.JSONDecodeError:
# JSON is still incomplete, continue accumulating
pass

Complete args: {'title': 'Artificial Intelligence: An Overview', 'content': '# Artificial Intelligence: An Overview...

### ​ Programmatic tool calling

Programmatic tool calling requires:

- Claude Sonnet 4.5 or Opus 4.5.

You must specify the `advanced-tool-use-2025-11-20` beta header to enable programmatic tool calling.

Tools can be configured to be callable from Claude’s code execution environment, reducing latency and token consumption in contexts involving large data processing or multi-tool workflows.Refer to Claude’s programmatic tool calling guide for details. To use this feature:

- Include the code execution built-in tool in your set of tools
- Specify `extras={"allowed_callers": ["code_execution_20250825"]}` on tools you wish to call programmatically

See below for a full example with `create_agent`.

You can specify `reuse_last_container` on initialization to automatically reuse code execution containers from previous model responses.

from langchain.agents import create_agent
from langchain.tools import tool
from langchain_anthropic import ChatAnthropic

@tool(extras={"allowed_callers": ["code_execution_20250825"]})

tools = [\
{"type": "code_execution_20250825", "name": "code_execution"},\
get_weather,\
]

model = ChatAnthropic(
model="claude-sonnet-4-5",
betas=["advanced-tool-use-2025-11-20"],
reuse_last_container=True,
)

agent = create_agent(model, tools=tools)

input_query = {
"role": "user",
"content": "What's the weather in Boston?",
}

result = agent.invoke({"messages": [input_query]})

## ​ Multimodal

Claude supports image and PDF inputs as content blocks, both in Anthropic’s native format (see docs for vision and PDF support) as well as LangChain’s standard format.

### ​ Supported input methods

| Method | Image | PDF |
| --- | --- | --- |
| Base64 inline data | ✅ | ✅ |
| HTTP/HTTPS URLs | ✅ | ✅ |
| Files API | ✅ | ✅ |

The Files API can also be used to upload files to a container for use with Claude’s built-in code-execution tools. See the code execution section for details.

### ​ Image input

Provide image inputs along with text using a `HumanMessage` with list content format.

URL

Base64 encoded

Files API

from langchain_anthropic import ChatAnthropic
from langchain.messages import HumanMessage

message = HumanMessage(
content=[\
{"type": "text", "text": "Describe the image at the URL."},\
{\
"type": "image",\
"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",\
},\
]
)
response = model.invoke([message])

### ​ PDF input

Provide PDF file inputs along with text.

message = HumanMessage(
content=[\
{"type": "text", "text": "Summarize this document."},\
{\
"type": "file",\
"url": "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf",\
"mime_type": "application/pdf",\
},\
]
)
response = model.invoke([message])

## ​ Extended thinking

Some Claude models support an extended thinking feature, which will output the step-by-step reasoning process that led to its final answer.See compatible models in the Claude documentation.To use extended thinking, specify the `thinking` parameter when initializing `ChatAnthropic`. If needed, it can also be passed in as a parameter during invocation.You will need to specify a token budget to use this feature. See usage example below:

Initialization param

Invocation param

import json
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
model="claude-sonnet-4-5-20250929",
max_tokens=5000,
thinking={"type": "enabled", "budget_tokens": 2000},
)

response = model.invoke("What is the cube root of 50.653?")
print(json.dumps(response.content_blocks, indent=2))

[\
{\
"type": "reasoning",\
"reasoning": "To find the cube root of 50.653, I need to find the value of $x$ such that $x^3 = 50.653$.\n\nI can try to estimate this first. \n$3^3 = 27$\n$4^3 = 64$\n\nSo the cube root of 50.653 will be somewhere between 3 and 4, but closer to 4.\n\nLet me try to compute this more precisely. I can use the cube root function:\n\ncube root of 50.653 = 50.653^(1/3)\n\nLet me calculate this:\n50.653^(1/3) \u2248 3.6998\n\nLet me verify:\n3.6998^3 \u2248 50.6533\n\nThat's very close to 50.653, so I'm confident that the cube root of 50.653 is approximately 3.6998.\n\nActually, let me compute this more precisely:\n50.653^(1/3) \u2248 3.69981\n\nLet me verify once more:\n3.69981^3 \u2248 50.652998\n\nThat's extremely close to 50.653, so I'll say that the cube root of 50.653 is approximately 3.69981.",\
"extras": {"signature": "ErUBCkYIBxgCIkB0UjV..."}\
},\
{\
"type": "text",\
"text": "The cube root of 50.653 is approximately 3.6998.\n\nTo verify: 3.6998\u00b3 = 50.6530, which is very close to our original number.",\
}\
]

The Claude Messages API handles thinking differently across Claude Sonnet 3.7 and Claude 4 models.Refer to the Claude docs for more info.

## ​ Effort

Certain Claude models support an effort feature, which controls how many tokens Claude uses when responding. This is useful for balancing response quality against latency and cost.

model = ChatAnthropic(
model="claude-opus-4-5-20251101",
effort="medium", # # Options: "high", "medium", "low"
)

response = model.invoke("Analyze the trade-offs between microservices and monolithic architectures")

Setting `effort` to `"high"` produces exactly the same behavior as omitting the parameter altogether.

See the Claude documentation for detail on when to use different effort levels and to see supported models.

## ​ Citations

Anthropic supports a citations feature that lets Claude attach context to its answers based on source documents supplied by the user.When document or `search_result` content blocks with `"citations": {"enabled": True}` are included in a query, Claude may generate citations in its response.

### ​ Simple example

In this example we pass a plain text document. In the background, Claude automatically chunks the input text into sentences, which are used when generating citations.

messages = [\
{\
"role": "user",\
"content": [\
{\
"type": "document",\
"source": {\
"type": "text",\
"media_type": "text/plain",\
"data": "The grass is green. The sky is blue.",\
},\
"title": "My Document",\
"context": "This is a trustworthy document.",\
"citations": {"enabled": True},\
},\
{"type": "text", "text": "What color is the grass and sky?"},\
],\
}\
]
response = model.invoke(messages)
response.content

[{'text': 'Based on the document, ', 'type': 'text'},\
{'text': 'the grass is green',\
'type': 'text',\
'citations': [{'type': 'char_location',\
'cited_text': 'The grass is green. ',\
'document_index': 0,\
'document_title': 'My Document',\
'start_char_index': 0,\
'end_char_index': 20}]},\
{'text': ', and ', 'type': 'text'},\
{'text': 'the sky is blue',\
'type': 'text',\
'citations': [{'type': 'char_location',\
'cited_text': 'The sky is blue.',\
'document_index': 0,\
'document_title': 'My Document',\
'start_char_index': 20,\
'end_char_index': 36}]},\
{'text': '.', 'type': 'text'}]

### ​ In tool results (agentic RAG)

Claude supports a search\_result content block representing citable results from queries against a knowledge base or other custom source. These content blocks can be passed to claude both top-line (as in the above example) and within a tool result. This allows Claude to cite elements of its response using the result of a tool call.To pass search results in response to tool calls, define a tool that returns a list of `search_result` content blocks in Anthropic’s native format. For example:

"""Access my knowledge base."""

# Run a search (e.g., with a LangChain vector store)
results = vector_store.similarity_search(query=query, k=2)

# Package results into search_result blocks
return [\
{\
"type": "search_result",\
# Customize fields as desired, using document metadata or otherwise\
"title": "My Document Title",\
"source": "Source description or provenance",\
"citations": {"enabled": True},\
"content": [{"type": "text", "text": doc.page_content}],\
}\
for doc in results\
]

End to end example with LangGraph

Here we demonstrate an end-to-end example in which we populate a LangChain vector store with sample documents and equip Claude with a tool that queries those documents.The tool here takes a search query and a `category` string literal, but any valid tool signature can be used.This example requires `langchain-openai` and `numpy` to be installed:

pip install langchain-openai numpy

from typing import Literal

from langchain.chat_models import init_chat_model
from langchain.embeddings import init_embeddings
from langchain_core.documents import Document
from langchain_core.vectorstores import InMemoryVectorStore
from langgraph.checkpoint.memory import InMemorySaver
from langchain.agents import create_agent

# Set up vector store
# Ensure you set your OPENAI_API_KEY environment variable
embeddings = init_embeddings("openai:text-embedding-3-small")
vector_store = InMemoryVectorStore(embeddings)

document_1 = Document(
id="1",
page_content=(
"To request vacation days, submit a leave request form through the "
"HR portal. Approval will be sent by email."
),
metadata={
"category": "HR Policy",
"doc_title": "Leave Policy",
"provenance": "Leave Policy - page 1",
},
)
document_2 = Document(
id="2",
page_content="Managers will review vacation requests within 3 business days.",
metadata={
"category": "HR Policy",
"doc_title": "Leave Policy",
"provenance": "Leave Policy - page 2",
},
)
document_3 = Document(
id="3",
page_content=(
"Employees with over 6 months tenure are eligible for 20 paid vacation days "
"per year."
),
metadata={
"category": "Benefits Policy",
"doc_title": "Benefits Guide 2025",
"provenance": "Benefits Policy - page 1",
},
)

documents = [document_1, document_2, document_3]
vector_store.add_documents(documents=documents)

# Define tool
async def retrieval_tool(
query: str, category: Literal["HR Policy", "Benefits Policy"]

return doc.metadata.get("category") == category

results = vector_store.similarity_search(
query=query, k=2, filter=_filter_function
)

return [\
{\
"type": "search_result",\
"title": doc.metadata["doc_title"],\
"source": doc.metadata["provenance"],\
"citations": {"enabled": True},\
"content": [{"type": "text", "text": doc.page_content}],\
}\
for doc in results\
]

# Create agent
model = init_chat_model("claude-haiku-4-5-20251001")

checkpointer = InMemorySaver()
agent = create_agent(model, [retrieval_tool], checkpointer=checkpointer)

# Invoke on a query
config = {"configurable": {"thread_id": "session_1"}}

input_message = {
"role": "user",
"content": "How do I request vacation days?",
}
async for step in agent.astream(
{"messages": [input_message]},
config,
stream_mode="values",
):
step["messages"][-1].pretty_print()

### ​ Using with text splitters

Anthropic also lets you specify your own splits using custom document types. LangChain text splitters can be used to generate meaningful splits for this purpose. See the below example, where we split the LangChain `README.md` (a markdown document) and pass it to Claude as context:This example requires `langchain-text-splitters` to be installed:

pip install langchain-text-splitters

import requests
from langchain_anthropic import ChatAnthropic
from langchain_text_splitters import MarkdownTextSplitter

def format_to_anthropic_documents(documents: list[str]):
return {
"type": "document",
"source": {
"type": "content",
"content": [{"type": "text", "text": document} for document in documents],
},
"citations": {"enabled": True},
}

# Pull readme
get_response = requests.get(
"https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md"
)
readme = get_response.text

# Split into chunks
splitter = MarkdownTextSplitter(
chunk_overlap=0,
chunk_size=50,
)
documents = splitter.split_text(readme)

# Construct message
message = {
"role": "user",
"content": [\
format_to_anthropic_documents(documents),\
{"type": "text", "text": "Give me a link to LangChain's tutorials."},\
],
}

# Query model
model = ChatAnthropic(model="claude-haiku-4-5-20251001")
response = model.invoke([message])

See all 41 lines

## ​ Prompt caching

Anthropic supports caching of elements of your prompts, including messages, tool definitions, tool results, images and documents. This allows you to re-use large documents, instructions, few-shot documents, and other data to reduce latency and costs.To enable caching on an element of a prompt, mark its associated content block using the `cache_control` key. See examples below:

Only certain Claude models support prompt caching. See the Claude documentation for details.

### ​ Messages

import requests
from langchain_anthropic import ChatAnthropic

# Pull LangChain readme

messages = [\
{\
"role": "system",\
"content": [\
{\
"type": "text",\
"text": "You are a technology expert.",\
},\
{\
"type": "text",\
"text": f"{readme}",\
"cache_control": {"type": "ephemeral"},\
},\
],\
},\
{\
"role": "user",\
"content": "What's LangChain, according to its README?",\
},\
]

response_1 = model.invoke(messages)
response_2 = model.invoke(messages)

usage_1 = response_1.usage_metadata["input_token_details"]
usage_2 = response_2.usage_metadata["input_token_details"]

print(f"First invocation:\n{usage_1}")
print(f"\nSecond:\n{usage_2}")

First invocation:
{'cache_read': 0, 'cache_creation': 1458}

Second:
{'cache_read': 1458, 'cache_creation': 0}

Alternatively, you may enable prompt caching at invocation time. You may want to conditionally cache based on runtime conditions, such as the length of the context. This is useful for app-level decisions about what to cache.

response = model.invoke(
messages,
cache_control={"type": "ephemeral"},
)

**Extended caching**The cache lifetime is 5 minutes by default. If this is too short, you can apply one hour caching by enabling the `"extended-cache-ttl-2025-04-11"` beta header and specifying `"cache_control": {"type": "ephemeral", "ttl": "1h"}` on the message.

Example

model = ChatAnthropic(
model="claude-sonnet-4-5-20250929",
betas=["extended-cache-ttl-2025-04-11"],
)

messages = [\
{\
"role": "user",\
"content": [\
{\
"type": "text",\
"text": f"{long_text}",\
"cache_control": {"type": "ephemeral", "ttl": "1h"},\
},\
],\
}\
]

Details of cached token counts will be included on the `InputTokenDetails` of response’s `usage_metadata`:

response = model.invoke(messages)
response.usage_metadata

{
"input_tokens": 1500,
"output_tokens": 200,
"total_tokens": 1700,
"input_token_details": {
"cache_read": 0,
"cache_creation": 1000,
"ephemeral_1h_input_tokens": 750,
"ephemeral_5m_input_tokens": 250,
}
}

### ​ Caching tools

# For demonstration purposes, we artificially expand the
# tool description.
description = (
"Get the weather at a location. "
f"By the way, check out this readme: {readme}"
)

@tool(description=description, extras={"cache_control": {"type": "ephemeral"}})

return "It's sunny."

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
model_with_tools = model.bind_tools([get_weather])
query = "What's the weather in San Francisco?"

response_1 = model_with_tools.invoke(query)
response_2 = model_with_tools.invoke(query)

See all 29 lines

First invocation:
{'cache_read': 0, 'cache_creation': 1809}

Second:
{'cache_read': 1809, 'cache_creation': 0}

### ​ Incremental caching in conversational applications

Prompt caching can be used in multi-turn conversations to maintain context from earlier messages without redundant processing.We can enable incremental caching by marking the final message with `cache_control`. Claude will automatically use the longest previously-cached prefix for follow-up messages.Below, we implement a simple chatbot that incorporates this feature. We follow the LangChain chatbot tutorial, but add a custom reducer that automatically marks the last content block in each user message with `cache_control`:

Chatbot with incremental prompt caching

import requests
from langchain_anthropic import ChatAnthropic
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, StateGraph, add_messages
from typing_extensions import Annotated, TypedDict

# Update last user message
for i in range(len(right) - 1, -1, -1):
if right[i].type == "human":
right[i].content[-1]["cache_control"] = {"type": "ephemeral"}
break

return add_messages(left, right)

class State(TypedDict):
messages: Annotated[list, messages_reducer]

workflow = StateGraph(state_schema=State)

# Define the function that calls the model
def call_model(state: State):
response = model.invoke(state["messages"])
return {"messages": [response]}

# Define the (single) node in the graph
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Add memory
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

See all 46 lines

from langchain.messages import HumanMessage

config = {"configurable": {"thread_id": "abc123"}}

query = "Hi! I'm Bob."

input_message = HumanMessage([{"type": "text", "text": query}])
output = app.invoke({"messages": [input_message]}, config)
output["messages"][-1].pretty_print()
print(f"\n{output['messages'][-1].usage_metadata['input_token_details']}")

================================== Ai Message ==================================

Hello, Bob! It's nice to meet you. How are you doing today? Is there something I can help you with?

{'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}

query = f"Check out this readme: {readme}"

I can see you've shared the README from the LangChain GitHub repository. This is the documentation for LangChain, which is a popular framework for building applications powered by Large Language Models (LLMs). Here's a summary of what the README contains:

LangChain is:
- A framework for developing LLM-powered applications
- Helps chain together components and integrations to simplify AI application development
- Provides a standard interface for models, embeddings, vector stores, etc.

Key features/benefits:
- Real-time data augmentation (connect LLMs to diverse data sources)
- Model interoperability (swap models easily as needed)
- Large ecosystem of integrations

The LangChain ecosystem includes:
- LangSmith - For evaluations and observability
- LangGraph - For building complex agents with customizable architecture
- LangSmith - For deployment and scaling of agents

The README also mentions installation instructions (`pip install -U langchain`) and links to various resources including tutorials, how-to guides, conceptual guides, and API references.

Is there anything specific about LangChain you'd like to know more about, Bob?

{'cache_read': 0, 'cache_creation': 1846, 'ephemeral_5m_input_tokens': 1846, 'ephemeral_1h_input_tokens': 0}

query = "What was my name again?"

Your name is Bob. You introduced yourself at the beginning of our conversation.

{'cache_read': 1846, 'cache_creation': 278, 'ephemeral_5m_input_tokens': 278, 'ephemeral_1h_input_tokens': 0}

In the LangSmith trace, toggling “raw output” will show exactly what messages are sent to the chat model, including `cache_control` keys.

## ​ Token counting

You can count tokens in messages before sending them to the model using `get_num_tokens_from_messages()`. This uses Anthropic’s official token counting API.

Message token counting

from langchain_anthropic import ChatAnthropic
from langchain.messages import HumanMessage, SystemMessage

messages = [\
SystemMessage(content="You are a scientist"),\
HumanMessage(content="Hello, Claude"),\
]

token_count = model.get_num_tokens_from_messages(messages)
print(token_count)

14

Tool token counting

You can also count tokens when using tools:

from langchain.tools import tool

@tool(parse_docstring=True)

"""Get the current weather in a given location

Args:
location: The city and state, e.g. San Francisco, CA
"""
return "Sunny"

messages = [\
HumanMessage(content="What's the weather like in San Francisco?"),\
]

token_count = model.get_num_tokens_from_messages(messages, tools=[get_weather])
print(token_count)

586

## ​ Context management

Anthropic supports a context editing feature that will automatically manage the model’s context window (e.g., by clearing tool results).See the Claude documentation for more details and configuration options.

model = ChatAnthropic(
model="claude-sonnet-4-5-20250929",
betas=["context-management-2025-06-27"],
context_management={"edits": [{"type": "clear_tool_uses_20250919"}]},
)
model_with_tools = model.bind_tools([{"type": "web_search_20250305", "name": "web_search"}])
response = model_with_tools.invoke("Search for recent developments in AI")

## ​ Extended context window

Claude Sonnet 4 and 4.5 support a 1-million token context window, available in beta for organizations in usage tier 4 and organizations with custom rate limits.To enable the extended context window, specify the `context-1m-2025-08-07` beta header:

model = ChatAnthropic(
model="claude-sonnet-4-5-20250929",
betas=["context-1m-2025-08-07"],
)

long_document = """
This is a very long document that would benefit from the extended 1M
context window...
[imagine this continues for hundreds of thousands of tokens]
"""

messages = [\
HumanMessage(f"""\
Please analyze this document and provide a summary:\
\
{long_document}\
\
What are the key themes and main conclusions?\
""")\
]

response = model.invoke(messages)

See the Claude documentation for detail.

## ​ Structured output

Structured output requires:

Anthropic supports a native structured output feature, which guarantees that its responses adhere to a given schema.You can access this feature in individual model calls, or by specifying the response format of a LangChain agent. See below for examples.

Individual model calls

Use the `with_structured_output` method to generate a structured model response. Specify `method="json_schema"` to enable Anthropic’s native structured output feature; otherwise the method defaults to using function calling.

from langchain_anthropic import ChatAnthropic
from pydantic import BaseModel, Field

class Movie(BaseModel):
"""A movie with details."""
title: str = Field(..., description="The title of the movie")
year: int = Field(..., description="The year the movie was released")
director: str = Field(..., description="The director of the movie")
rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie, method="json_schema")
response = model_with_structure.invoke("Provide details about the movie Inception")
response

Movie(title='Inception', year=2010, director='Christopher Nolan', rating=8.8)

Agent response format

Specify `response_format` with `ProviderStrategy` to engage Anthropic’s structured output feature when generating its final response.

from langchain.agents import create_agent
from langchain.agents.structured_output import ProviderStrategy
from pydantic import BaseModel

class Weather(BaseModel):
temperature: float
condition: str

"""Get the weather at a location."""
return "Sunny and 75 degrees F."

agent = create_agent(
model="anthropic:claude-sonnet-4-5",
tools=[weather_tool],
response_format=ProviderStrategy(Weather),
)

result = agent.invoke({
"messages": [{"role": "user", "content": "What's the weather in SF?"}]
})

result["structured_response"]

Weather(temperature=75.0, condition='Sunny')

## ​ Built-in tools

Anthropic supports a variety of built-in client and server-side tools.Server-side tools (e.g., web search) are passed to the model and executed by Anthropic. Client-side tools (e.g., bash tool) require you to implement the callback execution logic in your application and return results to the model.In either case, you make tools accessible to your chat model by using `bind_tools` on the model instance.Importantly, client-side tools require you to implement the execution logic. See the relevant sections below for examples.

**Middleware vs tools**For client-side tools (e.g. bash, text editor, memory), you may opt to use middleware, which provide production-ready implementations that contain built-in execution, state management, and security policies.Use middleware when you want a turnkey solution; use tools (documented below) when you need custom execution logic or want to use `bind_tools` directly.

**Beta tools**If binding a beta tool to your chat model, LangChain will automatically add the required beta header for you.

### ​ Bash tool

Claude supports a client-side bash tool that allows it to execute shell commands in a persistent bash session. This enables system operations, script execution, and command-line automation.

**Important: You must provide the execution environment**LangChain handles the API integration (sending/receiving tool calls), but **you are responsible** for:

- Setting up a sandboxed computing environment (Docker, VM, etc.)
- Implementing command execution and output capture
- Passing results for implementation guidance.

**Requirements:**

- Claude 4 models or Claude Sonnet 3.7

- Anthropic type

- create\_agent

- Dict

import subprocess

from anthropic.types.beta import BetaToolBash20250124Param
from langchain_anthropic import ChatAnthropic
from langchain.messages import HumanMessage, ToolMessage
from langchain.tools import tool

tool_spec = BetaToolBash20250124Param(
name="bash",
type="bash_20250124",
)

@tool(extras={"provider_tool_definition": tool_spec})
def bash(*, command: str, restart: bool = False, **kw):
"""Execute a bash command."""
if restart:
return "Bash session restarted"
try:
result = subprocess.run(
command,
shell=True,
capture_output=True,
text=True,
timeout=30,
)
return result.stdout + result.stderr
except Exception as e:
return f"Error: {e}"

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
model_with_bash = model.bind_tools([bash])

# Initial request
messages = [HumanMessage("List all files in the current directory")]
response = model_with_bash.invoke(messages)
print(response.content_blocks)

# Tool execution loop
while response.tool_calls:
# Execute each tool call
tool_messages = []
for tool_call in response.tool_calls:
result = bash.invoke(tool_call)
tool_messages.append(result)

# Continue conversation with tool results
messages = [*messages, response, *tool_messages]
response = model_with_bash.invoke(messages)
print(response.content_blocks)

See all 51 lines

from anthropic.types.beta import BetaToolBash20250124Param
from langchain.agents import create_agent
from langchain_anthropic import ChatAnthropic
from langchain.tools import tool

@tool(extras={"provider_tool_definition": tool_spec})
def bash(*, command: str, restart: bool = False, **kw):
"""Execute a bash command."""
if restart:
return "Bash session restarted"
result = subprocess.run(
command,
shell=True,
capture_output=True,
text=True,
)
return result.stdout + result.stderr

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[bash],
)

result = agent.invoke({"messages": [{"role": "user", "content": "List files"}]})

for message in result["messages"]:
message.pretty_print()

See all 36 lines

bash_tool = {
"type": "bash_20250124",
"name": "bash",
}

model_with_bash = model.bind_tools([bash_tool])
response = model_with_bash.invoke(
"List all Python files in the current directory"
)
# You must handle execution of the bash command in response.tool_calls via a tool execution loop

Using `create_agent` handles the tool execution loop automatically.`response.tool_calls` will contain the bash command Claude wants to execute. You must run this command in your environment and pass the result back.

[{'type': 'text',\
'text': "I'll list the Python files in the current directory for you."},\
{'type': 'tool_call',\
'name': 'bash',\
'args': {'command': 'ls -la *.py'},\
'id': 'toolu_01ABC123...'}]

The bash tool supports two parameters:

- `command` (required): The bash command to execute
- `restart` (optional): Set to `true` to restart the bash session

For a “batteries-included” implementation, consider using `ClaudeBashToolMiddleware` which provides persistent sessions, Docker isolation, output redaction, and startup/shutdown commands out of the box.

### ​ Code execution

Claude can use a server-side code execution tool to execute code in a sandboxed environment.

The code sandbox does not have internet access, thus you may only use packages that are pre-installed in the environment. See the Claude docs for more info.

from anthropic.types.beta import BetaCodeExecutionTool20250825Param
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(
model="claude-sonnet-4-5-20250929",
# (Optional) Enable the param below to automatically
# pass back in container IDs from previous response
reuse_last_container=True,
)

code_tool = BetaCodeExecutionTool20250825Param(
name="code_execution",
type="code_execution_20250825",
)
model_with_tools = model.bind_tools([code_tool])

response = model_with_tools.invoke(
"Calculate the mean and standard deviation of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
)

from anthropic.types.beta import BetaCodeExecutionTool20250825Param
from langchain.agents import create_agent
from langchain_anthropic import ChatAnthropic

code_tool = BetaCodeExecutionTool20250825Param(
name="code_execution",
type="code_execution_20250825",
)

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[code_tool],
)

result = agent.invoke({
"messages": [{"role": "user", "content": "Calculate mean and std of [1,2,3,4,5]"}]
})

model = ChatAnthropic(
model="claude-sonnet-4-5-20250929",
)

code_tool = {"type": "code_execution_20250825", "name": "code_execution"}
model_with_tools = model.bind_tools([code_tool])

Use with Files API

Using the Files API, Claude can write code to access files for data analysis and other purposes. See example below:

import anthropic
from anthropic.types.beta import BetaCodeExecutionTool20250825Param
from langchain_anthropic import ChatAnthropic

client = anthropic.Anthropic()
file = client.beta.files.upload(
file=open("/path/to/sample_data.csv", "rb")
)
file_id = file.id

# Run inference

input_message = {
"role": "user",
"content": [\
{\
"type": "text",\
"text": "Please plot these data and tell me what you see.",\
},\
{\
"type": "container_upload",\
"file_id": file_id,\
},\
]
}
response = model_with_tools.invoke([input_message])

Note that Claude may generate files as part of its code execution. You can access these files using the Files API:

# Take all file outputs for demonstration purposes
file_ids = []
for block in response.content:
if block["type"] == "bash_code_execution_tool_result":
file_ids.extend(
content["file_id"]
for content in block.get("content", {}).get("content", [])
if "file_id" in content
)

for i, file_id in enumerate(file_ids):
file_content = client.beta.files.download(file_id)
file_content.write_to_file(f"/path/to/file_{i}.png")

**Available tool versions:**

- `code_execution_20250522` (legacy)
- `code_execution_20250825` (recommended)

### ​ Computer use

Claude supports client-side computer use capabilities, allowing it to interact with desktop environments through screenshots, mouse control, and keyboard input.

- Setting up a sandboxed computing environment (Linux VM, Docker container, etc.)
- Implementing a virtual display (e.g., Xvfb)
- Executing Claude’s tool calls (screenshot, mouse clicks, keyboard input)
- Passing results to help you get started.

- Claude Opus 4.5, Claude 4, or Claude Sonnet 3.7

import base64
from typing import Literal

from anthropic.types.beta import BetaToolComputerUse20250124Param
from langchain_anthropic import ChatAnthropic
from langchain.messages import HumanMessage, ToolMessage
from langchain.tools import tool

DISPLAY_WIDTH = 1024
DISPLAY_HEIGHT = 768

tool_spec = BetaToolComputerUse20250124Param(
name="computer",
type="computer_20250124",
display_width_px=DISPLAY_WIDTH,
display_height_px=DISPLAY_HEIGHT,
display_number=1,
)

@tool(extras={"provider_tool_definition": tool_spec})
def computer(
*,
action: Literal[\
"key", "type", "mouse_move", "left_click", "left_click_drag",\
"right_click", "middle_click", "double_click", "screenshot",\
"cursor_position", "scroll"\
],
coordinate: list[int] | None = None,
text: str | None = None,
**kw
):
"""Control the computer display."""
if action == "screenshot":
# Take screenshot and return base64-encoded image
# Implementation depends on your display setup (e.g., Xvfb, pyautogui)
return {"type": "image", "data": "base64_screenshot_data..."}
elif action == "left_click" and coordinate:
# Execute click at coordinate
return f"Clicked at {coordinate}"
elif action == "type" and text:
# Type text
return f"Typed: {text}"
# ... implement other actions
return f"Executed {action}"

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
model_with_computer = model.bind_tools([computer])

messages = [HumanMessage("Take a screenshot to see what's on the screen")]
response = model_with_computer.invoke(messages)
print(response.content_blocks)

while response.tool_calls:
tool_messages = []
for tool_call in response.tool_calls:
result = computer.invoke(tool_call["args"])
tool_messages.append(
ToolMessage(content=str(result), tool_call_id=tool_call["id"])
)

messages = [*messages, response, *tool_messages]
response = model_with_computer.invoke(messages)
print(response.content_blocks)

See all 65 lines

from anthropic.types.beta import BetaToolComputerUse20250124Param
from langchain.agents import create_agent
from langchain_anthropic import ChatAnthropic
from langchain.tools import tool

tool_spec = BetaToolComputerUse20250124Param(
name="computer",
type="computer_20250124",
display_width_px=1024,
display_height_px=768,
)

@tool(extras={"provider_tool_definition": tool_spec})
def computer(
*,
action: Literal[\
"key", "type", "mouse_move", "left_click", "left_click_drag",\
"right_click", "middle_click", "double_click", "screenshot",\
"cursor_position", "scroll"\
],
coordinate: list[int] | None = None,
text: str | None = None,
**kw
):
"""Control the computer display."""
if action == "screenshot":
return {"type": "image", "data": "base64_screenshot_data..."}
elif action == "left_click" and coordinate:
return f"Clicked at {coordinate}"
elif action == "type" and text:
return f"Typed: {text}"
return f"Executed {action}"

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[computer],
)

result = agent.invoke({
"messages": [{"role": "user", "content": "Take a screenshot"}]
})

See all 48 lines

computer_tool = {
"type": "computer_20250124",
"name": "computer",
"display_width_px": 1024,
"display_height_px": 768,
"display_number": 1,
}

model_with_computer = model.bind_tools([computer_tool])
response = model_with_computer.invoke(
"Take a screenshot to see what's on the screen"
)
# You must handle execution of the computer actions in response.tool_calls via a tool execution loop

Using `create_agent` handles the tool execution loop automatically.`response.tool_calls` will contain the computer action Claude wants to perform. You must execute this action in your environment and pass the result back.

[{'type': 'text',\
'text': "I'll take a screenshot to see what's currently on the screen."},\
{'type': 'tool_call',\
'name': 'computer',\
'args': {'action': 'screenshot'},\
'id': 'toolu_01RNsqAE7dDZujELtacNeYv9'}]

- `computer_20250124` (for Claude 4 and Claude Sonnet 3.7)
- `computer_20251124` (for Claude Opus 4.5)

### ​ Remote MCP

Claude can use a server-side MCP connector tool for model-generated calls to remote MCP servers.

from anthropic.types.beta import BetaMCPToolsetParam
from langchain_anthropic import ChatAnthropic

mcp_servers = [\
{\
"type": "url",\
"url": "https://docs.langchain.com/mcp",\
"name": "LangChain Docs",\
}\
]

model = ChatAnthropic(
model="claude-sonnet-4-5-20250929",
mcp_servers=mcp_servers,
)

mcp_tool = BetaMCPToolsetParam(
type="mcp_toolset",
mcp_server_name="LangChain Docs",
)

response = model.invoke(
"What are LangChain content blocks?",
tools=[mcp_tool],
)

from anthropic.types.beta import BetaMCPToolsetParam
from langchain.agents import create_agent
from langchain_anthropic import ChatAnthropic

agent = create_agent(
model=ChatAnthropic(
model="claude-sonnet-4-5-20250929",
mcp_servers=mcp_servers,
),
tools=[mcp_tool],
)

result = agent.invoke({
"messages": [{"role": "user", "content": "What are LangChain content blocks?"}]
})

mcp_servers = [\
{\
"type": "url",\
"url": "https://docs.langchain.com/mcp",\
"name": "LangChain Docs",\
# "tool_configuration": { # optional configuration\
# "enabled": True,\
# "allowed_tools": ["ask_question"],\
# },\
# "authorization_token": "PLACEHOLDER", # optional authorization if needed\
}\
]

response = model.invoke(
"What are LangChain content blocks?",
tools=[{"type": "mcp_toolset", "mcp_server_name": "LangChain Docs"}],
)
response.content_blocks

### ​ Text editor

Claude supports a client-side text editor tool can be used to view and modify text local files. See docs here for details.

from anthropic.types.beta import BetaToolTextEditor20250728Param
from langchain_anthropic import ChatAnthropic
from langchain.messages import HumanMessage, ToolMessage
from langchain.tools import tool

tool_spec = BetaToolTextEditor20250728Param(
name="str_replace_based_edit_tool",
type="text_editor_20250728",
)

# Simple in-memory file storage for demonstration
files: dict[str, str] = {
"/workspace/primes.py": "def is_prime(n):\n if n < 2\n return False\n return True"
}

@tool(extras={"provider_tool_definition": tool_spec})
def str_replace_based_edit_tool(
*,
command: Literal["view", "create", "str_replace", "insert", "undo_edit"],
path: str,
file_text: str | None = None,
old_str: str | None = None,
new_str: str | None = None,
insert_line: int | None = None,
view_range: list[int] | None = None,
**kw
):
"""View and edit text files."""
if command == "view":
if path not in files:
return f"Error: File {path} not found"
content = files[path]
if view_range:
lines = content.splitlines()
start, end = view_range[0] - 1, view_range[1]
return "\n".join(lines[start:end])
return content
elif command == "create":
files[path] = file_text or ""
return f"Created {path}"
elif command == "str_replace" and old_str is not None:
if path not in files:
return f"Error: File {path} not found"
files[path] = files[path].replace(old_str, new_str or "", 1)
return f"Replaced in {path}"
# ... implement other commands
return f"Executed {command} on {path}"

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
model_with_tools = model.bind_tools([str_replace_based_edit_tool])

messages = [HumanMessage("There's a syntax error in my primes.py file. Can you fix it?")]
response = model_with_tools.invoke(messages)
print(response.content_blocks)

while response.tool_calls:
tool_messages = []
for tool_call in response.tool_calls:
result = str_replace_based_edit_tool.invoke(tool_call["args"])
tool_messages.append(
ToolMessage(content=result, tool_call_id=tool_call["id"])
)

messages = [*messages, response, *tool_messages]
response = model_with_tools.invoke(messages)
print(response.content_blocks)

See all 70 lines

from anthropic.types.beta import BetaToolTextEditor20250728Param
from langchain.agents import create_agent
from langchain_anthropic import ChatAnthropic
from langchain.tools import tool

# Simple in-memory file storage

@tool(extras={"provider_tool_definition": tool_spec})
def str_replace_based_edit_tool(
*,
command: Literal["view", "create", "str_replace", "insert", "undo_edit"],
path: str,
file_text: str | None = None,
old_str: str | None = None,
new_str: str | None = None,
**kw
):
"""View and edit text files."""
if command == "view":
return files.get(path, f"Error: File {path} not found")
elif command == "create":
files[path] = file_text or ""
return f"Created {path}"
elif command == "str_replace" and old_str is not None:
if path not in files:
return f"Error: File {path} not found"
files[path] = files[path].replace(old_str, new_str or "", 1)
return f"Replaced in {path}"
return f"Executed {command} on {path}"

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[str_replace_based_edit_tool],
)

result = agent.invoke({
"messages": [{"role": "user", "content": "Fix the syntax error in /workspace/primes.py"}]
})

See all 53 lines

editor_tool = {"type": "text_editor_20250728", "name": "str_replace_based_edit_tool"}

model_with_tools = model.bind_tools([editor_tool])

response = model_with_tools.invoke(
"There's a syntax error in my primes.py file. Can you help me fix it?"
)
# You must handle execution of the text editor commands in response.tool_calls via a tool execution loop

Using `create_agent` handles the tool execution loop automatically.

[{'name': 'str_replace_based_edit_tool',\
'args': {'command': 'view', 'path': '/root'},\
'id': 'toolu_011BG5RbqnfBYkD8qQonS9k9',\
'type': 'tool_call'}]

- `text_editor_20250124` (legacy)
- `text_editor_20250728` (recommended)

For a “batteries-included” implementation, consider using `StateClaudeTextEditorMiddleware` or `FilesystemClaudeTextEditorMiddleware` which provide LangGraph state integration or filesystem persistence, path validation, and other features.

### ​ Web fetching

Claude can use a server-side web fetching tool to retrieve full content from specified web pages and PDF documents and ground its responses with citations.

from anthropic.types.beta import BetaWebFetchTool20250910Param
from langchain_anthropic import ChatAnthropic

fetch_tool = BetaWebFetchTool20250910Param(
name="web_fetch",
type="web_fetch_20250910",
max_uses=3,
)

model_with_tools = model.bind_tools([fetch_tool])

response = model_with_tools.invoke(
"Please analyze the content at
)

from anthropic.types.beta import BetaWebFetchTool20250910Param
from langchain.agents import create_agent
from langchain_anthropic import ChatAnthropic

agent = create_agent(
model=ChatAnthropic(model="claude-haiku-4-5-20251001"),
tools=[fetch_tool],
)

result = agent.invoke({
"messages": [{"role": "user", "content": "Analyze .com/"}]
})

fetch_tool = {"type": "web_fetch_20250910", "name": "web_fetch", "max_uses": 3}

### ​ Web search

Claude can use a server-side web search tool to run searches and ground its responses with citations.

from anthropic.types.beta import BetaWebSearchTool20250305Param
from langchain_anthropic import ChatAnthropic

search_tool = BetaWebSearchTool20250305Param(
name="web_search",
type="web_search_20250305",
max_uses=3,
)

model_with_tools = model.bind_tools([search_tool])

response = model_with_tools.invoke("How do I update a web app to TypeScript 5.5?")

from anthropic.types.beta import BetaWebSearchTool20250305Param
from langchain.agents import create_agent
from langchain_anthropic import ChatAnthropic

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[search_tool],
)

result = agent.invoke({
"messages": [{"role": "user", "content": "How do I update a web app to TypeScript 5.5?"}]
})

search_tool = {"type": "web_search_20250305", "name": "web_search", "max_uses": 3}

### ​ Memory tool

Claude supports a memory tool for client-side storage and retrieval of context across conversational threads. See docs here for details.

from anthropic.types.beta import BetaMemoryTool20250818Param
from langchain_anthropic import ChatAnthropic
from langchain.messages import HumanMessage, ToolMessage
from langchain.tools import tool

tool_spec = BetaMemoryTool20250818Param(
name="memory",
type="memory_20250818",
)

# Simple in-memory storage for demonstration purposes
memory_store: dict[str, str] = {
"/memories/interests": "User enjoys Python programming and hiking"
}

@tool(extras={"provider_tool_definition": tool_spec})
def memory(
*,
command: Literal["view", "create", "str_replace", "insert", "delete", "rename"],
path: str,
content: str | None = None,
old_str: str | None = None,
new_str: str | None = None,
insert_line: int | None = None,
new_path: str | None = None,
**kw,
):
"""Manage persistent memory across conversations."""
if command == "view":
if path == "/memories":
# List all memories
return "\n".join(memory_store.keys()) or "No memories stored"
return memory_store.get(path, f"No memory at {path}")
elif command == "create":
memory_store[path] = content or ""
return f"Created memory at {path}"
elif command == "str_replace" and old_str is not None:
if path in memory_store:
memory_store[path] = memory_store[path].replace(old_str, new_str or "", 1)
return f"Updated {path}"
elif command == "delete":
memory_store.pop(path, None)
return f"Deleted {path}"

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
model_with_tools = model.bind_tools([memory])

messages = [HumanMessage("What are my interests?")]
response = model_with_tools.invoke(messages)
print(response.content_blocks)

while response.tool_calls:
tool_messages = []
for tool_call in response.tool_calls:
result = memory.invoke(tool_call["args"])
tool_messages.append(ToolMessage(content=result, tool_call_id=tool_call["id"]))

See all 68 lines

[{'type': 'text',\
'text': "I'll check my memory to see what information I have about your interests."},\
{'type': 'tool_call',\
'name': 'memory',\
'args': {'command': 'view', 'path': '/memories'},\
'id': 'toolu_01XeP9sxx44rcZHFNqXSaKqh'}]

from anthropic.types.beta import BetaMemoryTool20250818Param
from langchain.agents import create_agent
from langchain_anthropic import ChatAnthropic
from langchain.tools import tool

# Simple in-memory storage

@tool(extras={"provider_tool_definition": tool_spec})
def memory(
*,
command: Literal["view", "create", "str_replace", "insert", "delete", "rename"],
path: str,
content: str | None = None,
old_str: str | None = None,
new_str: str | None = None,
**kw
):
"""Manage persistent memory across conversations."""
if command == "view":
if path == "/memories":
return "\n".join(memory_store.keys()) or "No memories stored"
return memory_store.get(path, f"No memory at {path}")
elif command == "create":
memory_store[path] = content or ""
return f"Created memory at {path}"
elif command == "str_replace" and old_str is not None:
if path in memory_store:
memory_store[path] = memory_store[path].replace(old_str, new_str or "", 1)
return f"Updated {path}"
elif command == "delete":
memory_store.pop(path, None)
return f"Deleted {path}"
return f"Executed {command} on {path}"

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[memory],
)

result = agent.invoke({
"messages": [{"role": "user", "content": "What are my interests?"}]
})

See all 57 lines

model = ChatAnthropic(
model="claude-sonnet-4-5-20250929",
)
model_with_tools = model.bind_tools([{"type": "memory_20250818", "name": "memory"}])

response = model_with_tools.invoke("What are my interests?")
response.content_blocks
# You must handle execution of the memory commands in response.tool_calls via a tool execution loop

For a “batteries-included” implementation, consider using `StateClaudeMemoryMiddleware` or `FilesystemClaudeMemoryMiddleware` which provide LangGraph state integration or filesystem persistence, automatic system prompt injection, and other features.

### ​ Tool search

Claude supports a server-side tool search feature that enables dynamic tool discovery and loading. Instead of loading all tool definitions into the context window upfront, Claude can search your tool catalog and load only the tools it needs.This is useful when:

- You have 10+ tools available in your system
- Tool definitions are consuming significant tokens
- You’re experiencing tool selection accuracy issues with large tool sets

There are two tool search variants:

- **Regex** (`tool_search_tool_regex_20251119`): Claude constructs regex patterns to search for tools
- **BM25** (`tool_search_tool_bm25_20251119`): Claude uses natural language queries to search for tools

Use the `extras` parameter to specify `defer_loading` on LangChain tools:

from anthropic.types.beta import BetaToolSearchToolRegex20251119Param
from langchain_anthropic import ChatAnthropic
from langchain.tools import tool

@tool(extras={"defer_loading": True})

"""Get the current weather for a location.

Args:
location: City name
unit: Temperature unit (celsius or fahrenheit)
"""
return f"Weather in {location}: Sunny"

"""Search through files in the workspace.

Args:
query: Search query
"""
return f"Found files matching '{query}'"

tool_search = BetaToolSearchToolRegex20251119Param(
name="tool_search_tool_regex",
type="tool_search_tool_regex_20251119",
)

model_with_tools = model.bind_tools([\
tool_search,\
get_weather,\
search_files,\
])
response = model_with_tools.invoke("What's the weather in San Francisco?")

from anthropic.types.beta import BetaToolSearchToolRegex20251119Param
from langchain.agents import create_agent
from langchain_anthropic import ChatAnthropic
from langchain.tools import tool

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[\
tool_search,\
get_weather,\
search_files,\
],
)

result = agent.invoke({
"messages": [{"role": "user", "content": "What's the weather in San Francisco?"}]
})

See all 47 lines

model_with_tools = model.bind_tools([\
{"type": "tool_search_tool_regex_20251119", "name": "tool_search_tool_regex"},\
get_weather,\
search_files,\
])
response = model_with_tools.invoke("What's the weather in San Francisco?")

get\_weatherTool SearchModelUserget\_weatherTool SearchModelUser"What's the weather in San Francisco?"tool\_search\_tool\_regex(pattern="weather")tool\_references: \[get\_weather\]get\_weather(location="San Francisco")Weather resultResponse with weather info

**Key points:**

- Tools with `defer_loading: True` are only loaded when Claude discovers them via search
- Keep your 3-5 most frequently used tools as non-deferred for optimal performance
- Both variants search tool names, descriptions, argument names, and argument descriptions

See the Claude documentation for more details on tool search, including usage with MCP servers and client-side implementations.

## ​ Response metadata

ai_msg = model.invoke(messages)
ai_msg.response_metadata

{
"id": "msg_013xU6FHEGEq76aP4RgFerVT",
"model": "claude-sonnet-4-5-20250929",
"stop_reason": "end_turn",
"stop_sequence": None,
"usage": {"input_tokens": 25, "output_tokens": 11},
}

## ​ Token usage metadata

ai_msg = model.invoke(messages)
ai_msg.usage_metadata

{"input_tokens": 25, "output_tokens": 11, "total_tokens": 36}

Message chunks containing token usage will be included during streaming by
default:

stream = model.stream(messages)
full = next(stream)
for chunk in stream:
full += chunk
full.usage_metadata

These can be disabled by setting `stream_usage=False` in the stream method or when initializing `ChatAnthropic`.

* * *

## ​ API reference

For detailed documentation of all features and configuration options, head to the `ChatAnthropic` API reference.

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/openai

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

ChatOpenAI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Integration details
- Model features
- Setup
- Installation
- Credentials
- Instantiation
- Invocation
- Streaming usage metadata
- Using with Azure OpenAI
- Tool calling
- Bind tools
- Strict mode
- Tool calls
- Structured output and tool calls
- Custom tools
- Responses API
- Web search
- Image generation
- File search
- Computer use
- Code interpreter
- Remote MCP
- Managing conversation state
- Manually manage state
- Passing previous\_response\_id
- Reasoning output
- Fine-tuning
- Multimodal Inputs (images, PDFs, audio)
- Predicted output
- Audio Generation (Preview)
- Prompt caching
- Manual caching
- Cache key strategies
- Model-level caching
- Flex processing
- API reference

You can find information about OpenAI’s latest models, their costs, context windows, and supported input types in the OpenAI Platform docs.

**API Reference**For detailed documentation of all features and configuration options, head to the `ChatOpenAI` API reference.

**Chat Completions API compatibility**`ChatOpenAI` is fully compatible with OpenAI’s Chat Completions API. If you are looking to connect to other model providers that support the Chat Completions API, you can do so – see instructions.

## ​ Overview

### ​ Integration details

| Class | Package | Local | Serializable | JS/TS Support | Downloads | Latest Version |
| --- | --- | --- | --- | --- | --- | --- |
| `ChatOpenAI` | `langchain-openai` | ❌ | beta | ✅ (npm) | ![Downloads per month](https://pypi.org/project/langchain-openai/) | ![PyPI - Latest version](https://pypi.org/project/langchain-openai/) |

### ​ Model features

| Tool calling | Structured output | Image input | Audio input | Video input | Token-level streaming | Native async | Token usage | Logprobs |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ |

## ​ Setup

To access OpenAI models you’ll need to install the `langchain-openai` integration package and acquire an OpenAI Platform API key.

### ​ Installation

pip

uv

Copy

pip install -U langchain-openai

### ​ Credentials

Head to the OpenAI Platform to sign up and generate an API key. Once you’ve done this set the `OPENAI_API_KEY` environment variable in your environment:

import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")

If you want to get automated tracing of your model calls you can also set your LangSmith API key:

os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
os.environ["LANGSMITH_TRACING"] = "true"

## ​ Instantiation

Now we can instantiate our model object and generate responses:

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
model="gpt-5-nano",
# stream_usage=True,
# temperature=None,
# max_tokens=None,
# timeout=None,
# reasoning_effort="low",
# max_retries=2,
# api_key="...", # If you prefer to pass api key in directly
# base_url="...",
# organization="...",
# other params...
)

See the `ChatOpenAI` API Reference for the full set of available model parameters.

**Token parameter deprecation**OpenAI deprecated `max_tokens` in favor of `max_completion_tokens` in September 2024. While `max_tokens` is still supported for backwards compatibility, it’s automatically converted to `max_completion_tokens` internally.

* * *

## ​ Invocation

messages = [\
(\
"system",\
"You are a helpful assistant that translates English to French. Translate the user sentence.",\
),\
("human", "I love programming."),\
]
ai_msg = llm.invoke(messages)
ai_msg

AIMessage(content="J'adore la programmation.", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 31, 'total_tokens': 36}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'stop', 'logprobs': None}, id='run-63219b22-03e3-4561-8cc4-78b7c7c3a3ca-0', usage_metadata={'input_tokens': 31, 'output_tokens': 5, 'total_tokens': 36})

print(ai_msg.text)

J'adore la programmation.

## ​ Streaming usage metadata

OpenAI’s Chat Completions API does not stream token usage statistics by default (see API reference here).To recover token counts when streaming with `ChatOpenAI` or `AzureChatOpenAI`, set `stream_usage=True` as an initialization parameter or on invocation:

llm = ChatOpenAI(model="gpt-4.1-mini", stream_usage=True)

## ​ Using with Azure OpenAI

Using Azure OpenAI v1 API with API Key

To use `ChatOpenAI` with Azure OpenAI, set the `base_url` to your Azure endpoint with `/openai/v1/` appended:

llm = ChatOpenAI(
model="gpt-5-mini", # Your Azure deployment name
base_url="https://{your-resource-name}.openai.azure.com/openai/v1/",
api_key="your-azure-api-key"
)

response = llm.invoke("Hello, how are you?")
print(response.content)

Using Azure OpenAI with Microsoft Entra ID

The v1 API adds native support for Microsoft Entra ID (formerly Azure AD) authentication with automatic token refresh. Pass a token provider callable to the `api_key` parameter:

from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from langchain_openai import ChatOpenAI

# Create a token provider that handles automatic refresh
token_provider = get_bearer_token_provider(
DefaultAzureCredential(),
"https://cognitiveservices.azure.com/.default"
)

llm = ChatOpenAI(
model="gpt-5-mini", # Your Azure deployment name
base_url="https://{your-resource-name}.openai.azure.com/openai/v1/",
api_key=token_provider # Callable that handles token refresh
)

# Use the model as normal
messages = [\
("system", "You are a helpful assistant."),\
("human", "Translate 'I love programming' to French.")\
]
response = llm.invoke(messages)
print(response.content)

The token provider is a callable that automatically retrieves and refreshes authentication tokens, eliminating the need to manually manage token expiration.

**Installation requirements**To use Microsoft Entra ID authentication, install the Azure Identity library:

pip install azure-identity

You can also pass a token provider callable to the `api_key` parameter when using
asynchronous functions. You must import DefaultAzureCredential from `azure.identity.aio`:

from azure.identity.aio import DefaultAzureCredential
from langchain_openai import ChatOpenAI

credential = DefaultAzureCredential()

llm_async = ChatOpenAI(
model="gpt-5-nano",
api_key=credential
)

# Use async methods when using async callable
response = await llm_async.ainvoke("Hello!")

When using an async callable for the API key, you must use async methods (`ainvoke`, `astream`, etc.). Sync methods will raise an error.

## ​ Tool calling

OpenAI has a tool calling (we use “tool calling” and “function calling” interchangeably here) API that lets you describe tools and their arguments, and have the model return a JSON object with a tool to invoke and the inputs to that tool. tool-calling is extremely useful for building tool-using chains and agents, and for getting structured outputs from models more generally.

### ​ Bind tools

With `ChatOpenAI.bind_tools`, we can easily pass in Pydantic classes, dict schemas, LangChain tools, or even functions as tools to the model. Under the hood these are converted to an OpenAI tool schemas, which looks like:

{
"name": "...",
"description": "...",
"parameters": {...} # JSONSchema
}

…and are passed in every model invocation.

from pydantic import BaseModel, Field

class GetWeather(BaseModel):
"""Get the current weather in a given location"""

location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

llm_with_tools = llm.bind_tools([GetWeather])

ai_msg = llm_with_tools.invoke(
"what is the weather like in San Francisco",
)
ai_msg

AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 68, 'total_tokens': 85}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1617c9b2-dda5-4120-996b-0333ed5992e2-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'call_o9udf3EVOWiV4Iupktpbpofk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85})

### ​ Strict mode

As of Aug 6, 2024, OpenAI supports a `strict` argument when calling tools that will enforce that the tool argument schema is respected by the model. See more.

If `strict=True` the tool definition will also be validated, and a subset of JSON schema are accepted. Crucially, schema cannot have optional args (those with default values).Read the full docs on what types of schema are supported.

llm_with_tools = llm.bind_tools([GetWeather], strict=True)
ai_msg = llm_with_tools.invoke(
"what is the weather like in San Francisco",
)
ai_msg

AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_jUqhd8wzAIzInTJl72Rla8ht', 'function': {'arguments': '{"location":"San Francisco, CA"}', 'name': 'GetWeather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 68, 'total_tokens': 85}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5e3356a9-132d-4623-8e73-dd5a898cf4a6-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': 'call_jUqhd8wzAIzInTJl72Rla8ht', 'type': 'tool_call'}], usage_metadata={'input_tokens': 68, 'output_tokens': 17, 'total_tokens': 85})

### ​ Tool calls

Notice that the AIMessage has a `tool_calls` attribute. This contains in a standardized ToolCall format that is model-provider agnostic.

ai_msg.tool_calls

[{'name': 'GetWeather',\
'args': {'location': 'San Francisco, CA'},\
'id': 'call_jUqhd8wzAIzInTJl72Rla8ht',\
'type': 'tool_call'}]

For more on binding tools and tool call outputs, head to the tool calling docs.

### ​ Structured output and tool calls

OpenAI’s structured output feature can be used simultaneously with tool-calling. The model will either generate tool calls or a response adhering to a desired schema. See example below:

from langchain_openai import ChatOpenAI
from pydantic import BaseModel

"""Get weather at a location."""
return "It's sunny."

class OutputSchema(BaseModel):
"""Schema for response."""

answer: str
justification: str

llm = ChatOpenAI(model="gpt-4.1")

structured_llm = llm.bind_tools(
[get_weather],
response_format=OutputSchema,
strict=True,
)

# Response contains tool calls:
tool_call_response = structured_llm.invoke("What is the weather in SF?")

# structured_response.additional_kwargs["parsed"] contains parsed output
structured_response = structured_llm.invoke(
"What weighs more, a pound of feathers or a pound of gold?"
)

### ​ Custom tools

Custom tools support tools with arbitrary string inputs. They can be particularly useful when you expect your string arguments to be long or complex.

from langchain_openai import ChatOpenAI, custom_tool
from langchain.agents import create_agent

@custom_tool

"""Execute python code."""
return "27"

llm = ChatOpenAI(model="gpt-5", use_responses_api=True)

agent = create_agent(llm, [execute_code])

input_message = {"role": "user", "content": "Use the tool to calculate 3^3."}
for step in agent.stream(
{"messages": [input_message]},
stream_mode="values",
):
step["messages"][-1].pretty_print()

================================ Human Message =================================

Use the tool to calculate 3^3.
================================== Ai Message ==================================

[{'id': 'rs_68b7336cb72081a080da70bf5e980e4e0d6082d28f91357a', 'summary': [], 'type': 'reasoning'}, {'call_id': 'call_qyKsJ4XlGRudbIJDrXVA2nQa', 'input': 'print(3**3)', 'name': 'execute_code', 'type': 'custom_tool_call', 'id': 'ctc_68b7336f718481a0b39584cd35fbaa5d0d6082d28f91357a', 'status': 'completed'}]
Tool Calls:
execute_code (call_qyKsJ4XlGRudbIJDrXVA2nQa)
Call ID: call_qyKsJ4XlGRudbIJDrXVA2nQa
Args:
__arg1: print(3**3)
================================= Tool Message =================================
Name: execute_code

[{'type': 'custom_tool_call_output', 'output': '27'}]
================================== Ai Message ==================================

[{'type': 'text', 'text': '27', 'annotations': [], 'id': 'msg_68b73371e9e081a0927f54f88f2cd7a20d6082d28f91357a'}]

Context-free grammars

OpenAI supports the specification of a context-free grammar for custom tool inputs in `lark` or `regex` format. See OpenAI docs for details. The `format` parameter can be passed into `@custom_tool` as shown below:

grammar = """
start: expr

| term

| factor
factor: INT
SP: " "
ADD: "+"
MUL: "*"
%import common.INT
"""

format_ = {"type": "grammar", "syntax": "lark", "definition": grammar}

@custom_tool(format=format_)

"""Do a mathematical operation."""
return "27"

agent = create_agent(llm, [do_math])

[{'id': 'rs_68b733f066a48194a41001c0cc1081760811f11b6f4bae47', 'summary': [], 'type': 'reasoning'}, {'call_id': 'call_7hTYtlTj9NgWyw8AQGqETtV9', 'input': '3 * 3 * 3', 'name': 'do_math', 'type': 'custom_tool_call', 'id': 'ctc_68b733f3a0a08194968b8338d33ad89f0811f11b6f4bae47', 'status': 'completed'}]
Tool Calls:
do_math (call_7hTYtlTj9NgWyw8AQGqETtV9)
Call ID: call_7hTYtlTj9NgWyw8AQGqETtV9
Args:
__arg1: 3 * 3 * 3
================================= Tool Message =================================
Name: do_math

[{'type': 'text', 'text': '27', 'annotations': [], 'id': 'msg_68b733f4bb008194937130796372bd0f0811f11b6f4bae47'}]

## ​ Responses API

OpenAI supports a Responses API that is oriented toward building agentic applications. It includes a suite of built-in tools, including web and file search. It also supports management of conversation state, allowing you to continue a conversational thread without explicitly passing in previous messages, as well as the output from reasoning processes.`ChatOpenAI` will route to the Responses API if one of these features is used. You can also specify `use_responses_api=True` when instantiating `ChatOpenAI`.

### ​ Web search

To trigger a web search, pass `{"type": "web_search_preview"}` to the model as you would another tool.

**You can also pass built-in tools as invocation params:**

llm.invoke("...", tools=[{"type": "web_search_preview"}])

llm = ChatOpenAI(model="gpt-4.1-mini")

tool = {"type": "web_search_preview"}
llm_with_tools = llm.bind_tools([tool])

response = llm_with_tools.invoke("What was a positive news story from today?")

Note that the response includes structured content blocks that include both the text of the response and OpenAI annotations citing its sources. The output message will also contain information from any tool invocations:

response.content_blocks

[{'type': 'server_tool_call',\
'name': 'web_search',\
'args': {'query': 'positive news stories today', 'type': 'search'},\
'id': 'ws_68cd6f8d72e4819591dab080f4b0c340080067ad5ea8144a'},\
{'type': 'server_tool_result',\
'tool_call_id': 'ws_68cd6f8d72e4819591dab080f4b0c340080067ad5ea8144a',\
'status': 'success'},\
{'type': 'text',\
'text': 'Here are some positive news stories from today...',\
'annotations': [{'end_index': 410,\
'start_index': 337,\
'title': 'Positive News | Real Stories. Real Positive Impact',\
'type': 'citation',\
'url': 'https://www.positivenews.press/?utm_source=openai'},\
{'end_index': 969,\
'start_index': 798,\
'title': "From Green Innovation to Community Triumphs: Uplifting US Stories Lighting Up September 2025 | That's Great News",\
'type': 'citation',\
'url': 'https://info.thatsgreatnews.com/from-green-innovation-to-community-triumphs-uplifting-us-stories-lighting-up-september-2025/?utm_source=openai'},\
'id': 'msg_68cd6f8e8d448195a807b89f483a1277080067ad5ea8144a'}]\
```\
\
**You can recover just the text content of the response as a string by using `response.text`. For example, to stream response text:**\
\
Copy\
\
```\
for token in llm_with_tools.stream("..."):\
print(token.text, end="|")\
```\
\
See the streaming guide for more detail.\
\
### ​ Image generation\
\

\
To trigger an image generation, pass `{"type": "image_generation"}` to the model as you would another tool.\
\
You can also pass built-in tools as invocation params:\
\
Copy\
\
```\
llm.invoke("...", tools=[{"type": "image_generation"}])\
```\
\
Copy\
\
```\
from langchain_openai import ChatOpenAI\
\
llm = ChatOpenAI(model="gpt-4.1-mini")\
\
tool = {"type": "image_generation", "quality": "low"}\
\
llm_with_tools = llm.bind_tools([tool])\
\
ai_message = llm_with_tools.invoke(\
"Draw a picture of a cute fuzzy cat with an umbrella"\
)\
```\
\
Copy\
\
```\
import base64\
\
from IPython.display import Image\
\
image = next(\
item for item in ai_message.content_blocks if item["type"] == "image"\
)\
Image(base64.b64decode(image["base64"]), width=200)\
```\
\

### ​ File search\
\
\
To trigger a file search, pass a file search tool to the model as you would another tool. You will need to populate an OpenAI-managed vector store and include the vector store ID in the tool definition. See OpenAI documentation for more detail.\
\
Copy\
\
```\
from langchain_openai import ChatOpenAI\
\
llm = ChatOpenAI(\
model="gpt-4.1-mini",\
include=["file_search_call.results"], # optionally include search results\
)\
\
openai_vector_store_ids = [\
"vs_...", # your IDs here\
]\
\
tool = {\
"type": "file_search",\
"vector_store_ids": openai_vector_store_ids,\
}\
llm_with_tools = llm.bind_tools([tool])\
\
response = llm_with_tools.invoke("What is deep research by OpenAI?")\
print(response.text)\
```\
\
Copy\
\
```\
Deep Research by OpenAI is...\
```\
\
As with web search, the response will include content blocks with citations:\
\
Copy\
\
```\
[block["type"] for block in response.content_blocks]\
```\
\
Copy\
\
```\
['server_tool_call', 'server_tool_result', 'text']\
```\
\
Copy\
\
```\
text_block = next(block for block in response.content_blocks if block["type"] == "text")\
\
text_block["annotations"][:2]\
```\
\
Copy\
\
```\
[{'type': 'citation',\
'title': 'deep_research_blog.pdf',\
'extras': {'file_id': 'file-3UzgX7jcC8Dt9ZAFzywg5k', 'index': 2712}},\
{'type': 'citation',\
'title': 'deep_research_blog.pdf',\
'extras': {'file_id': 'file-3UzgX7jcC8Dt9ZAFzywg5k', 'index': 2712}}]\
```\
\
It will also include information from the built-in tool invocations:\
\
Copy\
\
```\
response.content_blocks[0]\
```\
\
Copy\
\
```\
{'type': 'server_tool_call',\
'name': 'file_search',\
'id': 'fs_68cd704c191c81959281b3b2ec6b139908f8f7fb31b1123c',\
'args': {'queries': ['deep research by OpenAI']}}\
```\
\
### ​ Computer use\
\
`ChatOpenAI` supports the `"computer-use-preview"` model, which is a specialized model for the built-in computer use tool. To enable, pass a computer use tool as you would pass another tool.Currently, tool outputs for computer use are present in the message `content` field. To reply to the computer use tool call, construct a `ToolMessage` with `{"type": "computer_call_output"}` in its `additional_kwargs`. The content of the message will be a screenshot. Below, we demonstrate a simple example.First, load two screenshots:\
\
Copy\
\
```\
import base64\
\
def load_png_as_base64(file_path):\
with open(file_path, "rb") as image_file:\
encoded_string = base64.b64encode(image_file.read())\
return encoded_string.decode("utf-8")\
\
screenshot_1_base64 = load_png_as_base64(\
"/path/to/screenshot_1.png"\
) # perhaps a screenshot of an application\
screenshot_2_base64 = load_png_as_base64(\
"/path/to/screenshot_2.png"\
) # perhaps a screenshot of the Desktop\
```\
\
Copy\
\
```\
from langchain_openai import ChatOpenAI\
\
# Initialize model\
llm = ChatOpenAI(model="computer-use-preview", truncation="auto")\
\
# Bind computer-use tool\
tool = {\
"type": "computer_use_preview",\
"display_width": 1024,\
"display_height": 768,\
"environment": "browser",\
}\
llm_with_tools = llm.bind_tools([tool])\
\
# Construct input message\
input_message = {\
"role": "user",\
"content": [\
{\
"type": "text",\
"text": (\
"Click the red X to close and reveal my Desktop. "\
"Proceed, no confirmation needed."\
),\
},\
{\
"type": "input_image",\
"image_url": f"data:image/png;base64,{screenshot_1_base64}",\
},\
],\
}\
\
# Invoke model\
response = llm_with_tools.invoke(\
[input_message],\
reasoning={\
"generate_summary": "concise",\
},\
)\
```\
\
The response will include a call to the computer-use tool in its `content`:\
\
Copy\
\
```\
response.content\
```\
\
Copy\
\
```\
[{'id': 'rs_685da051742c81a1bb35ce46a9f3f53406b50b8696b0f590',\
'summary': [{'text': "Clicking red 'X' to show desktop",\
'type': 'summary_text'}],\
'type': 'reasoning'},\
{'id': 'cu_685da054302481a1b2cc43b56e0b381706b50b8696b0f590',\
'action': {'button': 'left', 'type': 'click', 'x': 14, 'y': 38},\
'call_id': 'call_zmQerFBh4PbBE8mQoQHkfkwy',\
'pending_safety_checks': [],\
'status': 'completed',\
'type': 'computer_call'}]\
```\
\
We next construct a `ToolMessage` with these properties:\
\
1. It has a `tool_call_id` matching the `call_id` from the computer-call.\
2. It has `{"type": "computer_call_output"}` in its `additional_kwargs`.\
3. Its content is either an `image_url` or an `input_image` output block (see OpenAI docs for formatting).\
\
Copy\
\
```\
from langchain.messages import ToolMessage\
\
tool_call_id = next(\
item["call_id"] for item in response.content if item["type"] == "computer_call"\
)\
\
tool_message = ToolMessage(\
content=[\
{\
"type": "input_image",\
"image_url": f"data:image/png;base64,{screenshot_2_base64}",\
}\
],\
# content=f"data:image/png;base64,{screenshot_2_base64}", # <-- also acceptable\
tool_call_id=tool_call_id,\
additional_kwargs={"type": "computer_call_output"},\
)\
```\
\
We can now invoke the model again using the message history:\
\
Copy\
\
```\
messages = [\
input_message,\
response,\
tool_message,\
]\
\
response_2 = llm_with_tools.invoke(\
messages,\
reasoning={\
"generate_summary": "concise",\
},\
)\
```\
\
Copy\
\
```\
response_2.text\
```\
\
Copy\
\
```\
'VS Code has been closed, and the desktop is now visible.'\
```\
\
Instead of passing back the entire sequence, we can also use the `previous_response_id`:\
\
Copy\
\
```\
previous_response_id = response.response_metadata["id"]\
\
response_2 = llm_with_tools.invoke(\
[tool_message],\
previous_response_id=previous_response_id,\
reasoning={\
"generate_summary": "concise",\
},\
)\
```\
\
Copy\
\
```\
response_2.text\
```\
\
Copy\
\
```\
'The VS Code window is closed, and the desktop is now visible. Let me know if you need any further assistance.'\
```\
\
### ​ Code interpreter\
\
OpenAI implements a code interpreter tool to support the sandboxed generation and execution of code.\
\
Example use\
\
Copy\
\
```\
from langchain_openai import ChatOpenAI\
\
llm = ChatOpenAI(\
model="gpt-4.1-mini",\
include=["code_interpreter_call.outputs"], # optionally include outputs\
)\
\
llm_with_tools = llm.bind_tools(\
[\
{\
"type": "code_interpreter",\
# Create a new container\
"container": {"type": "auto"},\
}\
]\
)\
response = llm_with_tools.invoke(\
"Write and run code to answer the question: what is 3^3?"\
)\
```\
\
Note that the above command created a new container. We can also specify an existing container ID:\
\
Copy\
\
```\
code_interpreter_calls = [\
item for item in response.content if item["type"] == "code_interpreter_call"\
]\
assert len(code_interpreter_calls) == 1\
container_id = code_interpreter_calls[0]["extras"]["container_id"]\
\
llm_with_tools = llm.bind_tools(\
[\
{\
"type": "code_interpreter",\
# Use an existing container\
"container": container_id,\
}\
]\
)\
```\
\
### ​ Remote MCP\
\
OpenAI implements a remote MCP tool that allows for model-generated calls to MCP servers.\
\
Example use\
\
Copy\
\
```\
from langchain_openai import ChatOpenAI\
\
llm = ChatOpenAI(model="gpt-4.1-mini")\
\
llm_with_tools = llm.bind_tools(\
[\
{\
"type": "mcp",\
"server_label": "deepwiki",\
"server_url": "https://mcp.deepwiki.com/mcp",\
"require_approval": "never",\
}\
]\
)\
response = llm_with_tools.invoke(\
"What transport protocols does the 2025-03-26 version of the MCP "\
"spec (modelcontextprotocol/modelcontextprotocol) support?"\
)\
```\
\
MCP Approvals\
\
OpenAI will at times request approval before sharing data with a remote MCP server.In the above command, we instructed the model to never require approval. We can also configure the model to always request approval, or to always request approval for specific tools:\
\
Copy\
\
```\
llm_with_tools = llm.bind_tools(\
[\
{\
"type": "mcp",\
"server_label": "deepwiki",\
"server_url": "https://mcp.deepwiki.com/mcp",\
"require_approval": {\
"always": {\
"tool_names": ["read_wiki_structure"]\
}\
}\
}\
]\
)\
response = llm_with_tools.invoke(\
"What transport protocols does the 2025-03-26 version of the MCP "\
"spec (modelcontextprotocol/modelcontextprotocol) support?"\
)\
```\
\
Responses may then include blocks with type `"mcp_approval_request"`.To submit approvals for an approval request, structure it into a content block in an input message:\
\
Copy\
\
```\
approval_message = {\
"role": "user",\
"content": [\
{\
"type": "mcp_approval_response",\
"approve": True,\
"approval_request_id": block["id"],\
}\
for block in response.content\
if block["type"] == "mcp_approval_request"\
]\
}\
\
next_response = llm_with_tools.invoke(\
[approval_message],\
# continue existing thread\
previous_response_id=response.response_metadata["id"]\
)\
```\
\
### ​ Managing conversation state\
\
The Responses API supports management of conversation state.\
\
#### ​ Manually manage state\
\
You can manage the state manually or using LangGraph, as with other chat models:\
\
Copy\
\
```\
from langchain_openai import ChatOpenAI\
\
llm = ChatOpenAI(model="gpt-4.1-mini", use_responses_api=True)\
\
first_query = "Hi, I'm Bob."\
messages = [{"role": "user", "content": first_query}]\
\
response = llm.invoke(messages)\
print(response.text)\
```\
\
Copy\
\
```\
Hi Bob! Nice to meet you. How can I assist you today?\
```\
\
Copy\
\
```\
second_query = "What is my name?"\
\
messages.extend(\
[\
response,\
{"role": "user", "content": second_query},\
]\
)\
second_response = llm.invoke(messages)\
print(second_response.text)\
```\
\
Copy\
\
```\
You mentioned that your name is Bob. How can I assist you further, Bob?\
```\
\
**You can use LangGraph to manage conversational threads for you in a variety of backends, including in-memory and Postgres. See this tutorial to get started.**\
\
#### ​ Passing `previous_response_id`\
\
When using the Responses API, LangChain messages will include an `"id"` field in its metadata. Passing this ID to subsequent invocations will continue the conversation. Note that this is equivalent to manually passing in messages from a billing perspective.\
\
Copy\
\
```\
second_response = llm.invoke(\
"What is my name?",\
previous_response_id=response.id,\
)\
print(second_response.text)\
```\
\
Copy\
\
```\
Your name is Bob. How can I help you today, Bob?\
```\
\
ChatOpenAI can also automatically specify `previous_response_id` using the last response in a message sequence:\
\
Copy\
\
```\
from langchain_openai import ChatOpenAI\
\
llm = ChatOpenAI(\
model="gpt-4.1-mini",\
use_previous_response_id=True,\
)\
```\
\
If we set `use_previous_response_id=True`, input messages up to the most recent response will be dropped from request payloads, and `previous_response_id` will be set using the ID of the most recent response.That is,\
\
Copy\
\
```\
llm.invoke(\
[\
HumanMessage("Hello"),\
AIMessage("Hi there!", id="resp_123"),\
HumanMessage("How are you?"),\
]\
)\
```\
\
…is equivalent to:\
\
Copy\
\
```\
llm.invoke([HumanMessage("How are you?")], previous_response_id="resp_123")\
```\
\
### ​ Reasoning output\
\
Some OpenAI models will generate separate text content illustrating their reasoning process. See OpenAI’s reasoning documentation for details.OpenAI can return a summary of the model’s reasoning (although it doesn’t expose the raw reasoning tokens). To configure `ChatOpenAI` to return this summary, specify the `reasoning` parameter. `ChatOpenAI` will automatically route to the Responses API if this parameter is set.\
\
Copy\
\
```\
from langchain_openai import ChatOpenAI\
\
reasoning = {\
"effort": "medium", # 'low', 'medium', or 'high'\
"summary": "auto", # 'detailed', 'auto', or None\
}\
\
llm = ChatOpenAI(model="gpt-5-nano", reasoning=reasoning)\
response = llm.invoke("What is 3^3?")\
\
# Output\
response.text\
```\
\
Copy\
\
```\
'3³ = 3 × 3 × 3 = 27.'\
```\
\
Copy\
\
```\
# Reasoning\
for block in response.content_blocks:\
if block["type"] == "reasoning":\
print(block["reasoning"])\
```\
\
Copy\
\
```\
**Calculating the power of three**\
\
The user is asking about 3 raised to the power of 3. That's a pretty simple calculation! I know that 3^3 equals 27, so I can say, "3 to the power of 3 equals 27." I might also include a quick explanation that it's 3 multiplied by itself three times: 3 × 3 × 3 = 27. So, the answer is definitely 27.\
```\
\
**Troubleshooting: Empty responses from reasoning models**If you’re getting empty responses from reasoning models like `gpt-5-nano`, this is likely due to restrictive token limits. The model uses tokens for internal reasoning and may not have any left for the final output.Ensure `max_tokens` is set to `None` or increase the token limit to allow sufficient tokens for both reasoning and output generation.\
\
* * *\
\
## ​ Fine-tuning\
\
You can call fine-tuned OpenAI models by passing in your corresponding `modelName` parameter.This generally takes the form of `ft:{OPENAI_MODEL_NAME}:{ORG_NAME}::{MODEL_ID}`. For example:\
\
Copy\
\
```\
fine_tuned_model = ChatOpenAI(\
temperature=0, model_name="ft:gpt-3.5-turbo-0613:langchain::7qTVM5AR"\
)\
\
fine_tuned_model.invoke(messages)\
```\
\
Copy\
\
```\
AIMessage(content="J'adore la programmation.", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 31, 'total_tokens': 39}, 'model_name': 'ft:gpt-3.5-turbo-0613:langchain::7qTVM5AR', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0f39b30e-c56e-4f3b-af99-5c948c984146-0', usage_metadata={'input_tokens': 31, 'output_tokens': 8, 'total_tokens': 39})\
```\
\
* * *\
\
## ​ Multimodal Inputs (images, PDFs, audio)\
\
OpenAI has models that support multimodal inputs. You can pass in images, PDFs, or audio to these models. For more information on how to do this in LangChain, head to the multimodal inputs docs.You can see the list of models that support different modalities in OpenAI’s documentation.For all modalities, LangChain supports both its cross-provider standard as well as OpenAI’s native content-block format.To pass multimodal data into `ChatOpenAI`, create a content block containing the data and incorporate it into a message, e.g., as below:\
\
Copy\
\
```\
message = {\
"role": "user",\
"content": [\
{\
"type": "text",\
# Update prompt as desired\
"text": "Describe the (image / PDF / audio...)",\
},\
content_block,\
],\
}\
```\
\
See below for examples of content blocks.\
\
Images\
\
Refer to examples in the how-to guide here.\
\
URLs\
\
Copy\
\
```\
# LangChain format\
content_block = {\
"type": "image",\
"url": url_string,\
}\
\
# OpenAI Chat Completions format\
content_block = {\
"type": "image_url",\
"image_url": {"url": url_string},\
}\
```\
\
In-line base64 data\
\
Copy\
\
```\
content_block = {\
"type": "image",\
"base64": base64_string,\
"mime_type": "image/jpeg",\
}\
\
content_block = {\
"type": "image_url",\
"image_url": {\
"url": f"data:image/jpeg;base64,{base64_string}",\
},\
}\
```\
\
PDFs\
\
Note: OpenAI requires file-names be specified for PDF inputs. When using LangChain’s format, include the `filename` key.Read more here.Refer to examples in the how-to guide here.\
\
In-line base64 data\
\
Copy\
\
```\
content_block = {\
"type": "file",\
"base64": base64_string,\
"mime_type": "application/pdf",\
"filename": "my-file.pdf",\
}\
\
content_block = {\
"type": "file",\
"file": {\
"filename": "my-file.pdf",\
"file_data": f"data:application/pdf;base64,{base64_string}",\
}\
}\
```\
\
Audio\
\
See supported models, e.g., `"gpt-4o-audio-preview"`.Refer to examples in the how-to guide here.\
\
In-line base64 data\
\
Copy\
\
```\
content_block = {\
"type": "audio",\
"mime_type": "audio/wav", # or appropriate mime-type\
"base64": base64_string,\
}\
\
content_block = {\
"type": "input_audio",\
"input_audio": {"data": base64_string, "format": "wav"},\
}\
```\
\
* * *\
\
## ​ Predicted output\

\
Some OpenAI models (such as their `gpt-4o` and `gpt-4o-mini` series) support Predicted Outputs, which allow you to pass in a known portion of the LLM’s expected output ahead of time to reduce latency. This is useful for cases such as editing text or code, where only a small part of the model’s output will change.Here’s an example:\
\
Copy\
\
```\
code = """\

/// Represents a user with a first name, last name, and username.\

public class User\
{\

/// Gets or sets the user's first name.\

public string FirstName { get; set; }\
\

/// Gets or sets the user's last name.\

public string LastName { get; set; }\
\

/// Gets or sets the user's username.\

public string Username { get; set; }\
}\
"""\
\
llm = ChatOpenAI(model="gpt-4o")\
query = (\
"Replace the Username property with an Email property. "\
"Respond only with code, and with no markdown formatting."\
)\
response = llm.invoke(\
[{"role": "user", "content": query}, {"role": "user", "content": code}],\
prediction={"type": "content", "content": code},\
)\
print(response.content)\
print(response.response_metadata)\
```\
\
Copy\
\
```\

/// Represents a user with a first name, last name, and email.\

/// Gets or sets the user's email.\

public string Email { get; set; }\
}\
{'token_usage': {'completion_tokens': 226, 'prompt_tokens': 166, 'total_tokens': 392, 'completion_tokens_details': {'accepted_prediction_tokens': 49, 'audio_tokens': None, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 107}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_45cf54deae', 'finish_reason': 'stop', 'logprobs': None}\
```\
\
Predictions are billed as additional tokens and may increase your usage and costs in exchange for this reduced latency.\
\
## ​ Audio Generation (Preview)\

\
OpenAI has a new audio generation feature that allows you to use audio inputs and outputs with the `gpt-4o-audio-preview` model.\
\
Copy\
\
```\
from langchain_openai import ChatOpenAI\
\
llm = ChatOpenAI(\
model="gpt-4o-audio-preview",\
temperature=0,\
model_kwargs={\
"modalities": ["text", "audio"],\
"audio": {"voice": "alloy", "format": "wav"},\
},\
)\
\
output_message = llm.invoke(\
[\
("human", "Are you made by OpenAI? Just answer yes or no"),\
]\
)\
```\
\
`output_message.additional_kwargs['audio']` will contain a dictionary like\
\
Copy\
\
```\
{\
'data': '<audio data b64-encoded',\
'expires_at': 1729268602,\
'id': 'audio_67127d6a44348190af62c1530ef0955a',\
'transcript': 'Yes.'\
}\
```\
\
…and the format will be what was passed in `model_kwargs['audio']['format']`.We can also pass this message with audio data .\
\
Copy\
\
```\
history = [\
("human", "Are you made by OpenAI? Just answer yes or no"),\
output_message,\
("human", "And what is your name? Just give your name."),\
]\
second_output_message = llm.invoke(history)\
```\
\
* * *\
\
## ​ Prompt caching\
\
OpenAI’s prompt caching feature automatically caches prompts longer than 1024 tokens to reduce costs and improve response times. This feature is enabled for all recent models (`gpt-4o` and newer).\
\
### ​ Manual caching\
\
You can use the `prompt_cache_key` parameter to influence OpenAI’s caching and optimize cache hit rates:\
\
Copy\
\
```\
from langchain_openai import ChatOpenAI\
\
llm = ChatOpenAI(model="gpt-4o")\
\
# Use a cache key for repeated prompts\
messages = [\
{"role": "system", "content": "You are a helpful assistant that translates English to French."},\
{"role": "user", "content": "I love programming."},\
]\
\
response = llm.invoke(\
messages,\
prompt_cache_key="translation-assistant-v1"\
)\
\
# Check cache usage\
cache_read_tokens = response.usage_metadata.input_token_details.cache_read\
print(f"Cached tokens used: {cache_read_tokens}")\
```\
\
Cache hits require the prompt prefix to match exactly\
\
### ​ Cache key strategies\
\
You can use different cache key strategies based on your application’s needs:\
\
Copy\
\
```\
# Static cache keys for consistent prompt templates\
customer_response = llm.invoke(\
messages,\
prompt_cache_key="customer-support-v1"\
)\
\
support_response = llm.invoke(\
messages,\
prompt_cache_key="internal-support-v1"\
)\
\
# Dynamic cache keys based on context\
user_type = "premium"\
cache_key = f"assistant-{user_type}-v1"\
response = llm.invoke(messages, prompt_cache_key=cache_key)\
```\
\
### ​ Model-level caching\
\
You can also set a default cache key at the model level using `model_kwargs`:\
\
Copy\
\
```\
llm = ChatOpenAI(\
model="gpt-4o-mini",\
model_kwargs={"prompt_cache_key": "default-cache-v1"}\
)\
\
# Uses default cache key\
response1 = llm.invoke(messages)\
\
# Override with specific cache key\
response2 = llm.invoke(messages, prompt_cache_key="override-cache-v1")\
```\
\
* * *\
\
## ​ Flex processing\
\
OpenAI offers a variety of service tiers. The “flex” tier offers cheaper pricing for requests, with the trade-off that responses may take longer and resources might not always be available. This approach is best suited for non-critical tasks, including model testing, data enhancement, or jobs that can be run asynchronously.To use it, initialize the model with `service_tier="flex"`:\
\
Copy\
\
```\
llm = ChatOpenAI(model="o4-mini", service_tier="flex")\
```\
\
Note that this is a beta feature that is only available for a subset of models. See OpenAI docs for more detail.\
\
* * *\
\
## ​ API reference\
\
For detailed documentation of all features and configuration options, head to the `ChatOpenAI` API reference.\
\
* * *\
\
Edit this page on GitHub or file an issue.\
\
Connect these docs to Claude, VSCode, and more via MCP for real-time answers.\
\
Was this page helpful?\
\
YesNo\
\
Ctrl+I\
\
Assistant\
\
Responses are generated using AI and may contain mistakes.\
\

---

# https://docs.langchain.com/oss/python/releases/langchain-v1

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Releases

What's new in LangChain v1

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- LangChain v1
- LangGraph v1
- Migration guides

##### Policies

- Release policy
- Security

On this page

- create\_agent
- Middleware
- Prebuilt middleware
- Custom middleware
- Built on LangGraph
- Structured output
- Standard content blocks
- Benefits
- Simplified package
- Namespace
- langchain-classic
- Migration guide
- Reporting issues
- Additional resources
- See also

**LangChain v1 is a focused, production-ready foundation for building agents.** We’ve streamlined the framework around three core improvements:

**create\_agent** \\
\\
The new standard for building agents in LangChain, replacing `langgraph.prebuilt.create_react_agent`. **Standard content blocks** \\
\\
A new `content_blocks` property that provides unified access to modern LLM features across providers. **Simplified namespace** \\
\\
The `langchain` namespace has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to `langchain-classic`.

To upgrade,

pip

uv

Copy

pip install -U langchain

For a complete list of changes, see the migration guide.

## ​ `create_agent`

`create_agent` is the standard way to build agents in LangChain 1.0. It provides a simpler interface than `langgraph.prebuilt.create_react_agent` while offering greater customization potential by using middleware.

from langchain.agents import create_agent

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[search_web, analyze_data, send_email],
system_prompt="You are a helpful research assistant."
)

result = agent.invoke({
"messages": [\
{"role": "user", "content": "Research AI safety trends"}\
]
})

Under the hood, `create_agent` is built on the basic agent loop — calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:

For more information, see Agents.

### ​ Middleware

Middleware is the defining feature of `create_agent`. It offers a highly customizable entry-point, raising the ceiling for what you can build.Great agents require context engineering: getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.

#### ​ Prebuilt middleware

LangChain provides a few prebuilt middlewares for common patterns, including:

- `PIIMiddleware`: Redact sensitive information before sending to the model
- `SummarizationMiddleware`: Condense conversation history when it gets too long
- `HumanInTheLoopMiddleware`: Require approval for sensitive tool calls

from langchain.agents import create_agent
from langchain.agents.middleware import (
PIIMiddleware,
SummarizationMiddleware,
HumanInTheLoopMiddleware
)

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[read_email, send_email],
middleware=[\
PIIMiddleware("email", strategy="redact", apply_to_input=True),\
PIIMiddleware(\
"phone_number",\
detector=(\
r"(?:\+?\d{1,3}[\s.-]?)?"\
r"(?:\(?\d{2,4}\)?[\s.-]?)?"\
r"\d{3,4}[\s.-]?\d{4}"\
),\
strategy="block"\
),\
SummarizationMiddleware(\
model="claude-sonnet-4-5-20250929",\
trigger={"tokens": 500}\
),\
HumanInTheLoopMiddleware(\
interrupt_on={\
"send_email": {\
"allowed_decisions": ["approve", "edit", "reject"]\
}\
}\
),\
]
)

#### ​ Custom middleware

You can also build custom middleware to fit your needs. Middleware exposes hooks at each step in an agent’s execution:

Build custom middleware by implementing any of these hooks on a subclass of the `AgentMiddleware` class:

| Hook | When it runs | Use cases |
| --- | --- | --- |
| `before_agent` | Before calling the agent | Load memory, validate input |
| `before_model` | Before each LLM call | Update prompts, trim messages |
| `wrap_model_call` | Around each LLM call | Intercept and modify requests/responses |
| `wrap_tool_call` | Around each tool call | Intercept and modify tool execution |
| `after_model` | After each LLM response | Validate output, apply guardrails |
| `after_agent` | After agent completes | Save results, cleanup |

Example custom middleware:

from dataclasses import dataclass
from typing import Callable

from langchain_openai import ChatOpenAI

from langchain.agents.middleware import (
AgentMiddleware,
ModelRequest
)
from langchain.agents.middleware.types import ModelResponse

@dataclass
class Context:
user_expertise: str = "beginner"

class ExpertiseBasedToolMiddleware(AgentMiddleware):
def wrap_model_call(
self,
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse]

user_level = request.runtime.context.user_expertise

if user_level == "expert":
# More powerful model
model = ChatOpenAI(model="gpt-5")
tools = [advanced_search, data_analysis]
else:
# Less powerful model
model = ChatOpenAI(model="gpt-5-nano")
tools = [simple_search, basic_calculator]

return handler(request.override(model=model, tools=tools))

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[\
simple_search,\
advanced_search,\
basic_calculator,\
data_analysis\
],
middleware=[ExpertiseBasedToolMiddleware()],
context_schema=Context
)

See all 45 lines

For more information, see the complete middleware guide.

### ​ Built on LangGraph

Because `create_agent` is built on LangGraph, you automatically get built in support for long running and reliable agents via:

## Persistence

Conversations automatically persist across sessions with built-in checkpointing

## Streaming

Stream tokens, tool calls, and reasoning traces in real-time

## Human-in-the-loop

Pause agent execution for human approval before sensitive actions

## Time travel

Rewind conversations to any point and explore alternate paths and prompts

You don’t need to learn LangGraph to use these features—they work out of the box.

### ​ Structured output

`create_agent` has improved structured output generation:

- **Main loop integration**: Structured output is now generated in the main loop instead of requiring an additional LLM call
- **Structured output strategy**: Models can choose between calling tools or using provider-side structured output generation
- **Cost reduction**: Eliminates extra expense from additional LLM calls

from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy
from pydantic import BaseModel

class Weather(BaseModel):
temperature: float
condition: str

"""Get the weather for a city."""
return f"it's sunny and 70 degrees in {city}"

agent = create_agent(
"gpt-4o-mini",
tools=[weather_tool],
response_format=ToolStrategy(Weather)
)

result = agent.invoke({
"messages": [{"role": "user", "content": "What's the weather in SF?"}]
})

print(repr(result["structured_response"]))
# results in `Weather(temperature=70.0, condition='sunny')`

**Error handling**: Control error handling via the `handle_errors` parameter to `ToolStrategy`:

- **Parsing errors**: Model generates data that doesn’t match desired structure
- **Multiple tool calls**: Model generates 2+ tool calls for structured output schemas

* * *

## ​ Standard content blocks

Content block support is currently only available for the following integrations:

- `langchain-anthropic`
- `langchain-aws`
- `langchain-openai`
- `langchain-google-genai`
- `langchain-ollama`

Broader support for content blocks will be rolled out gradually across more providers.

The new `content_blocks` property introduces a standard representation for message content that works across providers:

from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
response = model.invoke("What's the capital of France?")

# Unified access to content blocks
for block in response.content_blocks:
if block["type"] == "reasoning":
print(f"Model reasoning: {block['reasoning']}")
elif block["type"] == "text":
print(f"Response: {block['text']}")
elif block["type"] == "tool_call":
print(f"Tool call: {block['name']}({block['args']})")

### ​ Benefits

- **Provider agnostic**: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider
- **Type safe**: Full type hints for all content block types
- **Backward compatible**: Standard content can be loaded lazily, so there are no associated breaking changes

For more information, see our guide on content blocks.

## ​ Simplified package

LangChain v1 streamlines the `langchain` package namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality:

### ​ Namespace

| Module | What’s available | Notes |
| --- | --- | --- |
| `langchain.agents` | `create_agent`, `AgentState` | Core agent creation functionality |
| `langchain.messages` | Message types, content blocks, `trim_messages` | Re-exported from `langchain-core` |
| `langchain.tools` | `@tool`, `BaseTool`, injection helpers | Re-exported from `langchain-core` |
| `langchain.chat_models` | `init_chat_model`, `BaseChatModel` | Unified model initialization |
| `langchain.embeddings` | `Embeddings`, `init_embeddings` | Embedding models |

Most of these are re-exported from `langchain-core` for convenience, which gives you a focused API surface for building agents.

# Agent building

# Messages and content
from langchain.messages import AIMessage, HumanMessage

# Tools
from langchain.tools import tool

# Model initialization
from langchain.chat_models import init_chat_model
from langchain.embeddings import init_embeddings

### ​ `langchain-classic`

Legacy functionality has moved to `langchain-classic` to keep the core packages lean and focused.**What’s in `langchain-classic`:**

- Legacy chains and chain implementations
- Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)
- The indexing API
- The hub module (for managing prompts programmatically)
- `langchain-community` exports
- Other deprecated functionality

If you use any of this functionality, install `langchain-classic`:

pip install langchain-classic

Then update your imports:

from langchain import ...
from langchain_classic import ...

from langchain.chains import ...
from langchain_classic.chains import ...

from langchain.retrievers import ...
from langchain_classic.retrievers import ...

from langchain import hub
from langchain_classic import hub

## ​ Migration guide

See our migration guide for help updating your code to LangChain v1.

## ​ Reporting issues

Please report any issues discovered with 1.0 on GitHub using the `'v1'` label.

## ​ Additional resources

**LangChain 1.0** \\
\\
Read the announcement **Middleware guide** \\
\\
Deep dive into middleware **Agents Documentation** \\
\\
Full agent documentation **Message Content** \\
\\
New content blocks API **Migration guide** \\
\\
How to migrate to LangChain v1 **GitHub** \\
\\
Report issues or contribute

## ​ See also

- Versioning – Understanding version numbers
- Release policy – Detailed release policies

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Changelog\\
\\
Previous What's new in LangGraph v1\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/migrate/langchain-v1

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Migration guides

LangChain v1 migration guide

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

- LangChain v1
- LangGraph v1

##### Policies

- Release policy
- Security

On this page

- Simplified package
- Namespace
- langchain-classic
- Migrate to create\_agent
- Import path
- Prompts
- Static prompt rename
- SystemMessage to string
- Dynamic prompts
- Pre-model hook
- Post-model hook
- Custom state
- Defining state via state\_schema
- Defining state via middleware
- State type restrictions
- Model
- Dynamic model selection
- Pre-bound models
- Tools
- Handling tool errors
- Structured output
- Node changes
- Tool and provider strategies
- Prompted output removed
- Streaming node name rename
- Runtime context
- Standard content
- What changed
- Read standardized content
- Create multimodal messages
- Example block shapes
- Serialize standard content
- Simplified package
- Namespace
- langchain-classic
- Breaking changes
- Dropped Python 3.9 support
- Updated return type for chat models
- Default message format for OpenAI Responses API
- Default max\_tokens in langchain-anthropic
- Legacy code moved to langchain-classic
- Removal of deprecated APIs
- Text property
- example parameter removed from AIMessage
- Minor changes
- Archived docs

This guide outlines the major changes between LangChain v1 and previous versions.

## ​ Simplified package

The `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.

### ​ Namespace

| Module | What’s available | Notes |
| --- | --- | --- |
| `langchain.agents` | `create_agent`, `AgentState` | Core agent creation functionality |
| `langchain.messages` | Message types, content blocks, `trim_messages` | Re-exported from `langchain-core` |
| `langchain.tools` | `@tool`, `BaseTool`, injection helpers | Re-exported from `langchain-core` |
| `langchain.chat_models` | `init_chat_model`, `BaseChatModel` | Unified model initialization |
| `langchain.embeddings` | `init_embeddings`, `Embeddings` | Embedding models |

### ​ `langchain-classic`

If you were using any of the following from the `langchain` package, you’ll need to install `langchain-classic` and update your imports:

- Legacy chains (`LLMChain`, `ConversationChain`, etc.)
- Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)
- The indexing API
- The hub module (for managing prompts programmatically)
- Embeddings modules (e.g. `CacheBackedEmbeddings` and community embeddings)
- `langchain-community` re-exports
- Other deprecated functionality

v1 (new)

v0 (old)

Copy

# Chains
from langchain_classic.chains import LLMChain

# Retrievers
from langchain_classic.retrievers import ...

# Indexing
from langchain_classic.indexes import ...

# Hub
from langchain_classic import hub

Install with:

pip

uv

pip install langchain-classic

* * *

## ​ Migrate to `create_agent`

Prior to v1.0, we recommended using `langgraph.prebuilt.create_react_agent` to build agents. Now, we recommend you use `langchain.agents.create_agent` to build agents.The table below outlines what functionality has changed from `create_react_agent` to `create_agent`:

| Section | TL;DR - What’s changed |
| --- | --- |
| Import path | Package moved from `langgraph.prebuilt` to `langchain.agents` |
| Prompts | Parameter renamed to `system_prompt`, dynamic prompts use middleware |
| Pre-model hook | Replaced by middleware with `before_model` method |
| Post-model hook | Replaced by middleware with `after_model` method |
| Custom state | `TypedDict` only, can be defined via `state_schema` or middleware |
| Model | Dynamic selection via middleware, pre-bound models not supported |
| Tools | Tool error handling moved to middleware with `wrap_tool_call` |
| Structured output | prompted output removed, use `ToolStrategy`/`ProviderStrategy` |
| Streaming node name | Node name changed from `"agent"` to `"model"` |
| Runtime context | Dependency injection via `context` argument instead of `config["configurable"]` |
| Namespace | Streamlined to focus on agent building blocks, legacy code moved to `langchain-classic` |

### ​ Import path

The import path for the agent prebuilt has changed from `langgraph.prebuilt` to `langchain.agents`.
The name of the function has changed from `create_react_agent` to `create_agent`:

from langgraph.prebuilt import create_react_agent
from langchain.agents import create_agent

For more information, see Agents.

#### ​ Static prompt rename

The `prompt` parameter has been renamed to `system_prompt`:

from langchain.agents import create_agent

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[check_weather],
system_prompt="You are a helpful assistant"
)

#### ​ `SystemMessage` to string

If using `SystemMessage` objects in the system prompt, extract the string content:

#### ​ Dynamic prompts

Dynamic prompts are a core context engineering pattern— they adapt what you tell the model based on the current conversation state. To do this, use the `@dynamic_prompt` decorator:

from dataclasses import dataclass

from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest
from langgraph.runtime import Runtime

@dataclass
class Context:
user_role: str = "user"

@dynamic_prompt

user_role = request.runtime.context.user_role
base_prompt = "You are a helpful assistant."

if user_role == "expert":
prompt = (
f"{base_prompt} Provide detailed technical responses."
)
elif user_role == "beginner":
prompt = (
f"{base_prompt} Explain concepts simply and avoid jargon."
)
else:
prompt = base_prompt

return prompt

agent = create_agent(
model="gpt-4o",
tools=tools,
middleware=[dynamic_prompt],
context_schema=Context
)

# Use with context
agent.invoke(
{"messages": [{"role": "user", "content": "Explain async programming"}]},
context=Context(user_role="expert")
)

### ​ Pre-model hook

Pre-model hooks are now implemented as middleware with the `before_model` method.
This new pattern is more extensible—you can define multiple middlewares to run before the model is called,
reusing common patterns across different agents.Common use cases include:

- Summarizing conversation history
- Trimming messages
- Input guardrails, like PII redaction

v1 now has summarization middleware as a built in option:

from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=tools,
middleware=[\
SummarizationMiddleware(\
model="claude-sonnet-4-5-20250929",\
trigger={"tokens": 1000}\
)\
]
)

### ​ Post-model hook

Post-model hooks are now implemented as middleware with the `after_model` method.
This new pattern is more extensible—you can define multiple middlewares to run after the model is called,
reusing common patterns across different agents.Common use cases include:

- Human in the loop
- Output guardrails

v1 has a built in middleware for human in the loop approval for tool calls:

from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[read_email, send_email],
middleware=[\
HumanInTheLoopMiddleware(\
interrupt_on={\
"send_email": {\
"description": "Please review this email before sending",\
"allowed_decisions": ["approve", "reject"]\
}\
}\
)\
]
)

### ​ Custom state

Custom state extends the default agent state with additional fields. You can define custom state in two ways:

1. **Via `state_schema` on `create_agent`** \- Best for state used in tools
2. **Via middleware** \- Best for state managed by specific middleware hooks and tools attached to said middleware

Defining custom state via middleware is preferred over defining it via `state_schema` on `create_agent` because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.`state_schema` is still supported for backwards compatibility on `create_agent`.

#### ​ Defining state via `state_schema`

Use the `state_schema` parameter when your custom state needs to be accessed by tools:

from langchain.tools import tool, ToolRuntime
from langchain.agents import create_agent, AgentState

# Define custom state extending AgentState
class CustomState(AgentState):
user_name: str

@tool
def greet(
runtime: ToolRuntime[None, CustomState]

"""Use this to greet the user by name."""
user_name = runtime.state.get("user_name", "Unknown")
return f"Hello {user_name}!"

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[greet],
state_schema=CustomState
)

#### ​ Defining state via middleware

Middleware can also define custom state by setting the `state_schema` attribute.
This helps to keep state extensions conceptually scoped to the relevant middleware and tools.

from langchain.agents.middleware import AgentState, AgentMiddleware
from typing_extensions import NotRequired
from typing import Any

class CustomState(AgentState):
model_call_count: NotRequired[int]

class CallCounterMiddleware(AgentMiddleware[CustomState]):
state_schema = CustomState

count = state.get("model_call_count", 0)

return {"jump_to": "end"}
return None

return {"model_call_count": state.get("model_call_count", 0) + 1}

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[...],
middleware=[CallCounterMiddleware()]
)

See the middleware documentation for more details on defining custom state via middleware.

#### ​ State type restrictions

`create_agent` only supports `TypedDict` for state schemas. Pydantic models and dataclasses are no longer supported.

from langchain.agents import AgentState, create_agent

# AgentState is a TypedDict
class CustomAgentState(AgentState):
user_id: str

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=tools,
state_schema=CustomAgentState
)

Simply inherit from `langchain.agents.AgentState` instead of `BaseModel` or decorating with `dataclass`.
If you need to perform validation, handle it in middleware hooks instead.

### ​ Model

Dynamic model selection allows you to choose different models based on runtime context (e.g., task complexity, cost constraints, or user preferences). `create_react_agent` released in v0.6 of `langgraph-prebuilt` supported dynamic model and tool selection via a callable passed to the `model` parameter.This functionality has been ported to the middleware interface in v1.

#### ​ Dynamic model selection

from langchain.agents import create_agent
from langchain.agents.middleware import (
AgentMiddleware, ModelRequest
)
from langchain.agents.middleware.types import ModelResponse
from langchain_openai import ChatOpenAI
from typing import Callable

basic_model = ChatOpenAI(model="gpt-5-nano")
advanced_model = ChatOpenAI(model="gpt-5")

class DynamicModelMiddleware(AgentMiddleware):

model = advanced_model
else:
model = basic_model
return handler(request.override(model=model))

self.messages_threshold = messages_threshold

agent = create_agent(
model=basic_model,
tools=tools,
middleware=[DynamicModelMiddleware(messages_threshold=10)]
)

#### ​ Pre-bound models

To better support structured output, `create_agent` no longer accepts pre-bound models with tools or configuration:

# No longer supported
model_with_tools = ChatOpenAI().bind_tools([some_tool])
agent = create_agent(model_with_tools, tools=[])

# Use instead
agent = create_agent("gpt-4o-mini", tools=[some_tool])

Dynamic model functions can return pre-bound models if structured output is _not_ used.

### ​ Tools

The `tools` argument to `create_agent` accepts a list of:

- LangChain `BaseTool` instances (functions decorated with `@tool`)
- Callable objects (functions) with proper type hints and a docstring
- `dict` that represents a built-in provider tools

The argument will no longer accept `ToolNode` instances.

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[check_weather, search_web]
)

#### ​ Handling tool errors

You can now configure the handling of tool errors with middleware implementing the `wrap_tool_call` method.

from langchain.agents import create_agent
from langchain.agents.middleware import wrap_tool_call
from langchain.messages import ToolMessage

@wrap_tool_call
def handle_tool_errors(request, handler):
"""Handle tool execution errors with custom messages."""
try:
return handler(request)
except Exception as e:
# Only handle errors that occur during tool execution due to invalid inputs
# that pass schema validation but fail at runtime (e.g., invalid SQL syntax).
# Do NOT handle:
# - Network failures (use tool retry middleware instead)
# - Incorrect tool implementation errors (should bubble up)
# - Schema mismatch errors (already auto-handled by the framework)
# Return a custom error message to the model
return ToolMessage(
content=f"Tool error: Please check your input and try again. ({str(e)})",
tool_call_id=request.tool_call["id"]
)

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[check_weather, search_web],
middleware=[handle_tool_errors]
)

#### ​ Node changes

Structured output used to be generated in a separate node from the main agent. This is no longer the case.
We generate structured output in the main loop, reducing cost and latency.

#### ​ Tool and provider strategies

In v1, there are two new structured output strategies:

- `ToolStrategy` uses artificial tool calling to generate structured output
- `ProviderStrategy` uses provider-native structured output generation

from langchain.agents import create_agent
from langchain.agents.structured_output import ToolStrategy, ProviderStrategy
from pydantic import BaseModel

class OutputSchema(BaseModel):
summary: str
sentiment: str

# Using ToolStrategy
agent = create_agent(
model="gpt-4o-mini",
tools=tools,
# explicitly using tool strategy
response_format=ToolStrategy(OutputSchema)
)

#### ​ Prompted output removed

**Prompted output** is no longer supported via the `response_format` argument. Compared to strategies
like artificial tool calling and provider native structured output, prompted output has not proven to be particularly reliable.

### ​ Streaming node name rename

When streaming events from agents, the node name has changed from `"agent"` to `"model"` to better reflect the node’s purpose.

### ​ Runtime context

When you invoke an agent, it’s often the case that you want to pass two types of data:

- Dynamic state that changes throughout the conversation (e.g., message history)
- Static context that doesn’t change during the conversation (e.g., user metadata)

In v1, static context is supported by setting the `context` parameter to `invoke` and `stream`.

@dataclass
class Context:
user_id: str
session_id: str

agent = create_agent(
model=model,
tools=tools,
context_schema=Context
)

result = agent.invoke(
{"messages": [{"role": "user", "content": "Hello"}]},
context=Context(user_id="123", session_id="abc")
)

The old `config["configurable"]` pattern still works for backward compatibility, but using the new `context` parameter is recommended for new applications or applications migrating to v1.

## ​ Standard content

In v1, messages gain provider-agnostic standard content blocks. Access them via `message.content_blocks` for a consistent, typed view across providers. The existing `message.content` field remains unchanged for strings or provider-native structures.

### ​ What changed

- New `content_blocks` property on messages for normalized content
- Standardized block shapes, documented in Messages
- Optional serialization of standard blocks into `content` via `LC_OUTPUT_VERSION=v1` or `output_version="v1"`

### ​ Read standardized content

from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-5-nano")
response = model.invoke("Explain AI")

for block in response.content_blocks:
if block["type"] == "reasoning":
print(block.get("reasoning"))
elif block["type"] == "text":
print(block.get("text"))

### ​ Create multimodal messages

from langchain.messages import HumanMessage

message = HumanMessage(content_blocks=[\
{"type": "text", "text": "Describe this image."},\
{"type": "image", "url": "https://example.com/image.jpg"},\
])
res = model.invoke([message])

### ​ Example block shapes

# Text block
text_block = {
"type": "text",
"text": "Hello world",
}

# Image block
image_block = {
"type": "image",
"url": "https://example.com/image.png",
"mime_type": "image/png",
}

See the content blocks reference for more details.

### ​ Serialize standard content

Standard content blocks are **not serialized** into the `content` attribute by default. If you need to access standard content blocks in the `content` attribute (e.g., when sending messages to a client), you can opt-in to serializing them into `content`.

Environment variable

Initialization parameter

export LC_OUTPUT_VERSION=v1

Learn more: Messages, Standard content blocks, and Multimodal.

### ​ Namespace

### ​ `langchain-classic`

**Installation**:

uv pip install langchain-classic

## ​ Breaking changes

### ​ Dropped Python 3.9 support

All LangChain packages now require **Python 3.10 or higher**. Python 3.9 reaches end of life in October 2025.

### ​ Updated return type for chat models

The return type signature for chat model invocation has been fixed from `BaseMessage` to `AIMessage`. Custom chat models implementing `bind_tools` should update their return signature:

def bind_tools(
...

### ​ Default message format for OpenAI Responses API

When interacting with the Responses API, `langchain-openai` now defaults to storing response items in message `content`. To restore previous behavior, set the `LC_OUTPUT_VERSION` environment variable to `v0`, or specify `output_version="v0"` when instantiating `ChatOpenAI`.

# Enforce previous behavior with output_version flag
model = ChatOpenAI(model="gpt-4o-mini", output_version="v0")

### ​ Default `max_tokens` in `langchain-anthropic`

The `max_tokens` parameter in `langchain-anthropic` now defaults to higher values based on the model chosen, rather than the previous default of `1024`. If you relied on the old default, explicitly set `max_tokens=1024`.

### ​ Legacy code moved to `langchain-classic`

Existing functionality outside the focus of standard interfaces and agents has been moved to the `langchain-classic` package. See the Simplified namespace section for details on what’s available in the core `langchain` package and what moved to `langchain-classic`.

### ​ Removal of deprecated APIs

Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the deprecation notices from previous versions for replacement APIs.

### ​ Text property

Use of the `.text()` method on message objects should drop the parentheses, as it is now a property:

# Property access
text = response.text

# Deprecated method call
text = response.text()

Existing usage patterns (i.e., `.text()`) will continue to function but now emit a warning. The method form will be removed in v2.

### ​ `example` parameter removed from `AIMessage`

The `example` parameter has been removed from `AIMessage` objects. We recommend migrating to use `additional_kwargs` for passing extra metadata as needed.

## ​ Minor changes

- `AIMessageChunk` objects now include a `chunk_position` attribute with position `'last'` to indicate the final chunk in a stream. This allows for clearer handling of streamed messages. If the chunk is not the final one, `chunk_position` will be `None`.
- `LanguageModelOutputVar` is now typed to `AIMessage` instead of `BaseMessage`.
- The logic for merging message chunks (`AIMessageChunk.add`) has been updated with more sophisticated selection handling for the final id for the merged chunk. It prioritizes provider-assigned IDs over LangChain-generated IDs.
- We now open files with `utf-8` encoding by default.
- Standard tests now use multimodal content blocks.

## ​ Archived docs

Old docs are archived for reference:

- v0.3 docs content
- v0.3 API reference

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

What's new in LangGraph v1\\
\\
Previous LangGraph v1 migration guide\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/releases/langgraph-v1

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Releases

What's new in LangGraph v1

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- LangChain v1
- LangGraph v1
- Migration guides

##### Policies

- Release policy
- Security

On this page

- Deprecation of create\_react\_agent
- Reporting issues
- Additional resources
- See also

**LangGraph v1 is a stability-focused release for the agent runtime.** It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.It’s designed to work hand-in-hand with LangChain v1 (whose `create_agent` is built on LangGraph) so you can start high-level and drop down to granular control when needed.

## Stable core APIs

Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.

## Reliability, by default

Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.

## Seamless with LangChain v1

LangChain’s `create_agent` runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.

To upgrade,

pip

uv

Copy

pip install -U langgraph

## ​ Deprecation of `create_react_agent`

The LangGraph `create_react_agent` prebuilt has been deprecated in favor of LangChain’s `create_agent`. It provides a simpler interface, and offers greater customization potential through the introduction of middleware.

- For information on the new `create_agent` API, see the LangChain v1 release notes.
- For information on migrating from `create_react_agent` to `create_agent`, see the LangChain v1 migration guide.

## ​ Reporting issues

Please report any issues discovered with 1.0 on GitHub using the `'v1'` label.

## ​ Additional resources

**LangGraph 1.0** \\
\\
Read the announcement **Overview** \\
\\
What LangGraph is and when to use it **Graph API** \\
\\
Build graphs with state, nodes, and edges **LangChain Agents** \\
\\
High-level agents built on LangGraph **Migration guide** \\
\\
How to migrate to LangGraph v1 **GitHub** \\
\\
Report issues or contribute

## ​ See also

- Versioning – Understanding version numbers
- Release policy – Detailed release policies

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

What's new in LangChain v1\\
\\
Previous LangChain v1 migration guide\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/migrate/langgraph-v1

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Migration guides

LangGraph v1 migration guide

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

- LangChain v1
- LangGraph v1

##### Policies

- Release policy
- Security

On this page

- Summary of changes
- Deprecations
- create\_react\_agent → create\_agent
- Breaking changes
- Dropped Python 3.9 support

This guide outlines changes in LangGraph v1 and how to migrate from previous versions. For a high-level overview of changes, see the what’s new page.To upgrade:

pip

uv

Copy

pip install -U langgraph langchain-core

## ​ Summary of changes

LangGraph v1 is largely backwards compatible with previous versions. The main change is the deprecation of `create_react_agent` in favor of LangChain’s new `create_agent` function.

## ​ Deprecations

The following table lists all items deprecated in LangGraph v1:

| Deprecated item | Alternative |
| --- | --- |
| `create_react_agent` | `langchain.agents.create_agent` |
| `AgentState` | `langchain.agents.AgentState` |
| `AgentStatePydantic` | `langchain.agents.AgentState` (no more pydantic state) |
| `AgentStateWithStructuredResponse` | `langchain.agents.AgentState` |
| `AgentStateWithStructuredResponsePydantic` | `langchain.agents.AgentState` (no more pydantic state) |
| `HumanInterruptConfig` | `langchain.agents.middleware.human_in_the_loop.InterruptOnConfig` |
| `ActionRequest` | `langchain.agents.middleware.human_in_the_loop.InterruptOnConfig` |
| `HumanInterrupt` | `langchain.agents.middleware.human_in_the_loop.HITLRequest` |
| `ValidationNode` | Tools automatically validate input with `create_agent` |
| `MessageGraph` | `StateGraph` with a `messages` key, like `create_agent` provides |

## ​ `create_react_agent` → `create_agent`

LangGraph v1 deprecates the `create_react_agent` prebuilt. Use LangChain’s `create_agent`, which runs on LangGraph and adds a flexible middleware system.See the LangChain v1 docs for details:

- Release notes
- Migration guide

v1 (new)

v0 (old)

from langchain.agents import create_agent

agent = create_agent(
model,
tools,
system_prompt="You are a helpful assistant.",
)

## ​ Breaking changes

### ​ Dropped Python 3.9 support

All LangChain packages now require **Python 3.10 or higher**. Python 3.9 reached end of life in October 2025.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangChain v1 migration guide\\
\\
Previous Release policy\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/common-errors)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LiveKit Trace with Pipecat Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/versioning)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Upload files with traces Contributing to documentation

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/release-policy)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

Release policy Trace with LangChain (Python and JS/TS) Contributing to documentation

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/security-policy)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

Security policy Trace with LangChain (Python and JS/TS) Contributing to documentation

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/releases/changelog/rss.xml)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/agents):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview Build a voice agent with LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/tools).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration LangChain integrations packages

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/structured-output):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Structured output Models

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/releases/langchain-v1)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) What's new in LangChain v1

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/migrate/langchain-v1)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph v1 migration guide LangChain v1 migration guide

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/releases/langgraph-v1)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

What's new in LangGraph v1 LangGraph Python SDK

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/migrate/langgraph-v1)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph v1 migration guide What's new in LangGraph v1

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/knowledge-base

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangChain

Build a semantic search engine with LangChain

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Overview
- Concepts
- Setup
- Installation
- LangSmith
- 1\. Documents and Document Loaders
- Loading documents
- Splitting
- 2\. Embeddings
- 3\. Vector stores
- 4\. Retrievers
- Next steps

## ​ Overview

This tutorial will familiarize you with LangChain’s document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data— from (vector) databases and other sources — for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG.Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.

### ​ Concepts

This guide focuses on retrieval of text data. We will cover the following concepts:

- Documents and document loaders;
- Text splitters;
- Embeddings;
- Vector stores and retrievers.

## ​ Setup

### ​ Installation

This tutorial requires the `langchain-community` and `pypdf` packages:

pip

conda

Copy

pip install langchain-community pypdf

For more details, see our Installation guide.

### ​ LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with LangSmith.After you sign up at the link above, make sure to set your environment variables to start logging traces:

export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

Or, if in a notebook, you can set them with:

import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()

## ​ 1\. Documents and Document Loaders

LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:

- `page_content`: a string representing the content;
- `metadata`: a dict containing arbitrary metadata;
- `id`: (optional) a string identifier for the document.

The `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual `Document` object often represents a chunk of a larger document.We can generate sample documents when desired:

from langchain_core.documents import Document

documents = [\
Document(\
page_content="Dogs are great companions, known for their loyalty and friendliness.",\
metadata={"source": "mammal-pets-doc"},\
),\
Document(\
page_content="Cats are independent pets that often enjoy their own space.",\
metadata={"source": "mammal-pets-doc"},\
),\
]

However, the LangChain ecosystem implements document loaders that integrate with hundreds of common sources. This makes it easy to incorporate data from these sources into your AI application.

### ​ Loading documents

Let’s load a PDF into a sequence of `Document` objects. Here is a sample PDF — a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders.

from langchain_community.document_loaders import PyPDFLoader

file_path = "../example_data/nke-10k-2023.pdf"
loader = PyPDFLoader(file_path)

docs = loader.load()

print(len(docs))

107

`PyPDFLoader` loads one `Document` object per PDF page. For each, we can easily access:

- The string content of the page;
- Metadata containing the file name and page number.

print(f"{docs[0].page_content[:200]}\n")
print(docs[0].metadata)

Table of Contents
UNITED STATES
SECURITIES AND EXCHANGE COMMISSION
Washington, D.C. 20549
FORM 10-K
(Mark One)
☑ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934
FO

{'source': '../example_data/nke-10k-2023.pdf', 'page': 0}

### ​ Splitting

For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve `Document` objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not “washed out” by surrounding text.We can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters
with 200 characters of overlap between chunks. The overlap helps
mitigate the possibility of separating a statement from important
context related to it. We use the
`RecursiveCharacterTextSplitter`,
which will recursively split the document using common separators like
new lines until each chunk is the appropriate size. This is the
recommended text splitter for generic text use cases.We set `add_start_index=True` so that the character index where each
split Document starts within the initial Document is preserved as
metadata attribute “start\_index”.

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
chunk_size=1000, chunk_overlap=200, add_start_index=True
)
all_splits = text_splitter.split_documents(docs)

print(len(all_splits))

514

## ​ 2\. Embeddings

Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.LangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Let’s select a model:

- OpenAI

- Azure

- Google Gemini

- Google Vertex

- AWS

- HuggingFace

- Ollama

- Cohere

- MistralAI

- Nomic

- NVIDIA

- Voyage AI

- IBM watsonx

- Fake

- Isaacus

pip install -U "langchain-openai"

if not os.environ.get("OPENAI_API_KEY"):
os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

if not os.environ.get("AZURE_OPENAI_API_KEY"):
os.environ["AZURE_OPENAI_API_KEY"] = getpass.getpass("Enter API key for Azure: ")

from langchain_openai import AzureOpenAIEmbeddings

embeddings = AzureOpenAIEmbeddings(
azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
openai_api_version=os.environ["AZURE_OPENAI_API_VERSION"],
)

pip install -qU langchain-google-genai

if not os.environ.get("GOOGLE_API_KEY"):
os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter API key for Google Gemini: ")

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

pip install -qU langchain-google-vertexai

from langchain_google_vertexai import VertexAIEmbeddings

embeddings = VertexAIEmbeddings(model="text-embedding-005")

pip install -qU langchain-aws

from langchain_aws import BedrockEmbeddings

embeddings = BedrockEmbeddings(model_id="amazon.titan-embed-text-v2:0")

pip install -qU langchain-huggingface

from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

pip install -qU langchain-ollama

from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(model="llama3")

pip install -qU langchain-cohere

if not os.environ.get("COHERE_API_KEY"):
os.environ["COHERE_API_KEY"] = getpass.getpass("Enter API key for Cohere: ")

from langchain_cohere import CohereEmbeddings

embeddings = CohereEmbeddings(model="embed-english-v3.0")

pip install -qU langchain-mistralai

if not os.environ.get("MISTRALAI_API_KEY"):
os.environ["MISTRALAI_API_KEY"] = getpass.getpass("Enter API key for MistralAI: ")

from langchain_mistralai import MistralAIEmbeddings

embeddings = MistralAIEmbeddings(model="mistral-embed")

pip install -qU langchain-nomic

if not os.environ.get("NOMIC_API_KEY"):
os.environ["NOMIC_API_KEY"] = getpass.getpass("Enter API key for Nomic: ")

from langchain_nomic import NomicEmbeddings

embeddings = NomicEmbeddings(model="nomic-embed-text-v1.5")

pip install -qU langchain-nvidia-ai-endpoints

if not os.environ.get("NVIDIA_API_KEY"):
os.environ["NVIDIA_API_KEY"] = getpass.getpass("Enter API key for NVIDIA: ")

from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings

embeddings = NVIDIAEmbeddings(model="NV-Embed-QA")

pip install -qU langchain-voyageai

if not os.environ.get("VOYAGE_API_KEY"):
os.environ["VOYAGE_API_KEY"] = getpass.getpass("Enter API key for Voyage AI: ")

from langchain-voyageai import VoyageAIEmbeddings

embeddings = VoyageAIEmbeddings(model="voyage-3")

pip install -qU langchain-ibm

if not os.environ.get("WATSONX_APIKEY"):
os.environ["WATSONX_APIKEY"] = getpass.getpass("Enter API key for IBM watsonx: ")

from langchain_ibm import WatsonxEmbeddings

embeddings = WatsonxEmbeddings(
model_id="ibm/slate-125m-english-rtrvr",
url="https://us-south.ml.cloud.ibm.com",

)

pip install -qU langchain-core

from langchain_core.embeddings import DeterministicFakeEmbedding

embeddings = DeterministicFakeEmbedding(size=4096)

pip install -qU langchain-isaacus

if not os.environ.get("ISAACUS_API_KEY"):
os.environ["ISAACUS_API_KEY"] = getpass.getpass("Enter API key for Isaacus: ")

from langchain_isaacus import IsaacusEmbeddings

embeddings = IsaacusEmbeddings(model="kanon-2-embedder")

vector_1 = embeddings.embed_query(all_splits[0].page_content)
vector_2 = embeddings.embed_query(all_splits[1].page_content)

assert len(vector_1) == len(vector_2)
print(f"Generated vectors of length {len(vector_1)}\n")
print(vector_1[:10])

Generated vectors of length 1536

[-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946]

Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.

## ​ 3\. Vector stores

LangChain VectorStore objects contain methods for adding text and `Document` objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let’s select a vector store:

- In-memory

- Amazon OpenSearch

- AstraDB

- Chroma

- FAISS

- Milvus

- MongoDB

- PGVector

- PGVectorStore

- Pinecone

- Qdrant

pip install -U "langchain-core"

from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)

pip install -qU boto3

from opensearchpy import RequestsHttpConnection

service = "es" # must set the service as 'es'
region = "us-east-2"
credentials = boto3.Session(
aws_access_key_id="xxxxxx", aws_secret_access_key="xxxxx"
).get_credentials()
awsauth = AWS4Auth("xxxxx", "xxxxxx", region, service, session_token=credentials.token)

vector_store = OpenSearchVectorSearch.from_documents(
docs,
embeddings,
opensearch_url="host url",
http_auth=awsauth,
timeout=300,
use_ssl=True,
verify_certs=True,
connection_class=RequestsHttpConnection,
index_name="test-index",
)

pip install -U "langchain-astradb"

from langchain_astradb import AstraDBVectorStore

vector_store = AstraDBVectorStore(
embedding=embeddings,
api_endpoint=ASTRA_DB_API_ENDPOINT,
collection_name="astra_vector_langchain",
token=ASTRA_DB_APPLICATION_TOKEN,
namespace=ASTRA_DB_NAMESPACE,
)

pip install -qU langchain-chroma

from langchain_chroma import Chroma

vector_store = Chroma(
collection_name="example_collection",
embedding_function=embeddings,
persist_directory="./chroma_langchain_db", # Where to save data locally, remove if not necessary
)

pip install -qU langchain-community faiss-cpu

import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS

embedding_dim = len(embeddings.embed_query("hello world"))
index = faiss.IndexFlatL2(embedding_dim)

vector_store = FAISS(
embedding_function=embeddings,
index=index,
docstore=InMemoryDocstore(),
index_to_docstore_id={},
)

pip install -qU langchain-milvus

from langchain_milvus import Milvus

URI = "./milvus_example.db"

vector_store = Milvus(
embedding_function=embeddings,
connection_args={"uri": URI},
index_params={"index_type": "FLAT", "metric_type": "L2"},
)

pip install -qU langchain-mongodb

from langchain_mongodb import MongoDBAtlasVectorSearch

vector_store = MongoDBAtlasVectorSearch(
embedding=embeddings,
collection=MONGODB_COLLECTION,
index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,
relevance_score_fn="cosine",
)

pip install -qU langchain-postgres

from langchain_postgres import PGVector

vector_store = PGVector(
embeddings=embeddings,
collection_name="my_docs",
connection="postgresql+psycopg://...",
)

from langchain_postgres import PGEngine, PGVectorStore

pg_engine = PGEngine.from_connection_string(
url="postgresql+psycopg://..."
)

vector_store = PGVectorStore.create_sync(
engine=pg_engine,
table_name='test_table',
embedding_service=embedding
)

pip install -qU langchain-pinecone

from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

pc = Pinecone(api_key=...)
index = pc.Index(index_name)

vector_store = PineconeVectorStore(embedding=embeddings, index=index)

pip install -qU langchain-qdrant

from qdrant_client.models import Distance, VectorParams
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient

client = QdrantClient(":memory:")

vector_size = len(embeddings.embed_query("sample text"))

if not client.collection_exists("test"):
client.create_collection(
collection_name="test",
vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
)
vector_store = QdrantVectorStore(
client=client,
collection_name="test",
embedding=embeddings,
)

Having instantiated our vector store, we can now index the documents.

ids = vector_store.add_documents(documents=all_splits)

Note that most vector store implementations will allow you to connect to an existing vector store— e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail.Once we’ve instantiated a `VectorStore` that contains documents, we can query it. VectorStore includes methods for querying:

- Synchronously and asynchronously;
- By string query and by vector;
- With and without returning similarity scores;
- By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results).

The methods will generally include a list of Document objects in their outputs.**Usage**Embeddings typically represent text as a “dense” vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.Return documents based on similarity to a string query:

results = vector_store.similarity_search(
"How many distribution centers does Nike have in the US?"
)

print(results[0])

page_content='direct to consumer operations sell products through the following number of retail stores in the United States:
U.S. RETAIL STORES NUMBER
NIKE Brand factory stores 213
NIKE Brand in-line stores (including employee-only stores) 74
Converse stores (including factory stores) 82
TOTAL 369
In the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.
2023 FORM 10-K 2' metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}

Async query:

results = await vector_store.asimilarity_search("When was Nike incorporated?")

page_content='Table of Contents
PART I
ITEM 1. BUSINESS
GENERAL
NIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"
"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.
Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is
the largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores
and sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}

Return scores:

# Note that providers implement different scores; the score here
# is a distance metric that varies inversely with similarity.

results = vector_store.similarity_search_with_score("What was Nike's revenue in 2023?")
doc, score = results[0]
print(f"Score: {score}\n")
print(doc)

Score: 0.23699893057346344

page_content='Table of Contents
FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS
The following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:
FISCAL 2023 COMPARED TO FISCAL 2022
•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.
The increase was due to higher revenues in North America, Europe, Middle East & Africa ("EMEA"), APLA and Greater China, which contributed approximately 7, 6,
2 and 1 percentage points to NIKE, Inc. Revenues, respectively.
•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This
increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale
equivalent basis.' metadata={'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}

Return documents based on similarity to an embedded query:

embedding = embeddings.embed_query("How were Nike's margins impacted in 2023?")

results = vector_store.similarity_search_by_vector(embedding)
print(results[0])

page_content='Table of Contents
GROSS MARGIN
FISCAL 2023 COMPARED TO FISCAL 2022
For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to
43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:
*Wholesale equivalent
The decrease in gross margin for fiscal 2023 was primarily due to:
•Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as
product mix;
•Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in
the prior period resulting from lower available inventory supply;
•Unfavorable changes in net foreign currency exchange rates, including hedges; and
•Lower off-price margin, on a wholesale equivalent basis.
This was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}

Learn more:

- API Reference
- Integration-specific docs

## ​ 4\. Retrievers

LangChain `VectorStore` objects do not subclass Runnable. LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous `invoke` and `batch` operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).We can create a simple version of this ourselves, without subclassing `Retriever`. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the `similarity_search` method:

from typing import List

from langchain_core.documents import Document
from langchain_core.runnables import chain

@chain

return vector_store.similarity_search(query, k=1)

retriever.batch(
[\
"How many distribution centers does Nike have in the US?",\
"When was Nike incorporated?",\
],
)

[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213 \nNIKE Brand in-line stores (including employee-only stores) 74 \nConverse stores (including factory stores) 82 \nTOTAL 369 \nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2')],\
[Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"\n"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales')]]

Vectorstores implement an `as_retriever` method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific `search_type` and `search_kwargs` attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:

retriever = vector_store.as_retriever(
search_type="similarity",
search_kwargs={"k": 1},
)

`VectorStoreRetriever` supports search types of `"similarity"` (default), `"mmr"` (maximum marginal relevance, described above), and `"similarity_score_threshold"`. We can use the latter to threshold documents output by the retriever by similarity score.Retrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial.

## ​ Next steps

You’ve now seen how to build a semantic search engine over a PDF document.For more on document loaders:

- Overview
- Available integrations

For more on embeddings:

For more on vector stores:

For more on RAG, see:

- Build a Retrieval Augmented Generation (RAG) App

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Learn\\
\\
Previous Build a RAG agent with LangChain\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/sql-agent

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangChain

Build a SQL agent

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Overview
- Concepts
- Setup
- Installation
- LangSmith
- 1\. Select an LLM
- 2\. Configure the database
- 3\. Add tools for database interactions
- 4\. Use create\_agent
- 5\. Run the agent
- (Optional) Use Studio
- 6\. Implement human-in-the-loop review
- Next steps

## ​ Overview

In this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain agents.At a high level, the agent will:

1

Fetch the available tables and schemas from the database

2

Decide which tables are relevant to the question

3

Fetch the schemas for the relevant tables

4

Generate a query based on the question and information from the schemas

5

Double-check the query for common mistakes using an LLM

6

Execute the query and return the results

7

Correct mistakes surfaced by the database engine until the query is successful

8

Formulate a response based on the results

Building Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent’s needs. This will mitigate, though not eliminate, the risks of building a model-driven system.

### ​ Concepts

We will cover the following concepts:

- Tools for reading from SQL databases
- LangChain agents
- Human-in-the-loop processes

## ​ Setup

### ​ Installation

pip

Copy

pip install langchain langgraph langchain-community

### ​ LangSmith

Set up LangSmith to inspect what is happening inside your chain or agent. Then set the following environment variables:

export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

## ​ 1\. Select an LLM

Select a model that supports tool-calling:

- OpenAI

- Anthropic

- Azure

- Google Gemini

- AWS Bedrock

- HuggingFace

👉 Read the OpenAI chat model integration docs

pip install -U "langchain[openai]"

init\_chat\_model

Model Class

import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")

👉 Read the Anthropic chat model integration docs

pip install -U "langchain[anthropic]"

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")

👉 Read the Azure chat model integration docs

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
"azure_openai:gpt-4.1",
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)

👉 Read the Google GenAI chat model integration docs

pip install -U "langchain[google-genai]"

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")

👉 Read the AWS Bedrock chat model integration docs

pip install -U "langchain[aws]"

from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
#

model = init_chat_model(
"anthropic.claude-3-5-sonnet-20240620-v1:0",
model_provider="bedrock_converse",
)

👉 Read the HuggingFace chat model integration docs

pip install -U "langchain[huggingface]"

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

model = init_chat_model(
"microsoft/Phi-3-mini-4k-instruct",
model_provider="huggingface",
temperature=0.7,
max_tokens=1024,
)

The output shown in the examples below used OpenAI.

## ​ 2\. Configure the database

You will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the `chinook` database, which is a sample database that represents a digital media store.For convenience, we have hosted the database (`Chinook.db`) on a public GCS bucket.

import requests, pathlib

url = "https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db"
local_path = pathlib.Path("Chinook.db")

if local_path.exists():
print(f"{local_path} already exists, skipping download.")
else:
response = requests.get(url)
if response.status_code == 200:
local_path.write_bytes(response.content)
print(f"File downloaded and saved as {local_path}")
else:
print(f"Failed to download the file. Status code: {response.status_code}")

We will use a handy SQL database wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:

from langchain_community.utilities import SQLDatabase

db = SQLDatabase.from_uri("sqlite:///Chinook.db")

print(f"Dialect: {db.dialect}")
print(f"Available tables: {db.get_usable_table_names()}")
print(f'Sample output: {db.run("SELECT * FROM Artist LIMIT 5;")}')

Dialect: sqlite
Available tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']
Sample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]

## ​ 3\. Add tools for database interactions

Use the `SQLDatabase` wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:

from langchain_community.agent_toolkits import SQLDatabaseToolkit

toolkit = SQLDatabaseToolkit(db=db, llm=model)

tools = toolkit.get_tools()

for tool in tools:
print(f"{tool.name}: {tool.description}\n")

sql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.

sql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3

sql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.

sql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!

## ​ 4\. Use `create_agent`

Use `create_agent` to build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command, which the tools will execute. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.Initialize the agent with a descriptive system prompt to customize its behavior:

system_prompt = """
You are an agent designed to interact with a SQL database.
Given an input question, create a syntactically correct {dialect} query to run,
then look at the results of the query and return the answer. Unless the user
specifies a specific number of examples they wish to obtain, always limit your
query to at most {top_k} results.

You can order the results by a relevant column to return the most interesting
examples in the database. Never query for all the columns from a specific table,
only ask for the relevant columns given the question.

You MUST double check your query before executing it. If you get an error while
executing a query, rewrite the query and try again.

DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the
database.

To start you should ALWAYS look at the tables in the database to see what you
can query. Do NOT skip this step.

Then you should query the schema of the most relevant tables.
""".format(
dialect=db.dialect,
top_k=5,
)

Now, create an agent with the model, tools, and prompt:

from langchain.agents import create_agent

agent = create_agent(
model,
tools,
system_prompt=system_prompt,
)

## ​ 5\. Run the agent

Run the agent on a sample query and observe its behavior:

question = "Which genre on average has the longest tracks?"

for step in agent.stream(
{"messages": [{"role": "user", "content": question}]},
stream_mode="values",
):
step["messages"][-1].pretty_print()

================================ Human Message =================================

Which genre on average has the longest tracks?
================================== Ai Message ==================================
Tool Calls:
sql_db_list_tables (call_BQsWg8P65apHc8BTJ1NPDvnM)
Call ID: call_BQsWg8P65apHc8BTJ1NPDvnM
Args:
================================= Tool Message =================================
Name: sql_db_list_tables

Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track
================================== Ai Message ==================================
Tool Calls:
sql_db_schema (call_i89tjKECFSeERbuACYm4w0cU)
Call ID: call_i89tjKECFSeERbuACYm4w0cU
Args:
table_names: Track, Genre
================================= Tool Message =================================
Name: sql_db_schema

CREATE TABLE "Genre" (
"GenreId" INTEGER NOT NULL,
"Name" NVARCHAR(120),
PRIMARY KEY ("GenreId")
)

/*
3 rows from Genre table:
GenreId	Name
1	Rock
2	Jazz
3	Metal
*/

CREATE TABLE "Track" (
"TrackId" INTEGER NOT NULL,
"Name" NVARCHAR(200) NOT NULL,
"AlbumId" INTEGER,
"MediaTypeId" INTEGER NOT NULL,
"GenreId" INTEGER,
"Composer" NVARCHAR(220),
"Milliseconds" INTEGER NOT NULL,
"Bytes" INTEGER,
"UnitPrice" NUMERIC(10, 2) NOT NULL,
PRIMARY KEY ("TrackId"),
FOREIGN KEY("MediaTypeId") REFERENCES "MediaType" ("MediaTypeId"),
FOREIGN KEY("GenreId") REFERENCES "Genre" ("GenreId"),
FOREIGN KEY("AlbumId") REFERENCES "Album" ("AlbumId")
)

/*
3 rows from Track table:
TrackId	Name	AlbumId	MediaTypeId	GenreId	Composer	Milliseconds	Bytes	UnitPrice
1	For Those About To Rock (We Salute You)	1	1	1	Angus Young, Malcolm Young, Brian Johnson	343719	11170334	0.99
2	Balls to the Wall	2	2	1	U. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann	342562	5510424	0.99
3	Fast As a Shark	3	2	1	F. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman	230619	3990994	0.99
*/
================================== Ai Message ==================================
Tool Calls:
sql_db_query_checker (call_G64yYm6R6UauiVPCXJZMA49b)
Call ID: call_G64yYm6R6UauiVPCXJZMA49b
Args:
query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;
================================= Tool Message =================================
Name: sql_db_query_checker

SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;
================================== Ai Message ==================================
Tool Calls:
sql_db_query (call_AnO3SrhD0ODJBxh6dHMwvHwZ)
Call ID: call_AnO3SrhD0ODJBxh6dHMwvHwZ
Args:
query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5;
================================= Tool Message =================================
Name: sql_db_query

[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]
================================== Ai Message ==================================

On average, the genre with the longest tracks is "Sci Fi & Fantasy" with an average track length of approximately 2,911,783 milliseconds. This is followed by "Science Fiction," "Drama," "TV Shows," and "Comedy."

The agent correctly wrote a query, checked the query, and ran it to inform its final response.

You can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the LangSmith trace.

### ​ (Optional) Use Studio

Studio provides a “client side” loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like “Tell me the scheme of the database” or “Show me the invoices for the 5 top customers”. You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.

Run your agent in Studio

In addition to the previously mentioned packages, you will need to:

In directory you will run in, you will need a `langgraph.json` file with the following contents:

{
"dependencies": ["."],
"graphs": {
"agent": "./sql_agent.py:agent",
"graph": "./sql_agent_langgraph.py:graph"
},
"env": ".env"
}

Create a file `sql_agent.py` and insert this:

#sql_agent.py for studio
import pathlib

from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain_community.agent_toolkits import SQLDatabaseToolkit
from langchain_community.utilities import SQLDatabase
import requests

# Initialize an LLM

# Get the database, store it locally

# Create the tools

# Use create_agent

## ​ 6\. Implement human-in-the-loop review

It can be prudent to check the agent’s SQL queries before they are executed for any unintended actions or inefficiencies.LangChain agents feature support for built-in human-in-the-loop middleware to add oversight to agent tool calls. Let’s configure the agent to pause for human review on calling the `sql_db_query` tool:

from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
model,
tools,
system_prompt=system_prompt,
middleware=[\
HumanInTheLoopMiddleware(\
interrupt_on={"sql_db_query": True},\
description_prefix="Tool execution pending approval",\
),\
],
checkpointer=InMemorySaver(),
)

We’ve added a checkpointer to our agent to allow execution to be paused and resumed. See the human-in-the-loop guide for detalis on this as well as available middleware configurations.

On running the agent, it will now pause for review before executing the `sql_db_query` tool:

question = "Which genre on average has the longest tracks?"
config = {"configurable": {"thread_id": "1"}}

for step in agent.stream(
{"messages": [{"role": "user", "content": question}]},
config,
stream_mode="values",
):
if "__interrupt__" in step:
print("INTERRUPTED:")
interrupt = step["__interrupt__"][0]
for request in interrupt.value["action_requests"]:
print(request["description"])
elif "messages" in step:
step["messages"][-1].pretty_print()
else:
pass

...

INTERRUPTED:
Tool execution pending approval

Tool: sql_db_query
Args: {'query': 'SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name ORDER BY AvgTrackLength DESC LIMIT 1;'}

We can resume execution, in this case accepting the query, using Command:

from langgraph.types import Command

for step in agent.stream(
Command(resume={"decisions": [{"type": "approve"}]}),
config,
stream_mode="values",
):
if "messages" in step:
step["messages"][-1].pretty_print()
elif "__interrupt__" in step:
print("INTERRUPTED:")
interrupt = step["__interrupt__"][0]
for request in interrupt.value["action_requests"]:
print(request["description"])
else:
pass

================================== Ai Message ==================================
Tool Calls:
sql_db_query (call_7oz86Epg7lYRqi9rQHbZPS1U)
Call ID: call_7oz86Epg7lYRqi9rQHbZPS1U
Args:
query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgDuration FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgDuration DESC LIMIT 5;
================================= Tool Message =================================
Name: sql_db_query

The genre with the longest average track length is "Sci Fi & Fantasy" with an average duration of about 2,911,783 milliseconds, followed by "Science Fiction" and "Drama."

Refer to the human-in-the-loop guide for details.

## ​ Next steps

For deeper customization, check out this tutorial for implementing a SQL agent directly using LangGraph primitives.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a RAG agent with LangChain\\
\\
Previous Build a voice agent with LangChain\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/voice-agent

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangChain

Build a voice agent with LangChain

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Overview
- What are voice agents?
- How do voice agents work?

- 2\. Speech-to-Speech Architecture (S2S)
- Demo Application Overview
- Architecture
- Setup
- 1\. Speech-to-text
- Key Concepts
- Implementation
- 2\. LangChain agent
- Key Concepts
- Implementation
- 3\. Text-to-speech
- Key Concepts
- Implementation
- Putting It All Together

## ​ Overview

Chat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.Voice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.

### ​ What are voice agents?

Voice agents are agents that can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.They’re suited for a variety of use cases, including:

- Customer support
- Personal assistants
- Hands-free interfaces
- Coaching and training

### ​ How do voice agents work?

At a high level, every voice agent needs to handle three tasks:

1. **Listen** \- capture audio and transcribe it
2. **Think** \- interpret intent, reason, plan

The Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).

User Audio

Speech-to-Text

LangChain Agent

Text-to-Speech

Audio Output

**Pros:**

- Full control over each component (swap STT/TTS providers as needed)
- Access to latest capabilities from modern text-modality models
- Transparent behavior with clear boundaries between components

**Cons:**

- Requires orchestrating multiple services
- Additional complexity in managing the pipeline
- Conversion from speech to text loses information (e.g., tone, emotion)

#### ​ 2\. Speech-to-Speech Architecture (S2S)

Speech-to-speech uses a multimodal model that processes audio input and generates audio output natively.

Multimodal Model

- Simpler architecture with fewer moving parts
- Typically lower latency for simple interactions
- Direct audio processing captures tone and other nuances of speech

- Limited model options, greater risk of provider lock-in
- Features may lag behind text-modality models
- Less transparency in how audio is processed
- Reduced controllability and customization options

This guide demonstrates the **sandwich architecture** to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.

### ​ Demo Application Overview

We’ll walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using AssemblyAI for STT and Cartesia for TTS (although adapters can be built for most providers).An end-to-end reference application is available in the voice-sandwich-demo repository. We will walk through that application here.The demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.

### ​ Architecture

The demo implements a streaming pipeline where each stage processes data asynchronously:**Client (Browser)**

- Captures microphone audio and encodes it as PCM
- Establishes WebSocket connection to the backend server
- Streams audio chunks to the server in real-time
- Receives and plays back synthesized speech audio

**Server (Python)**

- Accepts WebSocket connections from clients
- Orchestrates the three-step pipeline: - Speech-to-text (STT): Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events
- Agent: Processes transcripts with LangChain agent, streams response tokens
- Text-to-speech (TTS): Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks
- Returns synthesized audio to the client for playback

The pipeline uses async generators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.

## ​ Setup

For detailed installation instructions and setup, see the repository README.

## ​ 1\. Speech-to-text

The STT stage transforms an incoming audio stream into text transcripts. The implementation uses a producer-consumer pattern to handle audio streaming and transcript reception concurrently.

### ​ Key Concepts

**Producer-Consumer Pattern**: Audio chunks are sent to the STT service concurrently with receiving transcript events. This allows transcription to begin before all audio has arrived.**Event Types**:

- `stt_chunk`: Partial transcripts provided as the STT service processes audio
- `stt_output`: Final, formatted transcripts that trigger agent processing

**WebSocket Connection**: Maintains a persistent connection to AssemblyAI’s real-time STT API, configured for 16kHz PCM audio with automatic turn formatting.

### ​ Implementation

Copy

from typing import AsyncIterator
import asyncio
from assemblyai_stt import AssemblyAISTT
from events import VoiceAgentEvent

async def stt_stream(
audio_stream: AsyncIterator[bytes],

"""
Transform stream: Audio (Bytes) → Voice Events (VoiceAgentEvent)

Uses a producer-consumer pattern where:
- Producer: Reads audio chunks and sends them to AssemblyAI
- Consumer: Receives transcription events from AssemblyAI
"""
stt = AssemblyAISTT(sample_rate=16000)

async def send_audio():
"""Background task that pumps audio chunks to AssemblyAI."""
try:
async for audio_chunk in audio_stream:
await stt.send_audio(audio_chunk)
finally:
# Signal completion when audio stream ends
await stt.close()

# Launch audio sending in background
send_task = asyncio.create_task(send_audio())

try:
# Receive and yield transcription events as they arrive
async for event in stt.receive_events():
yield event
finally:
# Cleanup
with contextlib.suppress(asyncio.CancelledError):
send_task.cancel()
await send_task
await stt.close()

The application implements an AssemblyAI client to manage the WebSocket connection and message parsing. See below for implementations; similar adapters can be constructed for other STT providers.

AssemblyAI Client

class AssemblyAISTT:
def __init__(self, api_key: str | None = None, sample_rate: int = 16000):
self.api_key = api_key or os.getenv("ASSEMBLYAI_API_KEY")
self.sample_rate = sample_rate
self._ws: WebSocketClientProtocol | None = None

"""Send PCM audio bytes to AssemblyAI."""
ws = await self._ensure_connection()
await ws.send(audio_chunk)

"""Yield STT events as they arrive from AssemblyAI."""
async for raw_message in self._ws:
message = json.loads(raw_message)

if message["type"] == "Turn":
# Final formatted transcript
if message.get("turn_is_formatted"):
yield STTOutputEvent.create(message["transcript"])
# Partial transcript
else:
yield STTChunkEvent.create(message["transcript"])

"""Establish WebSocket connection if not already connected."""
if self._ws is None:
url = f"wss://streaming.assemblyai.com/v3/ws?sample_rate={self.sample_rate}&format_turns=true"
self._ws = await websockets.connect(
url,
additional_headers={"Authorization": self.api_key}
)
return self._ws

## ​ 2\. LangChain agent

The agent stage processes text transcripts through a LangChain agent and streams the response tokens. In this case, we stream all text content blocks generated by the agent.

### ​ Key Concepts

**Streaming Responses**: The agent uses `stream_mode="messages"` to emit response tokens as they’re generated, rather than waiting for the complete response. This enables the TTS stage to begin synthesis immediately.**Conversation Memory**: A checkpointer maintains conversation state across turns using a unique thread ID. This allows the agent to reference previous exchanges in the conversation.

### ​ Implementation

from uuid import uuid4
from langchain.agents import create_agent
from langchain.messages import HumanMessage
from langgraph.checkpoint.memory import InMemorySaver

# Define agent tools

"""Add an item to the customer's sandwich order."""
return f"Added {quantity} x {item} to the order."

"""Confirm the final order with the customer."""
return f"Order confirmed: {order_summary}. Sending to kitchen."

# Create agent with tools and memory
agent = create_agent(
model="anthropic:claude-haiku-4-5", # Select your model
tools=[add_to_order, confirm_order],
system_prompt="""You are a helpful sandwich shop assistant.
Your goal is to take the user's order. Be concise and friendly.
Do NOT use emojis, special characters, or markdown.
Your responses will be read by a text-to-speech engine.""",
checkpointer=InMemorySaver(),
)

async def agent_stream(
event_stream: AsyncIterator[VoiceAgentEvent],

"""
Transform stream: Voice Events → Voice Events (with Agent Responses)

Passes through all upstream events and adds agent_chunk events
when processing STT transcripts.
"""
# Generate unique thread ID for conversation memory
thread_id = str(uuid4())

async for event in event_stream:
# Pass through all upstream events
yield event

# Process final transcripts through the agent
if event.type == "stt_output":
# Stream agent response with conversation context
stream = agent.astream(
{"messages": [HumanMessage(content=event.transcript)]},
{"configurable": {"thread_id": thread_id}},
stream_mode="messages",
)

# Yield agent response chunks as they arrive
async for message, _ in stream:
if message.text:
yield AgentChunkEvent.create(message.text)

## ​ 3\. Text-to-speech

The TTS stage synthesizes agent response text into audio and streams it Key Concepts

**Concurrent Processing**: The implementation merges two async streams:

- **Upstream processing**: Passes through all events and sends agent text chunks to the TTS provider
- **Audio reception**: Receives synthesized audio chunks from the TTS provider

**Streaming TTS**: Some providers (such as Cartesia) begin synthesizing audio as soon as it receives text, enabling audio play Implementation

from cartesia_tts import CartesiaTTS
from utils import merge_async_iters

async def tts_stream(
event_stream: AsyncIterator[VoiceAgentEvent],

"""
Transform stream: Voice Events → Voice Events (with Audio)

Merges two concurrent streams:
1. process_upstream(): passes through events and sends text to Cartesia
2. tts.receive_events(): yields audio chunks from Cartesia
"""
tts = CartesiaTTS()

"""Process upstream events and send agent text to Cartesia."""
async for event in event_stream:
# Pass through all events
# Send agent text to Cartesia for synthesis
yield event
if event.type == "agent_chunk":
await tts.send_text(event.text)

# Merge upstream events with TTS audio events
# Both streams run concurrently
try:
async for event in merge_async_iters(
process_upstream(),
tts.receive_events()
):
yield event
finally:
await tts.close()

The application implements an Cartesia client to manage the WebSocket connection and audio streaming. See below for implementations; similar adapters can be constructed for other TTS providers.

Cartesia Client

import base64
import json
import websockets

class CartesiaTTS:
def __init__(
self,
api_key: Optional[str] = None,
voice_id: str = "f6ff7c0c-e396-40a9-a70b-f7607edb6937",
model_id: str = "sonic-3",
sample_rate: int = 24000,
encoding: str = "pcm_s16le",
):
self.api_key = api_key or os.getenv("CARTESIA_API_KEY")
self.voice_id = voice_id
self.model_id = model_id
self.sample_rate = sample_rate
self.encoding = encoding
self._ws: WebSocketClientProtocol | None = None

"""Generate a valid context_id for Cartesia."""
timestamp = int(time.time() * 1000)
counter = self._context_counter
self._context_counter += 1
return f"ctx_{timestamp}_{counter}"

"""Send text to Cartesia for synthesis."""
if not text or not text.strip():
return

ws = await self._ensure_connection()
payload = {
"model_id": self.model_id,
"transcript": text,
"voice": {
"mode": "id",
"id": self.voice_id,
},
"output_format": {
"container": "raw",
"encoding": self.encoding,
"sample_rate": self.sample_rate,
},
"language": self.language,
"context_id": self._generate_context_id(),
}
await ws.send(json.dumps(payload))

"""Yield audio chunks as they arrive from Cartesia."""
async for raw_message in self._ws:
message = json.loads(raw_message)

# Decode and yield audio chunks
if "data" in message and message["data"]:
audio_chunk = base64.b64decode(message["data"])
if audio_chunk:
yield TTSChunkEvent.create(audio_chunk)

"""Establish WebSocket connection if not already connected."""
if self._ws is None:
url = (
f"wss://api.cartesia.ai/tts/websocket"
f"?api_key={self.api_key}&cartesia_version={self.cartesia_version}"
)
self._ws = await websockets.connect(url)

return self._ws

## ​ Putting It All Together

The complete pipeline chains the three stages together:

from langchain_core.runnables import RunnableGenerator

pipeline = (
RunnableGenerator(stt_stream) # Audio → STT events
| RunnableGenerator(agent_stream) # STT events → Agent events
| RunnableGenerator(tts_stream) # Agent events → TTS audio
)

# Use in WebSocket endpoint
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
await websocket.accept()

async def websocket_audio_stream():
"""Yield audio bytes from WebSocket."""
while True:
data = await websocket.receive_bytes()
yield data

# Transform audio through pipeline
output_stream = pipeline.atransform(websocket_audio_stream())

# Send TTS audio to compose each step of the pipeline. This is an abstraction LangChain uses internally to manage streaming across components.Each stage processes events independently and concurrently: audio transcription begins as soon as audio arrives, the agent starts reasoning as soon as a transcript is available, and speech synthesis begins as soon as agent text is generated. This architecture can achieve sub-700ms latency to support natural conversation.For more on building agents with LangChain, see the Agents guide.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a SQL agent\\
\\
Previous Build a personal assistant with subagents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent/subagents-personal-assistant

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Multi-agent

Build a personal assistant with subagents

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Overview
- Why use a supervisor?
- Concepts
- Setup
- Installation
- LangSmith
- Components
- 1\. Define tools
- 2\. Create specialized sub-agents
- Create a calendar agent
- Create an email agent
- 3\. Wrap sub-agents as tools
- 4\. Create the supervisor agent
- 5\. Use the supervisor
- Example 1: Simple single-domain request
- Example 2: Complex multi-domain request
- Complete working example
- Understanding the architecture
- 6\. Add human-in-the-loop review
- 7\. Advanced: Control information flow
- Pass additional conversational context to sub-agents
- Control what supervisor receives
- 8\. Key takeaways
- Next steps

## ​ Overview

The **supervisor pattern** is a multi-agent architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow.In this tutorial, you’ll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:

- A **calendar agent** that handles scheduling, availability checking, and event management.
- An **email agent** that manages communication, drafts messages, and sends notifications.

We will also incorporate human-in-the-loop review to allow users to approve, edit, and reject actions (such as outbound emails) as desired.

### ​ Why use a supervisor?

Multi-agent architectures allow you to partition tools across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).

### ​ Concepts

We will cover the following concepts:

- Multi-agent systems
- Human-in-the-loop review

## ​ Setup

### ​ Installation

This tutorial requires the `langchain` package:

pip

conda

Copy

pip install langchain

For more details, see our Installation guide.

### ​ LangSmith

Set up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:

bash

python

export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

### ​ Components

We will need to select a chat model from LangChain’s suite of integrations:

- OpenAI

- Anthropic

- Azure

- Google Gemini

- AWS Bedrock

- HuggingFace

👉 Read the OpenAI chat model integration docs

pip install -U "langchain[openai]"

init\_chat\_model

Model Class

import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")

👉 Read the Anthropic chat model integration docs

pip install -U "langchain[anthropic]"

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")

👉 Read the Azure chat model integration docs

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
"azure_openai:gpt-4.1",
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)

👉 Read the Google GenAI chat model integration docs

pip install -U "langchain[google-genai]"

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")

👉 Read the AWS Bedrock chat model integration docs

pip install -U "langchain[aws]"

from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
#

model = init_chat_model(
"anthropic.claude-3-5-sonnet-20240620-v1:0",
model_provider="bedrock_converse",
)

👉 Read the HuggingFace chat model integration docs

pip install -U "langchain[huggingface]"

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

model = init_chat_model(
"microsoft/Phi-3-mini-4k-instruct",
model_provider="huggingface",
temperature=0.7,
max_tokens=1024,
)

## ​ 1\. Define tools

Start by defining the tools that require structured inputs. In real applications, these would call actual APIs (Google Calendar, SendGrid, etc.). For this tutorial, you’ll use stubs to demonstrate the pattern.

from langchain.tools import tool

@tool
def create_calendar_event(
title: str,
start_time: str, # ISO format: "2024-01-15T14:00:00"
end_time: str, # ISO format: "2024-01-15T15:00:00"
attendees: list[str], # email addresses
location: str = ""

"""Create a calendar event. Requires exact ISO datetime format."""
# Stub: In practice, this would call Google Calendar API, Outlook API, etc.
return f"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees"

@tool
def send_email(
to: list[str], # email addresses
subject: str,
body: str,
cc: list[str] = []

"""Send an email via email API. Requires properly formatted addresses."""
# Stub: In practice, this would call SendGrid, Gmail API, etc.
return f"Email sent to {', '.join(to)} - Subject: {subject}"

@tool
def get_available_time_slots(
attendees: list[str],
date: str, # ISO format: "2024-01-15"
duration_minutes: int

"""Check calendar availability for given attendees on a specific date."""
# Stub: In practice, this would query calendar APIs
return ["09:00", "14:00", "16:00"]

## ​ 2\. Create specialized sub-agents

Next, we’ll create specialized sub-agents that handle each domain.

### ​ Create a calendar agent

The calendar agent understands natural language scheduling requests and translates them into precise API calls. It handles date parsing, availability checking, and event creation.

from langchain.agents import create_agent

CALENDAR_AGENT_PROMPT = (
"You are a calendar scheduling assistant. "
"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') "
"into proper ISO datetime formats. "
"Use get_available_time_slots to check availability when needed. "
"Use create_calendar_event to schedule events. "
"Always confirm what was scheduled in your final response."
)

calendar_agent = create_agent(
model,
tools=[create_calendar_event, get_available_time_slots],
system_prompt=CALENDAR_AGENT_PROMPT,
)

Test the calendar agent to see how it handles natural language scheduling:

query = "Schedule a team meeting next Tuesday at 2pm for 1 hour"

for step in calendar_agent.stream(
{"messages": [{"role": "user", "content": query}]}
):
for update in step.values():
for message in update.get("messages", []):
message.pretty_print()

================================== Ai Message ==================================
Tool Calls:
get_available_time_slots (call_EIeoeIi1hE2VmwZSfHStGmXp)
Call ID: call_EIeoeIi1hE2VmwZSfHStGmXp
Args:
attendees: []
date: 2024-06-18
duration_minutes: 60
================================= Tool Message =================================
Name: get_available_time_slots

["09:00", "14:00", "16:00"]
================================== Ai Message ==================================
Tool Calls:
create_calendar_event (call_zgx3iJA66Ut0W8S3NpT93kEB)
Call ID: call_zgx3iJA66Ut0W8S3NpT93kEB
Args:
title: Team Meeting
start_time: 2024-06-18T14:00:00
end_time: 2024-06-18T15:00:00
attendees: []
================================= Tool Message =================================
Name: create_calendar_event

Event created: Team Meeting from 2024-06-18T14:00:00 to 2024-06-18T15:00:00 with 0 attendees
================================== Ai Message ==================================

The team meeting has been scheduled for next Tuesday, June 18th, at 2:00 PM and will last for 1 hour. If you need to add attendees or a location, please let me know!

The agent parses “next Tuesday at 2pm” into ISO format (“2024-01-16T14:00:00”), calculates the end time, calls `create_calendar_event`, and returns a natural language confirmation.

### ​ Create an email agent

The email agent handles message composition and sending. It focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication.

EMAIL_AGENT_PROMPT = (
"You are an email assistant. "
"Compose professional emails based on natural language requests. "
"Extract recipient information and craft appropriate subject lines and body text. "
"Use send_email to send the message. "
"Always confirm what was sent in your final response."
)

email_agent = create_agent(
model,
tools=[send_email],
system_prompt=EMAIL_AGENT_PROMPT,
)

Test the email agent with a natural language request:

query = "Send the design team a reminder about reviewing the new mockups"

for step in email_agent.stream(
{"messages": [{"role": "user", "content": query}]}
):
for update in step.values():
for message in update.get("messages", []):
message.pretty_print()

================================== Ai Message ==================================
Tool Calls:
send_email (call_OMl51FziTVY6CRZvzYfjYOZr)
Call ID: call_OMl51FziTVY6CRZvzYfjYOZr
Args:
to: ['design-team@example.com']
subject: Reminder: Please Review the New Mockups
body: Hi Design Team,

This is a friendly reminder to review the new mockups at your earliest convenience. Your feedback is important to ensure that we stay on track with our project timeline.

Please let me know if you have any questions or need additional information.

Thank you!

Best regards,
================================= Tool Message =================================
Name: send_email

Email sent to design-team@example.com - Subject: Reminder: Please Review the New Mockups
================================== Ai Message ==================================

I've sent a reminder to the design team asking them to review the new mockups. If you need any further communication on this topic, just let me know!

The agent infers the recipient from the informal request, crafts a professional subject line and body, calls `send_email`, and returns a confirmation. Each sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task.

## ​ 3\. Wrap sub-agents as tools

Now wrap each sub-agent as a tool that the supervisor can invoke. This is the key architectural step that creates the layered system. The supervisor will see high-level tools like “schedule\_event”, not low-level tools like “create\_calendar\_event”.

@tool

"""Schedule calendar events using natural language.

Use this when the user wants to create, modify, or check calendar appointments.
Handles date/time parsing, availability checking, and event creation.

Input: Natural language scheduling request (e.g., 'meeting with design team
next Tuesday at 2pm')
"""
result = calendar_agent.invoke({
"messages": [{"role": "user", "content": request}]
})
return result["messages"][-1].text

"""Send emails using natural language.

Use this when the user wants to send notifications, reminders, or any email
communication. Handles recipient extraction, subject generation, and email
composition.

Input: Natural language email request (e.g., 'send them a reminder about
the meeting')
"""
result = email_agent.invoke({
"messages": [{"role": "user", "content": request}]
})
return result["messages"][-1].text

The tool descriptions help the supervisor decide when to use each tool, so make them clear and specific. We return only the sub-agent’s final response, as the supervisor doesn’t need to see intermediate reasoning or tool calls.

## ​ 4\. Create the supervisor agent

Now create the supervisor that orchestrates the sub-agents. The supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level.

SUPERVISOR_PROMPT = (
"You are a helpful personal assistant. "
"You can schedule calendar events and send emails. "
"Break down user requests into appropriate tool calls and coordinate the results. "
"When a request involves multiple actions, use multiple tools in sequence."
)

supervisor_agent = create_agent(
model,
tools=[schedule_event, manage_email],
system_prompt=SUPERVISOR_PROMPT,
)

## ​ 5\. Use the supervisor

Now test your complete system with complex requests that require coordination across multiple domains:

### ​ Example 1: Simple single-domain request

query = "Schedule a team standup for tomorrow at 9am"

for step in supervisor_agent.stream(
{"messages": [{"role": "user", "content": query}]}
):
for update in step.values():
for message in update.get("messages", []):
message.pretty_print()

================================== Ai Message ==================================
Tool Calls:
schedule_event (call_mXFJJDU8bKZadNUZPaag8Lct)
Call ID: call_mXFJJDU8bKZadNUZPaag8Lct
Args:
request: Schedule a team standup for tomorrow at 9am with Alice and Bob.
================================= Tool Message =================================
Name: schedule_event

The team standup has been scheduled for tomorrow at 9:00 AM with Alice and Bob. If you need to make any changes or add more details, just let me know!
================================== Ai Message ==================================

The team standup with Alice and Bob is scheduled for tomorrow at 9:00 AM. If you need any further arrangements or adjustments, please let me know!

The supervisor identifies this as a calendar task, calls `schedule_event`, and the calendar agent handles date parsing and event creation.

For full transparency into the information flow, including prompts and responses for each chat model call, check out the LangSmith trace for the above run.

### ​ Example 2: Complex multi-domain request

query = (
"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, "
"and send them an email reminder about reviewing the new mockups."
)

================================== Ai Message ==================================
Tool Calls:
schedule_event (call_YA68mqF0koZItCFPx0kGQfZi)
Call ID: call_YA68mqF0koZItCFPx0kGQfZi
Args:
request: meeting with the design team next Tuesday at 2pm for 1 hour
manage_email (call_XxqcJBvVIuKuRK794ZIzlLxx)
Call ID: call_XxqcJBvVIuKuRK794ZIzlLxx
Args:
request: send the design team an email reminder about reviewing the new mockups
================================= Tool Message =================================
Name: schedule_event

Your meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm. Let me know if you need to add more details or make any changes!
================================= Tool Message =================================
Name: manage_email

I've sent an email reminder to the design team requesting them to review the new mockups. If you need to include more information or recipients, just let me know!
================================== Ai Message ==================================

Your meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm.

I've also sent an email reminder to the design team, asking them to review the new mockups.

Let me know if you'd like to add more details to the meeting or include additional information in the email!

The supervisor recognizes this requires both calendar and email actions, calls `schedule_event` for the meeting, then calls `manage_email` for the reminder. Each sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.

Refer to the LangSmith trace to see the detailed information flow for the above run, including individual chat model prompts and responses.

### ​ Complete working example

Here’s everything together in a runnable script:

Show View complete code

"""
Personal Assistant Supervisor Example

This example demonstrates the tool calling pattern for multi-agent systems.
A supervisor agent coordinates specialized sub-agents (calendar and email)
that are wrapped as tools.
"""

from langchain.tools import tool
from langchain.agents import create_agent
from langchain.chat_models import init_chat_model

# ============================================================================
# Step 1: Define low-level API tools (stubbed)

"""Create a calendar event. Requires exact ISO datetime format."""
return f"Event created: {title} from {start_time} to {end_time} with {len(attendees)} attendees"

"""Send an email via email API. Requires properly formatted addresses."""
return f"Email sent to {', '.join(to)} - Subject: {subject}"

"""Check calendar availability for given attendees on a specific date."""
return ["09:00", "14:00", "16:00"]

# Step 2: Create specialized sub-agents

model = init_chat_model("claude-haiku-4-5-20251001") # for example

calendar_agent = create_agent(
model,
tools=[create_calendar_event, get_available_time_slots],
system_prompt=(
"You are a calendar scheduling assistant. "
"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') "
"into proper ISO datetime formats. "
"Use get_available_time_slots to check availability when needed. "
"Use create_calendar_event to schedule events. "
"Always confirm what was scheduled in your final response."
)
)

email_agent = create_agent(
model,
tools=[send_email],
system_prompt=(
"You are an email assistant. "
"Compose professional emails based on natural language requests. "
"Extract recipient information and craft appropriate subject lines and body text. "
"Use send_email to send the message. "
"Always confirm what was sent in your final response."
)
)

# Step 3: Wrap sub-agents as tools for the supervisor

# Step 4: Create the supervisor agent

supervisor_agent = create_agent(
model,
tools=[schedule_event, manage_email],
system_prompt=(
"You are a helpful personal assistant. "
"You can schedule calendar events and send emails. "
"Break down user requests into appropriate tool calls and coordinate the results. "
"When a request involves multiple actions, use multiple tools in sequence."
)
)

# Step 5: Use the supervisor

if __name__ == "__main__":
# Example: User request requiring both calendar and email coordination
user_request = (
"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, "
"and send them an email reminder about reviewing the new mockups."
)

print("User Request:", user_request)
print("\n" + "="*80 + "\n")

for step in supervisor_agent.stream(
{"messages": [{"role": "user", "content": user_request}]}
):
for update in step.values():
for message in update.get("messages", []):
message.pretty_print()

### ​ Understanding the architecture

Your system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results.This separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.

## ​ 6\. Add human-in-the-loop review

It can be prudent to incorporate human-in-the-loop review of sensitive actions. LangChain includes built-in middleware to review tool calls, in this case the tools invoked by sub-agents.Let’s add human-in-the-loop review to both sub-agents:

- We configure the `create_calendar_event` and `send_email` tools to interrupt, permitting all response types (`approve`, `edit`, `reject`)
- We add a checkpointer **only to the top-level agent**. This is required to pause and resume execution.

from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver

calendar_agent = create_agent(
model,
tools=[create_calendar_event, get_available_time_slots],
system_prompt=CALENDAR_AGENT_PROMPT,
middleware=[\
HumanInTheLoopMiddleware(\
interrupt_on={"create_calendar_event": True},\
description_prefix="Calendar event pending approval",\
),\
],
)

email_agent = create_agent(
model,
tools=[send_email],
system_prompt=EMAIL_AGENT_PROMPT,
middleware=[\
HumanInTheLoopMiddleware(\
interrupt_on={"send_email": True},\
description_prefix="Outbound email pending approval",\
),\
],
)

supervisor_agent = create_agent(
model,
tools=[schedule_event, manage_email],
system_prompt=SUPERVISOR_PROMPT,
checkpointer=InMemorySaver(),
)

Let’s repeat the query. Note that we gather interrupt events into a list to access downstream:

config = {"configurable": {"thread_id": "6"}}

interrupts = []
for step in supervisor_agent.stream(
{"messages": [{"role": "user", "content": query}]},
config,
):
for update in step.values():
if isinstance(update, dict):
for message in update.get("messages", []):
message.pretty_print()
else:
interrupt_ = update[0]
interrupts.append(interrupt_)
print(f"\nINTERRUPTED: {interrupt_.id}")

================================== Ai Message ==================================
Tool Calls:
schedule_event (call_t4Wyn32ohaShpEZKuzZbl83z)
Call ID: call_t4Wyn32ohaShpEZKuzZbl83z
Args:
request: Schedule a meeting with the design team next Tuesday at 2pm for 1 hour.
manage_email (call_JWj4vDJ5VMnvkySymhCBm4IR)
Call ID: call_JWj4vDJ5VMnvkySymhCBm4IR
Args:
request: Send an email reminder to the design team about reviewing the new mockups before our meeting next Tuesday at 2pm.

INTERRUPTED: 4f994c9721682a292af303ec1a46abb7

INTERRUPTED: 2b56f299be313ad8bc689eff02973f16

This time we’ve interrupted execution. Let’s inspect the interrupt events:

for interrupt_ in interrupts:
for request in interrupt_.value["action_requests"]:
print(f"INTERRUPTED: {interrupt_.id}")
print(f"{request['description']}\n")

INTERRUPTED: 4f994c9721682a292af303ec1a46abb7
Calendar event pending approval

Tool: create_calendar_event
Args: {'title': 'Meeting with the Design Team', 'start_time': '2024-06-18T14:00:00', 'end_time': '2024-06-18T15:00:00', 'attendees': ['design team']}

INTERRUPTED: 2b56f299be313ad8bc689eff02973f16
Outbound email pending approval

Tool: send_email
Args: {'to': ['designteam@example.com'], 'subject': 'Reminder: Review New Mockups Before Meeting Next Tuesday at 2pm', 'body': "Hello Team,\n\nThis is a reminder to review the new mockups ahead of our meeting scheduled for next Tuesday at 2pm. Your feedback and insights will be valuable for our discussion and next steps.\n\nPlease ensure you've gone through the designs and are ready to share your thoughts during the meeting.\n\nThank you!\n\nBest regards,\n[Your Name]"}

We can specify decisions for each interrupt by referring to its ID using a `Command`. Refer to the human-in-the-loop guide for additional details. For demonstration purposes, here we will accept the calendar event, but edit the subject of the outbound email:

from langgraph.types import Command

resume = {}
for interrupt_ in interrupts:
if interrupt_.id == "2b56f299be313ad8bc689eff02973f16":
# Edit email
edited_action = interrupt_.value["action_requests"][0].copy()
edited_action["arguments"]["subject"] = "Mockups reminder"
resume[interrupt_.id] = {
"decisions": [{"type": "edit", "edited_action": edited_action}]
}
else:
resume[interrupt_.id] = {"decisions": [{"type": "approve"}]}

interrupts = []
for step in supervisor_agent.stream(
Command(resume=resume),
config,
):
for update in step.values():
if isinstance(update, dict):
for message in update.get("messages", []):
message.pretty_print()
else:
interrupt_ = update[0]
interrupts.append(interrupt_)
print(f"\nINTERRUPTED: {interrupt_.id}")

================================= Tool Message =================================
Name: schedule_event

Your meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.
================================= Tool Message =================================
Name: manage_email

Your email reminder to the design team has been sent. Here’s what was sent:

- Recipient: designteam@example.com
- Subject: Mockups reminder
- Body: A reminder to review the new mockups before the meeting next Tuesday at 2pm, with a request for feedback and readiness for discussion.

Let me know if you need any further assistance!
================================== Ai Message ==================================

- Your meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm.
- An email reminder has been sent to the design team about reviewing the new mockups before the meeting.

Let me know if you need any further assistance!

The run proceeds with our input.

## ​ 7\. Advanced: Control information flow

By default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.

### ​ Pass additional conversational context to sub-agents

from langchain.tools import tool, ToolRuntime

@tool
def schedule_event(
request: str,
runtime: ToolRuntime

"""Schedule calendar events using natural language."""
# Customize context received by sub-agent
original_user_message = next(
message for message in runtime.state["messages"]
if message.type == "human"
)
prompt = (
"You are assisting with the following user inquiry:\n\n"
f"{original_user_message.text}\n\n"
"You are tasked with the following sub-request:\n\n"
f"{request}"
)
result = calendar_agent.invoke({
"messages": [{"role": "user", "content": prompt}],
})
return result["messages"][-1].text

This allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like “schedule it for the same time tomorrow” (referencing a previous conversation).

You can see the full context received by the sub agent in the chat model call of the LangSmith trace.

### ​ Control what supervisor receives

You can also customize what information flows

# Option 1: Return just the confirmation message
return result["messages"][-1].text

# Option 2: Return structured data
# return json.dumps({
# "status": "success",
# "event_id": "evt_123",
# "summary": result["messages"][-1].text
# })

**Important:** Make sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but don’t include the results in their final response.

## ​ 8\. Key takeaways

The supervisor pattern creates layers of abstraction where each layer has a clear responsibility. When designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts. Write clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.

**When to use the supervisor pattern**Use the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents don’t need to converse directly with users.For simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use handoffs instead. For peer-to-peer collaboration between agents, consider other multi-agent patterns.

## ​ Next steps

Learn about handoffs for agent-to-agent conversations, explore context engineering to fine-tune information flow, read the multi-agent overview to compare different patterns, and use LangSmith to debug and monitor your multi-agent system.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a voice agent with LangChain\\
\\
Previous Build customer support with handoffs\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent/handoffs-customer-support

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Multi-agent

Build customer support with handoffs

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Setup
- Installation
- LangSmith
- Select an LLM
- 1\. Define custom state
- 2\. Create tools that manage workflow state
- 3\. Define step configurations
- 4\. Create step-based middleware
- 5\. Create the agent
- 6\. Test the workflow
- 7\. Understanding state transitions
- Turn 1: Initial message
- Turn 2: After warranty recorded
- Turn 3: After issue classified
- 8\. Manage message history
- 9\. Add flexibility: Go back
- Complete example
- Next steps

The state machine pattern describes workflows where an agent’s behavior changes as it moves through different states of a task. This tutorial shows how to implement a state machine by using tool calls to dynamically change a single agent’s configuration—updating its available tools and instructions based on the current state. The state can be determined from multiple sources: the agent’s past actions (tool calls), external state (such as API call results), or even initial user input (for example, by running a classifier to determine user intent).In this tutorial, you’ll build a customer support agent that does the following:

- Collects warranty information before proceeding.
- Classifies issues as hardware or software.
- Provides solutions or escalates to human support.
- Maintains conversation state across multiple turns.

Unlike the subagents pattern where sub-agents are called as tools, the **state machine pattern** uses a single agent whose configuration changes based on workflow progress. Each “step” is just a different configuration (system prompt + tools) of the same underlying agent, selected dynamically based on state.Here’s the workflow we’ll build:

✅ Yes

❌ No

🔩 Hardware

💻 Software

💬 Customer reports

an issue

Is the device

under warranty?

What type

of issue?

Provide warranty

repair instructions

Provide troubleshooting

steps

Escalate to human

for paid repair options

✅ Issue Resolved

## ​ Setup

### ​ Installation

This tutorial requires the `langchain` package:

pip

uv

conda

Copy

pip install langchain

For more details, see our Installation guide.

### ​ LangSmith

Set up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:

bash

python

export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

### ​ Select an LLM

Select a chat model from LangChain’s suite of integrations:

- OpenAI

- Anthropic

- Azure

- Google Gemini

- AWS Bedrock

- HuggingFace

👉 Read the OpenAI chat model integration docs

pip install -U "langchain[openai]"

init\_chat\_model

Model Class

import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")

👉 Read the Anthropic chat model integration docs

pip install -U "langchain[anthropic]"

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")

👉 Read the Azure chat model integration docs

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
"azure_openai:gpt-4.1",
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)

👉 Read the Google GenAI chat model integration docs

pip install -U "langchain[google-genai]"

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")

👉 Read the AWS Bedrock chat model integration docs

pip install -U "langchain[aws]"

from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
#

model = init_chat_model(
"anthropic.claude-3-5-sonnet-20240620-v1:0",
model_provider="bedrock_converse",
)

👉 Read the HuggingFace chat model integration docs

pip install -U "langchain[huggingface]"

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

model = init_chat_model(
"microsoft/Phi-3-mini-4k-instruct",
model_provider="huggingface",
temperature=0.7,
max_tokens=1024,
)

## ​ 1\. Define custom state

First, define a custom state schema that tracks which step is currently active:

from langchain.agents import AgentState
from typing_extensions import NotRequired
from typing import Literal

# Define the possible workflow steps
SupportStep = Literal["warranty_collector", "issue_classifier", "resolution_specialist"]

class SupportState(AgentState):
"""State for customer support workflow."""
current_step: NotRequired[SupportStep]
warranty_status: NotRequired[Literal["in_warranty", "out_of_warranty"]]
issue_type: NotRequired[Literal["hardware", "software"]]

The `current_step` field is the core of the state machine pattern - it determines which configuration (prompt + tools) is loaded on each turn.

## ​ 2\. Create tools that manage workflow state

Create tools that update the workflow state. These tools allow the agent to record information and transition to the next step.The key is using `Command` to update state, including the `current_step` field:

from langchain.tools import tool, ToolRuntime
from langchain.messages import ToolMessage
from langgraph.types import Command

@tool
def record_warranty_status(
status: Literal["in_warranty", "out_of_warranty"],
runtime: ToolRuntime[None, SupportState],

"""Record the customer's warranty status and transition to issue classification."""
return Command(
update={
"messages": [\
ToolMessage(\
content=f"Warranty status recorded as: {status}",\
tool_call_id=runtime.tool_call_id,\
)\
],
"warranty_status": status,
"current_step": "issue_classifier",
}
)

@tool
def record_issue_type(
issue_type: Literal["hardware", "software"],
runtime: ToolRuntime[None, SupportState],

"""Record the type of issue and transition to resolution specialist."""
return Command(
update={
"messages": [\
ToolMessage(\
content=f"Issue type recorded as: {issue_type}",\
tool_call_id=runtime.tool_call_id,\
)\
],
"issue_type": issue_type,
"current_step": "resolution_specialist",
}
)

@tool

"""Escalate the case to a human support specialist."""
# In a real system, this would create a ticket, notify staff, etc.
return f"Escalating to human support. Reason: {reason}"

"""Provide a solution to the customer's issue."""
return f"Solution provided: {solution}"

Notice how `record_warranty_status` and `record_issue_type` return `Command` objects that update both the data (`warranty_status`, `issue_type`) AND the `current_step`. This is how the state machine works - tools control workflow progression.

## ​ 3\. Define step configurations

Define prompts and tools for each step. First, define the prompts for each step:

View complete prompt definitions

# Define prompts as constants for easy reference
WARRANTY_COLLECTOR_PROMPT = """You are a customer support agent helping with device issues.

CURRENT STAGE: Warranty verification

At this step, you need to:
1. Greet the customer warmly
2. Ask if their device is under warranty
3. Use record_warranty_status to record their response and move to the next step

Be conversational and friendly. Don't ask multiple questions at once."""

ISSUE_CLASSIFIER_PROMPT = """You are a customer support agent helping with device issues.

CURRENT STAGE: Issue classification
CUSTOMER INFO: Warranty status is {warranty_status}

At this step, you need to:
1. Ask the customer to describe their issue
2. Determine if it's a hardware issue (physical damage, broken parts) or software issue (app crashes, performance)
3. Use record_issue_type to record the classification and move to the next step

If unclear, ask clarifying questions before classifying."""

RESOLUTION_SPECIALIST_PROMPT = """You are a customer support agent helping with device issues.

CURRENT STAGE: Resolution
CUSTOMER INFO: Warranty status is {warranty_status}, issue type is {issue_type}

At this step, you need to:
1. For SOFTWARE issues: provide troubleshooting steps using provide_solution
2. For HARDWARE issues:
- If IN WARRANTY: explain warranty repair process using provide_solution
- If OUT OF WARRANTY: escalate_to_human for paid repair options

Be specific and helpful in your solutions."""

Then map step names to their configurations using a dictionary:

# Step configuration: maps step name to (prompt, tools, required_state)
STEP_CONFIG = {
"warranty_collector": {
"prompt": WARRANTY_COLLECTOR_PROMPT,
"tools": [record_warranty_status],
"requires": [],
},
"issue_classifier": {
"prompt": ISSUE_CLASSIFIER_PROMPT,
"tools": [record_issue_type],
"requires": ["warranty_status"],
},
"resolution_specialist": {
"prompt": RESOLUTION_SPECIALIST_PROMPT,
"tools": [provide_solution, escalate_to_human],
"requires": ["warranty_status", "issue_type"],
},
}

This dictionary-based configuration makes it easy to:

- See all steps at a glance
- Add new steps (just add another entry)
- Understand the workflow dependencies (`requires` field)
- Use prompt templates with state variables (e.g., `{warranty_status}`)

## ​ 4\. Create step-based middleware

Create middleware that reads `current_step` from state and applies the appropriate configuration. We’ll use the `@wrap_model_call` decorator for a clean implementation:

from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable

@wrap_model_call
def apply_step_config(
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

"""Configure agent behavior based on the current step."""
# Get current step (defaults to warranty_collector for first interaction)
current_step = request.state.get("current_step", "warranty_collector")

# Look up step configuration
stage_config = STEP_CONFIG[current_step]

# Validate required state exists
for key in stage_config["requires"]:
if request.state.get(key) is None:
raise ValueError(f"{key} must be set before reaching {current_step}")

# Format prompt with state values (supports {warranty_status}, {issue_type}, etc.)
system_prompt = stage_config["prompt"].format(**request.state)

# Inject system prompt and step-specific tools
request = request.override(
system_prompt=system_prompt,
tools=stage_config["tools"],
)

return handler(request)

This middleware:

1. **Reads current step**: Gets `current_step` from state (defaults to “warranty\_collector”).
2. **Looks up configuration**: Finds the matching entry in `STEP_CONFIG`.
3. **Validates dependencies**: Ensures required state fields exist.
4. **Formats prompt**: Injects state values into the prompt template.
5. **Applies configuration**: Overrides the system prompt and available tools.

The `request.override()` method is key - it allows us to dynamically change the agent’s behavior based on state without creating separate agent instances.

## ​ 5\. Create the agent

Now create the agent with the step-based middleware and a checkpointer for state persistence:

from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver

# Collect all tools from all step configurations
all_tools = [\
record_warranty_status,\
record_issue_type,\
provide_solution,\
escalate_to_human,\
]

# Create the agent with step-based configuration
agent = create_agent(
model,
tools=all_tools,
state_schema=SupportState,
middleware=[apply_step_config],
checkpointer=InMemorySaver(),
)

**Why a checkpointer?** The checkpointer maintains state across conversation turns. Without it, the `current_step` state would be lost between user messages, breaking the workflow.

## ​ 6\. Test the workflow

Test the complete workflow:

from langchain.messages import HumanMessage
import uuid

# Configuration for this conversation thread
thread_id = str(uuid.uuid4())
config = {"configurable": {"thread_id": thread_id}}

# Turn 1: Initial message - starts with warranty_collector step
print("=== Turn 1: Warranty Collection ===")
result = agent.invoke(
{"messages": [HumanMessage("Hi, my phone screen is cracked")]},
config
)
for msg in result['messages']:
msg.pretty_print()

# Turn 2: User responds about warranty
print("\n=== Turn 2: Warranty Response ===")
result = agent.invoke(
{"messages": [HumanMessage("Yes, it's still under warranty")]},
config
)
for msg in result['messages']:
msg.pretty_print()
print(f"Current step: {result.get('current_step')}")

# Turn 3: User describes the issue
print("\n=== Turn 3: Issue Description ===")
result = agent.invoke(
{"messages": [HumanMessage("The screen is physically cracked from dropping it")]},
config
)
for msg in result['messages']:
msg.pretty_print()
print(f"Current step: {result.get('current_step')}")

# Turn 4: Resolution
print("\n=== Turn 4: Resolution ===")
result = agent.invoke(
{"messages": [HumanMessage("What should I do?")]},
config
)
for msg in result['messages']:
msg.pretty_print()

Expected flow:

1. **Warranty verification step**: Asks about warranty status
2. **Issue classification step**: Asks about the problem, determines it’s hardware
3. **Resolution step**: Provides warranty repair instructions

## ​ 7\. Understanding state transitions

Let’s trace what happens at each turn:

### ​ Turn 1: Initial message

{
"messages": [HumanMessage("Hi, my phone screen is cracked")],
"current_step": "warranty_collector" # Default value
}

Middleware applies:

- System prompt: `WARRANTY_COLLECTOR_PROMPT`
- Tools: `[record_warranty_status]`

### ​ Turn 2: After warranty recorded

Tool call: `record_warranty_status("in_warranty")` returns:

Command(update={
"warranty_status": "in_warranty",
"current_step": "issue_classifier" # State transition!
})

Next turn, middleware applies:

- System prompt: `ISSUE_CLASSIFIER_PROMPT` (formatted with `warranty_status="in_warranty"`)
- Tools: `[record_issue_type]`

### ​ Turn 3: After issue classified

Tool call: `record_issue_type("hardware")` returns:

Command(update={
"issue_type": "hardware",
"current_step": "resolution_specialist" # State transition!
})

- System prompt: `RESOLUTION_SPECIALIST_PROMPT` (formatted with `warranty_status` and `issue_type`)
- Tools: `[provide_solution, escalate_to_human]`

The key insight: **Tools drive the workflow** by updating `current_step`, and **middleware responds** by applying the appropriate configuration on the next turn.

## ​ 8\. Manage message history

As the agent progresses through steps, message history grows. Use summarization middleware to compress earlier messages while preserving conversational context:

from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
model,
tools=all_tools,
state_schema=SupportState,
middleware=[\
apply_step_config,\
SummarizationMiddleware(\
model="gpt-4o-mini",\
trigger=("tokens", 4000),\
keep=("messages", 10)\
)\
],
checkpointer=InMemorySaver(),
)

See the short-term memory guide for other memory management techniques.

## ​ 9\. Add flexibility: Go back

Some workflows need to allow users to

# Update the resolution_specialist configuration to include these tools
STEP_CONFIG["resolution_specialist"]["tools"].extend([\
go_back_to_warranty,\
go_back_to_classification\
])

Update the resolution specialist’s prompt to mention these tools:

If the customer indicates any information was wrong, use:
- go_back_to_warranty to correct warranty status
- go_back_to_classification to correct issue type

Now the agent can handle corrections:

result = agent.invoke(
{"messages": [HumanMessage("Actually, I made a mistake - my device is out of warranty")]},
config
)
# Agent will call go_back_to_warranty and restart the warranty verification step

## ​ Complete example

Here’s everything together in a runnable script:

Show Complete code

"""
Customer Support State Machine Example

This example demonstrates the state machine pattern.
A single agent dynamically changes its behavior based on the current_step state,
creating a state machine for sequential information collection.
"""

import uuid

from langgraph.checkpoint.memory import InMemorySaver
from langgraph.types import Command
from typing import Callable, Literal
from typing_extensions import NotRequired

from langchain.agents import AgentState, create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse, SummarizationMiddleware
from langchain.chat_models import init_chat_model
from langchain.messages import HumanMessage, ToolMessage
from langchain.tools import tool, ToolRuntime

model = init_chat_model("anthropic:claude-3-5-sonnet-latest")

class SupportState(AgentState):
"""State for customer support workflow."""

current_step: NotRequired[SupportStep]
warranty_status: NotRequired[Literal["in_warranty", "out_of_warranty"]]
issue_type: NotRequired[Literal["hardware", "software"]]

"""Escalate the case to a human support specialist."""
return f"Escalating to human support. Reason: {reason}"

# Define prompts as constants

CURRENT STEP: Warranty verification

CURRENT STEP: Issue classification
CUSTOMER INFO: Warranty status is {warranty_status}

CURRENT STEP: Resolution
CUSTOMER INFO: Warranty status is {warranty_status}, issue type is {issue_type}

"""Configure agent behavior based on the current step."""
current_step = request.state.get("current_step", "warranty_collector")

step_config = STEP_CONFIG[current_step]

for key in step_config["requires"]:
if request.state.get(key) is None:
raise ValueError(f"{key} must be set before reaching {current_step}")

# Format prompt with state values
system_prompt = step_config["prompt"].format(**request.state)

request = request.override(
system_prompt=system_prompt,
tools=step_config["tools"],
)

# Create the agent with step-based configuration and summarization

# ============================================================================
# Test the workflow

if __name__ == "__main__":
thread_id = str(uuid.uuid4())
config = {"configurable": {"thread_id": thread_id}}

result = agent.invoke(
{"messages": [HumanMessage("Hi, my phone screen is cracked")]},
config
)

result = agent.invoke(
{"messages": [HumanMessage("Yes, it's still under warranty")]},
config
)

result = agent.invoke(
{"messages": [HumanMessage("The screen is physically cracked from dropping it")]},
config
)

result = agent.invoke(
{"messages": [HumanMessage("What should I do?")]},
config
)
for msg in result['messages']:
msg.pretty_print()

## ​ Next steps

- Learn about the subagents pattern for centralized orchestration
- Explore middleware for more dynamic behaviors
- Read the multi-agent overview to compare patterns
- Use LangSmith to debug and monitor your multi-agent system

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a personal assistant with subagents\\
\\
Previous Build a multi-source knowledge base with routing\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent/router-knowledge-base

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Multi-agent

Build a multi-source knowledge base with routing

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Overview
- Why use a router?
- Concepts
- Setup
- Installation
- LangSmith
- Select an LLM
- 1\. Define state
- 2\. Define tools for each vertical
- 3\. Create specialized agents
- 4\. Build the router workflow
- 5\. Compile the workflow
- 6\. Use the router
- 7\. Understanding the architecture
- Classification phase
- Parallel execution with Send
- Result collection with reducers
- Synthesis phase
- 8\. Complete working example
- 9\. Advanced: Stateful routers
- Tool wrapper approach
- Full persistence approach
- 10\. Key takeaways
- Next steps

## ​ Overview

The **router pattern** is a multi-agent architecture where a routing step classifies input and directs it to specialized agents, with results synthesized into a combined response. This pattern excels when your organization’s knowledge lives across distinct **verticals**—separate knowledge domains that each require their own agent with specialized tools and prompts.In this tutorial, you’ll build a multi-source knowledge base router that demonstrates these benefits through a realistic enterprise scenario. The system will coordinate three specialists:

- A **GitHub agent** that searches code, issues, and pull requests.
- A **Notion agent** that searches internal documentation and wikis.
- A **Slack agent** that searches relevant threads and discussions.

When a user asks “How do I authenticate API requests?”, the router decomposes the query into source-specific sub-questions, routes them to the relevant agents in parallel, and synthesizes results into a coherent answer.

Query

Classify

GitHub agent

Notion agent

Slack agent

Synthesize

Combined answer

### ​ Why use a router?

The router pattern provides several advantages:

- **Parallel execution**: Query multiple sources simultaneously, reducing latency compared to sequential approaches.
- **Specialized agents**: Each vertical has focused tools and prompts optimized for its domain.
- **Selective routing**: Not every query needs every source—the router intelligently selects relevant verticals.
- **Targeted sub-questions**: Each agent receives a question tailored to its domain, improving result quality.
- **Clean synthesis**: Results from multiple sources are combined into a single, coherent response.

### ​ Concepts

We will cover the following concepts:

- Multi-agent systems
- StateGraph for workflow orchestration
- Send API for parallel execution

**Router vs. Subagents**: The subagents pattern can also route to multiple agents. Use the router pattern when you need specialized preprocessing, custom routing logic, or want explicit control over parallel execution. Use the subagents pattern when you want the LLM to decide which agents to call dynamically.

## ​ Setup

### ​ Installation

This tutorial requires the `langchain` and `langgraph` packages:

pip

uv

conda

Copy

pip install langchain langgraph

For more details, see our Installation guide.

### ​ LangSmith

Set up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:

bash

python

export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

### ​ Select an LLM

Select a chat model from LangChain’s suite of integrations:

- OpenAI

- Anthropic

- Azure

- Google Gemini

- AWS Bedrock

- HuggingFace

👉 Read the OpenAI chat model integration docs

pip install -U "langchain[openai]"

init\_chat\_model

Model Class

import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")

👉 Read the Anthropic chat model integration docs

pip install -U "langchain[anthropic]"

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")

👉 Read the Azure chat model integration docs

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
"azure_openai:gpt-4.1",
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)

👉 Read the Google GenAI chat model integration docs

pip install -U "langchain[google-genai]"

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")

👉 Read the AWS Bedrock chat model integration docs

pip install -U "langchain[aws]"

from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
#

model = init_chat_model(
"anthropic.claude-3-5-sonnet-20240620-v1:0",
model_provider="bedrock_converse",
)

👉 Read the HuggingFace chat model integration docs

pip install -U "langchain[huggingface]"

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

model = init_chat_model(
"microsoft/Phi-3-mini-4k-instruct",
model_provider="huggingface",
temperature=0.7,
max_tokens=1024,
)

## ​ 1\. Define state

First, define the state schemas. We use three types:

- **`AgentInput`**: Simple state passed to each subagent (just a query)
- **`AgentOutput`**: Result returned by each subagent (source name + result)
- **`RouterState`**: Main workflow state tracking the query, classifications, results, and final answer

from typing import Annotated, Literal, TypedDict
import operator

class AgentInput(TypedDict):
"""Simple input state for each subagent."""
query: str

class AgentOutput(TypedDict):
"""Output from each subagent."""
source: str
result: str

class Classification(TypedDict):
"""A single routing decision: which agent to call with what query."""
source: Literal["github", "notion", "slack"]
query: str

class RouterState(TypedDict):
query: str
classifications: list[Classification]
results: Annotated[list[AgentOutput], operator.add] # Reducer collects parallel results
final_answer: str

The `results` field uses a **reducer** (`operator.add` in Python, a concat function in JS) to collect outputs from parallel agent executions into a single list.

## ​ 2\. Define tools for each vertical

Create tools for each knowledge domain. In a production system, these would call actual APIs. For this tutorial, we use stub implementations that return mock data. We define 7 tools across 3 verticals: GitHub (search code, issues, PRs), Notion (search docs, get page), and Slack (search messages, get thread).

from langchain.tools import tool

@tool

"""Search code in GitHub repositories."""
return f"Found code matching '{query}' in {repo}: authentication middleware in src/auth.py"

"""Search GitHub issues and pull requests."""
return f"Found 3 issues matching '{query}': #142 (API auth docs), #89 (OAuth flow), #203 (token refresh)"

"""Search pull requests for implementation details."""
return f"PR #156 added JWT authentication, PR #178 updated OAuth scopes"

"""Search Notion workspace for documentation."""
return f"Found documentation: 'API Authentication Guide' - covers OAuth2 flow, API keys, and JWT tokens"

"""Get a specific Notion page by ID."""
return f"Page content: Step-by-step authentication setup instructions"

"""Search Slack messages and threads."""
return f"Found discussion in #engineering: 'Use Bearer tokens for API auth, see docs for refresh flow'"

"""Get a specific Slack thread."""
return f"Thread discusses best practices for API key rotation"

See all 43 lines

## ​ 3\. Create specialized agents

Create an agent for each vertical. Each agent has domain-specific tools and a prompt optimized for its knowledge source. All three follow the same pattern—only the tools and system prompt differ.

from langchain.agents import create_agent
from langchain.chat_models import init_chat_model

model = init_chat_model("openai:gpt-4o")

github_agent = create_agent(
model,
tools=[search_code, search_issues, search_prs],
system_prompt=(
"You are a GitHub expert. Answer questions about code, "
"API references, and implementation details by searching "
"repositories, issues, and pull requests."
),
)

notion_agent = create_agent(
model,
tools=[search_notion, get_page],
system_prompt=(
"You are a Notion expert. Answer questions about internal "
"processes, policies, and team documentation by searching "
"the organization's Notion workspace."
),
)

slack_agent = create_agent(
model,
tools=[search_slack, get_thread],
system_prompt=(
"You are a Slack expert. Answer questions by searching "
"relevant threads and discussions where team members have "
"shared knowledge and solutions."
),
)

See all 34 lines

## ​ 4\. Build the router workflow

Now build the router workflow using a StateGraph. The workflow has four main steps:

1. **Classify**: Analyze the query and determine which agents to invoke with what sub-questions
2. **Route**: Fan out to selected agents in parallel using `Send`
3. **Query agents**: Each agent receives a simple `AgentInput` and returns an `AgentOutput`
4. **Synthesize**: Combine collected results into a coherent response

from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send

router_llm = init_chat_model("openai:gpt-4o-mini")

# Define structured output schema for the classifier
class ClassificationResult(BaseModel):
"""Result of classifying a user query into agent-specific sub-questions."""
classifications: list[Classification] = Field(
description="List of agents to invoke with their targeted sub-questions"
)

"""Classify query and determine which agents to invoke."""
structured_llm = router_llm.with_structured_output(ClassificationResult)

result = structured_llm.invoke([\
{\
"role": "system",\
"content": """Analyze this query and determine which knowledge bases to consult.\
For each relevant source, generate a targeted sub-question optimized for that source.\
\
Available sources:\
- github: Code, API references, implementation details, issues, pull requests\
- notion: Internal documentation, processes, policies, team wikis\
- slack: Team discussions, informal knowledge sharing, recent conversations\
\
Return ONLY the sources that are relevant to the query. Each source should have\
a targeted sub-question optimized for that specific knowledge domain.\
\
Example for "How do I authenticate API requests?":\
- github: "What authentication code exists? Search for auth middleware, JWT handling"\
- notion: "What authentication documentation exists? Look for API auth guides"\
(slack omitted because it's not relevant for this technical question)"""\
},\
{"role": "user", "content": state["query"]}\
])

return {"classifications": result.classifications}

"""Fan out to agents based on classifications."""
return [\
Send(c["source"], {"query": c["query"]})\
for c in state["classifications"]\
]

"""Query the GitHub agent."""
result = github_agent.invoke({
"messages": [{"role": "user", "content": state["query"]}]
})
return {"results": [{"source": "github", "result": result["messages"][-1].content}]}

"""Query the Notion agent."""
result = notion_agent.invoke({
"messages": [{"role": "user", "content": state["query"]}]
})
return {"results": [{"source": "notion", "result": result["messages"][-1].content}]}

"""Query the Slack agent."""
result = slack_agent.invoke({
"messages": [{"role": "user", "content": state["query"]}]
})
return {"results": [{"source": "slack", "result": result["messages"][-1].content}]}

"""Combine results from all agents into a coherent answer."""
if not state["results"]:
return {"final_answer": "No results found from any knowledge source."}

# Format results for synthesis
formatted = [\
f"**From {r['source'].title()}:**\n{r['result']}"\
for r in state["results"]\
]

synthesis_response = router_llm.invoke([\
{\
"role": "system",\
"content": f"""Synthesize these search results to answer the original question: "{state['query']}"\
\
- Combine information from multiple sources without redundancy\
- Highlight the most relevant and actionable information\
- Note any discrepancies between sources\
- Keep the response concise and well-organized"""\
},\
{"role": "user", "content": "\n\n".join(formatted)}\
])

return {"final_answer": synthesis_response.content}

## ​ 5\. Compile the workflow

Now assemble the workflow by connecting nodes with edges. The key is using `add_conditional_edges` with the routing function to enable parallel execution:

workflow = (
StateGraph(RouterState)
.add_node("classify", classify_query)
.add_node("github", query_github)
.add_node("notion", query_notion)
.add_node("slack", query_slack)
.add_node("synthesize", synthesize_results)
.add_edge(START, "classify")
.add_conditional_edges("classify", route_to_agents, ["github", "notion", "slack"])
.add_edge("github", "synthesize")
.add_edge("notion", "synthesize")
.add_edge("slack", "synthesize")
.add_edge("synthesize", END)
.compile()
)

The `add_conditional_edges` call connects the classify node to the agent nodes through the `route_to_agents` function. When `route_to_agents` returns multiple `Send` objects, those nodes execute in parallel.

## ​ 6\. Use the router

Test your router with queries that span multiple knowledge domains:

result = workflow.invoke({
"query": "How do I authenticate API requests?"
})

print("Original query:", result["query"])
print("\nClassifications:")
for c in result["classifications"]:
print(f" {c['source']}: {c['query']}")
print("\n" + "=" * 60 + "\n")
print("Final Answer:")
print(result["final_answer"])

Expected output:

Original query: How do I authenticate API requests?

Classifications:
github: What authentication code exists? Search for auth middleware, JWT handling
notion: What authentication documentation exists? Look for API auth guides

Final Answer:
To authenticate API requests, you have several options:

1. **JWT Tokens**: The recommended approach for most use cases.
Implementation details are in `src/auth.py` (PR #156).

2. **OAuth2 Flow**: For third-party integrations, follow the OAuth2
flow documented in Notion's 'API Authentication Guide'.

3. **API Keys**: For server-to-server communication, use Bearer tokens
in the Authorization header.

For token refresh handling, see issue #203 and PR #178 for the latest
OAuth scope updates.

The router analyzed the query, classified it to determine which agents to invoke (GitHub and Notion, but not Slack for this technical question), queried both agents in parallel, and synthesized the results into a coherent answer.

## ​ 7\. Understanding the architecture

The router workflow follows a clear pattern:

### ​ Classification phase

The `classify_query` function uses **structured output** to analyze the user’s query and determine which agents to invoke. This is where the routing intelligence lives:

- Uses a Pydantic model (Python) or Zod schema (JS) to ensure valid output
- Returns a list of `Classification` objects, each with a `source` and targeted `query`
- Only includes relevant sources—irrelevant ones are simply omitted

This structured approach is more reliable than free-form JSON parsing and makes the routing logic explicit.

### ​ Parallel execution with Send

The `route_to_agents` function maps classifications to `Send` objects. Each `Send` specifies the target node and the state to pass:

# Classifications: [{"source": "github", "query": "..."}, {"source": "notion", "query": "..."}]
# Becomes:
[Send("github", {"query": "..."}), Send("notion", {"query": "..."})]
# Both agents execute simultaneously, each receiving only the query it needs

Each agent node receives a simple `AgentInput` with just a `query` field—not the full router state. This keeps the interface clean and explicit.

### ​ Result collection with reducers

Agent results flow

{"results": [{"source": "github", "result": "..."}]}

The reducer (`operator.add` in Python) concatenates these lists, collecting all parallel results into `state["results"]`.

### ​ Synthesis phase

After all agents complete, the `synthesize_results` function iterates over the collected results:

- Waits for all parallel branches to complete (LangGraph handles this automatically)
- References the original query to ensure the answer addresses what the user asked
- Combines information from all sources without redundancy

**Partial results**: In this tutorial, all selected agents must complete before synthesis. For more advanced patterns where you want to handle partial results or timeouts, see the map-reduce guide.

## ​ 8\. Complete working example

Here’s everything together in a runnable script:

Show View complete code

"""
Multi-Source Knowledge Router Example

This example demonstrates the router pattern for multi-agent systems.
A router classifies queries, routes them to specialized agents in parallel,
and synthesizes results into a combined response.
"""

import operator
from typing import Annotated, Literal, TypedDict

from langchain.agents import create_agent
from langchain.chat_models import init_chat_model
from langchain.tools import tool
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from pydantic import BaseModel, Field

# State definitions

class RouterState(TypedDict):
query: str
classifications: list[Classification]
results: Annotated[list[AgentOutput], operator.add]
final_answer: str

# Structured output schema for classifier

# Tools

# Models and agents
model = init_chat_model("openai:gpt-4o")
router_llm = init_chat_model("openai:gpt-4o-mini")

# Workflow nodes

result = structured_llm.invoke([\
{\
"role": "system",\
"content": """Analyze this query and determine which knowledge bases to consult.\
For each relevant source, generate a targeted sub-question optimized for that source.\
\
Available sources:\
- github: Code, API references, implementation details, issues, pull requests\
- notion: Internal documentation, processes, policies, team wikis\
- slack: Team discussions, informal knowledge sharing, recent conversations\
\
Return ONLY the sources that are relevant to the query."""\
},\
{"role": "user", "content": state["query"]}\
])

# Build workflow

if __name__ == "__main__":
result = workflow.invoke({
"query": "How do I authenticate API requests?"
})

print("Original query:", result["query"])
print("\nClassifications:")
for c in result["classifications"]:
print(f" {c['source']}: {c['query']}")
print("\n" + "=" * 60 + "\n")
print("Final Answer:")
print(result["final_answer"])

## ​ 9\. Advanced: Stateful routers

The router we’ve built so far is **stateless**—each request is handled independently with no memory between calls. For multi-turn conversations, you need a **stateful** approach.

### ​ Tool wrapper approach

The simplest way to add conversation memory is to wrap the stateless router as a tool that a conversational agent can call:

from langgraph.checkpoint.memory import InMemorySaver

"""Search across multiple knowledge sources (GitHub, Notion, Slack).

Use this to find information about code, documentation, or team discussions.
"""
result = workflow.invoke({"query": query})
return result["final_answer"]

conversational_agent = create_agent(
model,
tools=[search_knowledge_base],
system_prompt=(
"You are a helpful assistant that answers questions about our organization. "
"Use the search_knowledge_base tool to find information across our code, "
"documentation, and team discussions."
),
checkpointer=InMemorySaver(),
)

This approach keeps the router stateless while the conversational agent handles memory and context. The user can have a multi-turn conversation, and the agent will call the router tool as needed.

config = {"configurable": {"thread_id": "user-123"}}

result = conversational_agent.invoke(
{"messages": [{"role": "user", "content": "How do I authenticate API requests?"}]},
config
)
print(result["messages"][-1].content)

result = conversational_agent.invoke(
{"messages": [{"role": "user", "content": "What about rate limiting for those endpoints?"}]},
config
)
print(result["messages"][-1].content)

The tool wrapper approach is recommended for most use cases. It provides clean separation: the router handles multi-source querying, while the conversational agent handles context and memory.

### ​ Full persistence approach

If you need the router itself to maintain state—for example, to use previous search results in routing decisions—use persistence to store message history at the router level.

**Stateful routers add complexity.** When routing to different agents across turns, conversations may feel inconsistent if agents have different tones or prompts. Consider the handoffs pattern or subagents pattern instead—both provide clearer semantics for multi-turn conversations with different agents.

## ​ 10\. Key takeaways

The router pattern excels when you have:

- **Distinct verticals**: Separate knowledge domains that each require specialized tools and prompts
- **Parallel query needs**: Questions that benefit from querying multiple sources simultaneously
- **Synthesis requirements**: Results from multiple sources need to be combined into a coherent response

The pattern has three phases: **decompose** (analyze the query and generate targeted sub-questions), **route** (execute queries in parallel), and **synthesize** (combine results).

**When to use the router pattern**Use the router pattern when you have multiple independent knowledge sources, need low-latency parallel queries, and want explicit control over routing logic.For simpler cases with dynamic tool selection, consider the subagents pattern. For workflows where agents need to converse with users sequentially, consider handoffs.

## ​ Next steps

- Learn about handoffs for agent-to-agent conversations
- Explore the subagents pattern for centralized orchestration
- Read the multi-agent overview to compare different patterns
- Use LangSmith to debug and monitor your router

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build customer support with handoffs\\
\\
Previous Build a SQL assistant with on-demand skills\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent/skills-sql-assistant

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Multi-agent

Build a SQL assistant with on-demand skills

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- How it works
- Setup
- Installation
- LangSmith
- Select an LLM
- 1\. Define skills
- 2\. Create skill loading tool
- 3\. Build skill middleware
- 4\. Create the agent with skill support
- 5\. Test progressive disclosure
- 6\. Advanced: Add constraints with custom state
- Complete example
- Implementation variations
- Progressive disclosure and context engineering
- Next steps

This tutorial shows how to use **progressive disclosure** \- a context management technique where the agent loads information on-demand rather than upfront - to implement **skills** (specialized prompt-based instructions). The agent loads skills via tool calls, rather than dynamically changing the system prompt, discovering and loading only the skills it needs for each task.**Use case:** Imagine building an agent to help write SQL queries across different business verticals in a large enterprise. Your organization might have separate datastores for each vertical, or a single monolithic database with thousands of tables. Either way, loading all schemas upfront would overwhelm the context window. Progressive disclosure solves this by loading only the relevant schema when needed. This architecture also enables different product owners and stakeholders to independently contribute and maintain skills for their specific business verticals.**What you’ll build:** A SQL query assistant with two skills (sales analytics and inventory management). The agent sees lightweight skill descriptions in its system prompt, then loads full database schemas and business logic through tool calls only when relevant to the user’s query.

For a more complete example of a SQL agent with query execution, error correction, and validation, see our SQL Agent tutorial. This tutorial focuses on the progressive disclosure pattern which can be applied to any domain.

Progressive disclosure was popularized by Anthropic as a technique for building scalable agent skills systems. This approach uses a three-level architecture (metadata → core content → detailed resources) where agents load information only as needed. For more on this technique, see Equipping agents for the real world with Agent Skills.

## ​ How it works

Here’s the flow when a user asks for a SQL query:

💬 User: Write SQL query

for high-value customers

📋 Agent sees skill descriptions:

• sales\_analytics

• inventory\_management

🤔 Need sales schema

🔧 load\_skill

'sales\_analytics'

📊 Schema loaded:

customers, orders tables

\+ business logic

✍️ Agent writes SQL query

using schema knowledge

✅ Returns valid SQL

following business rules

**Why progressive disclosure:**

- **Reduces context usage** \- load only the 2-3 skills needed for a task, not all available skills
- **Enables team autonomy** \- different teams can develop specialized skills independently (similar to other multi-agent architectures)
- **Scales efficiently** \- add dozens or hundreds of skills without overwhelming context
- **Simplifies conversation history** \- single agent with one conversation thread

**What are skills:** Skills, as popularized by Claude Code, are primarily prompt-based: self-contained units of specialized instructions for specific business tasks. In Claude Code, skills are exposed as directories with files on the file system, discovered through file operations. Skills guide behavior through prompts and can provide information about tool usage or include sample code for a coding agent to execute.

Skills with progressive disclosure can be viewed as a form of RAG (Retrieval-Augmented Generation), where each skill is a retrieval unit—though not necessarily backed by embeddings or keyword search, but by tools for browsing content (like file operations or, in this tutorial, direct lookup).

**Trade-offs:**

- **Latency**: Loading skills on-demand requires additional tool calls, which adds latency to the first request that needs each skill
- **Workflow control**: Basic implementations rely on prompting to guide skill usage - you cannot enforce hard constraints like “always try skill A before skill B” without custom logic

**Implementing your own skills system**When building your own skills implementation (as we do in this tutorial), the core concept is progressive disclosure - loading information on-demand. Beyond that, you have full flexibility in implementation:

- **Storage**: databases, S3, in-memory data structures, or any backend
- **Discovery**: direct lookup (this tutorial), RAG for large skill collections, file system scanning, or API calls
- **Loading logic**: customize latency characteristics and add logic to search through skill content or rank relevance
- **Side effects**: define what happens when a skill loads, such as exposing tools associated with that skill (covered in section 8)

This flexibility lets you optimize for your specific requirements around performance, storage, and workflow control.

## ​ Setup

### ​ Installation

This tutorial requires the `langchain` package:

pip

uv

conda

Copy

pip install langchain

For more details, see our Installation guide.

### ​ LangSmith

Set up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:

bash

python

export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

### ​ Select an LLM

Select a chat model from LangChain’s suite of integrations:

- OpenAI

- Anthropic

- Azure

- Google Gemini

- AWS Bedrock

- HuggingFace

👉 Read the OpenAI chat model integration docs

pip install -U "langchain[openai]"

init\_chat\_model

Model Class

import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")

👉 Read the Anthropic chat model integration docs

pip install -U "langchain[anthropic]"

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")

👉 Read the Azure chat model integration docs

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
"azure_openai:gpt-4.1",
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)

👉 Read the Google GenAI chat model integration docs

pip install -U "langchain[google-genai]"

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")

👉 Read the AWS Bedrock chat model integration docs

pip install -U "langchain[aws]"

from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
#

model = init_chat_model(
"anthropic.claude-3-5-sonnet-20240620-v1:0",
model_provider="bedrock_converse",
)

👉 Read the HuggingFace chat model integration docs

pip install -U "langchain[huggingface]"

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

model = init_chat_model(
"microsoft/Phi-3-mini-4k-instruct",
model_provider="huggingface",
temperature=0.7,
max_tokens=1024,
)

## ​ 1\. Define skills

First, define the structure for skills. Each skill has a name, a brief description (shown in the system prompt), and full content (loaded on-demand):

from typing import TypedDict

class Skill(TypedDict):
"""A skill that can be progressively disclosed to the agent."""
name: str # Unique identifier for the skill
description: str # 1-2 sentence description to show in system prompt
content: str # Full skill content with detailed instructions

Now define example skills for a SQL query assistant. The skills are designed to be **lightweight in description** (shown to the agent upfront) but **detailed in content** (loaded only when needed):

View complete skill definitions

SKILLS: list[Skill] = [\
{\
"name": "sales_analytics",\
"description": "Database schema and business logic for sales data analysis including customers, orders, and revenue.",\
"content": """# Sales Analytics Schema\
\
## Tables\
\
### customers\
- customer_id (PRIMARY KEY)\
- name\
- email\
- signup_date\
- status (active/inactive)\
- customer_tier (bronze/silver/gold/platinum)\
\
### orders\
- order_id (PRIMARY KEY)\

- order_date\
- status (pending/completed/cancelled/refunded)\
- total_amount\
- sales_region (north/south/east/west)\
\
### order_items\
- item_id (PRIMARY KEY)\

- product_id\
- quantity\
- unit_price\
- discount_percent\
\
## Business Logic\
\
**Active customers**: status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'\
\
**Revenue calculation**: Only count orders with status = 'completed'. Use total_amount from orders table, which already accounts for discounts.\
\
**Customer lifetime value (CLV)**: Sum of all completed order amounts for a customer.\
\

## Example Query\
\
\
-- Get top 10 customers by revenue in the last quarter\
SELECT\
c.customer_id,\
c.name,\
c.customer_tier,\
SUM(o.total_amount) as total_revenue\
FROM customers c\
JOIN orders o ON c.customer_id = o.customer_id\
WHERE o.status = 'completed'\

GROUP BY c.customer_id, c.name, c.customer_tier\
ORDER BY total_revenue DESC\
LIMIT 10;\
""",\
},\
{\
"name": "inventory_management",\
"description": "Database schema and business logic for inventory tracking including products, warehouses, and stock levels.",\
"content": """# Inventory Management Schema\
\
### products\
\
- product_id (PRIMARY KEY)\
- product_name\
- sku\
- category\
- unit_cost\
- reorder_point (minimum stock level before reordering)\
- discontinued (boolean)\
\
### warehouses\
- warehouse_id (PRIMARY KEY)\
- warehouse_name\
- location\
- capacity\
\
### inventory\
- inventory_id (PRIMARY KEY)\

- quantity_on_hand\
- last_updated\
\
### stock_movements\
- movement_id (PRIMARY KEY)\

- movement_type (inbound/outbound/transfer/adjustment)\
- quantity (positive for inbound, negative for outbound)\
- movement_date\
- reference_number\
\

\
**Products needing reorder**: Products where total quantity_on_hand across all warehouses is less than or equal to the product's reorder_point\
\
**Active products only**: Exclude products where discontinued = true unless specifically analyzing discontinued items\
\
**Stock valuation**: quantity_on_hand * unit_cost for each product\
\
\
-- Find products below reorder point across all warehouses\
SELECT\
p.product_id,\
p.product_name,\
p.reorder_point,\
SUM(i.quantity_on_hand) as total_stock,\
p.unit_cost,\
(p.reorder_point - SUM(i.quantity_on_hand)) as units_to_reorder\
FROM products p\
JOIN inventory i ON p.product_id = i.product_id\
WHERE p.discontinued = false\
GROUP BY p.product_id, p.product_name, p.reorder_point, p.unit_cost\
HAVING SUM(i.quantity_on_hand) <= p.reorder_point\
ORDER BY units_to_reorder DESC;\
""",\
},\
]

## ​ 2\. Create skill loading tool

Create a tool to load full skill content on-demand:

from langchain.tools import tool

@tool

"""Load the full content of a skill into the agent's context.

Use this when you need detailed information about how to handle a specific
type of request. This will provide you with comprehensive instructions,
policies, and guidelines for the skill area.

Args:
skill_name: The name of the skill to load (e.g., "expense_reporting", "travel_booking")
"""
# Find and return the requested skill
for skill in SKILLS:
if skill["name"] == skill_name:
return f"Loaded skill: {skill_name}\n\n{skill['content']}"

# Skill not found
available = ", ".join(s["name"] for s in SKILLS)
return f"Skill '{skill_name}' not found. Available skills: {available}"

The `load_skill` tool returns the full skill content as a string, which becomes part of the conversation as a ToolMessage. For more details on creating and using tools, see the Tools guide.

## ​ 3\. Build skill middleware

Create custom middleware that injects skill descriptions into the system prompt. This middleware makes skills discoverable without loading their full content upfront.

This guide demonstrates creating custom middleware. For a comprehensive guide on middleware concepts and patterns, see the custom middleware documentation.

from langchain.agents.middleware import ModelRequest, ModelResponse, AgentMiddleware
from langchain.messages import SystemMessage
from typing import Callable

class SkillMiddleware(AgentMiddleware):
"""Middleware that injects skill descriptions into the system prompt."""

# Register the load_skill tool as a class variable
tools = [load_skill]

def __init__(self):
"""Initialize and generate the skills prompt from SKILLS."""
# Build skills prompt from the SKILLS list
skills_list = []
for skill in SKILLS:
skills_list.append(
f"- **{skill['name']}**: {skill['description']}"
)
self.skills_prompt = "\n".join(skills_list)

def wrap_model_call(
self,
request: ModelRequest,
handler: Callable[[ModelRequest], ModelResponse],

"""Sync: Inject skill descriptions into system prompt."""
# Build the skills addendum
skills_addendum = (
f"\n\n## Available Skills\n\n{self.skills_prompt}\n\n"
"Use the load_skill tool when you need detailed information "
"about handling a specific type of request."
)

# Append to system message content blocks
new_content = list(request.system_message.content_blocks) + [\
{"type": "text", "text": skills_addendum}\
]
new_system_message = SystemMessage(content=new_content)
modified_request = request.override(system_message=new_system_message)
return handler(modified_request)

The middleware appends skill descriptions to the system prompt, making the agent aware of available skills without loading their full content. The `load_skill` tool is registered as a class variable, making it available to the agent.

**Production consideration**: This tutorial loads the skill list in `__init__` for simplicity. In a production system, you may want to load skills in the `before_agent` hook instead, allowing them to be refreshed periodically to reflect up-to-date changes (e.g., when new skills are added or existing ones are modified). See the before\_agent hook documentation for details.

## ​ 4\. Create the agent with skill support

Now create the agent with the skill middleware and a checkpointer for state persistence:

from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver

# Create the agent with skill support
agent = create_agent(
model,
system_prompt=(
"You are a SQL query assistant that helps users "
"write queries against business databases."
),
middleware=[SkillMiddleware()],
checkpointer=InMemorySaver(),
)

The agent now has access to skill descriptions in its system prompt and can call `load_skill` to retrieve full skill content when needed. The checkpointer maintains conversation history across turns.

## ​ 5\. Test progressive disclosure

Test the agent with a question that requires skill-specific knowledge:

import uuid

# Configuration for this conversation thread
thread_id = str(uuid.uuid4())
config = {"configurable": {"thread_id": thread_id}}

# Ask for a SQL query
result = agent.invoke(
{
"messages": [\
{\
"role": "user",\
"content": (\
"Write a SQL query to find all customers "\
"who made orders over $1000 in the last month"\
),\
}\
]
},
config
)

# Print the conversation
for message in result["messages"]:
if hasattr(message, 'pretty_print'):
message.pretty_print()
else:
print(f"{message.type}: {message.content}")

Expected output:

================================ Human Message =================================

Write a SQL query to find all customers who made orders over $1000 in the last month
================================== Ai Message ==================================
Tool Calls:
load_skill (call_abc123)
Call ID: call_abc123
Args:
skill_name: sales_analytics
================================= Tool Message =================================
Name: load_skill

Loaded skill: sales_analytics

# Sales Analytics Schema

## Tables

### customers
- customer_id (PRIMARY KEY)
- name
- email
- signup_date
- status (active/inactive)
- customer_tier (bronze/silver/gold/platinum)

### orders
- order_id (PRIMARY KEY)

- order_date
- status (pending/completed/cancelled/refunded)
- total_amount
- sales_region (north/south/east/west)

[... rest of schema ...]

## Business Logic

**Revenue calculation**: Only count orders with `status = 'completed'`

================================== Ai Message ==================================

Here's a SQL query to find all customers who made orders over $1000 in the last month:

\`\`\`sql
SELECT DISTINCT
c.customer_id,
c.name,
c.email,
c.customer_tier
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id

AND o.status = 'completed'

ORDER BY c.customer_id;
\`\`\`

This query:
- Joins customers with their orders

- Only includes completed orders (as per the business logic)
- Restricts to orders from the last month
- Returns distinct customers to avoid duplicates if they made multiple qualifying orders

The agent saw the lightweight skill description in its system prompt, recognized the question required sales database knowledge, called `load_skill("sales_analytics")` to get the full schema and business logic, and then used that information to write a correct query following the database conventions.

## ​ 6\. Advanced: Add constraints with custom state

Optional: Track loaded skills and enforce tool constraints

You can add constraints to enforce that certain tools are only available after specific skills have been loaded. This requires tracking which skills have been loaded in custom agent state.

### ​ Define custom state

First, extend the agent state to track loaded skills:

from langchain.agents.middleware import AgentState

class CustomState(AgentState):
skills_loaded: NotRequired[list[str]] # Track which skills have been loaded #

### ​ Update load\_skill to modify state

Modify the `load_skill` tool to update state when a skill is loaded:

from langgraph.types import Command
from langchain.tools import tool, ToolRuntime
from langchain.messages import ToolMessage

Args:
skill_name: The name of the skill to load
"""
for skill in SKILLS:
if skill["name"] == skill_name:
skill_content = f"Loaded skill: {skill_name}\n\n{skill['content']}"

# Update state to track loaded skill
return Command(
update={
"messages": [\
ToolMessage(\
content=skill_content,\
tool_call_id=runtime.tool_call_id,\
)\
],
"skills_loaded": [skill_name],
}
)

available = ", ".join(s["name"] for s in SKILLS)
return Command(
update={
"messages": [\
ToolMessage(\
content=f"Skill '{skill_name}' not found. Available skills: {available}",\
tool_call_id=runtime.tool_call_id,\
)\
]
}
)

### ​ Create constrained tool

Create a tool that’s only usable after a specific skill has been loaded:

@tool
def write_sql_query(
query: str,
vertical: str,
runtime: ToolRuntime,

"""Write and validate a SQL query for a specific business vertical.

This tool helps format and validate SQL queries. You must load the
appropriate skill first to understand the database schema.

Args:
query: The SQL query to write
vertical: The business vertical (sales_analytics or inventory_management)
"""
# Check if the required skill has been loaded
skills_loaded = runtime.state.get("skills_loaded", [])

if vertical not in skills_loaded:
return (
f"Error: You must load the '{vertical}' skill first "
f"to understand the database schema before writing queries. "
f"Use load_skill('{vertical}') to load the schema."
)

# Validate and format the query
return (
f"SQL Query for {vertical}:\n\n"
f"```sql\n{query}\n```\n\n"
f"✓ Query validated against {vertical} schema\n"
f"Ready to execute against the database."
)

### ​ Update middleware and agent

Update the middleware to use the custom state schema:

class SkillMiddleware(AgentMiddleware[CustomState]):
"""Middleware that injects skill descriptions into the system prompt."""

state_schema = CustomState
tools = [load_skill, write_sql_query]

# ... rest of the middleware implementation stays the same

Create the agent with the middleware that registers the constrained tool:

Now if the agent tries to use `write_sql_query` before loading the required skill, it will receive an error message prompting it to load the appropriate skill (e.g., `sales_analytics` or `inventory_management`) first. This ensures the agent has the necessary schema knowledge before attempting to validate queries.

## ​ Complete example

View complete runnable script

Here’s a complete, runnable implementation combining all the pieces from this tutorial:

import uuid
from typing import TypedDict, NotRequired
from langchain.tools import tool
from langchain.agents import create_agent
from langchain.agents.middleware import ModelRequest, ModelResponse, AgentMiddleware
from langchain.messages import SystemMessage
from langgraph.checkpoint.memory import InMemorySaver
from typing import Callable

# Define skill structure
class Skill(TypedDict):
"""A skill that can be progressively disclosed to the agent."""
name: str
description: str
content: str

# Define skills with schemas and business logic
SKILLS: list[Skill] = [\
{\
"name": "sales_analytics",\
"description": "Database schema and business logic for sales data analysis including customers, orders, and revenue.",\
"content": """# Sales Analytics Schema\
\
\
### customers\
### orders\
- customer_id (PRIMARY KEY)\
- name\
- email\
- signup_date\
- status (active/inactive)\
- customer_tier (bronze/silver/gold/platinum)\
\
- order_id (PRIMARY KEY)\

### order_items\
- order_date\
- status (pending/completed/cancelled/refunded)\
- total_amount\
- sales_region (north/south/east/west)\
\
- item_id (PRIMARY KEY)\

- product_id\
- quantity\
- unit_price\
- discount_percent\
\
\
**Active customers**: status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'\
\
**Revenue calculation**: Only count orders with status = 'completed'. Use total_amount from orders table, which already accounts for discounts.\
\
**Customer lifetime value (CLV)**: Sum of all completed order amounts for a customer.\
\

GROUP BY c.customer_id, c.name, c.customer_tier\
ORDER BY total_revenue DESC\
LIMIT 10;\
""",\
},\
{\
"name": "inventory_management",\
"description": "Database schema and business logic for inventory tracking including products, warehouses, and stock levels.",\
"content": """# Inventory Management Schema\
\
\
### products\
- product_id (PRIMARY KEY)\
- product_name\
- sku\
- category\
- unit_cost\
- reorder_point (minimum stock level before reordering)\
- discontinued (boolean)\
\
### warehouses\
### inventory\
- warehouse_id (PRIMARY KEY)\
- warehouse_name\
- location\
- capacity\
\
- inventory_id (PRIMARY KEY)\

### stock_movements\
- quantity_on_hand\
- last_updated\
\
- movement_id (PRIMARY KEY)\

- movement_type (inbound/outbound/transfer/adjustment)\
- quantity (positive for inbound, negative for outbound)\
- movement_date\
- reference_number\
\
\

\
**Products needing reorder**: Products where total quantity_on_hand across all warehouses is less than or equal to the product's reorder_point\
\
**Active products only**: Exclude products where discontinued = true unless specifically analyzing discontinued items\
\
**Stock valuation**: quantity_on_hand * unit_cost for each product\
\
\
-- Find products below reorder point across all warehouses\
SELECT\
p.product_id,\
p.product_name,\
p.reorder_point,\
SUM(i.quantity_on_hand) as total_stock,\
p.unit_cost,\
(p.reorder_point - SUM(i.quantity_on_hand)) as units_to_reorder\
FROM products p\
JOIN inventory i ON p.product_id = i.product_id\
WHERE p.discontinued = false\
GROUP BY p.product_id, p.product_name, p.reorder_point, p.unit_cost\
HAVING SUM(i.quantity_on_hand) <= p.reorder_point\
ORDER BY units_to_reorder DESC;\
""",\
},\
]

# Create skill loading tool

Args:
skill_name: The name of the skill to load (e.g., "sales_analytics", "inventory_management")
"""

# Create skill middleware

def __init__(self):
"""Initialize and generate the skills prompt from SKILLS."""
skills_list = []
for skill in SKILLS:
skills_list.append(
f"- **{skill['name']}**: {skill['description']}"
)
self.skills_prompt = "\n".join(skills_list)

"""Sync: Inject skill descriptions into system prompt."""
skills_addendum = (
f"\n\n## Available Skills\n\n{self.skills_prompt}\n\n"
"Use the load_skill tool when you need detailed information "
"about handling a specific type of request."
)

# Initialize your chat model (replace with your model)
# Example: from langchain_anthropic import ChatAnthropic
# model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4")

# Example usage
if __name__ == "__main__":
thread_id = str(uuid.uuid4())
config = {"configurable": {"thread_id": thread_id}}

result = agent.invoke(
{
"messages": [\
{\
"role": "user",\
"content": (\
"Write a SQL query to find all customers "\
"who made orders over $1000 in the last month"\
),\
}\
]
},
config
)

This complete example includes:

- Skill definitions with full database schemas
- The `load_skill` tool for on-demand loading
- `SkillMiddleware` that injects skill descriptions into the system prompt
- Agent creation with middleware and checkpointer
- Example usage showing how the agent loads skills and writes SQL queries

To run this, you’ll need to:

1. Install required packages: `pip install langchain langchain-openai langgraph`
2. Set your API key (e.g., `export OPENAI_API_KEY=...`)
3. Replace the model initialization with your preferred LLM provider

## ​ Implementation variations

View implementation options and trade-offs

This tutorial implemented skills as in-memory Python dictionaries loaded through tool calls. However, there are several ways to implement progressive disclosure with skills:**Storage backends:**

- **In-memory** (this tutorial): Skills defined as Python data structures, fast access, no I/O overhead
- **File system** (Claude Code approach): Skills as directories with files, discovered via file operations like `read_file`
- **Remote storage**: Skills in S3, databases, Notion, or APIs, fetched on-demand

**Skill discovery** (how the agent learns which skills exist):

- **System prompt listing**: Skill descriptions in system prompt (used in this tutorial)
- **File-based**: Discover skills by scanning directories (Claude Code approach)
- **Registry-based**: Query a skill registry service or API for available skills
- **Dynamic lookup**: List available skills via a tool call

**Progressive disclosure strategies** (how skill content is loaded):

- **Single load**: Load entire skill content in one tool call (used in this tutorial)
- **Paginated**: Load skill content in multiple pages/chunks for large skills
- **Search-based**: Search within a specific skill’s content for relevant sections (e.g., using grep/read operations on skill files)
- **Hierarchical**: Load skill overview first, then drill into specific subsections

**Size considerations** (uncalibrated mental model - optimize for your system):

- **Small skills** (< 1K tokens / ~750 words): Can be included directly in system prompt and cached with prompt caching for cost savings and faster responses
- **Medium skills** (1-10K tokens / ~750-7.5K words): Benefit from on-demand loading to avoid context overhead (this tutorial)

The choice depends on your requirements: in-memory is fastest but requires redeployment for skill updates, while file-based or remote storage enables dynamic skill management without code changes.

## ​ Progressive disclosure and context engineering

Combining with few-shot prompting and other techniques

Progressive disclosure is fundamentally a **context engineering technique** \- you’re managing what information is available to the agent and when. This tutorial focused on loading database schemas, but the same principles apply to other types of context.

### ​ Combining with few-shot prompting

For the SQL query use case, you could extend progressive disclosure to dynamically load **few-shot examples** that match the user’s query:**Example approach:**

1. User asks: “Find customers who haven’t ordered in 6 months”
2. Agent loads `sales_analytics` schema (as shown in this tutorial)
3. Agent also loads 2-3 relevant example queries (via semantic search or tag-based lookup):
- Query for finding inactive customers
- Query with date-based filtering
- Query joining customers and orders tables
4. Agent writes query using both schema knowledge AND example patterns

This combination of progressive disclosure (loading schemas on-demand) and dynamic few-shot prompting (loading relevant examples) creates a powerful context engineering pattern that scales to large knowledge bases while providing high-quality, grounded outputs.

## ​ Next steps

- Learn about middleware for more dynamic agent behaviors
- Explore context engineering techniques for managing agent context
- Explore the handoffs pattern for sequential workflows
- Read the subagents pattern for parallel task routing
- See multi-agent patterns for other approaches to specialized agents
- Use LangSmith to debug and monitor skill loading

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a multi-source knowledge base with routing\\
\\
Previous Build a custom RAG agent with LangGraph\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/agentic-rag

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangGraph

Build a custom RAG agent with LangGraph

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Overview
- Concepts
- Setup
- 1\. Preprocess documents
- 2\. Create a retriever tool
- 3\. Generate query
- 4\. Grade documents
- 5\. Rewrite question
- 6\. Generate an answer
- 7\. Assemble the graph
- 8\. Run the agentic RAG

## ​ Overview

In this tutorial we will build a retrieval agent using LangGraph.LangChain offers built-in agent implementations, implemented using LangGraph primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a retrieval agent. Retrieval agents are useful when you want an LLM to make a decision about whether to retrieve context from a vectorstore or respond to the user directly.By the end of the tutorial we will have done the following:

1. Fetch and preprocess documents that will be used for retrieval.
2. Index those documents for semantic search and create a retriever tool for the agent.
3. Build an agentic RAG system that can decide when to use the retriever tool.

### ​ Concepts

We will cover the following concepts:

- Retrieval using document loaders, text splitters, embeddings, and vector stores
- The LangGraph Graph API, including state, nodes, edges, and conditional edges.

## ​ Setup

Let’s download the required packages and set our API keys:

Copy

pip install -U langgraph "langchain[openai]" langchain-community langchain-text-splitters bs4

import getpass
import os

def _set_env(key: str):
if key not in os.environ:
os.environ[key] = getpass.getpass(f"{key}:")

_set_env("OPENAI_API_KEY")

Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.

## ​ 1\. Preprocess documents

1. Fetch documents to use in our RAG system. We will use three of the most recent pages from Lilian Weng’s excellent blog. We’ll start by fetching the content of the pages using `WebBaseLoader` utility:

from langchain_community.document_loaders import WebBaseLoader

urls = [\
"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/",\
"https://lilianweng.github.io/posts/2024-07-07-hallucination/",\
"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/",\
]

docs = [WebBaseLoader(url).load() for url in urls]

docs[0][0].page_content.strip()[:1000]

2. Split the fetched documents into smaller chunks for indexing into our vectorstore:

from langchain_text_splitters import RecursiveCharacterTextSplitter

docs_list = [item for sublist in docs for item in sublist]

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
chunk_size=100, chunk_overlap=50
)
doc_splits = text_splitter.split_documents(docs_list)

doc_splits[0].page_content.strip()

## ​ 2\. Create a retriever tool

Now that we have our split documents, we can index them into a vector store that we’ll use for semantic search.

1. Use an in-memory vector store and OpenAI embeddings:

from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings

vectorstore = InMemoryVectorStore.from_documents(
documents=doc_splits, embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

2. Create a retriever tool using the `@tool` decorator:

from langchain.tools import tool

@tool

"""Search and return information about Lilian Weng blog posts."""
docs = retriever.invoke(query)
return "\n\n".join([doc.page_content for doc in docs])

retriever_tool = retrieve_blog_posts

3. Test the tool:

retriever_tool.invoke({"query": "types of reward hacking"})

## ​ 3\. Generate query

Now we will start building components ( nodes and edges) for our agentic RAG graph.Note that the components will operate on the `MessagesState` — graph state that contains a `messages` key with a list of chat messages.

1. Build a `generate_query_or_respond` node. It will call an LLM to generate a response based on the current graph state (list of messages). Given the input messages, it will decide to retrieve using the retriever tool, or respond directly to the user. Note that we’re giving the chat model access to the `retriever_tool` we created earlier via `.bind_tools`:

from langgraph.graph import MessagesState
from langchain.chat_models import init_chat_model

response_model = init_chat_model("gpt-4o", temperature=0)

def generate_query_or_respond(state: MessagesState):
"""Call the model to generate a response based on the current state. Given
the question, it will decide to retrieve using the retriever tool, or simply respond to the user.
"""
response = (
response_model
.bind_tools([retriever_tool]).invoke(state["messages"])
)
return {"messages": [response]}

2. Try it on a random input:

input = {"messages": [{"role": "user", "content": "hello!"}]}
generate_query_or_respond(input)["messages"][-1].pretty_print()

**Output:**

================================== Ai Message ==================================

Hello! How can I help you today?

3. Ask a question that requires semantic search:

input = {
"messages": [\
{\
"role": "user",\
"content": "What does Lilian Weng say about types of reward hacking?",\
}\
]
}
generate_query_or_respond(input)["messages"][-1].pretty_print()

================================== Ai Message ==================================
Tool Calls:
retrieve_blog_posts (call_tYQxgfIlnQUDMdtAhdbXNwIM)
Call ID: call_tYQxgfIlnQUDMdtAhdbXNwIM
Args:
query: types of reward hacking

## ​ 4\. Grade documents

1. Add a conditional edge — `grade_documents` — to determine whether the retrieved documents are relevant to the question. We will use a model with a structured output schema `GradeDocuments` for document grading. The `grade_documents` function will return the name of the node to go to based on the grading decision (`generate_answer` or `rewrite_question`):

from pydantic import BaseModel, Field
from typing import Literal

GRADE_PROMPT = (
"You are a grader assessing relevance of a retrieved document to a user question. \n "
"Here is the retrieved document: \n\n {context} \n\n"
"Here is the user question: {question} \n"
"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n"
"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."
)

class GradeDocuments(BaseModel):
"""Grade documents using a binary score for relevance check."""

binary_score: str = Field(
description="Relevance score: 'yes' if relevant, or 'no' if not relevant"
)

grader_model = init_chat_model("gpt-4o", temperature=0)

def grade_documents(
state: MessagesState,

"""Determine whether the retrieved documents are relevant to the question."""
question = state["messages"][0].content
context = state["messages"][-1].content

prompt = GRADE_PROMPT.format(question=question, context=context)
response = (
grader_model
.with_structured_output(GradeDocuments).invoke(
[{"role": "user", "content": prompt}]
)
)
score = response.binary_score

if score == "yes":
return "generate_answer"
else:
return "rewrite_question"

2. Run this with irrelevant documents in the tool response:

from langchain_core.messages import convert_to_messages

input = {
"messages": convert_to_messages(
[\
{\
"role": "user",\
"content": "What does Lilian Weng say about types of reward hacking?",\
},\
{\
"role": "assistant",\
"content": "",\
"tool_calls": [\
{\
"id": "1",\
"name": "retrieve_blog_posts",\
"args": {"query": "types of reward hacking"},\
}\
],\
},\
{"role": "tool", "content": "meow", "tool_call_id": "1"},\
]
)
}
grade_documents(input)

3. Confirm that the relevant documents are classified as such:

input = {
"messages": convert_to_messages(
[\
{\
"role": "user",\
"content": "What does Lilian Weng say about types of reward hacking?",\
},\
{\
"role": "assistant",\
"content": "",\
"tool_calls": [\
{\
"id": "1",\
"name": "retrieve_blog_posts",\
"args": {"query": "types of reward hacking"},\
}\
],\
},\
{\
"role": "tool",\
"content": "reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering",\
"tool_call_id": "1",\
},\
]
)
}
grade_documents(input)

## ​ 5\. Rewrite question

1. Build the `rewrite_question` node. The retriever tool can return potentially irrelevant documents, which indicates a need to improve the original user question. To do so, we will call the `rewrite_question` node:

from langchain.messages import HumanMessage

REWRITE_PROMPT = (
"Look at the input and try to reason about the underlying semantic intent / meaning.\n"
"Here is the initial question:"
"\n ------- \n"
"{question}"
"\n ------- \n"
"Formulate an improved question:"
)

def rewrite_question(state: MessagesState):
"""Rewrite the original user question."""
messages = state["messages"]
question = messages[0].content
prompt = REWRITE_PROMPT.format(question=question)
response = response_model.invoke([{"role": "user", "content": prompt}])
return {"messages": [HumanMessage(content=response.content)]}

2. Try it out:

input = {
"messages": convert_to_messages(
[\
{\
"role": "user",\
"content": "What does Lilian Weng say about types of reward hacking?",\
},\
{\
"role": "assistant",\
"content": "",\
"tool_calls": [\
{\
"id": "1",\
"name": "retrieve_blog_posts",\
"args": {"query": "types of reward hacking"},\
}\
],\
},\
{"role": "tool", "content": "meow", "tool_call_id": "1"},\
]
)
}

response = rewrite_question(input)
print(response["messages"][-1].content)

What are the different types of reward hacking described by Lilian Weng, and how does she explain them?

## ​ 6\. Generate an answer

1. Build `generate_answer` node: if we pass the grader checks, we can generate the final answer based on the original question and the retrieved context:

GENERATE_PROMPT = (
"You are an assistant for question-answering tasks. "
"Use the following pieces of retrieved context to answer the question. "
"If you don't know the answer, just say that you don't know. "
"Use three sentences maximum and keep the answer concise.\n"
"Question: {question} \n"
"Context: {context}"
)

def generate_answer(state: MessagesState):
"""Generate an answer."""
question = state["messages"][0].content
context = state["messages"][-1].content
prompt = GENERATE_PROMPT.format(question=question, context=context)
response = response_model.invoke([{"role": "user", "content": prompt}])
return {"messages": [response]}

2. Try it:

input = {
"messages": convert_to_messages(
[\
{\
"role": "user",\
"content": "What does Lilian Weng say about types of reward hacking?",\
},\
{\
"role": "assistant",\
"content": "",\
"tool_calls": [\
{\
"id": "1",\
"name": "retrieve_blog_posts",\
"args": {"query": "types of reward hacking"},\
}\
],\
},\
{\
"role": "tool",\
"content": "reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering",\
"tool_call_id": "1",\
},\
]
)
}

response = generate_answer(input)
response["messages"][-1].pretty_print()

Lilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering. She considers reward hacking as a broad concept that includes both of these categories. Reward hacking occurs when an agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended behaviors.

## ​ 7\. Assemble the graph

Now we’ll assemble all the nodes and edges into a complete graph:

- Start with a `generate_query_or_respond` and determine if we need to call `retriever_tool`
- Route to next step using `tools_condition`:

- If `generate_query_or_respond` returned `tool_calls`, call `retriever_tool` to retrieve context
- Otherwise, respond directly to the user
- Grade retrieved document content for relevance to the question (`grade_documents`) and route to next step:

- If not relevant, rewrite the question using `rewrite_question` and then call `generate_query_or_respond` again
- If relevant, proceed to `generate_answer` and generate final response using the `ToolMessage` with the retrieved document context

from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode, tools_condition

workflow = StateGraph(MessagesState)

# Define the nodes we will cycle between
workflow.add_node(generate_query_or_respond)
workflow.add_node("retrieve", ToolNode([retriever_tool]))
workflow.add_node(rewrite_question)
workflow.add_node(generate_answer)

workflow.add_edge(START, "generate_query_or_respond")

# Decide whether to retrieve
workflow.add_conditional_edges(
"generate_query_or_respond",
# Assess LLM decision (call `retriever_tool` tool or respond to the user)
tools_condition,
{
# Translate the condition outputs to nodes in our graph
"tools": "retrieve",
END: END,
},
)

# Edges taken after the `action` node is called.
workflow.add_conditional_edges(
"retrieve",
# Assess agent decision
grade_documents,
)
workflow.add_edge("generate_answer", END)
workflow.add_edge("rewrite_question", "generate_query_or_respond")

# Compile
graph = workflow.compile()

Visualize the graph:

from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))

## ​ 8\. Run the agentic RAG

Now let’s test the complete graph by running it with a question:

for chunk in graph.stream(
{
"messages": [\
{\
"role": "user",\
"content": "What does Lilian Weng say about types of reward hacking?",\
}\
]
}
):
for node, update in chunk.items():
print("Update from node", node)
update["messages"][-1].pretty_print()
print("\n\n")

Update from node generate_query_or_respond
================================== Ai Message ==================================
Tool Calls:
retrieve_blog_posts (call_NYu2vq4km9nNNEFqJwefWKu1)
Call ID: call_NYu2vq4km9nNNEFqJwefWKu1
Args:
query: types of reward hacking

Update from node retrieve
================================= Tool Message ==================================
Name: retrieve_blog_posts

(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)
At a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.

Why does Reward Hacking Exist?#

Pan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size, (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy of three types of misspecified proxy rewards:

Let's Define Reward Hacking#
Reward shaping in RL is challenging. Reward hacking occurs when an RL agent exploits flaws or ambiguities in the reward function to obtain high rewards without genuinely learning the intended behaviors or completing the task as designed. In recent years, several related concepts have been proposed, all referring to some form of reward hacking:

Update from node generate_answer
================================== Ai Message ==================================

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a SQL assistant with on-demand skills\\
\\
Previous Build a custom SQL agent\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/sql-agent

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangGraph

Build a custom SQL agent

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Concepts
- Setup
- Installation
- LangSmith
- 1\. Select an LLM
- 2\. Configure the database
- 3\. Add tools for database interactions
- 4\. Define application steps
- 5\. Implement the agent
- 6\. Implement human-in-the-loop review
- Next steps

In this tutorial we will build a custom agent that can answer questions about a SQL database using LangGraph.LangChain offers built-in agent implementations, implemented using LangGraph primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a SQL agent. You can find a tutorial building a SQL agent using higher-level LangChain abstractions here.

Building Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent’s needs. This will mitigate, though not eliminate, the risks of building a model-driven system.

The prebuilt agent lets us get started quickly, but we relied on the system prompt to constrain its behavior— for example, we instructed the agent to always start with the “list tables” tool, and to always run a query-checker tool before executing the query.We can enforce a higher degree of control in LangGraph by customizing the agent. Here, we implement a simple ReAct-agent setup, with dedicated nodes for specific tool-calls. We will use the same \[state\] as the pre-built agent.

### ​ Concepts

We will cover the following concepts:

- Tools for reading from SQL databases
- The LangGraph Graph API, including state, nodes, edges, and conditional edges.
- Human-in-the-loop processes

## ​ Setup

### ​ Installation

pip

Copy

pip install langchain langgraph langchain-community

### ​ LangSmith

Set up LangSmith to inspect what is happening inside your chain or agent. Then set the following environment variables:

export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."

## ​ 1\. Select an LLM

Select a model that supports tool-calling:

- OpenAI

- Anthropic

- Azure

- Google Gemini

- AWS Bedrock

- HuggingFace

👉 Read the OpenAI chat model integration docs

pip install -U "langchain[openai]"

init\_chat\_model

Model Class

import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")

👉 Read the Anthropic chat model integration docs

pip install -U "langchain[anthropic]"

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")

👉 Read the Azure chat model integration docs

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
"azure_openai:gpt-4.1",
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)

👉 Read the Google GenAI chat model integration docs

pip install -U "langchain[google-genai]"

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")

👉 Read the AWS Bedrock chat model integration docs

pip install -U "langchain[aws]"

from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
#

model = init_chat_model(
"anthropic.claude-3-5-sonnet-20240620-v1:0",
model_provider="bedrock_converse",
)

👉 Read the HuggingFace chat model integration docs

pip install -U "langchain[huggingface]"

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

model = init_chat_model(
"microsoft/Phi-3-mini-4k-instruct",
model_provider="huggingface",
temperature=0.7,
max_tokens=1024,
)

The output shown in the examples below used OpenAI.

## ​ 2\. Configure the database

You will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the `chinook` database, which is a sample database that represents a digital media store.For convenience, we have hosted the database (`Chinook.db`) on a public GCS bucket.

import requests, pathlib

url = "https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db"
local_path = pathlib.Path("Chinook.db")

if local_path.exists():
print(f"{local_path} already exists, skipping download.")
else:
response = requests.get(url)
if response.status_code == 200:
local_path.write_bytes(response.content)
print(f"File downloaded and saved as {local_path}")
else:
print(f"Failed to download the file. Status code: {response.status_code}")

We will use a handy SQL database wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:

from langchain_community.utilities import SQLDatabase

db = SQLDatabase.from_uri("sqlite:///Chinook.db")

print(f"Dialect: {db.dialect}")
print(f"Available tables: {db.get_usable_table_names()}")
print(f'Sample output: {db.run("SELECT * FROM Artist LIMIT 5;")}')

Dialect: sqlite
Available tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']
Sample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]

## ​ 3\. Add tools for database interactions

Use the `SQLDatabase` wrapper available in the `langchain_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:

from langchain_community.agent_toolkits import SQLDatabaseToolkit

toolkit = SQLDatabaseToolkit(db=db, llm=model)

tools = toolkit.get_tools()

for tool in tools:
print(f"{tool.name}: {tool.description}\n")

sql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.

sql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3

sql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.

sql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!

## ​ 4\. Define application steps

We construct dedicated nodes for the following steps:

- Listing DB tables
- Calling the “get schema” tool
- Generating a query
- Checking the query

Putting these steps in dedicated nodes lets us (1) force tool-calls when needed, and (2) customize the prompts associated with each step.

from typing import Literal

from langchain.messages import AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode

get_schema_tool = next(tool for tool in tools if tool.name == "sql_db_schema")
get_schema_node = ToolNode([get_schema_tool], name="get_schema")

run_query_tool = next(tool for tool in tools if tool.name == "sql_db_query")
run_query_node = ToolNode([run_query_tool], name="run_query")

# Example: create a predetermined tool call
def list_tables(state: MessagesState):
tool_call = {
"name": "sql_db_list_tables",
"args": {},
"id": "abc123",
"type": "tool_call",
}
tool_call_message = AIMessage(content="", tool_calls=[tool_call])

list_tables_tool = next(tool for tool in tools if tool.name == "sql_db_list_tables")
tool_message = list_tables_tool.invoke(tool_call)
response = AIMessage(f"Available tables: {tool_message.content}")

return {"messages": [tool_call_message, tool_message, response]}

# Example: force a model to create a tool call
def call_get_schema(state: MessagesState):
# Note that LangChain enforces that all models accept `tool_choice="any"`

llm_with_tools = model.bind_tools([get_schema_tool], tool_choice="any")
response = llm_with_tools.invoke(state["messages"])

return {"messages": [response]}

generate_query_system_prompt = """
You are an agent designed to interact with a SQL database.
Given an input question, create a syntactically correct {dialect} query to run,
then look at the results of the query and return the answer. Unless the user
specifies a specific number of examples they wish to obtain, always limit your
query to at most {top_k} results.

You can order the results by a relevant column to return the most interesting
examples in the database. Never query for all the columns from a specific table,
only ask for the relevant columns given the question.

DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.
""".format(
dialect=db.dialect,
top_k=5,
)

def generate_query(state: MessagesState):
system_message = {
"role": "system",
"content": generate_query_system_prompt,
}
# We do not force a tool call here, to allow the model to
# respond naturally when it obtains the solution.
llm_with_tools = model.bind_tools([run_query_tool])
response = llm_with_tools.invoke([system_message] + state["messages"])

check_query_system_prompt = """
You are a SQL expert with a strong attention to detail.
Double check the {dialect} query for common mistakes, including:
- Using NOT IN with NULL values
- Using UNION when UNION ALL should have been used
- Using BETWEEN for exclusive ranges
- Data type mismatch in predicates
- Properly quoting identifiers
- Using the correct number of arguments for functions
- Casting to the correct data type
- Using the proper columns for joins

If there are any of the above mistakes, rewrite the query. If there are no mistakes,
just reproduce the original query.

You will call the appropriate tool to execute the query after running this check.
""".format(dialect=db.dialect)

def check_query(state: MessagesState):
system_message = {
"role": "system",
"content": check_query_system_prompt,
}

# Generate an artificial user message to check
tool_call = state["messages"][-1].tool_calls[0]
user_message = {"role": "user", "content": tool_call["args"]["query"]}
llm_with_tools = model.bind_tools([run_query_tool], tool_choice="any")
response = llm_with_tools.invoke([system_message, user_message])
response.id = state["messages"][-1].id

## ​ 5\. Implement the agent

We can now assemble these steps into a workflow using the Graph API. We define a conditional edge at the query generation step that will route to the query checker if a query is generated, or end if there are no tool calls present, such that the LLM has delivered a response to the query.

messages = state["messages"]
last_message = messages[-1]
if not last_message.tool_calls:
return END
else:
return "check_query"

builder = StateGraph(MessagesState)
builder.add_node(list_tables)
builder.add_node(call_get_schema)
builder.add_node(get_schema_node, "get_schema")
builder.add_node(generate_query)
builder.add_node(check_query)
builder.add_node(run_query_node, "run_query")

builder.add_edge(START, "list_tables")
builder.add_edge("list_tables", "call_get_schema")
builder.add_edge("call_get_schema", "get_schema")
builder.add_edge("get_schema", "generate_query")
builder.add_conditional_edges(
"generate_query",
should_continue,
)
builder.add_edge("check_query", "run_query")
builder.add_edge("run_query", "generate_query")

agent = builder.compile()

We visualize the application below:

from IPython.display import Image, display
from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles

display(Image(agent.get_graph().draw_mermaid_png()))

question = "Which genre on average has the longest tracks?"

for step in agent.stream(
{"messages": [{"role": "user", "content": question}]},
stream_mode="values",
):
step["messages"][-1].pretty_print()

================================ Human Message =================================

Which genre on average has the longest tracks?
================================== Ai Message ==================================

Available tables: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track
================================== Ai Message ==================================
Tool Calls:
sql_db_schema (call_yzje0tj7JK3TEzDx4QnRR3lL)
Call ID: call_yzje0tj7JK3TEzDx4QnRR3lL
Args:
table_names: Genre, Track
================================= Tool Message =================================
Name: sql_db_schema

CREATE TABLE "Genre" (
"GenreId" INTEGER NOT NULL,
"Name" NVARCHAR(120),
PRIMARY KEY ("GenreId")
)

/*
3 rows from Genre table:
GenreId	Name
1	Rock
2	Jazz
3	Metal
*/

CREATE TABLE "Track" (
"TrackId" INTEGER NOT NULL,
"Name" NVARCHAR(200) NOT NULL,
"AlbumId" INTEGER,
"MediaTypeId" INTEGER NOT NULL,
"GenreId" INTEGER,
"Composer" NVARCHAR(220),
"Milliseconds" INTEGER NOT NULL,
"Bytes" INTEGER,
"UnitPrice" NUMERIC(10, 2) NOT NULL,
PRIMARY KEY ("TrackId"),
FOREIGN KEY("MediaTypeId") REFERENCES "MediaType" ("MediaTypeId"),
FOREIGN KEY("GenreId") REFERENCES "Genre" ("GenreId"),
FOREIGN KEY("AlbumId") REFERENCES "Album" ("AlbumId")
)

/*
3 rows from Track table:
TrackId	Name	AlbumId	MediaTypeId	GenreId	Composer	Milliseconds	Bytes	UnitPrice
1	For Those About To Rock (We Salute You)	1	1	1	Angus Young, Malcolm Young, Brian Johnson	343719	11170334	0.99
2	Balls to the Wall	2	2	1	U. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann	342562	5510424	0.99
3	Fast As a Shark	3	2	1	F. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman	230619	3990994	0.99
*/
================================== Ai Message ==================================
Tool Calls:
sql_db_query (call_cb9ApLfZLSq7CWg6jd0im90b)
Call ID: call_cb9ApLfZLSq7CWg6jd0im90b
Args:
query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;
================================== Ai Message ==================================
Tool Calls:
sql_db_query (call_DMVALfnQ4kJsuF3Yl6jxbeAU)
Call ID: call_DMVALfnQ4kJsuF3Yl6jxbeAU
Args:
query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;
================================= Tool Message =================================
Name: sql_db_query

[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]
================================== Ai Message ==================================

The genre with the longest tracks on average is "Sci Fi & Fantasy," with an average track length of approximately 2,911,783 milliseconds. Other genres with relatively long tracks include "Science Fiction," "Drama," "TV Shows," and "Comedy."

See LangSmith trace for the above run.

## ​ 6\. Implement human-in-the-loop review

It can be prudent to check the agent’s SQL queries before they are executed for any unintended actions or inefficiencies.Here we leverage LangGraph’s human-in-the-loop features to pause the run before executing a SQL query and wait for human review. Using LangGraph’s persistence layer, we can pause the run indefinitely (or at least as long as the persistence layer is alive).Let’s wrap the `sql_db_query` tool in a node that receives human input. We can implement this using the interrupt function. Below, we allow for input to approve the tool call, edit its arguments, or provide user feedback.

from langchain_core.runnables import RunnableConfig
from langchain.tools import tool
from langgraph.types import interrupt

@tool(
run_query_tool.name,
description=run_query_tool.description,
args_schema=run_query_tool.args_schema
)
def run_query_tool_with_interrupt(config: RunnableConfig, **tool_input):
request = {
"action": run_query_tool.name,
"args": tool_input,
"description": "Please review the tool call"
}
response = interrupt([request])
# approve the tool call
if response["type"] == "accept":
tool_response = run_query_tool.invoke(tool_input, config)
# update tool call args
elif response["type"] == "edit":
tool_input = response["args"]["args"]
tool_response = run_query_tool.invoke(tool_input, config)
# respond to the LLM with user feedback
elif response["type"] == "response":
user_feedback = response["args"]
tool_response = user_feedback
else:
raise ValueError(f"Unsupported interrupt response type: {response['type']}")

return tool_response

# Redefine the tool node to use the interrupt version
run_query_node = ToolNode([run_query_tool_with_interrupt], name="run_query")

The above implementation follows the tool interrupt example in the broader human-in-the-loop guide. Refer to that guide for details and alternatives.

Let’s now re-assemble our graph. We will replace the programmatic check with human review. Note that we now include a checkpointer; this is required to pause and resume the run.

from langgraph.checkpoint.memory import InMemorySaver

messages = state["messages"]
last_message = messages[-1]
if not last_message.tool_calls:
return END
else:
return "run_query"

builder = StateGraph(MessagesState)
builder.add_node(list_tables)
builder.add_node(call_get_schema)
builder.add_node(get_schema_node, "get_schema")
builder.add_node(generate_query)
builder.add_node(run_query_node, "run_query")

builder.add_edge(START, "list_tables")
builder.add_edge("list_tables", "call_get_schema")
builder.add_edge("call_get_schema", "get_schema")
builder.add_edge("get_schema", "generate_query")
builder.add_conditional_edges(
"generate_query",
should_continue,
)
builder.add_edge("run_query", "generate_query")

checkpointer = InMemorySaver()
agent = builder.compile(checkpointer=checkpointer)

We can invoke the graph as before. This time, execution is interrupted:

import json

config = {"configurable": {"thread_id": "1"}}

for step in agent.stream(
{"messages": [{"role": "user", "content": question}]},
config,
stream_mode="values",
):
if "messages" in step:
step["messages"][-1].pretty_print()
elif "__interrupt__" in step:
action = step["__interrupt__"][0]
print("INTERRUPTED:")
for request in action.value:
print(json.dumps(request, indent=2))
else:
pass

...

INTERRUPTED:
{
"action": "sql_db_query",
"args": {
"query": "SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;"
},
"description": "Please review the tool call"
}

We can accept or edit the tool call using Command:

from langgraph.types import Command

for step in agent.stream(
Command(resume={"type": "accept"}),
# Command(resume={"type": "edit", "args": {"query": "..."}}),
config,
stream_mode="values",
):
if "messages" in step:
step["messages"][-1].pretty_print()
elif "__interrupt__" in step:
action = step["__interrupt__"][0]
print("INTERRUPTED:")
for request in action.value:
print(json.dumps(request, indent=2))
else:
pass

================================== Ai Message ==================================
Tool Calls:
sql_db_query (call_t4yXkD6shwdTPuelXEmY3sAY)
Call ID: call_t4yXkD6shwdTPuelXEmY3sAY
Args:
query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;
================================= Tool Message =================================
Name: sql_db_query

The genre with the longest average track length is "Sci Fi & Fantasy" with an average length of about 2,911,783 milliseconds. Other genres with long average track lengths include "Science Fiction," "Drama," "TV Shows," and "Comedy."

Refer to the human-in-the-loop guide for details.

## ​ Next steps

Check out the Evaluate a graph guide for evaluating LangGraph applications, including SQL agents like this one, using LangSmith.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a custom RAG agent with LangGraph\\
\\
Previous Component architecture\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/component-architecture

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Conceptual overviews

Component architecture

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Core component ecosystem
- How components connect
- Component categories
- Common patterns
- RAG (Retrieval-Augmented Generation)
- Agent with tools
- Multi-agent system
- Learn more

LangChain’s power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components.

## ​ Core component ecosystem

The diagram below shows how LangChain’s major components connect to form complete AI applications:

🎯 Orchestration

🤖 Generation

🔍 Retrieval

🔢 Embedding & storage

📥 Input processing

Text input

Document loaders

Text splitters

Documents

Embedding models

Vectors

Vector stores

User Query

Query vector

Retrievers

Relevant context

Chat models

Tools

Tool results

AI response

Agents

Memory

### ​ How components connect

Each component layer builds on the previous ones:

1. **Input processing** – Transform raw data into structured documents
2. **Embedding & storage** – Convert text into searchable vector representations
3. **Retrieval** – Find relevant information based on user queries
4. **Generation** – Use AI models to create responses, optionally with tools
5. **Orchestration** – Coordinate everything through agents and memory systems

## ​ Component categories

LangChain organizes components into these main categories:

| Category | Purpose | Key Components | Use Cases |
| --- | --- | --- | --- |
| **Models** | AI reasoning and generation | Chat models, LLMs, Embedding models | Text generation, reasoning, semantic understanding |
| **Tools** | External capabilities | APIs, databases, etc. | Web search, data access, computations |
| **Agents** | Orchestration and reasoning | ReAct agents, tool calling agents | Nondeterministic workflows, decision making |
| **Memory** | Context preservation | Message history, custom state | Conversations, stateful interactions |
| **Retrievers** | Information access | Vector retrievers, web retrievers | RAG, knowledge base search |
| **Document processing** | Data ingestion | Loaders, splitters, transformers | PDF processing, web scraping |
| **Vector Stores** | Semantic search | Chroma, Pinecone, FAISS | Similarity search, embeddings storage |

## ​ Common patterns

### ​ RAG (Retrieval-Augmented Generation)

User question

Retriever

Relevant docs

Chat model

Informed response

### ​ Agent with tools

Yes

No

User request

Agent

Need tool?

Call tool

Tool result

Final answer

### ​ Multi-agent system

Complex Task

Supervisor agent

Specialist agent 1

Specialist agent 2

Results

Coordinated response

## ​ Learn more

Now that you understand how components relate to each other, explore specific areas:

- Building your first RAG system
- Creating agents
- Working with tools
- Setting up memory
- Browse integrations

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a custom SQL agent\\
\\
Previous Memory overview\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/concepts/context

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Conceptual overviews

Context overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Static runtime context
- Dynamic runtime context
- Dynamic cross-conversation context
- See also

**Context engineering** is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:

1. By **mutability**:

- **Static context**: Immutable data that doesn’t change during execution (e.g., user metadata, database connections, tools)
- **Dynamic context**: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)

2. By **lifetime**:

- **Runtime context**: Data scoped to a single run or invocation
- **Cross-conversation context**: Data that persists across multiple conversations or sessions

Runtime context refers to local context: data and dependencies your code needs to run. It does **not** refer to:

- The LLM context, which is the data passed into the LLM’s prompt.
- The “context window”, which is the maximum number of tokens that can be passed to the LLM.

Runtime context is a form of dependency injection and can be used to optimize the LLM context. It lets to provide dependencies (like database connections, user IDs, or API clients) to your tools and nodes at runtime rather than hardcoding them. For example, you can use user metadata in the runtime context to fetch user preferences and feed them into the context window.

LangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions:

| Context type | Description | Mutability | Lifetime | Access method |
| --- | --- | --- | --- | --- |
| **Static runtime context** | User metadata, tools, db connections passed at startup | Static | Single run | `context` argument to `invoke`/`stream` |
| **Dynamic runtime context (state)** | Mutable data that evolves during a single run | Dynamic | Single run | LangGraph state object |
| **Dynamic cross-conversation context (store)** | Persistent data shared across conversations | Dynamic | Cross-conversation | LangGraph store |

## ​ Static runtime context

**Static runtime context** represents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via the `context` argument to `invoke`/`stream`. This data does not change during execution.

Copy

@dataclass
class ContextSchema:
user_name: str

graph.invoke(
{"messages": [{"role": "user", "content": "hi!"}]},
context={"user_name": "John Smith"}
)

- Agent prompt

- Workflow node

- In a tool

from dataclasses import dataclass
from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dynamic_prompt

user_name = request.runtime.context.user_name
return f"You are a helpful assistant. Address the user as {user_name}."

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[get_weather],
middleware=[personalized_prompt],
context_schema=ContextSchema
)

agent.invoke(
{"messages": [{"role": "user", "content": "what is the weather in sf"}]},
context=ContextSchema(user_name="John Smith")
)

See Agents for details.

from langgraph.runtime import Runtime

def node(state: State, runtime: Runtime[ContextSchema]):
user_name = runtime.context.user_name
...

- See the Graph API for details.

from langchain.tools import tool, ToolRuntime

@tool

"""Retrieve user information based on user ID."""
# simulate fetching user info from a database
email = get_user_email_from_db(runtime.context.user_name)
return email

See the tool calling guide for details.

The `Runtime` object can be used to access static context and other utilities like the active store and stream writer.
See the `Runtime` documentation for details.

## ​ Dynamic runtime context

**Dynamic runtime context** represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as short-term memory during a run.

- In an agent

- In a workflow

Example shows how to incorporate state into an agent **prompt**.State can also be accessed by the agent’s **tools**, which can read or update the state as needed. See tool calling guide for details.

from langchain.agents import create_agent
from langchain.agents.middleware import dynamic_prompt, ModelRequest
from langchain.agents import AgentState

class CustomState(AgentState):
user_name: str

user_name = request.state.get("user_name", "User")
return f"You are a helpful assistant. User's name is {user_name}"

agent = create_agent(
model="claude-sonnet-4-5-20250929",
tools=[...],
state_schema=CustomState,
middleware=[personalized_prompt],
)

agent.invoke({
"messages": "hi!",
"user_name": "John Smith"
})

from typing_extensions import TypedDict
from langchain.messages import AnyMessage
from langgraph.graph import StateGraph

class CustomState(TypedDict):
messages: list[AnyMessage]
extra_field: int

def node(state: CustomState):
messages = state["messages"]
...
return {
"extra_field": state["extra_field"] + 1
}

builder = StateGraph(State)
builder.add_node(node)
builder.set_entry_point("node")
graph = builder.compile()

**Turning on memory**
Please see the memory guide for more details on how to enable memory. This is a powerful feature that allows you to persist the agent’s state across multiple invocations. Otherwise, the state is scoped only to a single run.

## ​ Dynamic cross-conversation context

**Dynamic cross-conversation context** represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as long-term memory across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).

## ​ See also

- Memory conceptual overview
- Short-term memory in LangChain
- Long-term memory in LangChain
- Memory in LangGraph

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Memory overview\\
\\
Previous Graph API overview\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/case-studies

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Additional resources

Case studies

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

This list of companies using LangGraph and their success stories is compiled from public sources. If your company uses LangGraph, we’d love for you to share your story and add it to the list. You’re also welcome to contribute updates based on publicly available information from other companies, such as blog posts or press releases.

| Company | Industry | Use case | Reference |
| --- | --- | --- | --- |
| AirTop | Software & Technology (GenAI Native) | Browser automation for AI agents | Case study, 2024 |
| AppFolio | Real Estate | Copilot for domain-specific task | Case study, 2024 |
| Athena Intelligence | Software & Technology (GenAI Native) | Research & summarization | Case study, 2024 |
| BlackRock | Financial Services | Copilot for domain-specific task | Interrupt talk, 2025 |
| Captide | Software & Technology (GenAI Native) | Data extraction | Case study, 2025 |
| Cisco CX | Software & Technology | Customer support | Interrupt Talk, 2025 |
| Cisco Outshift | Software & Technology | DevOps | Video story, 2025; Case study, 2025; Blog post, 2025 |
| Cisco TAC | Software & Technology | Customer support | Video story, 2025 |
| City of Hope | Non-profit | Copilot for domain-specific task | Video story, 2025 |
| C.H. Robinson | Logistics | Automation | Case study, 2025 |
| Definely | Legal | Copilot for domain-specific task | Case study, 2025 |
| Docent Pro | Travel | GenAI embedded product experiences | Case study, 2025 |
| Elastic | Software & Technology | Copilot for domain-specific task | Blog post, 2025 |
| Exa | Software & Technology (GenAI Native) | Search | Case study, 2025 |
| GitLab | Software & Technology | Code generation | Duo workflow docs |
| Harmonic | Software & Technology | Search | Case study, 2025 |
| Inconvo | Software & Technology | Code generation | Case study, 2025 |
| Infor | Software & Technology | GenAI embedded product experiences; customer support; copilot | Case study, 2025 |
| J.P. Morgan | Financial Services | Copilot for domain-specific task | Interrupt talk, 2025 |
| Klarna | Fintech | Copilot for domain-specific task | Case study, 2025 |
| Komodo Health | Healthcare | Copilot for domain-specific task | Blog post |
| LinkedIn | Social Media | Code generation; Search & discovery | Interrupt talk, 2025; Blog post, 2025; Blog post, 2024 |
| Minimal | E-commerce | Customer support | Case study, 2025 |
| Modern Treasury | Fintech | GenAI embedded product experiences | Video story, 2025 |
| Monday | Software & Technology | GenAI embedded product experiences | Interrupt talk, 2025 |
| Morningstar | Financial Services | Research & summarization | Video story, 2025 |
| OpenRecovery | Healthcare | Copilot for domain-specific task | Case study, 2024 |
| Pigment | Fintech | GenAI embedded product experiences | Video story, 2025 |
| Prosper | Fintech | Customer support | Video story, 2025 |
| Qodo | Software & Technology (GenAI Native) | Code generation | Blog post, 2025 |
| Rakuten | E-commerce / Fintech | Copilot for domain-specific task | Video story, 2025; Blog post, 2025 |
| Replit | Software & Technology | Code generation | Blog post, 2024; Breakout agent story, 2024; Fireside chat video, 2024 |
| Rexera | Real Estate (GenAI Native) | Copilot for domain-specific task | Case study, 2024 |
| Abu Dhabi Government | Government | Search | Case study, 2025 |
| Tradestack | Software & Technology (GenAI Native) | Copilot for domain-specific task | Case study, 2024 |
| Uber | Transportation | Developer productivity; Code generation | Interrupt talk, 2025; Presentation, 2024; Video, 2024 |
| Unify | Software & Technology (GenAI Native) | Copilot for domain-specific task | Interrupt talk, 2025; Blog post, 2024 |
| Vizient | Healthcare | Copilot for domain-specific task | Video story, 2025; Case study, 2025 |
| Vodafone | Telecommunications | Code generation; internal search | Case study, 2025 |
| WebToon | Media & Entertainment | Data extraction | Case study, 2025 |
| 11x | Software & Technology (GenAI Native) | Research & outreach | Interrupt talk, 2025 |

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangChain Academy\\
\\
Previous Get help\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/get-help

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Additional resources

Get help

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

On this page

- Learning resources
- Community support
- Professional support
- Contribute
- Stay connected

Connect with the LangChain community, access learning resources, and get the support you need to build with confidence.

## ​ Learning resources

Start your journey or deepen your knowledge with our comprehensive learning materials.

- **Chat LangChain**: Ask the docs anything about LangChain, powered by real-time docs
- **API Reference**: Complete documentation for all LangChain packages

## ​ Community support

Get help from fellow developers and the LangChain team through our active community channels.

- **Community Forum**: Ask questions, share solutions, and discuss best practices
- **Community Slack**: Connect with other builders and get quick help

## ​ Professional support

For enterprise needs and critical applications, access dedicated support channels.

- **Support portal**: Submit tickets and track support requests
- **LangSmith status**: Real-time status of LangSmith services and APIs

## ​ Contribute

Help us improve LangChain for everyone. Whether you’re fixing bugs, adding features, or improving documentation, we welcome your contributions.

- **Contributing Guide**: Everything you need to know about contributing to LangChain

## ​ Stay connected

Follow us for the latest updates, announcements, and community highlights.

- **X (Twitter)**: Daily updates and community spotlights
- **LinkedIn**: Professional network and company updates

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Case studies\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Multi-agent

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Overview
- Subagents
- Handoffs
- Skills
- Router
- Custom workflow
- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Why multi-agent?
- Patterns
- Choosing a pattern
- Visual overview
- Performance comparison
- One-shot request
- Repeat request
- Multi-domain
- Summary

Multi-agent systems coordinate specialized components to tackle complex workflows. However, not every complex task requires this approach — a single agent with the right (sometimes dynamic) tools and prompt can often achieve similar results.

## ​ Why multi-agent?

When developers say they need “multi-agent,” they’re usually looking for one or more of these capabilities:

- **Context management**: Provide specialized knowledge without overwhelming the model’s context window. If context were infinite and latency zero, you could dump all knowledge into a single prompt — but since it’s not, you need patterns to selectively surface relevant information.
- **Distributed development**: Allow different teams to develop and maintain capabilities independently, composing them into a larger system with clear boundaries.
- **Parallelization**: Spawn specialized workers for subtasks and execute them concurrently for faster results.

Multi-agent patterns are particularly valuable when a single agent has too many tools and makes poor decisions about which to use, when tasks require specialized knowledge with extensive context (long prompts and domain-specific tools), or when you need to enforce sequential constraints that unlock capabilities only after certain conditions are met.

At the center of multi-agent design is **context engineering**—deciding what information each agent sees. The quality of your system depends on ensuring each agent has access to the right data for its task.

## ​ Patterns

Here are the main patterns for building multi-agent systems, each suited to different use cases:

| Pattern | How it works |
| --- | --- |
| **Subagents** | A main agent coordinates subagents as tools. All routing passes through the main agent, which decides when and how to invoke each subagent. |
| **Handoffs** | Behavior changes dynamically based on state. Tool calls update a state variable that triggers routing or configuration changes, switching agents or adjusting the current agent’s tools and prompt. |
| **Skills** | Specialized prompts and knowledge loaded on-demand. A single agent stays in control while loading context from skills as needed. |
| **Router** | A routing step classifies input and directs it to one or more specialized agents. Results are synthesized into a combined response. |
| **Custom workflow** | Build bespoke execution flows with LangGraph, mixing deterministic logic and agentic behavior. Embed other patterns as nodes in your workflow. |

### ​ Choosing a pattern

Use this table to match your requirements to the right pattern:

| Pattern | Distributed development | Parallelization | Multi-hop | Direct user interaction |
| --- | --- | --- | --- | --- |
| **Subagents** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐ |
| **Handoffs** | — | — | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **Skills** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **Router** | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | — | ⭐⭐⭐ |

- **Distributed development**: Can different teams maintain components independently?
- **Parallelization**: Can multiple agents execute concurrently?
- **Multi-hop**: Does the pattern support calling multiple subagents in series?
- **Direct user interaction**: Can subagents converse directly with the user?

You can mix patterns! For example, a **subagents** architecture can invoke tools that invoke custom workflows or router agents. Subagents can even use the **skills** pattern to load context on-demand. The possibilities are endless!

### ​ Visual overview

- Subagents

- Handoffs

- Skills

- Router

A main agent coordinates subagents as tools. All routing passes through the main agent.

Agents transfer control to each other via tool calls. Each agent can hand off to others or respond directly to the user.

A single agent loads specialized prompts and knowledge on-demand while staying in control.

A routing step classifies input and directs it to specialized agents. Results are synthesized.

## ​ Performance comparison

Different patterns have different performance characteristics. Understanding these tradeoffs helps you choose the right pattern for your latency and cost requirements.**Key metrics:**

- **Model calls**: Number of LLM invocations. More calls = higher latency (especially if sequential) and higher per-request API costs.
- **Tokens processed**: Total context window usage across all calls. More tokens = higher processing costs and potential context limits.

A specialized coffee agent/skill can call a `buy_coffee` tool.

| Pattern | Model calls | Best fit |
| --- | --- | --- |
| **Subagents** | 4 | |
| **Handoffs** | 3 | ✅ |
| **Skills** | 3 | ✅ |
| **Router** | 3 | ✅ |

**4 model calls:**

**3 model calls:**

**Key insight:** Handoffs, Skills, and Router are most efficient for single tasks (3 calls each). Subagents adds one extra call because results flow back through the main agent—this overhead provides centralized control.

The user repeats the same request in the same conversation.

| Pattern | Turn 2 calls | Total (both turns) | Best fit |
| --- | --- | --- | --- |
| **Subagents** | 4 | 8 | |
| **Handoffs** | 2 | 5 | ✅ |
| **Skills** | 2 | 5 | ✅ |
| **Router** | 3 | 6 | |

**4 calls again → 8 total**

- Subagents are **stateless by design**—each invocation follows the same flow
- The main agent maintains conversation context, but subagents start fresh each time
- This provides strong context isolation but repeats the full flow

**2 calls → 5 total**

- The coffee agent is **still active** from turn 1 (state persists)
- No handoff needed—agent directly calls `buy_coffee` tool (call 1)
- Agent responds to user (call 2)
- **Saves 1 call by skipping the handoff**

- The skill context is **already loaded** in conversation history
- No need to reload—agent directly calls `buy_coffee` tool (call 1)
- Agent responds to user (call 2)
- **Saves 1 call by reusing loaded skill**

**3 calls again → 6 total**

- Routers are **stateless**—each request requires an LLM routing call
- Turn 2: Router LLM call (1) → Milk agent calls buy\_coffee (2) → Milk agent responds (3)
- Can be optimized by wrapping as a tool in a stateful agent

**Key insight:** Stateful patterns (Handoffs, Skills) save 40-50% of calls on repeat requests. Subagents maintain consistent cost per request—this stateless design provides strong context isolation but at the cost of repeated model calls.

Each language agent/skill contains ~2000 tokens of documentation. All patterns can make parallel tool calls.

| Pattern | Model calls | Total tokens | Best fit |
| --- | --- | --- | --- |
| **Subagents** | 5 | ~9K | ✅ |
| **Handoffs** | 7+ | ~14K+ | |
| **Skills** | 3 | ~15K | |
| **Router** | 5 | ~9K | ✅ |

**5 calls, ~9K tokens**

Each subagent works in **isolation** with only its relevant context. Total: **9K tokens**.

**7+ calls, ~14K+ tokens**

Handoffs executes **sequentially**—can’t research all three languages in parallel. Growing conversation history adds overhead. Total: **~14K+ tokens**.

**3 calls, ~15K tokens**

After loading, **every subsequent call processes all 6K tokens of skill documentation**. Subagents processes 67% fewer tokens overall due to context isolation. Total: **15K tokens**.

Router uses an **LLM for routing**, then invokes agents in parallel. Similar to Subagents but with explicit routing step. Total: **9K tokens**.

**Key insight:** For multi-domain tasks, patterns with parallel execution (Subagents, Router) are most efficient. Skills has fewer calls but high token usage due to context accumulation. Handoffs is inefficient here—it must execute sequentially and can’t leverage parallel tool calling for consulting multiple domains simultaneously.

### ​ Summary

Here’s how patterns compare across all three scenarios:

| Pattern | One-shot | Repeat request | Multi-domain |
| --- | --- | --- | --- |
| **Subagents** | 4 calls | 8 calls (4+4) | 5 calls, 9K tokens |
| **Handoffs** | 3 calls | 5 calls (3+2) | 7+ calls, 14K+ tokens |
| **Skills** | 3 calls | 5 calls (3+2) | 3 calls, 15K tokens |
| **Router** | 3 calls | 6 calls (3+3) | 5 calls, 9K tokens |

**Choosing a pattern:**

| Optimize for | Subagents | Handoffs | Skills | Router |
| --- | --- | --- | --- | --- |
| Single requests | | ✅ | ✅ | ✅ |
| Repeat requests | | ✅ | ✅ | |
| Parallel execution | ✅ | | | ✅ |
| Large-context domains | ✅ | | | ✅ |
| Simple, focused tasks | | | ✅ | |

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Human-in-the-loop\\
\\
Previous Subagents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/knowledge-base)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/rag)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Build a RAG agent with LangChain Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/sql-agent)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Build a custom SQL agent Build a SQL agent

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/voice-agent)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Build a voice agent with LangChain Trace with LangChain (Python and JS/TS) LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent/subagents-personal-assistant)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Build a personal assistant with subagents Learn Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent/handoffs-customer-support)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Learn Build a voice agent with LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent/router-knowledge-base)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Build a multi-source knowledge base with routing Trace with LangChain (Python and JS/TS) Learn

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent/skills-sql-assistant)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Learn Build a voice agent with LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/agentic-rag)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Build a custom RAG agent with LangGraph Build a RAG agent with LangChain LangGraph Python SDK

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/sql-agent)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph CLI LangGraph Python SDK Build a custom RAG agent with LangGraph

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/component-architecture)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Component architecture

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/concepts/memory)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Contributing to documentation Data storage and privacy Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/concepts/context)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Contributing to documentation Trace with LangChain (Python and JS/TS) Custom instrumentation

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/functional-api)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Use the functional API Functional API overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/case-studies)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK LangSmith reference Install LangGraph

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/get-help)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Build a voice agent with LangChain LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integrations by component

Chat models

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

Chat models are language models that use a sequence of messages as inputs and return messages as outputs (as opposed to traditional, plaintext LLMs).

## ​ Featured models

**While these LangChain classes support the indicated advanced feature**, you may need to refer to provider-specific documentation to learn which hosted models or backends support the feature.

| Model | Tool calling | Structured output | JSON mode | Local | Multimodal |
| --- | --- | --- | --- | --- | --- |
| `ChatAnthropic` | ✅ | ✅ | ✅ | ❌ | ✅ |
| `ChatOpenAI` | ✅ | ✅ | ✅ | ❌ | ✅ |
| `AzureChatOpenAI` | ✅ | ✅ | ✅ | ❌ | ✅ |
| `ChatVertexAI` | ✅ | ✅ | ✅ | ❌ | ✅ |
| `ChatGoogleGenerativeAI` | ✅ | ✅ | ✅ | ❌ | ✅ |
| `ChatGroq` | ✅ | ✅ | ✅ | ❌ | ❌ |
| `ChatBedrock` | ✅ | ✅ | ❌ | ❌ | ❌ |
| `ChatAmazonNova` | ✅ | ❌ | ❌ | ❌ | ✅ |
| `ChatHuggingFace` | ✅ | ✅ | ❌ | ✅ | ❌ |
| `ChatOllama` | ✅ | ✅ | ✅ | ✅ | ❌ |
| `ChatWatsonx` | ✅ | ✅ | ✅ | ❌ | ✅ |
| `ChatXAI` | ✅ | ✅ | ❌ | ❌ | ❌ |
| `ChatNVIDIA` | ✅ | ✅ | ✅ | ✅ | ✅ |
| `ChatCohere` | ✅ | ✅ | ❌ | ❌ | ❌ |
| `ChatMistralAI` | ✅ | ✅ | ❌ | ❌ | ❌ |
| `ChatTogether` | ✅ | ✅ | ✅ | ❌ | ❌ |
| `ChatFireworks` | ✅ | ✅ | ✅ | ❌ | ❌ |
| `ChatLlamaCpp` | ✅ | ✅ | ❌ | ✅ | ❌ |
| `ChatDatabricks` | ✅ | ✅ | ❌ | ❌ | ❌ |
| `ChatPerplexity` | ❌ | ✅ | ✅ | ❌ | ✅ |

## ​ Chat Completions API

Certain model providers offer endpoints that are compatible with OpenAI’s (legacy) Chat Completions API. In such case, you can use `ChatOpenAI` with a custom `base_url` to connect to these endpoints. Note that features built on top of the Chat Completions API may not be fully supported by `ChatOpenAI`; in such cases, consider using a provider-specific class if available (e.g. `ChatLiteLLM` (community-maintained) for LiteLLM).

Example: OpenRouter

To use OpenRouter, you will need to sign up for an account and obtain an API key.

Copy

from langchain_openai import ChatOpenAI

model = ChatOpenAI(
model="...", # Specify a model available on OpenRouter
api_key="OPENROUTER_API_KEY",
base_url="https://openrouter.ai/api/v1",
)

Refer to the OpenRouter documentation for more details.

To capture reasoning tokens,

1. Switch imports from `langchain_openai` to `langchain_deepseek`
2. Use `ChatDeepSeek` instead of `ChatOpenAI`. You will need to change param `base_url` to `api_base`.
3. Adjust reasoning parameters as needed under `extra_body`, e.g.:

model = ChatDeepSeek(
model="...",
api_key="...",
api_base="https://openrouter.ai/api/v1",
extra_body={"reasoning": {"enabled": True}},
)

This is a known limitation with `ChatOpenAI` and will be addressed in a future release.

## ​ All chat models

**Abso** \\
\\
View guide **AI21 Labs** \\
\\
View guide **AI/ML API** \\
\\
View guide **Alibaba Cloud PAI EAS** \\
\\
View guide **Amazon Nova** \\
\\
View guide **Anthropic** \\
\\
View guide **AzureAIChatCompletionsModel** \\
\\
View guide **Azure OpenAI** \\
\\
View guide **Azure ML Endpoint** \\
\\
View guide **Baichuan Chat** \\
\\
View guide **Baidu Qianfan** \\
\\
View guide **Baseten** \\
\\
View guide **AWS Bedrock** \\
\\
View guide **Cerebras** \\
\\
View guide **CloudflareWorkersAI** \\
\\
View guide **Cohere** \\
\\
View guide **ContextualAI** \\
\\
View guide **Coze Chat** \\
\\
View guide **Dappier AI** \\
\\
View guide **Databricks** \\
\\
View guide **DeepInfra** \\
\\
View guide **DeepSeek** \\
\\
View guide **Eden AI** \\
\\
View guide **EverlyAI** \\
\\
View guide **Featherless AI** \\
\\
View guide **Fireworks** \\
\\
View guide **ChatFriendli** \\
\\
View guide **Google Gemini** \\
\\
View guide **Google Cloud Vertex AI** \\
\\
View guide **GPTRouter** \\
\\
View guide **DigitalOcean Gradient** \\
\\
View guide **GreenNode** \\
\\
View guide **Groq** \\
\\
View guide **ChatHuggingFace** \\
\\
View guide **IBM watsonx.ai** \\
\\
View guide **JinaChat** \\
\\
View guide **Kinetica** \\
\\
View guide **Konko** \\
\\
View guide **LiteLLM** \\
\\
View guide **Llama 2 Chat** \\
\\
View guide **Llama API** \\
\\
View guide **LlamaEdge** \\
\\
View guide **Llama.cpp** \\
\\
View guide **maritalk** \\
\\
View guide **MiniMax** \\
\\
View guide **MistralAI** \\
\\
View guide **MLX** \\
\\
View guide **ModelScope** \\
\\
View guide **Moonshot** \\
\\
View guide **Naver** \\
\\
View guide **Nebius** \\
\\
View guide **Netmind** \\
\\
View guide **NVIDIA AI Endpoints** \\
\\
View guide **ChatOCIModelDeployment** \\
\\
View guide **OCIGenAI** \\
\\
View guide **ChatOctoAI** \\
\\
View guide **Ollama** \\
\\
View guide **OpenAI** \\
\\
View guide **Outlines** \\
\\
View guide **Perplexity** \\
\\
View guide **Pipeshift** \\
\\
View guide **ChatPredictionGuard** \\
\\
View guide **PremAI** \\
\\
View guide **PromptLayer ChatOpenAI** \\
\\
View guide **Qwen QwQ** \\
\\
View guide **Qwen** \\
\\
View guide **Reka** \\
\\
View guide **RunPod Chat Model** \\
\\
View guide **SambaNova** \\
\\
View guide **ChatSeekrFlow** \\
\\
View guide **Snowflake Cortex** \\
\\
View guide **SparkLLM Chat** \\
\\
View guide **Nebula (Symbl.ai)** \\
\\
View guide **Tencent Hunyuan** \\
\\
View guide **Together** \\
\\
View guide **Tongyi Qwen** \\
\\
View guide **Upstage** \\
\\
View guide **vLLM Chat** \\
\\
View guide **Volc Engine Maas** \\
\\
View guide **ChatWriter** \\
\\
View guide **xAI** \\
\\
View guide **Xinference** \\
\\
View guide **YandexGPT** \\
\\
View guide **ChatYI** \\
\\
View guide **Yuan2.0** \\
\\
View guide **ZHIPU AI** \\
\\
View guide

If you’d like to contribute an integration, see Contributing integrations.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

ChatGroq\\
\\
Previous Tools and toolkits\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/all_providers

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

All integration providers

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

Browse the complete collection of integrations available for Python. LangChain Python offers the most extensive ecosystem with 1000+ integrations across LLMs, chat models, retrievers, vector stores, document loaders, and more.

## ​ Providers

**Abso** \\
\\
Custom AI integration platform for enterprise workflows. **Acreom** \\
\\
Knowledge management platform with AI-powered organization. **ActiveLoop DeepLake** \\
\\
Vector database for AI applications with deep learning focus. **Ads4GPTs** \\
\\
Advertising platform for GPT applications and AI services. **AG-UI Protocol** \\
\\
Open event-based protocol for connecting LangGraph agents to any frontend. **AgentQL** \\
\\
Web scraping with natural language queries. **AI21** \\
\\
AI21 Labs’ Jurassic models for text generation. **AIM Tracking** \\
\\
Experiment tracking and management platform. **AI/ML API** \\
\\
Unified API for multiple AI and ML services. **AI Network** \\
\\
Decentralized AI computing network platform. **Airbyte** \\
\\
Data integration platform for ETL and ELT pipelines. **Airtable** \\
\\
Cloud-based spreadsheet and database platform. **Alchemy** \\
\\
Blockchain development platform and APIs. **Aleph Alpha** \\
\\
European AI company’s multilingual language models. **Alibaba Cloud** \\
\\
Alibaba’s cloud computing and AI services. **AnalyticDB** \\
\\
Alibaba Cloud’s real-time analytics database. **Anchor Browser** \\
\\
Browser automation and web scraping tools. **Annoy** \\
\\
Approximate nearest neighbors search library. **Anthropic** \\
\\
Claude models for advanced reasoning and conversation. **Anyscale** \\
\\
Distributed computing platform for ML workloads. **Apache Doris** \\
\\
Real-time analytical database management system. **Apache** \\
\\
Apache Software Foundation tools and libraries. **Apify** \\
\\
Web scraping and automation platform. **Apple** \\
\\
Apple’s machine learning and AI frameworks. **ArangoDB** \\
\\
Multi-model database with graph capabilities. **Arcee** \\
\\
Domain-specific language model training platform. **ArcGIS** \\
\\
Geographic information system platform. **Argilla** \\
\\
Data labeling and annotation platform for NLP. **Arize** \\
\\
ML observability and performance monitoring. **Arthur Tracking** \\
\\
AI model monitoring and governance platform. **arXiv** \\
\\
Academic paper repository and search platform. **Ascend** \\
\\
Data engineering and pipeline automation platform. **Ask News** \\
\\
Real-time news search and analysis API. **AssemblyAI** \\
\\
Speech-to-text and audio intelligence API. **assistant-ui** \\
\\
React framework for building AI chat interfaces with streaming support and LangGraph integration. **AstraDB** \\
\\
DataStax Astra DB vector database platform. **Atlas** \\
\\
Data visualization and exploration platform. **AwaDB** \\
\\
Vector database for AI and ML applications. **AWS** \\
\\
Amazon Web Services cloud platform and AI services. **AZLyrics** \\
\\
Song lyrics database and search platform. **Azure AI** \\
\\
Microsoft Azure AI and cognitive services. **BAAI** \\
\\
Beijing Academy of AI research and models. **Bagel** \\
\\
Vector database and semantic search platform. **BagelDB** \\
\\
Multi-modal AI database and storage system. **Baichuan** \\
\\
Chinese language model from Baichuan AI. **Baidu** \\
\\
Baidu’s AI services and language models. **BananaDev** \\
\\
Serverless GPU infrastructure for ML models. **Baseten** \\
\\
ML model deployment and serving platform. **Beam** \\
\\
Serverless GPU computing platform. **Beautiful Soup** \\
\\
HTML and XML parsing library for web scraping. **BibTeX** \\
\\
Bibliography management and citation format. **Bilibili** \\
\\
Chinese video sharing platform integration. **Bittensor** \\
\\
Decentralized AI network and incentive protocol. **Blackboard** \\
\\
Educational technology and learning management. **Bodo DataFrames** \\
\\
High-performance analytics and data processing. **BookendAI** \\
\\
AI-powered reading and research assistant. **Box** \\
\\
Cloud content management and collaboration. **Brave Search** \\
\\
Privacy-focused search engine API. **Breebs** \\
\\
AI knowledge management and retrieval platform. **Brightdata** \\
\\
Web data platform and proxy services. **Browserbase** \\
\\
Headless browser automation platform. **Browserless** \\
\\
Serverless browser automation service. **ByteDance** \\
\\
ByteDance’s AI models and services. **Cassandra** \\
\\
Distributed NoSQL database management system. **Cerebras** \\
\\
AI compute platform with specialized processors. **CerebriumAI** \\
\\
Serverless GPU platform for AI applications. **Chaindesk** \\
\\
No-code AI chatbot and automation platform. **Chroma** \\
\\
Open-source embedding database for AI apps. **Clarifai** \\
\\
Computer vision and AI model platform. **ClearML Tracking** \\
\\
ML experiment tracking and automation. **CopilotKit** \\
\\
React framework with pre-built UI components for AI copilots. **ClickHouse** \\
\\
Fast columnar database for analytics. **ClickUp** \\
\\
Project management and productivity platform. **Cloudflare** \\
\\
Web infrastructure and security services. **Clova** \\
\\
Naver’s AI assistant and NLP platform. **CnosDB** \\
\\
Time series database for IoT and analytics. **Cognee** \\
\\
Memory layer for AI applications and agents. **CogniSwitch** \\
\\
AI knowledge management and retrieval system. **Cohere** \\
\\
Language AI platform for enterprise applications. **College Confidential** \\
\\
College admissions and education platform. **Comet Tracking** \\
\\
ML experiment tracking and model management. **Confident** \\
\\
AI observability and monitoring platform. **Confluence** \\
\\
Team collaboration and documentation platform. **Connery** \\
\\
Plugin system for AI agents and applications. **Context** \\
\\
Context management for AI applications. **Contextual** \\
\\
Contextual AI and language understanding. **Couchbase** \\
\\
NoSQL cloud database platform. **Coze** \\
\\
Conversational AI platform and chatbot builder. **CrateDB** \\
\\
Distributed SQL database for machine data. **CTransformers** \\
\\
Python bindings for transformer models in C/C++. **CTranslate2** \\
\\
Fast inference engine for Transformer models. **Cube** \\
\\
Semantic layer for building data applications. **Dappier** \\
\\
Real-time AI data platform and API. **DashVector** \\
\\
Alibaba Cloud’s vector database service. **Databricks** \\
\\
Unified analytics platform for big data and ML. **Datadog** \\
\\
Monitoring and analytics platform for applications. **Datadog Logs** \\
\\
Log management and analysis platform. **DataForSEO** \\
\\
SEO and SERP data API platform. **DataHerald** \\
\\
Natural language to SQL query platform. **Daytona** \\
\\
Secure and elastic infrastructure for running your AI-generated code. **Dedoc** \\
\\
Document analysis and structure detection. **DeepInfra** \\
\\
Serverless inference for deep learning models. **DeepLake** \\
\\
Vector database for deep learning applications. **DeepSeek** \\
\\
Advanced reasoning and coding AI models. **DeepSparse** \\
\\
Inference runtime for sparse neural networks. **Dell** \\
\\
Dell Technologies AI and computing solutions. **Diffbot** \\
\\
Web data extraction and knowledge graph. **Dingo** \\
\\
Distributed vector database system. **Discord** \\
\\
Communication platform integration and bots. **Discord Shikenso** \\
\\
Discord analytics and moderation tools. **DocArray** \\
\\
Data structure for multimodal AI applications. **Docling** \\
\\
Document processing and AI integration. **Doctran** \\
\\
Document transformation and processing. **Docugami** \\
\\
Document AI and semantic processing. **Docusaurus** \\
\\
Documentation website generator and platform. **Dria** \\
\\
Decentralized knowledge retrieval network. **Dropbox** \\
\\
Cloud storage and file sharing platform. **DuckDB** \\
\\
In-process SQL OLAP database management system. **DuckDuckGo Search** \\
\\
Privacy-focused search engine integration. **E2B** \\
\\
Cloud development environment platform. **EdenAI** \\
\\
Unified API for multiple AI services. **Elasticsearch** \\
\\
Distributed search and analytics engine. **ElevenLabs** \\
\\
AI voice synthesis and speech platform. **EmbedChain** \\
\\
Framework for creating RAG applications. **Epsilla** \\
\\
Vector database for AI and ML applications. **Etherscan** \\
\\
Ethereum blockchain explorer and analytics. **EverlyAI** \\
\\
Serverless AI inference platform. **Evernote** \\
\\
Note-taking and organization platform. **Exa Search** \\
\\
AI-powered search engine for developers. **Facebook** \\
\\
Meta’s social platform integration and APIs. **FalkorDB** \\
\\
Graph database with ultra-low latency. **Fauna** \\
\\
Serverless, globally distributed database. **Featherless AI** \\
\\
Fast and efficient AI model serving. **Fiddler** \\
\\
AI observability and monitoring platform. **Figma** \\
\\
Design collaboration and prototyping platform. **FireCrawl** \\
\\
Web scraping and crawling API service. **Fireworks** \\
\\
Fast inference platform for open-source models. **Flyte** \\
\\
Workflow orchestration for ML and data processing. **FMP Data** \\
\\
Financial market data and analytics API. **ForefrontAI** \\
\\
Fine-tuning platform for language models. **Friendli** \\
\\
Optimized serving engine for AI models. **Galaxia** \\
\\
Prompt-driven engineering assistant. **Gel** \\
\\
Knowledge extraction and NLP platform. **GeoPandas** \\
\\
Geographic data analysis with Python. **Git** \\
\\
Version control system integration. **GitBook** \\
\\
Documentation platform and knowledge base. **GitHub** \\
\\
Code hosting and collaboration platform. **GitLab** \\
\\
DevOps platform and code repository. **GOAT** \\
\\
Tool use framework for AI agents. **Golden** \\
\\
Knowledge graph and data platform. **Google** \\
\\
Google’s AI services and cloud platform. **Google Serper** \\
\\
Google Search API service. **GooseAI** \\
\\
Fully managed NLP-as-a-Service platform. **GPT4All** \\
\\
Open-source LLM ecosystem for local deployment. **Gradient** \\
\\
AI model training and deployment platform. **DigitalOcean Gradient AI Platform** \\
\\
Single endpoint to multiple LLMs via serverless inference. **Graph RAG** \\
\\
Graph-based retrieval augmented generation. **GraphSignal** \\
\\
AI observability and monitoring platform. **GreenNode** \\
\\
Sustainable AI computing platform. **GROBID** \\
\\
Machine learning library for bibliographic data. **Groq** \\
\\
Ultra-fast inference with specialized hardware. **Gutenberg** \\
\\
Project Gutenberg digital library access. **Hacker News** \\
\\
Tech news and discussion platform. **Hazy Research** \\
\\
Machine learning research and tools. **Helicone** \\
\\
LLM observability and monitoring platform. **Hologres** \\
\\
Real-time interactive analytics service. **HTML2Text** \\
\\
HTML to plain text conversion utility. **Huawei** \\
\\
Huawei Cloud AI services and models. **Hugging Face** \\
\\
Open platform for ML models and datasets. **HyperBrowser** \\
\\
Web automation and scraping platform. **IBM** \\
\\
IBM Watson AI and enterprise solutions. **IEIT Systems** \\
\\
Enterprise AI and system integration. **iFixit** \\
\\
Repair guides and technical documentation. **iFlytek** \\
\\
Chinese speech and language AI platform. **IMSDb** \\
\\
Internet Movie Script Database access. **InfinispanVS** \\
\\
Distributed cache and data grid platform. **Infinity** \\
\\
High-performance embedding inference server. **Infino** \\
\\
Observability and monitoring platform. **Intel** \\
\\
Intel’s AI optimization tools and libraries. **Isaacus** \\
\\
Legal AI models, apps, and data. **IUGU** \\
\\
Brazilian payment processing platform. **Jaguar** \\
\\
Vector database and search platform. **Javelin AI Gateway** \\
\\
AI model gateway and management platform. **Jenkins** \\
\\
Automation server and CI/CD platform. **Jina** \\
\\
Neural search framework and cloud platform. **John Snow Labs** \\
\\
Enterprise NLP and healthcare AI platform. **Joplin** \\
\\
Open-source note taking and organization. **KDB.AI** \\
\\
Time-series vector database platform. **Kinetica** \\
\\
Real-time analytics and database platform. **KoboldAI** \\
\\
Browser-based AI writing assistant. **Konko** \\
\\
Generative AI platform and model hosting. **KoNLPy** \\
\\
Korean natural language processing toolkit. **Kuzu** \\
\\
Embedded graph database management system. **Label Studio** \\
\\
Data labeling and annotation platform. **LakeFS** \\
\\
Git-like version control for data lakes. **LanceDB** \\
\\
Developer-friendly embedded vector database. **LangChain Decorators** \\
\\
Syntactic sugar and utilities for LangChain. **LangFair** \\
\\
Bias testing framework for language models. **LangFuse** \\
\\
LLM engineering platform and observability. **Lantern** \\
\\
PostgreSQL vector database extension. **Lindorm** \\
\\
Alibaba Cloud’s multi-model database service. **LinkUp** \\
\\
Real-time job market data and search. **LiteLLM** \\
\\
Unified interface for 100+ LLM APIs. **LlamaIndex** \\
\\
Data framework for LLM applications. **LlamaCPP** \\
\\
Port of Meta’s LLaMA model in C/C++. **LlamaEdge** \\
\\
Edge computing platform for LLaMA models. **LlamaFile** \\
\\
Single-file executable for running LLMs. **LLMonitor** \\
\\
Observability platform for LLM applications. **LocalAI** \\
\\
Self-hosted OpenAI-compatible API server. **Log10** \\
\\
LLM data management and observability. **MariaDB** \\
\\
Open-source relational database management. **MaritALK** \\
\\
Brazilian Portuguese language model. **Marqo** \\
\\
End-to-end vector search engine. **MediaWiki Dump** \\
\\
Wikipedia and MediaWiki data processing. **Meilisearch** \\
\\
Lightning-fast search engine platform. **Memcached** \\
\\
Distributed memory caching system. **Memgraph** \\
\\
Real-time graph database platform. **Metal** \\
\\
Managed vector search and retrieval. **Microsoft** \\
\\
Microsoft Azure AI and enterprise services. **Milvus** \\
\\
Open-source vector database for AI applications. **MindsDB** \\
\\
AI layer for databases and data platforms. **Minimax** \\
\\
Chinese AI company’s language models. **MistralAI** \\
\\
Efficient open-source language models. **MLflow** \\
\\
ML lifecycle management platform. **MLflow Tracking** \\
\\
Experiment tracking and model registry. **MLX** \\
\\
Apple’s machine learning framework. **Modal** \\
\\
Serverless cloud computing for data science. **ModelScope** \\
\\
Alibaba’s open-source model hub. **Modern Treasury** \\
\\
Payment operations and treasury management. **Momento** \\
\\
Serverless cache and vector index. **MongoDB** \\
\\
Document-based NoSQL database platform. **MongoDB Atlas** \\
\\
Cloud-hosted MongoDB with vector search. **MotherDuck** \\
\\
Serverless analytics with DuckDB in the cloud. **Motorhead** \\
\\
Long-term memory for AI conversations. **MyScale** \\
\\
SQL-compatible vector database platform. **Naver** \\
\\
Naver’s AI services and language models. **Nebius** \\
\\
AI cloud platform and infrastructure. **Neo4j** \\
\\
Native graph database and analytics platform. **NetMind** \\
\\
Decentralized AI computing network. **Nimble** \\
\\
Web intelligence and data extraction. **NLP Cloud** \\
\\
Production-ready NLP API platform. **Nomic** \\
\\
Open-source embedding models and tools. **Notion** \\
\\
All-in-one workspace and collaboration platform. **Nuclia** \\
\\
AI-powered search and understanding platform. **NVIDIA** \\
\\
NVIDIA’s AI computing platform and models. **Obsidian** \\
\\
Connected note-taking and knowledge management. **OceanBase** \\
\\
Distributed relational database system. **OCI** \\
\\
Oracle Cloud Infrastructure AI services. **OctoAI** \\
\\
Efficient AI compute and model serving. **Ollama** \\
\\
Run Large Language Models (LLMs) locally. **Ontotext GraphDB** \\
\\
RDF database and semantic graph platform. **OpenAI** \\
\\
GPT models and comprehensive AI platform. **OpenDataLoader PDF** \\
\\
Safe, Open, High-Performance — PDF for AI **OpenGradient** \\
\\
AI model training and fine-tuning platform. **OpenLLM** \\
\\
Operating LLMs in production environment. **Open Agent Spec (PyAgentSpec)** \\
\\
Framework-agnostic declarative language by Oracle for defining agentic systems. Define agents and workflows in a portable JSON/YAML format that can be executed across different runtimes. **OpenSearch** \\
\\
Distributed search and analytics suite. **OpenWeatherMap** \\
\\
Weather data and forecasting API. **Oracle AI** \\
\\
Oracle’s AI and machine learning services. **Outline** \\
\\
Team knowledge base and wiki platform. **Outlines** \\
\\
Structured generation for language models. **Oxylabs** \\
\\
Web scraping and proxy services. **Pandas** \\
\\
Data analysis and manipulation library. **Parallel** \\
\\
AI-powered web search and content extraction for LLMs. **Perigon** \\
\\
Real-time news and media monitoring. **Permit** \\
\\
Authorization and access control platform. **Perplexity** \\
\\
AI-powered search and reasoning engine. **Petals** \\
\\
Distributed inference for Large Language Models. **PlainId** \\
\\
Authorization and access control platform. **PG Embedding** \\
\\
PostgreSQL vector embedding extensions. **pgvector** \\
\\
Vector similarity search for PostgreSQL. **Pinecone** \\
\\
Managed vector database for ML applications. **PipelineAI** \\
\\
ML pipeline and model deployment platform. **Pipeshift** \\
\\
AI-powered content moderation platform. **PolarisAIDataInsight** \\
\\
Document-loaders for various file formats. **Portkey** \\
\\
AI gateway and observability platform. **Predibase** \\
\\
Fine-tuning platform for Large Language Models. **PredictionGuard** \\
\\
AI model security and compliance platform. **PreMAI** \\
\\
AI platform for model deployment and management. **Privy** \\
\\
Wallets and payments for AI agents. **Prolog** \\
\\
Logic programming language integration. **PromptLayer** \\
\\
Prompt engineering and observability platform. **Psychic** \\
\\
Universal API for SaaS integrations. **PubMed** \\
\\
Biomedical literature database access. **PygmalionAI** \\
\\
Conversational AI model platform. **PyMuPDF4LLM** \\
\\
PDF processing optimized for LLM ingestion. **Qdrant** \\
\\
Vector similarity search engine. **Ragatouille** \\
\\
RAG toolkit with ColBERT indexing. **Rank BM25** \\
\\
BM25 ranking algorithm implementation. **Ray Serve** \\
\\
Scalable model serving framework. **Rebuff** \\
\\
Prompt injection detection and prevention. **Reddit** \\
\\
Social media platform integration and APIs. **Redis** \\
\\
In-memory data structure store and cache. **Remembrall** \\
\\
AI memory and context management. **Replicate** \\
\\
Cloud platform for running ML models. **Roam** \\
\\
Research and note-taking platform. **Robocorp** \\
\\
Python automation and RPA platform. **Rockset** \\
\\
Real-time analytics database platform. **RunPod** \\
\\
GPU cloud platform for AI workloads. **Salesforce** \\
\\
CRM platform and business automation. **SambaNova** \\
\\
AI platform with specialized hardware. **SAP** \\
\\
Enterprise software and AI solutions. **ScrapeGraph** \\
\\
AI-powered web scraping framework. **Scrapeless** \\
\\
Web scraping API and proxy service. **SearchAPI** \\
\\
Real-time search engine results API. **SearX** \\
\\
Privacy-respecting metasearch engine. **SemaDB** \\
\\
Vector database for semantic search. **SerpApi** \\
\\
Google Search results scraping API. **Shale Protocol** \\
\\
Decentralized AI inference protocol. **SingleStore** \\
\\
Distributed database with vector capabilities. **scikit-learn** \\
\\
Machine learning library for Python. **Slack** \\
\\
Business communication and collaboration. **Snowflake** \\
\\
Cloud data platform and analytics. **spaCy** \\
\\
Industrial-strength NLP library. **Spark** \\
\\
Unified analytics engine for big data. **SparkLLM** \\
\\
iFlytek’s multilingual language model. **Spreedly** \\
\\
Payment orchestration platform. **SQLite** \\
\\
Embedded relational database engine. **StackExchange** \\
\\
Q&A platform network integration. **StarRocks** \\
\\
High-performance analytical database. **StochasticAI** \\
\\
GPU cloud platform for ML acceleration. **Streamlit** \\
\\
Web app framework for data science. **Stripe** \\
\\
Online payment processing platform. **Supabase** \\
\\
Open-source Firebase alternative. **SurrealDB** \\
\\
Multi-model database for modern applications. **Symbl.ai Nebula** \\
\\
Conversation intelligence platform. **Tableau** \\
\\
Data visualization and business intelligence. **Taiga** \\
\\
Project management platform for agile teams. **Tair** \\
\\
Alibaba Cloud’s in-memory database. **Tavily** \\
\\
AI-optimized search API for applications. **Telegram** \\
\\
Messaging platform and bot integration. **Tencent** \\
\\
Tencent Cloud AI services and models. **TensorFlow Datasets** \\
\\
Collection of ready-to-use datasets. **TensorLake** \\
\\
Data infrastructure for ML applications. **Teradata** \\
\\
Autonomous AI platform with integrated vector search. **TiDB** \\
\\
Distributed SQL database platform. **TigerGraph** \\
\\
Scalable graph database and analytics. **Tigris** \\
\\
Globally distributed database platform. **Tilores** \\
\\
Entity resolution and data matching. **Timbr** \\
\\
Semantic layer for data integration and querying. **Together** \\
\\
Fast inference for open-source models. **ToMarkdown** \\
\\
HTML to Markdown conversion utility. **Toolbox LangChain** \\
\\
Extended toolkit for LangChain applications. **Transwarp** \\
\\
Big data platform and analytics suite. **Trello** \\
\\
Visual project management and collaboration. **Trubrics** \\
\\
LLM evaluation and analytics platform. **TrueFoundry** \\
\\
ML platform for model deployment. **TrueLens** \\
\\
Evaluation framework for LLM applications. **Twitter** \\
\\
Social media platform integration. **Typesense** \\
\\
Fast and typo-tolerant search engine. **UnDatasIO** \\
\\
Data extraction and processing platform. **Unstructured** \\
\\
Document processing and data extraction. **Upstage** \\
\\
Document AI and OCR platform. **Upstash** \\
\\
Serverless data platform for Redis and Kafka. **UpTrain** \\
\\
ML observability and evaluation platform. **USearch** \\
\\
Single-file vector search engine. **Valthera** \\
\\
AI platform for healthcare applications. **Valyu** \\
\\
AI-powered data analysis platform. **VDMS** \\
\\
Visual data management system. **Vearch** \\
\\
Distributed vector search engine. **Vectara** \\
\\
Neural search platform with built-in understanding. **Vectorize** \\
\\
Vector database and semantic search. **Vespa** \\
\\
Big data serving engine for vector search. **VLite** \\
\\
Simple vector database for embeddings. **VoyageAI** \\
\\
Embedding models and semantic search. **Weights & Biases** \\
\\
ML experiment tracking and collaboration. **Weights & Biases Tracking** \\
\\
Experiment tracking and model management. **Weights & Biases Tracing** \\
\\
LLM tracing and observability. **Weather** \\
\\
Weather data and forecasting services. **Weaviate** \\
\\
Open-source vector database with GraphQL. **WhatsApp** \\
\\
Messaging platform integration and automation. **WhyLabs Profiling** \\
\\
AI observability and data monitoring. **Wikipedia** \\
\\
Wikipedia content access and search. **Wolfram Alpha** \\
\\
Computational knowledge engine. **WRITER** \\
\\
Enterprise models and tools for building, activating, and supervising AI agents. **XAI** \\
\\
xAI’s Grok models for conversational AI. **Xata** \\
\\
Serverless database with vector search. **Xinference** \\
\\
Distributed inference framework for LLMs. **Yahoo** \\
\\
Yahoo services and data integration. **Yandex** \\
\\
Yandex AI services and language models. **YDB** \\
\\
Yandex Database distributed storage system. **YeagerAI** \\
\\
AI agent framework and development platform. **Yellowbrick** \\
\\
Data warehouse and analytics platform. **Yi** \\
\\
01.AI’s bilingual language models. **You** \\
\\
You.com search engine and AI platform. **YouTube** \\
\\
Video platform integration and content access. **Zep** \\
\\
Long-term memory for AI assistants. **ZeusDB** \\
\\
High-performance vector database. **ZhipuAI** \\
\\
ChatGLM and other Chinese language models. **Zilliz** \\
\\
Managed Milvus vector database service. **Zotero** \\
\\
Reference management and research tool.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangChain integrations packages\\
\\
Previous OpenAI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/middleware

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Middleware

Overview

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- The agent loop
- Additional resources

Middleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following:

- Tracking agent behavior with logging, analytics, and debugging.
- Transforming prompts, tool selection, and output formatting.
- Adding retries, fallbacks, and early termination logic.
- Applying rate limits, guardrails, and PII detection.

Add middleware by passing them to `create_agent`:

Copy

from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware

agent = create_agent(
model="gpt-4o",
tools=[...],
middleware=[\
SummarizationMiddleware(...),\
HumanInTheLoopMiddleware(...)\
],
)

## ​ The agent loop

The core agent loop involves calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:!Core agent loop diagramMiddleware exposes hooks before and after each of those steps:!Middleware flow diagram

## ​ Additional resources

**Built-in middleware** \\
\\
Explore built-in middleware for common use cases. **Custom middleware** \\
\\
Build your own middleware with hooks and decorators. **Middleware API reference** \\
\\
Complete API reference for middleware. **Testing agents** \\
\\
Test your agents with LangSmith.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Structured output\\
\\
Previous Built-in middleware\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/use-graph-api

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Graph API

Use the graph API

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Choosing APIs
- Graph API
- Use the graph API
- Functional API

- Runtime

On this page

- Setup
- Define and update state
- Define state
- Update state
- Process state updates with reducers
- MessagesState
- Bypass reducers with Overwrite
- Define input and output schemas
- Pass private state between nodes
- Use Pydantic models for graph state
- Add runtime configuration
- Add retry policies
- Add node caching
- Create a sequence of steps
- Create branches
- Run graph nodes in parallel
- Defer node execution
- Conditional branching
- Map-Reduce and the Send API
- Create and control loops
- Impose a recursion limit
- Async
- Combine control flow and state updates with Command
- Navigate to a node in a parent graph
- Use inside tools
- Visualize your graph
- Mermaid
- PNG

This guide demonstrates the basics of LangGraph’s Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph’s control features, including the Send API for map-reduce workflows and the Command API for combining state updates with “hops” across nodes.

## ​ Setup

Install `langgraph`:

pip

uv

Copy

pip install -U langgraph

**Set up LangSmith for better debugging**Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the docs.

## ​ Define and update state

Here we show how to define and update state in LangGraph. We will demonstrate:

1. How to use state to define a graph’s schema
2. How to use reducers to control how state updates are processed.

### ​ Define state

State in LangGraph can be a `TypedDict`, `Pydantic` model, or dataclass. Below we will use `TypedDict`. See this section for detail on using Pydantic.By default, graphs will have the same input and output schema, and the state determines that schema. See this section for how to define distinct input and output schemas.Let’s consider a simple example using messages. This represents a versatile formulation of state for many LLM applications. See our concepts page for more detail.

from langchain.messages import AnyMessage
from typing_extensions import TypedDict

class State(TypedDict):
messages: list[AnyMessage]
extra_field: int

This state tracks a list of message objects, as well as an extra integer field.

### ​ Update state

Let’s build an example graph with a single node. Our node is just a Python function that reads our graph’s state and makes updates to it. The first argument to this function will always be the state:

from langchain.messages import AIMessage

def node(state: State):
messages = state["messages"]
new_message = AIMessage("Hello!")
return {"messages": messages + [new_message], "extra_field": 10}

This node simply appends a message to our message list, and populates an extra field.

Nodes should return updates to the state directly, instead of mutating the state.

Let’s next define a simple graph containing this node. We use `StateGraph` to define a graph that operates on this state. We then use `add_node` populate our graph.

from langgraph.graph import StateGraph

builder = StateGraph(State)
builder.add_node(node)
builder.set_entry_point("node")
graph = builder.compile()

LangGraph provides built-in utilities for visualizing your graph. Let’s inspect our graph. See this section for detail on visualization.

from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))

from langchain.messages import HumanMessage

result = graph.invoke({"messages": [HumanMessage("Hi")]})
result

{'messages': [HumanMessage(content='Hi'), AIMessage(content='Hello!')], 'extra_field': 10}

Note that:

- We kicked off invocation by updating a single key of the state.
- We receive the entire state in the invocation result.

For convenience, we frequently inspect the content of message objects via pretty-print:

for message in result["messages"]:
message.pretty_print()

================================ Human Message ================================

Hi
================================== Ai Message ==================================

Hello!

### ​ Process state updates with reducers

Each key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.For `TypedDict` state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.In the earlier example, our node updated the `"messages"` key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:

from typing_extensions import Annotated

def add(left, right):
"""Can also import `add` from the `operator` built-in."""
return left + right

class State(TypedDict):
messages: Annotated[list[AnyMessage], add]
extra_field: int

Now our node can be simplified:

def node(state: State):
new_message = AIMessage("Hello!")
return {"messages": [new_message], "extra_field": 10}

from langgraph.graph import START

graph = StateGraph(State).add_node(node).add_edge(START, "node").compile()

result = graph.invoke({"messages": [HumanMessage("Hi")]})

#### ​ MessagesState

In practice, there are additional considerations for updating lists of messages:

- We may wish to update an existing message in the state.
- We may want to accept short-hands for message formats, such as OpenAI format.

LangGraph includes a built-in reducer `add_messages` that handles these considerations:

from langgraph.graph.message import add_messages

class State(TypedDict):
messages: Annotated[list[AnyMessage], add_messages]
extra_field: int

graph = StateGraph(State).add_node(node).set_entry_point("node").compile()

input_message = {"role": "user", "content": "Hi"}

result = graph.invoke({"messages": [input_message]})

This is a versatile representation of state for applications involving chat models. LangGraph includes a pre-built `MessagesState` for convenience, so that we can have:

from langgraph.graph import MessagesState

class State(MessagesState):
extra_field: int

### ​ Bypass reducers with `Overwrite`

In some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the `Overwrite` type for this purpose. When a node returns a value wrapped with `Overwrite`, the reducer is bypassed and the channel is set directly to that value.This is useful when you want to reset or replace accumulated state rather than merge it with existing values.

from langgraph.graph import StateGraph, START, END
from langgraph.types import Overwrite
from typing_extensions import Annotated, TypedDict
import operator

class State(TypedDict):
messages: Annotated[list, operator.add]

def add_message(state: State):
return {"messages": ["first message"]}

def replace_messages(state: State):
# Bypass the reducer and replace the entire messages list
return {"messages": Overwrite(["replacement message"])}

builder = StateGraph(State)
builder.add_node("add_message", add_message)
builder.add_node("replace_messages", replace_messages)
builder.add_edge(START, "add_message")
builder.add_edge("add_message", "replace_messages")
builder.add_edge("replace_messages", END)

graph = builder.compile()

result = graph.invoke({"messages": ["initial"]})
print(result["messages"])

['replacement message']

You can also use JSON format with the special key `"__overwrite__"`:

def replace_messages(state: State):
return {"messages": {"__overwrite__": ["replacement message"]}}

When nodes execute in parallel, only one node can use `Overwrite` on the same state key in a given super-step. If multiple nodes attempt to overwrite the same key in the same super-step, an `InvalidUpdateError` will be raised.

### ​ Define input and output schemas

By default, `StateGraph` operates with a single schema, and all nodes are expected to communicate using that schema. However, it’s also possible to define distinct input and output schemas for a graph.When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.Below, we’ll see how to define distinct input and output schema.

from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

# Define the schema for the input
class InputState(TypedDict):
question: str

# Define the schema for the output
class OutputState(TypedDict):
answer: str

# Define the overall schema, combining both input and output
class OverallState(InputState, OutputState):
pass

# Define the node that processes the input and generates an answer
def answer_node(state: InputState):
# Example answer and an extra key
return {"answer": "bye", "question": state["question"]}

# Build the graph with input and output schemas specified
builder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)
builder.add_node(answer_node) # Add the answer node
builder.add_edge(START, "answer_node") # Define the starting edge
builder.add_edge("answer_node", END) # Define the ending edge
graph = builder.compile() # Compile the graph

# Invoke the graph with an input and print the result
print(graph.invoke({"question": "hi"}))

{'answer': 'bye'}

Notice that the output of invoke only includes the output schema.

### ​ Pass private state between nodes

In some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn’t need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.Below, we’ll create an example sequential graph consisting of three nodes (node\_1, node\_2 and node\_3), where private data is passed between the first two steps (node\_1 and node\_2), while the third step (node\_3) only has access to the public overall state.

# The overall state of the graph (this is the public state shared across nodes)
class OverallState(TypedDict):
a: str

# Output from node_1 contains private data that is not part of the overall state
class Node1Output(TypedDict):
private_data: str

# The private data is only shared between node_1 and node_2

output = {"private_data": "set by node_1"}
print(f"Entered node `node_1`:\n\tInput: {state}.\n\tReturned: {output}")
return output

# Node 2 input only requests the private data available after node_1
class Node2Input(TypedDict):
private_data: str

output = {"a": "set by node_2"}
print(f"Entered node `node_2`:\n\tInput: {state}.\n\tReturned: {output}")
return output

# Node 3 only has access to the overall state (no access to private data from node_1)

output = {"a": "set by node_3"}
print(f"Entered node `node_3`:\n\tInput: {state}.\n\tReturned: {output}")
return output

# Connect nodes in a sequence
# node_2 accepts private data from node_1, whereas
# node_3 does not see the private data.
builder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])
builder.add_edge(START, "node_1")
graph = builder.compile()

# Invoke the graph with the initial state
response = graph.invoke(
{
"a": "set at start",
}
)

print()
print(f"Output of graph invocation: {response}")

Entered node `node_1`:
Input: {'a': 'set at start'}.
Returned: {'private_data': 'set by node_1'}
Entered node `node_2`:
Input: {'private_data': 'set by node_1'}.
Returned: {'a': 'set by node_2'}
Entered node `node_3`:
Input: {'a': 'set by node_2'}.
Returned: {'a': 'set by node_3'}

Output of graph invocation: {'a': 'set by node_3'}

### ​ Use Pydantic models for graph state

A StateGraph accepts a `state_schema` argument on initialization that specifies the “shape” of the state that the nodes in the graph can access and update.In our examples, we typically use a python-native `TypedDict` or `dataclass` for `state_schema`, but `state_schema` can be any type.Here, we’ll see how a Pydantic BaseModel can be used for `state_schema` to add run-time validation on **inputs**.

**Known Limitations**

- Currently, the output of the graph will **NOT** be an instance of a pydantic model.
- Run-time validation only occurs on inputs to the first node in the graph, not on subsequent nodes or outputs.
- The validation error trace from pydantic does not show which node the error arises in.
- Pydantic’s recursive validation can be slow. For performance-sensitive applications, you may want to consider using a `dataclass` instead.

from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict
from pydantic import BaseModel

class OverallState(BaseModel):
a: str

def node(state: OverallState):
return {"a": "goodbye"}

# Build the state graph
builder = StateGraph(OverallState)
builder.add_node(node) # node_1 is the first node
builder.add_edge(START, "node") # Start the graph with node_1
builder.add_edge("node", END) # End the graph after node_1
graph = builder.compile()

# Test the graph with a valid input
graph.invoke({"a": "hello"})

Invoke the graph with an **invalid** input

try:
graph.invoke({"a": 123}) # Should be a string
except Exception as e:
print("An exception was raised because `a` is an integer rather than a string.")
print(e)

An exception was raised because `a` is an integer rather than a string.
1 validation error for OverallState
a
Input should be a valid string [type=string_type, input_value=123, input_type=int]
For further information visit

See below for additional features of Pydantic model state:

Serialization Behavior

When using Pydantic models as state schemas, it’s important to understand how serialization works, especially when:

- Passing Pydantic objects as inputs
- Receiving outputs from the graph
- Working with nested Pydantic models

Let’s see these behaviors in action.

from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel

class NestedModel(BaseModel):
value: str

class ComplexState(BaseModel):
text: str
count: int
nested: NestedModel

def process_node(state: ComplexState):
# Node receives a validated Pydantic object
print(f"Input state type: {type(state)}")
print(f"Nested type: {type(state.nested)}")
# Return a dictionary update
return {"text": state.text + " processed", "count": state.count + 1}

# Build the graph
builder = StateGraph(ComplexState)
builder.add_node("process", process_node)
builder.add_edge(START, "process")
builder.add_edge("process", END)
graph = builder.compile()

# Create a Pydantic instance for input
input_state = ComplexState(text="hello", count=0, nested=NestedModel(value="test"))
print(f"Input object type: {type(input_state)}")

# Invoke graph with a Pydantic instance
result = graph.invoke(input_state)
print(f"Output type: {type(result)}")
print(f"Output content: {result}")

# Convert :
messages: List[AnyMessage]
context: str

def add_message(state: ChatState):
return {"messages": state.messages + [AIMessage(content="Hello there!")]}

builder = StateGraph(ChatState)
builder.add_node("add_message", add_message)
builder.add_edge(START, "add_message")
builder.add_edge("add_message", END)
graph = builder.compile()

# Create input with a message
initial_state = ChatState(
messages=[HumanMessage(content="Hi")], context="Customer support chat"
)

result = graph.invoke(initial_state)
print(f"Output: {result}")

# Convert Add runtime configuration

Sometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, _without polluting the graph state with these parameters_.To add runtime configuration:

1. Specify a schema for your configuration
2. Add the configuration to the function signature for nodes or conditional edges
3. Pass the configuration into the graph.

See below for a simple example:

from langgraph.graph import END, StateGraph, START
from langgraph.runtime import Runtime
from typing_extensions import TypedDict

# 1. Specify config schema
class ContextSchema(TypedDict):
my_runtime_value: str

# 2. Define a graph that accesses the config in a node
class State(TypedDict):
my_state_value: str

def node(state: State, runtime: Runtime[ContextSchema]):
if runtime.context["my_runtime_value"] == "a":
return {"my_state_value": 1}
elif runtime.context["my_runtime_value"] == "b":
return {"my_state_value": 2}
else:
raise ValueError("Unknown values.")

builder = StateGraph(State, context_schema=ContextSchema)
builder.add_node(node)
builder.add_edge(START, "node")
builder.add_edge("node", END)

# 3. Pass in configuration at runtime:
print(graph.invoke({}, context={"my_runtime_value": "a"}))
print(graph.invoke({}, context={"my_runtime_value": "b"}))

{'my_state_value': 1}
{'my_state_value': 2}

Extended example: specifying LLM at runtime

Below we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.

from dataclasses import dataclass

from langchain.chat_models import init_chat_model
from langgraph.graph import MessagesState, END, StateGraph, START
from langgraph.runtime import Runtime
from typing_extensions import TypedDict

@dataclass
class ContextSchema:
model_provider: str = "anthropic"

MODELS = {
"anthropic": init_chat_model("claude-haiku-4-5-20251001"),
"openai": init_chat_model("gpt-4.1-mini"),
}

def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):
model = MODELS[runtime.context.model_provider]
response = model.invoke(state["messages"])
return {"messages": [response]}

builder = StateGraph(MessagesState, context_schema=ContextSchema)
builder.add_node("model", call_model)
builder.add_edge(START, "model")
builder.add_edge("model", END)

# Usage
input_message = {"role": "user", "content": "hi"}
# With no configuration, uses default (Anthropic)
response_1 = graph.invoke({"messages": [input_message]}, context=ContextSchema())["messages"][-1]
# Or, can set OpenAI
response_2 = graph.invoke({"messages": [input_message]}, context={"model_provider": "openai"})["messages"][-1]

print(response_1.response_metadata["model_name"])
print(response_2.response_metadata["model_name"])

claude-haiku-4-5-20251001
gpt-4.1-mini-2025-04-14

Extended example: specifying model and system message at runtime

Below we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.

from dataclasses import dataclass
from langchain.chat_models import init_chat_model
from langchain.messages import SystemMessage
from langgraph.graph import END, MessagesState, StateGraph, START
from langgraph.runtime import Runtime
from typing_extensions import TypedDict

@dataclass
class ContextSchema:
model_provider: str = "anthropic"
system_message: str | None = None

def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):
model = MODELS[runtime.context.model_provider]
messages = state["messages"]
if (system_message := runtime.context.system_message):
messages = [SystemMessage(system_message)] + messages
response = model.invoke(messages)
return {"messages": [response]}

input_message = {"role": "user", "content": "hi"}
response = graph.invoke({"messages": [input_message]}, context={"model_provider": "openai", "system_message": "Respond in Italian."})
for message in response["messages"]:
message.pretty_print()

hi
================================== Ai Message ==================================

Ciao! Come posso aiutarti oggi?

## ​ Add retry policies

There are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.To configure a retry policy, pass the `retry_policy` parameter to the `add_node`. The `retry_policy` parameter takes in a `RetryPolicy` named tuple object. Below we instantiate a `RetryPolicy` object with the default parameters and associate it with a node:

from langgraph.types import RetryPolicy

builder.add_node(
"node_name",
node_function,
retry_policy=RetryPolicy(),
)

By default, the `retry_on` parameter uses the `default_retry_on` function, which retries on any exception except for the following:

- `ValueError`
- `TypeError`
- `ArithmeticError`
- `ImportError`
- `LookupError`
- `NameError`
- `SyntaxError`
- `RuntimeError`
- `ReferenceError`
- `StopIteration`
- `StopAsyncIteration`
- `OSError`

In addition, for exceptions from popular http request libraries such as `requests` and `httpx` it only retries on 5xx status codes.

Extended example: customizing retry policies

Consider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:

import sqlite3
from typing_extensions import TypedDict
from langchain.chat_models import init_chat_model
from langgraph.graph import END, MessagesState, StateGraph, START
from langgraph.types import RetryPolicy
from langchain_community.utilities import SQLDatabase
from langchain.messages import AIMessage

db = SQLDatabase.from_uri("sqlite:///:memory:")
model = init_chat_model("claude-haiku-4-5-20251001")

def query_database(state: MessagesState):
query_result = db.run("SELECT * FROM Artist LIMIT 10;")
return {"messages": [AIMessage(content=query_result)]}

def call_model(state: MessagesState):
response = model.invoke(state["messages"])
return {"messages": [response]}

# Define a new graph
builder = StateGraph(MessagesState)
builder.add_node(
"query_database",
query_database,
retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),
)
builder.add_node("model", call_model, retry_policy=RetryPolicy(max_attempts=5))
builder.add_edge(START, "model")
builder.add_edge("model", "query_database")
builder.add_edge("query_database", END)
graph = builder.compile()

## ​ Add node caching

Node caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph.To configure a cache policy, pass the `cache_policy` parameter to the `add_node` function. In the following example, a `CachePolicy` object is instantiated with a time to live of 120 seconds and the default `key_func` generator. Then it is associated with a node:

from langgraph.types import CachePolicy

builder.add_node(
"node_name",
node_function,
cache_policy=CachePolicy(ttl=120),
)

Then, to enable node-level caching for a graph, set the `cache` argument when compiling the graph. The example below uses `InMemoryCache` to set up a graph with in-memory cache, but `SqliteCache` is also available.

from langgraph.cache.memory import InMemoryCache

graph = builder.compile(cache=InMemoryCache())

## ​ Create a sequence of steps

**Prerequisites**
This guide assumes familiarity with the above section on state.

Here we demonstrate how to construct a simple sequence of steps. We will show:

1. How to build a sequential graph
2. Built-in short-hand for constructing similar graphs.

To add a sequence of nodes, we use the `add_node` and `add_edge` methods of our graph:

from langgraph.graph import START, StateGraph

builder = StateGraph(State)

# Add nodes
builder.add_node(step_1)
builder.add_node(step_2)
builder.add_node(step_3)

# Add edges
builder.add_edge(START, "step_1")
builder.add_edge("step_1", "step_2")
builder.add_edge("step_2", "step_3")

We can also use the built-in shorthand `.add_sequence`:

builder = StateGraph(State).add_sequence([step_1, step_2, step_3])
builder.add_edge(START, "step_1")

Why split application steps into a sequence with LangGraph?

LangGraph makes it easy to add an underlying persistence layer to your application.
This allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:

- How state updates are checkpointed
- How interruptions are resumed in human-in-the-loop workflows
- How we can “rewind” and branch-off executions using LangGraph’s time travel features

They also determine how execution steps are streamed, and how your application is visualized and debugged using Studio.Let’s demonstrate an end-to-end example. We will create a sequence of three steps:

1. Populate a value in a key of the state
2. Update the same value
3. Populate a different value

Let’s first define our state. This governs the schema of the graph, and can also specify how to apply updates. See this section for more detail.In our case, we will just keep track of two values:

from typing_extensions import TypedDict

class State(TypedDict):
value_1: str
value_2: int

Our nodes are just Python functions that read our graph’s state and make updates to it. The first argument to this function will always be the state:

def step_1(state: State):
return {"value_1": "a"}

def step_2(state: State):
current_value_1 = state["value_1"]
return {"value_1": f"{current_value_1} b"}

def step_3(state: State):
return {"value_2": 10}

Note that when issuing updates to the state, each node can just specify the value of the key it wishes to update.By default, this will **overwrite** the value of the corresponding key. You can also use reducers to control how updates are processed— for example, you can append successive updates to a key instead. See this section for more detail.

Finally, we define the graph. We use StateGraph to define a graph that operates on this state.We will then use `add_node` and `add_edge` to populate our graph and define its control flow.

**Specifying custom names**
You can specify custom names for nodes using `add_node`:

builder.add_node("my_node", step_1)

- `add_edge` takes the names of nodes, which for functions defaults to `node.__name__`.
- We must specify the entry point of the graph. For this we add an edge with the START node.
- The graph halts when there are no more nodes to execute.

We next compile our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a checkpointer, it would also be passed in here.

LangGraph provides built-in utilities for visualizing your graph. Let’s inspect our sequence. See this guide for detail on visualization.

graph.invoke({"value_1": "c"})

{'value_1': 'a b', 'value_2': 10}

- We kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.
- The value we passed in was overwritten by the first node.
- The second node updated the value.
- The third node populated a different value.

## ​ Create branches

Parallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional\_edges. Below are some examples showing how to add create branching dataflows that work for you.

### ​ Run graph nodes in parallel

In this example, we fan out from `Node A` to `B and C` and then fan in to `D`. With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on state reducers for more detail on updating state with reducers.

import operator
from typing import Annotated, Any
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
# The operator.add reducer fn makes this append-only
aggregate: Annotated[list, operator.add]

def a(state: State):
print(f'Adding "A" to {state["aggregate"]}')
return {"aggregate": ["A"]}

def b(state: State):
print(f'Adding "B" to {state["aggregate"]}')
return {"aggregate": ["B"]}

def c(state: State):
print(f'Adding "C" to {state["aggregate"]}')
return {"aggregate": ["C"]}

def d(state: State):
print(f'Adding "D" to {state["aggregate"]}')
return {"aggregate": ["D"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(c)
builder.add_node(d)
builder.add_edge(START, "a")
builder.add_edge("a", "b")
builder.add_edge("a", "c")
builder.add_edge("b", "d")
builder.add_edge("c", "d")
builder.add_edge("d", END)
graph = builder.compile()

graph.invoke({"aggregate": []}, {"configurable": {"thread_id": "foo"}})

Adding "A" to []
Adding "B" to ['A']
Adding "C" to ['A']
Adding "D" to ['A', 'B', 'C']

In the above example, nodes `"b"` and `"c"` are executed concurrently in the same superstep. Because they are in the same step, node `"d"` executes after both `"b"` and `"c"` are finished.Importantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.

Exception handling?

LangGraph executes nodes within supersteps, meaning that while parallel branches are executed in parallel, the entire superstep is **transactional**. If any of these branches raises an exception, **none** of the updates are applied to the state (the entire superstep errors).Importantly, when using a checkpointer, results from successful nodes within a superstep are saved, and don’t repeat when resumed.If you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:

1. You can write regular python code within your node to catch and handle exceptions.
2. You can set a **retry\_policy** to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn’t worry about performing redundant work.

Together, these let you perform parallel execution and fully control exception handling.

**Set max concurrency**
You can control the maximum number of concurrent tasks by setting `max_concurrency` in the configuration when invoking the graph.

graph.invoke({"value_1": "c"}, {"configurable": {"max_concurrency": 10}})

### ​ Defer node execution

Deferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows.The above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let’s add a node `"b_2"` in the `"b"` branch:

class State(TypedDict):
aggregate: Annotated[list, operator.add]

def b_2(state: State):
print(f'Adding "B_2" to {state["aggregate"]}')
return {"aggregate": ["B_2"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(b_2)
builder.add_node(c)
builder.add_node(d, defer=True)
builder.add_edge(START, "a")
builder.add_edge("a", "b")
builder.add_edge("a", "c")
builder.add_edge("b", "b_2")
builder.add_edge("b_2", "d")
builder.add_edge("c", "d")
builder.add_edge("d", END)
graph = builder.compile()

graph.invoke({"aggregate": []})

Adding "A" to []
Adding "B" to ['A']
Adding "C" to ['A']
Adding "B_2" to ['A', 'B', 'C']
Adding "D" to ['A', 'B', 'C', 'B_2']

In the above example, nodes `"b"` and `"c"` are executed concurrently in the same superstep. We set `defer=True` on node `d` so it will not execute until all pending tasks are finished. In this case, this means that `"d"` waits to execute until the entire `"b"` branch is finished.

### ​ Conditional branching

If your fan-out should vary at runtime based on the state, you can use `add_conditional_edges` to select one or more paths using the graph state. See example below, where node `a` generates a state update that determines the following node.

import operator
from typing import Annotated, Literal, Sequence
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

# Add a key to the state. We will set this key to determine
# how we branch.
class State(TypedDict):
aggregate: Annotated[list, operator.add]
which: str

def a(state: State):
print(f'Adding "A" to {state["aggregate"]}')
return {"aggregate": ["A"], "which": "c"}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(c)
builder.add_edge(START, "a")
builder.add_edge("b", END)
builder.add_edge("c", END)

# Fill in arbitrary logic here that uses the state
# to determine the next node
return state["which"]

builder.add_conditional_edges("a", conditional_edge)

result = graph.invoke({"aggregate": []})
print(result)

Adding "A" to []
Adding "C" to ['A']
{'aggregate': ['A', 'C'], 'which': 'c'}

Your conditional edges can route to multiple destination nodes. For example:

if state["which"] == "cd":
return ["c", "d"]
return ["b", "c"]

## ​ Map-Reduce and the Send API

LangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:

from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from typing_extensions import TypedDict, Annotated
import operator

class OverallState(TypedDict):
topic: str
subjects: list[str]
jokes: Annotated[list[str], operator.add]
best_selected_joke: str

def generate_topics(state: OverallState):
return {"subjects": ["lions", "elephants", "penguins"]}

def generate_joke(state: OverallState):
joke_map = {
"lions": "Why don't lions like fast food? Because they can't catch it!",
"elephants": "Why don't elephants use computers? They're afraid of the mouse!",
"penguins": "Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice."
}
return {"jokes": [joke_map[state["subject"]]]}

def continue_to_jokes(state: OverallState):
return [Send("generate_joke", {"subject": s}) for s in state["subjects"]]

def best_joke(state: OverallState):
return {"best_selected_joke": "penguins"}

builder = StateGraph(OverallState)
builder.add_node("generate_topics", generate_topics)
builder.add_node("generate_joke", generate_joke)
builder.add_node("best_joke", best_joke)
builder.add_edge(START, "generate_topics")
builder.add_conditional_edges("generate_topics", continue_to_jokes, ["generate_joke"])
builder.add_edge("generate_joke", "best_joke")
builder.add_edge("best_joke", END)
graph = builder.compile()

# Call the graph: here we call it to generate a list of jokes
for step in graph.stream({"topic": "animals"}):
print(step)

{'generate_topics': {'subjects': ['lions', 'elephants', 'penguins']}}
{'generate_joke': {'jokes': ["Why don't lions like fast food? Because they can't catch it!"]}}
{'generate_joke': {'jokes': ["Why don't elephants use computers? They're afraid of the mouse!"]}}
{'generate_joke': {'jokes': ['Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.']}}
{'best_joke': {'best_selected_joke': 'penguins'}}

## ​ Create and control loops

When creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition.You can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here.Let’s consider a simple graph with a loop to better understand how these mechanisms work.

To return the last value of your state instead of receiving a recursion limit error, see the next section.

When creating a loop, you can include a conditional edge that specifies a termination condition:

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)

if termination_condition(state):
return END
else:
return "b"

builder.add_edge(START, "a")
builder.add_conditional_edges("a", route)
builder.add_edge("b", "a")
graph = builder.compile()

To control the recursion limit, specify `"recursionLimit"` in the config. This will raise a `GraphRecursionError`, which you can catch and handle:

from langgraph.errors import GraphRecursionError

try:
graph.invoke(inputs, {"recursion_limit": 3})
except GraphRecursionError:
print("Recursion Error")

Let’s define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.

import operator
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

def a(state: State):
print(f'Node A sees {state["aggregate"]}')
return {"aggregate": ["A"]}

def b(state: State):
print(f'Node B sees {state["aggregate"]}')
return {"aggregate": ["B"]}

# Define nodes

# Define edges

if len(state["aggregate"]) < 7:
return "b"
else:
return END

Node A sees []
Node B sees ['A']
Node A sees ['A', 'B']
Node B sees ['A', 'B', 'A']
Node A sees ['A', 'B', 'A', 'B']
Node B sees ['A', 'B', 'A', 'B', 'A']
Node A sees ['A', 'B', 'A', 'B', 'A', 'B']

### ​ Impose a recursion limit

In some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph’s recursion limit. This will raise a `GraphRecursionError` after a given number of supersteps. We can then catch and handle this exception:

try:
graph.invoke({"aggregate": []}, {"recursion_limit": 4})
except GraphRecursionError:
print("Recursion Error")

Node A sees []
Node B sees ['A']
Node C sees ['A', 'B']
Node D sees ['A', 'B']
Node A sees ['A', 'B', 'C', 'D']
Recursion Error

Extended example: return state on hitting recursion limit

Instead of raising `GraphRecursionError`, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.LangGraph implements a special `RemainingSteps` annotation. Under the hood, it creates a `ManagedValue` channel — a state channel that will exist for the duration of our graph run and no longer.

import operator
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.managed.is_last_step import RemainingSteps

class State(TypedDict):
aggregate: Annotated[list, operator.add]
remaining_steps: RemainingSteps

if state["remaining_steps"] <= 2:
return END
else:
return "b"

# Test it out
result = graph.invoke({"aggregate": []}, {"recursion_limit": 4})
print(result)

Node A sees []
Node B sees ['A']
Node A sees ['A', 'B']
{'aggregate': ['A', 'B', 'A']}

Extended example: loops with branches

To better understand how the recursion limit works, let’s consider a more complex example. Below we implement a loop, but one step fans out into two nodes:

def c(state: State):
print(f'Node C sees {state["aggregate"]}')
return {"aggregate": ["C"]}

def d(state: State):
print(f'Node D sees {state["aggregate"]}')
return {"aggregate": ["D"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(c)
builder.add_node(d)

builder.add_edge(START, "a")
builder.add_conditional_edges("a", route)
builder.add_edge("b", "c")
builder.add_edge("b", "d")
builder.add_edge(["c", "d"], "a")
graph = builder.compile()

1. Node A
2. Node B
3. Nodes C and D
4. Node A
5. …

We have a loop of four supersteps, where nodes C and D are executed concurrently.Invoking the graph as before, we see that we complete two full “laps” before hitting the termination condition:

result = graph.invoke({"aggregate": []})

Node A sees []
Node B sees ['A']
Node D sees ['A', 'B']
Node C sees ['A', 'B']
Node A sees ['A', 'B', 'C', 'D']
Node B sees ['A', 'B', 'C', 'D', 'A']
Node D sees ['A', 'B', 'C', 'D', 'A', 'B']
Node C sees ['A', 'B', 'C', 'D', 'A', 'B']
Node A sees ['A', 'B', 'C', 'D', 'A', 'B', 'C', 'D']

However, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:

try:
result = graph.invoke({"aggregate": []}, {"recursion_limit": 4})
except GraphRecursionError:
print("Recursion Error")

## ​ Async

Using the async programming paradigm can produce significant performance improvements when running IO-bound code concurrently (e.g., making concurrent API requests to a chat model provider).To convert a `sync` implementation of the graph to an `async` implementation, you will need to:

1. Update `nodes` use `async def` instead of `def`.
2. Update the code inside to use `await` appropriately.
3. Invoke the graph with `.ainvoke` or `.astream` as desired.

Because many LangChain objects implement the Runnable Protocol which has `async` variants of all the `sync` methods it’s typically fairly quick to upgrade a `sync` graph to an `async` graph.See example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:

- OpenAI

- Anthropic

- Azure

- Google Gemini

- AWS Bedrock

- HuggingFace

👉 Read the OpenAI chat model integration docs

pip install -U "langchain[openai]"

init\_chat\_model

Model Class

import os
from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")

👉 Read the Anthropic chat model integration docs

pip install -U "langchain[anthropic]"

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")

👉 Read the Azure chat model integration docs

os.environ["AZURE_OPENAI_API_KEY"] = "..."
os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
"azure_openai:gpt-4.1",
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
)

👉 Read the Google GenAI chat model integration docs

pip install -U "langchain[google-genai]"

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")

👉 Read the AWS Bedrock chat model integration docs

pip install -U "langchain[aws]"

from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
#

model = init_chat_model(
"anthropic.claude-3-5-sonnet-20240620-v1:0",
model_provider="bedrock_converse",
)

👉 Read the HuggingFace chat model integration docs

pip install -U "langchain[huggingface]"

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

model = init_chat_model(
"microsoft/Phi-3-mini-4k-instruct",
model_provider="huggingface",
temperature=0.7,
max_tokens=1024,
)

from langchain.chat_models import init_chat_model
from langgraph.graph import MessagesState, StateGraph

async def node(state: MessagesState):
new_message = await llm.ainvoke(state["messages"])
return {"messages": [new_message]}

builder = StateGraph(MessagesState).add_node(node).set_entry_point("node")
graph = builder.compile()

input_message = {"role": "user", "content": "Hello"}
result = await graph.ainvoke({"messages": [input_message]})

**Async streaming**
See the streaming guide for examples of streaming with async.

## ​ Combine control flow and state updates with `Command`

It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:

return Command(
# state update
update={"foo": "bar"},
# control flow
goto="my_other_node"
)

We show an end-to-end example below. Let’s create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.

import random
from typing_extensions import TypedDict, Literal
from langgraph.graph import StateGraph, START
from langgraph.types import Command

# Define graph state
class State(TypedDict):
foo: str

# Define the nodes

print("Called A")
value = random.choice(["b", "c"])
# this is a replacement for a conditional edge function
if value == "b":
goto = "node_b"
else:
goto = "node_c"

# note how Command allows you to BOTH update the graph state AND route to the next node
# this is the state update
return Command(
update={"foo": value},
# this is a replacement for an edge
goto=goto,
)

def node_b(state: State):
print("Called B")
return {"foo": state["foo"] + "b"}

def node_c(state: State):
print("Called C")
return {"foo": state["foo"] + "c"}

We can now create the `StateGraph` with the above nodes. Notice that the graph doesn’t have conditional edges for routing! This is because control flow is defined with `Command` inside `node_a`.

builder = StateGraph(State)
builder.add_edge(START, "node_a")
builder.add_node(node_a)
builder.add_node(node_b)
builder.add_node(node_c)
# NOTE: there are no edges between nodes A, B and C!

You might have noticed that we used `Command` as a return type annotation, e.g. `Command[Literal["node_b", "node_c"]]`. This is necessary for the graph rendering and tells LangGraph that `node_a` can navigate to `node_b` and `node_c`.

from IPython.display import display, Image

graph.invoke({"foo": ""})

Called A
Called C

### ​ Navigate to a node in a parent graph

If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:

return Command(
update={"foo": "bar"},
goto="other_subgraph", # where `other_subgraph` is a node in the parent graph
graph=Command.PARENT
)

Let’s demonstrate this using the above example. We’ll do so by changing `nodeA` in the above example into a single-node graph that we’ll add as a subgraph to our parent graph.

**State updates with `Command.PARENT`**
When you send updates from a subgraph node to a parent graph node for a key that’s shared by both parent and subgraph state schemas, you **must** define a reducer for the key you’re updating in the parent graph state. See the example below.

import operator
from typing_extensions import Annotated

# NOTE: we define a reducer here
class State(TypedDict):
foo: Annotated[str, operator.add]

def node_a(state: State):
print("Called A")
value = random.choice(["a", "b"])
if value == "a":
goto = "node_b"
else:
goto = "node_c"

return Command(
update={"foo": value},
goto=goto,
# this tells LangGraph to navigate to node_b or node_c in the parent graph
# NOTE: this will navigate to the closest parent graph relative to the subgraph
graph=Command.PARENT,
)

subgraph = StateGraph(State).add_node(node_a).add_edge(START, "node_a").compile()

def node_b(state: State):
print("Called B")
# NOTE: since we've defined a reducer, we don't need to manually append
# new characters to existing 'foo' value. instead, reducer will append these
# automatically (via operator.add)
return {"foo": "b"}

def node_c(state: State):
print("Called C")
return {"foo": "c"}

builder = StateGraph(State)
builder.add_edge(START, "subgraph")
builder.add_node("subgraph", subgraph)
builder.add_node(node_b)
builder.add_node(node_c)

### ​ Use inside tools

A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return `Command(update={"my_custom_key": "foo", "messages": [...]})` from the tool:

@tool
def lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):
"""Use this to look up user information to better assist them with their questions."""
user_info = get_user_info(config.get("configurable", {}).get("user_id"))
return Command(
update={
# update the state keys
"user_info": user_info,
# update the message history
"messages": [ToolMessage("Successfully looked up user information", tool_call_id=tool_call_id)]
}
)

You MUST include `messages` (or any state key used for the message history) in `Command.update` when returning `Command` from a tool and the list of messages in `messages` MUST contain a `ToolMessage`. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).

If you are using tools that update state via `Command`, we recommend using prebuilt `ToolNode` which automatically handles tools returning `Command` objects and propagates them to the graph state. If you’re writing a custom node that calls tools, you would need to manually propagate `Command` objects returned by the tools as the update from the node.

## ​ Visualize your graph

Here we demonstrate how to visualize the graphs you create.You can visualize any arbitrary Graph, including StateGraph.Let’s have some fun by drawing fractals :).

import random
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
messages: Annotated[list, add_messages]

class MyNode:
def __init__(self, name: str):
self.name = name
def __call__(self, state: State):
return {"messages": [("assistant", f"Called node {self.name}")]}

return END
return "entry_node"

def add_fractal_nodes(builder, current_node, level, max_level):

return
# Number of nodes to create at this level
num_nodes = random.randint(1, 3) # Adjust randomness as needed
for i in range(num_nodes):
nm = ["A", "B", "C"][i]
node_name = f"node_{current_node}_{nm}"
builder.add_node(node_name, MyNode(node_name))
builder.add_edge(current_node, node_name)
# Recursively add more nodes
r = random.random()

add_fractal_nodes(builder, node_name, level + 1, max_level)

builder.add_conditional_edges(node_name, route, node_name)
else:
# End
builder.add_edge(node_name, END)

def build_fractal_graph(max_level: int):
builder = StateGraph(State)
entry_point = "entry_node"
builder.add_node(entry_point, MyNode(entry_point))
builder.add_edge(START, entry_point)
add_fractal_nodes(builder, entry_point, 1, max_level)
# Optional: set a finish point if required
builder.add_edge(entry_point, END) # or any specific node
return builder.compile()

app = build_fractal_graph(3)

### ​ Mermaid

We can also convert a graph class into Mermaid syntax.

print(app.get_graph().draw_mermaid())

%%{init: {'flowchart': {'curve': 'linear'}}}%%
graph TD;

ry_node(entry_node)
e_entry_node_A(node_entry_node_A)
e_entry_node_B(node_entry_node_B)
e_node_entry_node_B_A(node_node_entry_node_B_A)
e_node_entry_node_B_B(node_node_entry_node_B_B)
e_node_entry_node_B_C(node_node_entry_node_B_C)

ssDef default fill:#f2f0ff,line-height:1.2
ssDef first fill-opacity:0
ssDef last fill:#bfb6fc

### ​ PNG

If preferred, we could render the Graph into a `.png`. Here we could use three options:

- Using Mermaid.ink API (does not require additional packages)
- Using Mermaid + Pyppeteer (requires `pip install pyppeteer`)
- Using graphviz (which requires `pip install graphviz`)

**Using Mermaid.Ink**By default, `draw_mermaid_png()` uses Mermaid.Ink’s API to generate the diagram.

from IPython.display import Image, display
from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles

display(Image(app.get_graph().draw_mermaid_png()))

import nest_asyncio

nest_asyncio.apply() # Required for Jupyter Notebook to run async functions

display(
Image(
app.get_graph().draw_mermaid_png(
curve_style=CurveStyle.LINEAR,
node_colors=NodeStyles(first="#ffdfba", last="#baffc9", default="#fad7de"),
wrap_label_n_words=9,
output_file_path=None,
draw_method=MermaidDrawMethod.PYPPETEER,
background_color="white",
padding=10,
)
)
)

**Using Graphviz**

try:
display(Image(app.get_graph().draw_png()))
except ImportError:
print(
"You likely need to install dependencies for pygraphviz, see more here
)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Graph API overview\\
\\
Previous Functional API overview\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/graph-api).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Use the graph API Choosing between Graph and Functional APIs LangSmith Deployment components

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration All integration providers Contributing integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/all_providers)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers All integrations Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/models).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/middleware).The

Skip to main content.The#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/messages).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/middleware).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Built-in middleware How to add custom middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/use-graph-api)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

Use the graph API Choosing between Graph and Functional APIs What's new in LangGraph v1

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/structured-output).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Structured output Models

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/middleware)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Built-in middleware How to add custom middleware

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/concepts/memory).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Handoffs: Customer support
- Router: Knowledge base
- Skills: SQL assistant
- LangGraph

- Custom RAG agent
- Custom SQL agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### Additional resources

- LangChain Academy
- Case studies
- Get help

404

# Page not found

We couldn’t find the page you were looking for.

Contributing to documentation Data storage and privacy Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/long-term-memory).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Long-term memory Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/streaming).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/reference/langchain-python

Edit this page

# LangChain reference

Welcome to the LangChain package reference documentation!

Most users will primarily interact with the main `langchain` package, which provides the complete set of implementations for building LLM applications. The packages below form the foundation of the LangChain ecosystem, each serving a specific purpose in the architecture:

- **`langchain`**

* * *

The main entrypoint containing all implementations you need for building applications with LLMs.

Reference

- **`langchain-core`**

Core interfaces and abstractions used across the LangChain ecosystem.

- **`langchain-text-splitters`**

Text splitting utilities for document processing.

- **`langchain-mcp-adapters`**

Make MCP tools available in LangChain and LangGraph applications.

- **`langchain-tests`**

Standard tests suite used to validate LangChain integration package implementations.

- **`langchain-classic`**

Legacy `langchain` implementations and components.

Integration Packages

Looking for integrations with specific providers and services? Check out the integrations reference for packages that connect with popular LLM providers, vector stores, tools, and other services.

---

# https://docs.langchain.com/oss/python/reference/langchain-python)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Reference

- LangChain SDK
- LangGraph SDK
- Integrations
- Deep Agents

##### Errors

- Reference

##### Releases

- Versioning
- Changelog
- Releases

- Migration guides

##### Policies

- Release policy
- Security

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangSmith reference Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/openai

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Popular Providers

OpenAI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Model interfaces
- Tools and toolkits
- Retrievers
- Document loaders
- Other

This page covers all LangChain integrations with OpenAI

## ​ Model interfaces

**ChatOpenAI** \\
\\
OpenAI chat models.\\
\\
Get started **AzureChatOpenAI** \\
\\
Wrapper for OpenAI chat models hosted on Azure.\\
\\
Get started **OpenAIEmbeddings** \\
\\
OpenAI embedding models.\\
\\
Get started **AzureOpenAIEmbeddings** \\
\\
Wrapper for OpenAI embedding models hosted on Azure.\\
\\
Get started

## ​ Tools and toolkits

**Dall-E Image Generator** \\
\\
Text-to-image generation using OpenAI’s Dall-E models.\\
\\
Get started

## ​ Retrievers

**ChatGPTPluginRetriever** \\
\\
Retrieve real-time information; e.g., sports scores, stock prices, the latest news, etc.\\
\\
Get started

## ​ Document loaders

**ChatGPTLoader** \\
\\
Load `conversations.json` from your ChatGPT data export folder.\\
\\
Get started

## ​ Other

**OpenAI**\\
\\
(Legacy) OpenAI text completion models.\\
\\
Get started **AzureOpenAI** \\
\\
Wrapper for (legacy) OpenAI text completion models hosted on Azure.\\
\\
Get started **Adapter** \\
\\
Adapt LangChain models to OpenAI APIs.\\
\\
Get started **OpenAIModerationChain** \\
\\
Detect text that could be hateful, violent, etc.\\
\\
Get started

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

All integration providers\\
\\
Previous Anthropic (Claude)\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/anthropic

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Popular Providers

Anthropic (Claude)

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Model interfaces
- Other

This page covers all LangChain integrations with Anthropic, the makers of Claude.

## ​ Model interfaces

**ChatAnthropic** \\
\\
Anthropic chat models.\\
\\
Get started **Anthropic middleware** \\
\\
Anthropic-specific middleware for Claude models.\\
\\
Get started

## ​ Other

**AnthropicLLM**\\
\\
(Legacy) Anthropic text completion models.\\
\\
Get started

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

OpenAI\\
\\
Previous Google\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/google

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Popular Providers

Google

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Google Generative AI
- Chat models
- LLMs
- Embedding models
- Google Cloud
- Chat models
- LLMs
- Embedding models
- Document loaders
- Document transformers
- Vector stores
- Retrievers
- Tools
- Callbacks
- Evaluators
- Other Google products
- Document loaders
- Vector stores
- Retrievers
- Tools
- MCP
- Toolkits
- Chat loaders
- 3rd party integrations
- Search
- YouTube

This page covers all LangChain integrations with Google Gemini, Google Cloud, and other Google products (such as Google Maps, YouTube, and more).

**Unified SDK & Package Consolidation**As of `langchain-google-genai` 4.0.0, this package uses the consolidated `google-genai` SDK and now supports **both the Gemini Developer API and Vertex AI** backends.The `langchain-google-vertexai` package remains supported for Vertex AI platform-specific features (Model Garden, Vector Search, evaluation services, etc.).Read the full announcement and migration guide.

Not sure which package to use?

Google Generative AI (Gemini API & Vertex AI)

Access Google Gemini models via the **Gemini Developer API** or **Vertex AI**. The backend is selected automatically based on your configuration.

- **Gemini Developer API**: Quick setup with API key, ideal for individual developers and rapid prototyping
- **Vertex AI**: Enterprise features with Google Cloud integration (requires GCP project)

Use the `langchain-google-genai` package for chat models, LLMs, and embeddings.See integrations.

Google Cloud (Vertex AI Platform Services)

Access Vertex AI platform-specific services beyond Gemini models: Model Garden (Llama, Mistral, Anthropic), evaluation services, and specialized vision models.Use the `langchain-google-vertexai` package for platform services and specific packages (e.g., `langchain-google-community`, `langchain-google-cloud-sql-pg`) for other cloud services like databases and storage.See integrations.

See Google’s guide on migrating from the Gemini API to Vertex AI for more details on the differences.

Integration packages for Gemini models and the Vertex AI platform are maintained in the `langchain-google` repository.You can find a host of LangChain integrations with other Google APIs and services in the `langchain-google-community` package (listed on this page) and the `googleapis` GitHub organization.

* * *

## ​ Google Generative AI

Access Google Gemini models via the Gemini Developer API or Vertex AI using the unified `langchain-google-genai` package.

**Package consolidation**Certain `langchain-google-vertexai` classes for Gemini models are being deprecated in favor of the unified `langchain-google-genai` package. Please migrate to the new classes.Read the full announcement and migration guide.

### ​ Chat models

**ChatGoogleGenerativeAI** \\
\\
Google Gemini chat models via **Gemini Developer API** or **Vertex AI**.\\
\\
Get started

### ​ LLMs

**GoogleGenerativeAI** \\
\\
Access the same Gemini models (via **Gemini Developer API** or **Vertex AI**) using the (legacy) LLM text completion interface.\\
\\
Get started

### ​ Embedding models

**GoogleGenerativeAIEmbeddings** \\
\\
Gemini embedding models via **Gemini Developer API** or **Vertex AI**.\\
\\
Get started

## ​ Google Cloud

Access Vertex AI platform-specific services including Model Garden (Llama, Mistral, Anthropic), Vector Search, evaluation services, and specialized vision models.

### ​ Chat models

**For Gemini models**, use `ChatGoogleGenerativeAI` from `langchain-google-genai` instead of `ChatVertexAI`. It supports both Gemini Developer API and Vertex AI backends.The classes below focus on **Vertex AI platform services** that are _not_ available in the consolidated SDK.Read the full announcement and migration guide.

**ChatVertexAI**

**Deprecated** – Use `ChatGoogleGenerativeAI` for Gemini models instead.

Get started

**ChatAnthropicVertex** \\
\\
Anthropic on Vertex AI Model Garden\\
\\
Get started

VertexModelGardenLlama

Llama on Vertex AI Model Garden

Copy

from langchain_google_vertexai.model_garden_maas.llama import VertexModelGardenLlama

VertexModelGardenMistral

Mistral on Vertex AI Model Garden

from langchain_google_vertexai.model_garden_maas.mistral import VertexModelGardenMistral

GemmaChatLocalHF

Local Gemma model loaded from HuggingFace.

from langchain_google_vertexai.gemma import GemmaChatLocalHF

GemmaChatLocalKaggle

Local Gemma model loaded from Kaggle.

from langchain_google_vertexai.gemma import GemmaChatLocalKaggle

GemmaChatVertexAIModelGarden

Gemma on Vertex AI Model Garden

from langchain_google_vertexai.gemma import GemmaChatVertexAIModelGarden

VertexAIImageCaptioningChat

Implementation of the Image Captioning model as a chat.

from langchain_google_vertexai.vision_models import VertexAIImageCaptioningChat

VertexAIImageEditorChat

Given an image and a prompt, edit the image. Currently only supports mask-free editing.

from langchain_google_vertexai.vision_models import VertexAIImageEditorChat

VertexAIImageGeneratorChat

Generates an image from a prompt.

from langchain_google_vertexai.vision_models import VertexAIImageGeneratorChat

VertexAIVisualQnAChat

Chat implementation of a visual QnA model.

from langchain_google_vertexai.vision_models import VertexAIVisualQnAChat

### ​ LLMs

(legacy) string-in, string-out LLM interface.

**VertexAIModelGarden** \\
\\
Access Gemini, and hundreds of OSS models via Vertex AI Model Garden service.\\
\\
Get started

**VertexAI**

**Deprecated** – Use `GoogleGenerativeAI` for Gemini models instead.

Gemma:

Gemma local from Hugging Face

from langchain_google_vertexai.gemma import GemmaLocalHF

Gemma local from Kaggle

from langchain_google_vertexai.gemma import GemmaLocalKaggle

from langchain_google_vertexai.gemma import GemmaVertexAIModelGarden

Vertex AI image captioning

Implementation of the Image Captioning model as an LLM.

from langchain_google_vertexai.vision_models import VertexAIImageCaptioning

### ​ Embedding models

**VertexAIEmbeddings**

**Deprecated** – Use `GenerativeAIEmbeddings` instead.

### ​ Document loaders

Load documents from various Google Cloud sources.

**AlloyDB for PostgreSQL** \\
\\
Google Cloud AlloyDB is a fully managed PostgreSQL-compatible database service.\\
\\
Get started **BigQuery** \\
\\
Google Cloud BigQuery is a serverless data warehouse.\\
\\
Get started **Bigtable** \\
\\
Google Cloud Bigtable is a fully managed NoSQL Big Data database service.\\
\\
Get started **Cloud SQL for MySQL** \\
\\
Google Cloud SQL for MySQL is a fully-managed MySQL database service.\\
\\
Get started **Cloud SQL for SQL Server** \\
\\
Google Cloud SQL for SQL Server is a fully-managed SQL Server database service.\\
\\
Get started **Cloud SQL for PostgreSQL** \\
\\
Google Cloud SQL for PostgreSQL is a fully-managed PostgreSQL database service.\\
\\
Get started **Cloud Storage (directory)** \\
\\
Google Cloud Storage is a managed service for storing unstructured data.\\
\\
Get started **Cloud Storage (file)** \\
\\
Google Cloud Storage is a managed service for storing unstructured data.\\
\\
Get started **El Carro for Oracle Workloads** \\
\\
Google El Carro Oracle Operator runs Oracle databases in Kubernetes.\\
\\
Get started **Firestore (Native Mode)** \\
\\
Google Cloud Firestore is a NoSQL document database.\\
\\
Get started **Firestore (Datastore Mode)** \\
\\
Google Cloud Firestore in Datastore mode\\
\\
Get started **Memorystore for Redis** \\
\\
Google Cloud Memorystore for Redis is a fully managed Redis service.\\
\\
Get started **Spanner** \\
\\
Google Cloud Spanner is a fully managed, globally distributed relational database service.\\
\\
Get started **Speech-to-Text** \\
\\
Google Cloud Speech-to-Text transcribes audio files.\\
\\
Get started

## Cloud Vision loader

Load data using Google Cloud Vision API.

from langchain_google_community.vision import CloudVisionLoader

### ​ Document transformers

Transform documents using Google Cloud services.

**Document AI** \\
\\
Transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume.\\
\\
Get started **Google Translate** \\
\\
Translate text and HTML with the Google Cloud Translation API.\\
\\
Get started

### ​ Vector stores

Store and search vectors using Google Cloud databases and Vertex AI Vector Search.

**AlloyDB for PostgreSQL** \\
\\
Google Cloud AlloyDB is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability on Google Cloud. AlloyDB is 100% compatible with PostgreSQL.\\
\\
Get started **BigQuery Vector Search** \\
\\
BigQuery vector search lets you use GoogleSQL to do semantic search, using vector indexes for fast but approximate results, or using brute force for exact results.\\
\\
Get started **Memorystore for Redis** \\
\\
Vector store using Memorystore for Redis\\
\\
Get started **Spanner** \\
\\
Vector store using Cloud Spanner\\
\\
Get started **Firestore (Native Mode)** \\
\\
Vector store using Firestore\\
\\
Get started **Cloud SQL for MySQL** \\
\\
Vector store using Cloud SQL for MySQL\\
\\
Get started **Cloud SQL for PostgreSQL** \\
\\
Vector store using Cloud SQL for PostgreSQL.\\
\\
Get started **Vertex AI Vector Search** \\
\\
Formerly known as Vertex AI Matching Engine, provides a low latency vector database. These vector databases are commonly referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.\\
\\
Get started **With DataStore Backend** \\
\\
Vector search using Datastore for document storage.\\
\\
Get started

### ​ Retrievers

Retrieve information using Google Cloud services.

**Vertex AI Search** \\
\\
Build generative AI powered search engines using Vertex AI Search\\
\\
Get started **Document AI Warehouse** \\
\\
Search, store, and manage documents using Document AI Warehouse.\\
\\
Get started

Other retrievers

from langchain_google_community import VertexAIMultiTurnSearchRetriever
from langchain_google_community import VertexAISearchRetriever
from langchain_google_community import VertexAISearchSummaryTool

### ​ Tools

Integrate agents with various Google Cloud services.

**Text-to-Speech** \\
\\
Google Cloud Text-to-Speech synthesizes natural-sounding speech with 100+ voices in multiple languages.\\
\\
Get started

### ​ Callbacks

Track LLM/Chat model usage.

Vertex AI callback handler

Callback Handler that tracks `VertexAI` usage info.

from langchain_google_vertexai.callbacks import VertexAICallbackHandler

### ​ Evaluators

Evaluate model outputs using Vertex AI.

VertexPairWiseStringEvaluator

Pair-wise evaluation using Vertex AI models.

from langchain_google_vertexai.evaluators.evaluation import VertexPairWiseStringEvaluator

VertexStringEvaluator

Evaluate a single prediction string using Vertex AI models.

from langchain_google_vertexai.evaluators.evaluation import VertexStringEvaluator

## ​ Other Google products

Integrations with various Google services beyond the core Cloud Platform.

### ​ Document loaders

**Google Drive** \\
\\
Google Drive file storage. Currently supports Google Docs.\\
\\
Get started

### ​ Vector stores

**ScaNN (Local Index)** \\
\\
ScaNN is a method for efficient vector similarity search at scale.\\
\\
Get started

### ​ Retrievers

**Google Drive** \\
\\
Retrieve documents from Google Drive.\\
\\
Get started

### ​ Tools

**Google Search** \\
\\
Perform web searches using Google Custom Search Engine (CSE).\\
\\
Get started **Google Drive** \\
\\
Tools for interacting with Google Drive.\\
\\
Get started **Google Finance** \\
\\
Query financial data.\\
\\
Get started **Google Jobs** \\
\\
Query job listings.\\
\\
Get started **Google Lens** \\
\\
Perform visual searches.\\
\\
Get started **Google Places** \\
\\
Search for places information.\\
\\
Get started **Google Scholar** \\
\\
Search academic papers.\\
\\
Get started **Google Trends** \\
\\
Query Google Trends data.\\
\\
Get started

### ​ MCP

**MCP Toolbox** \\
\\
Simple and efficient way to connect to your databases, including those on Google Cloud like Cloud SQL and AlloyDB\\
\\
Get started

### ​ Toolkits

Collections of tools for specific Google services.

**Gmail** \\
\\
Toolkit to create, get, search, and send emails using the Gmail API.\\
\\
Get started

### ​ Chat loaders

**Gmail** \\
\\
Load chat history from Gmail threads.\\
\\
Get started

## ​ 3rd party integrations

Access Google services via unofficial third-party APIs.

### ​ Search

**SearchApi** \\
\\
searchapi.io provides API access to Google search results, YouTube, and more.\\
\\
Get started **SerpApi** \\
\\
SerpApi provides API access to Google search results.\\
\\
Get started **Serper.dev** \\
\\
serper.dev provides API access to Google search results.\\
\\
Get started

### ​ YouTube

**Search tool** \\
\\
Search YouTube videos without the official API.\\
\\
Get started **Audio loader** \\
\\
Download audio from YouTube videos.\\
\\
Get started **Transcripts loader** \\
\\
Load video transcripts.\\
\\
Get started

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Anthropic (Claude)\\
\\
Previous AWS (Amazon)\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/aws

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Popular Providers

AWS (Amazon)

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Chat models
- Bedrock Chat
- Bedrock Converse
- LLMs
- Bedrock
- Amazon API Gateway
- SageMaker Endpoint
- Embedding Models
- Bedrock
- SageMaker Endpoint
- Document loaders
- AWS S3 Directory and File
- Amazon Textract
- Amazon Athena
- AWS Glue
- Vector stores
- Amazon OpenSearch Service
- Amazon DocumentDB Vector Search
- Installation and Setup
- Deploy DocumentDB on AWS
- Amazon MemoryDB
- Retrievers
- Amazon Kendra
- Amazon Bedrock (Knowledge Bases)
- Tools
- AWS Lambda
- Graphs
- Amazon Neptune
- Amazon Neptune with Cypher
- Amazon Neptune with SPARQL
- Callbacks
- Bedrock token usage
- SageMaker Tracking
- Chains
- Amazon Comprehend Moderation Chain

This page covers all LangChain integrations with the Amazon Web Services (AWS) platform.

## ​ Chat models

See a usage example.

Copy

from langchain_aws import ChatBedrock

### ​ Bedrock Converse

AWS Bedrock maintains a Converse API
that provides a unified conversational interface for Bedrock models. This API does not
yet support custom models. You can see a list of all
models that are supported here.

**We recommend the Converse API for users who do not need to use custom models. It can be accessed using ChatBedrockConverse.**

from langchain_aws import ChatBedrockConverse

## ​ LLMs

### ​ Bedrock

from langchain_aws import BedrockLLM

from langchain_community.llms import AmazonAPIGateway

We use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`.See a usage example.

from langchain_aws import SagemakerEndpoint

## ​ Embedding Models

### ​ Bedrock

from langchain_aws import BedrockEmbeddings

### ​ SageMaker Endpoint

from langchain_community.embeddings import SagemakerEndpointEmbeddings
from langchain_community.llms.sagemaker_endpoint import ContentHandlerBase

## ​ Document loaders

See a usage example for S3DirectoryLoader.See a usage example for S3FileLoader.

from langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader

from langchain_community.document_loaders import AmazonTextractPDFLoader

from langchain_community.document_loaders.athena import AthenaLoader

from langchain_community.document_loaders.glue_catalog import GlueCatalogLoader

## ​ Vector stores

We need to install several python libraries.

pip

uv

pip install boto3 requests requests-aws4auth

from langchain_community.vectorstores import OpenSearchVectorSearch

#### ​ Installation and Setup

See detail configuration instructions.We need to install the `pymongo` python package.

pip install pymongo

#### ​ Deploy DocumentDB on AWS

Amazon DocumentDB (with MongoDB Compatibility) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.AWS offers services for computing, databases, storage, analytics, and other functionality. For an overview of all AWS services, see Cloud Computing with Amazon Web Services.See a usage example.

from langchain_community.vectorstores import DocumentDBVectorSearch

### ​ Amazon MemoryDB

Amazon MemoryDB is a durable, in-memory database service that delivers ultra-fast performance. MemoryDB is compatible with Redis OSS, a popular open source data store,
enabling you to quickly build applications using the same flexible and friendly Redis OSS APIs, and commands that they already use today.InMemoryVectorStore class provides a vectorstore to connect with Amazon MemoryDB.

from langchain_aws.vectorstores.inmemorydb import InMemoryVectorStore

vds = InMemoryVectorStore.from_documents(
chunks,
embeddings,
redis_url="rediss://cluster_endpoint:6379/ssl=True ssl_cert_reqs=none",
vector_schema=vector_schema,
index_name=INDEX_NAME,
)

## ​ Retrievers

We need to install the `langchain-aws` library.

pip install langchain-aws

from langchain_aws import AmazonKendraRetriever

from langchain_aws import AmazonKnowledgeBasesRetriever

## ​ Tools

We need to install `boto3` python library.

pip install boto3

from langchain_community.chat_message_histories import DynamoDBChatMessageHistory

## ​ Graphs

For the Cypher and SPARQL integrations below, we need to install the `langchain-aws` library.

### ​ Amazon Neptune with Cypher

from langchain_aws.graphs import NeptuneGraph
from langchain_aws.graphs import NeptuneAnalyticsGraph
from langchain_aws.chains import create_neptune_opencypher_qa_chain

### ​ Amazon Neptune with SPARQL

from langchain_aws.graphs import NeptuneRdfGraph
from langchain_aws.chains import create_neptune_sparql_qa_chain

## ​ Callbacks

### ​ Bedrock token usage

from langchain_community.callbacks.bedrock_anthropic_callback import BedrockAnthropicTokenUsageCallbackHandler

pip install google-search-results sagemaker

from langchain_community.callbacks import SageMakerCallbackHandler

## ​ Chains

We need to install the `boto3` and `nltk` libraries.

pip install boto3 nltk

from langchain_experimental.comprehend_moderation import AmazonComprehendModerationChain

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Google\\
\\
Previous Hugging Face\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/huggingface

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Popular Providers

Hugging Face

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Chat models
- ChatHuggingFace
- LLMs
- HuggingFaceEndpoint
- HuggingFacePipeline
- Embedding Models
- HuggingFaceEmbeddings
- HuggingFaceEndpointEmbeddings
- HuggingFaceInferenceAPIEmbeddings
- HuggingFaceInstructEmbeddings
- HuggingFaceBgeEmbeddings
- Document loaders
- Hugging Face dataset
- Hugging Face model loader
- Image captions
- Tools
- Hugging Face Hub Tools
- Hugging Face Text-to-Speech Model Inference.

This page covers all LangChain integrations with Hugging Face Hub and libraries like transformers, sentence transformers, and datasets.

## ​ Chat models

### ​ ChatHuggingFace

We can use the `Hugging Face` LLM classes or directly use the `ChatHuggingFace` class.See a usage example.

Copy

from langchain_huggingface import ChatHuggingFace

## ​ LLMs

### ​ HuggingFaceEndpoint

We can use the `HuggingFaceEndpoint` class to run open source models via serverless Inference Providers or via dedicated Inference Endpoints.See a usage example.

from langchain_huggingface import HuggingFaceEndpoint

### ​ HuggingFacePipeline

We can use the `HuggingFacePipeline` class to run open source models locally.See a usage example.

from langchain_huggingface import HuggingFacePipeline

## ​ Embedding Models

### ​ HuggingFaceEmbeddings

We can use the `HuggingFaceEmbeddings` class to run open source embedding models locally.See a usage example.

from langchain_huggingface import HuggingFaceEmbeddings

### ​ HuggingFaceEndpointEmbeddings

We can use the `HuggingFaceEndpointEmbeddings` class to run open source embedding models via a dedicated Inference Endpoint.See a usage example.

from langchain_huggingface import HuggingFaceEndpointEmbeddings

### ​ HuggingFaceInferenceAPIEmbeddings

We can use the `HuggingFaceInferenceAPIEmbeddings` class to run open source embedding models via Inference Providers.See a usage example.

from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings

### ​ HuggingFaceInstructEmbeddings

We can use the `HuggingFaceInstructEmbeddings` class to run open source embedding models locally.See a usage example.

from langchain_community.embeddings import HuggingFaceInstructEmbeddings

See a usage example.

from langchain_community.embeddings import HuggingFaceBgeEmbeddings

## ​ Document loaders

We need to install `datasets` python package.

pip

uv

pip install datasets

from langchain_community.document_loaders.hugging_face_dataset import HuggingFaceDatasetLoader

from langchain_community.document_loaders import HuggingFaceModelLoader

### ​ Image captions

It uses the Hugging Face models to generate image captions.We need to install several python packages.

pip install transformers pillow

from langchain_community.document_loaders import ImageCaptionLoader

## ​ Tools

We need to install several python packages.

pip install transformers huggingface_hub

from langchain_community.agent_toolkits.load_tools import load_huggingface_tool

from langchain_community.tools.audio import HuggingFaceTextToSpeechModelInference

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

AWS (Amazon)\\
\\
Previous Microsoft\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/microsoft

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Popular Providers

Microsoft

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Chat models
- Azure OpenAI
- Azure AI
- Azure ML Chat Online Endpoint
- LLMs
- Azure ML
- Azure OpenAI
- Embedding Models
- Azure OpenAI
- Azure AI
- Document loaders
- Azure AI Data
- Azure AI Document Intelligence
- Azure Blob Storage
- Microsoft OneDrive
- Microsoft OneDrive File
- Microsoft Word
- Microsoft Excel
- Microsoft SharePoint
- Microsoft PowerPoint
- Microsoft OneNote
- Playwright URL Loader
- Memory
- Azure Cosmos DB Chat Message History
- Vector Stores
- Azure Cosmos DB
- Azure Cosmos DB for MongoDB (vCore)
- Azure Cosmos DB NoSQL
- Azure Database for PostgreSQL
- Azure SQL Database
- Azure AI Search
- Retrievers
- Azure AI Search
- Vector Store
- Azure Database for PostgreSQL
- Tools
- Azure Container Apps dynamic sessions
- Bing Search
- Toolkits
- Azure AI Services
- Azure AI Services individual tools
- Azure Cognitive Services
- Azure AI Services individual tools
- Microsoft Office 365 email and calendar
- Office 365 individual tools
- Microsoft Azure PowerBI
- PowerBI individual tools
- PlayWright Browser Toolkit
- PlayWright Browser individual tools
- Graphs
- Azure Cosmos DB for Apache Gremlin
- Utilities
- Bing Search API
- More
- Microsoft Presidio

This page covers all LangChain integrations with Microsoft Azure and other Microsoft products.

## ​ Chat models

Microsoft offers three main options for accessing chat models through Azure:

1. Azure OpenAI \- Provides access to OpenAI’s powerful models like o3, 4.1, and other models through Microsoft Azure’s secure enterprise platform.
2. Azure AI \- Offers access to a variety of models from different providers including Anthropic, DeepSeek, Cohere, Phi and Mistral through a unified API.
3. Azure ML \- Allows deployment and management of your own custom models or fine-tuned open-source models with Azure Machine Learning.

pip

uv

Copy

pip install langchain-openai

Set the environment variables to get access to the `Azure OpenAI` service.

import os

os.environ["AZURE_OPENAI_ENDPOINT"] = "https://<your-endpoint.openai.azure.com/"
os.environ["AZURE_OPENAI_API_KEY"] = "your AzureOpenAI key"

See a usage example

from langchain_openai import AzureChatOpenAI

pip install -U langchain-azure-ai

Configure your API key and Endpoint.

export AZURE_AI_CREDENTIAL=your-api-key
export AZURE_AI_ENDPOINT=your-endpoint

from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel

llm = AzureAIChatCompletionsModel(
model_name="gpt-4o",
api_version="2024-05-01-preview",
)

### ​ Azure ML Chat Online Endpoint

See the documentation here for accessing chat
models hosted with Azure Machine Learning.

## ​ LLMs

### ​ Azure ML

See a usage example.

from langchain_community.llms.azureml_endpoint import AzureMLOnlineEndpoint

### ​ Azure OpenAI

from langchain_openai import AzureOpenAI

## ​ Embedding Models

Microsoft offers two main options for accessing embedding models through Azure:

### ​ Azure OpenAI

from langchain_openai import AzureOpenAIEmbeddings

### ​ Azure AI

from langchain_azure_ai.embeddings import AzureAIEmbeddingsModel

embed_model = AzureAIEmbeddingsModel(
model_name="text-embedding-ada-002"
)

## ​ Document loaders

>
> - `Microsoft OneLake`
> - `Azure Blob Storage`
> - `Azure Data Lake gen 2`

First, you need to install several python packages.

pip install azureml-fsspec, azure-ai-generative

from langchain.document_loaders import AzureAIDataLoader

First, you need to install a python package.

pip install azure-ai-documentintelligence

from langchain.document_loaders import AzureAIDocumentIntelligenceLoader

`Azure Blob Storage` is designed for:

- Serving images or documents directly to a browser.
- Storing files for distributed access.
- Streaming video and audio.
- Writing to log files.
- Storing data for backup and restore, disaster recovery, and archiving.
- Storing data for analysis by an on-premises or Azure-hosted service.

pip install langchain-azure-storage

See usage examples for the Azure Blob Storage Loader.

from langchain_azure_storage.document_loaders import AzureBlobStorageLoader

pip install o365

from langchain_community.document_loaders import OneDriveLoader

from langchain_community.document_loaders import OneDriveFileLoader

from langchain_community.document_loaders import UnstructuredWordDocumentLoader

The `UnstructuredExcelLoader` is used to load `Microsoft Excel` files. The loader works with both `.xlsx` and `.xls` files.
The page content will be the raw text of the Excel file. If you use the loader in `"elements"` mode, an HTML
representation of the Excel file will be available in the document metadata under the `text_as_html` key.See a usage example.

from langchain_community.document_loaders import UnstructuredExcelLoader

from langchain_community.document_loaders.sharepoint import SharePointLoader

from langchain_community.document_loaders import UnstructuredPowerPointLoader

### ​ Microsoft OneNote

First, let’s install dependencies:

pip install bs4 msal

from langchain_community.document_loaders.onenote import OneNoteLoader

pip install playwright unstructured

## ​ Memory

pip install langchain-azure-ai

Configure your Azure Cosmos DB connection:

from langchain_azure_ai.chat_message_histories import CosmosDBChatMessageHistory

history = CosmosDBChatMessageHistory(

)

## ​ Vector Stores

### ​ Azure Cosmos DB

AI agents can rely on Azure Cosmos DB as a unified memory system solution, enjoying speed, scale, and simplicity. This service successfully enabled OpenAI’s ChatGPT service to scale dynamically with high reliability and low maintenance. Powered by an atom-record-sequence engine, it is the world’s first globally distributed NoSQL, relational, and vector database service that offers a serverless mode.Below are two available Azure Cosmos DB APIs that can provide vector store functionalities.

##### Installation and Setup

See detailed configuration instructions.We need to install `langchain-azure-ai` and `pymongo` python packages.

pip install langchain-azure-ai pymongo

##### Deploy Azure Cosmos DB on Microsoft Azure

Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.With Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones.Sign Up for free to get started today.See a usage example.

from langchain_azure_ai.vectorstores import AzureCosmosDBMongoVCoreVectorSearch

##### Installation and Setup

See detail configuration instructions.We need to install `langchain-azure-ai` and `azure-cosmos` python packages.

pip install langchain-azure-ai azure-cosmos

##### Deploy Azure Cosmos DB on Microsoft Azure

Azure Cosmos DB offers a solution for modern apps and intelligent workloads by being very responsive with dynamic and elastic autoscale. It is available
in every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability.Sign Up for free to get started today.See a usage example.

from langchain_azure_ai.vectorstores import AzureCosmosDBNoSqlVectorSearch

See set up instructions for Azure Database for PostgreSQL.Simply use the connection string from your Azure Portal.Since Azure Database for PostgreSQL is open-source Postgres, you can use the LangChain’s Postgres support to connect to Azure Database for PostgreSQL.

By leveraging your current SQL Server databases for vector search, you can enhance data capabilities while minimizing expenses and avoiding the challenges of transitioning to new systems.

##### Installation and Setup

See detail configuration instructions.We need to install the `langchain-sqlserver` python package.

##### Deploy Azure SQL DB on Microsoft Azure

Sign Up for free to get started today.See a usage example.

from langchain_sqlserver import SQLServer_VectorStore

### ​ Azure AI Search

Azure AI Search is a cloud search service
that gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid
queries at scale. See here for usage examples.

from langchain_community.vectorstores.azuresearch import AzureSearch

## ​ Retrievers

>
> - A search engine for full text search over a search index containing user-owned content
> - Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation
> - Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more
> - Programmability through REST APIs and client libraries in Azure SDKs
> - Azure integration at the data layer, machine learning layer, and AI (AI Services)

See set up instructions.See a usage example.

from langchain_community.retrievers import AzureAISearchRetriever

## ​ Vector Store

See set up instructions for Azure Database for PostgreSQL.You need to enable pgvector extension in your database to use Postgres as a vector store. Once you have the extension enabled, you can use the PGVector in LangChain to connect to Azure Database for PostgreSQL.See a usage example. Simply use the connection string from your Azure Portal.

## ​ Tools

### ​ Azure Container Apps dynamic sessions

We need to get the `POOL_MANAGEMENT_ENDPOINT` environment variable from the Azure Container Apps service.
See the instructions here.We need to install a python package.

pip install langchain-azure-dynamic-sessions

from langchain_azure_dynamic_sessions import SessionsPythonREPLTool

### ​ Bing Search

Follow the documentation here to get a detail explanations and instructions of this tool.The environment variable `BING_SUBSCRIPTION_KEY` and `BING_SEARCH_URL` are required from Bing Search resource.

from langchain_community.tools.bing_search import BingSearchResults
from langchain_community.utilities import BingSearchAPIWrapper

api_wrapper = BingSearchAPIWrapper()
tool = BingSearchResults(api_wrapper=api_wrapper)

## ​ Toolkits

### ​ Azure AI Services

We need to install several python packages.

pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis

from langchain_community.agent_toolkits import azure_ai_services

#### ​ Azure AI Services individual tools

The `azure_ai_services` toolkit includes the following tools:

- Image Analysis: AzureAiServicesImageAnalysisTool
- Document Intelligence: AzureAiServicesDocumentIntelligenceTool
- Speech to Text: AzureAiServicesSpeechToTextTool
- Text to Speech: AzureAiServicesTextToSpeechTool
- Text Analytics for Health: AzureAiServicesTextAnalyticsForHealthTool

### ​ Azure Cognitive Services

from langchain_community.agent_toolkits import AzureCognitiveServicesToolkit

#### ​ Azure AI Services individual tools

The `azure_ai_services` toolkit includes the tools that queries the `Azure Cognitive Services`:

- `AzureCogsFormRecognizerTool`: Form Recognizer API
- `AzureCogsImageAnalysisTool`: Image Analysis API
- `AzureCogsSpeech2TextTool`: Speech2Text API
- `AzureCogsText2SpeechTool`: Text2Speech API
- `AzureCogsTextAnalyticsHealthTool`: Text Analytics for Health API

from langchain_community.tools.azure_cognitive_services import (
AzureCogsFormRecognizerTool,
AzureCogsImageAnalysisTool,
AzureCogsSpeech2TextTool,
AzureCogsText2SpeechTool,
AzureCogsTextAnalyticsHealthTool,
)

### ​ Microsoft Office 365 email and calendar

We need to install `O365` python package.

pip install O365

from langchain_community.agent_toolkits import O365Toolkit

#### ​ Office 365 individual tools

You can use individual tools from the Office 365 Toolkit:

- `O365CreateDraftMessage`: creating a draft email in Office 365
- `O365SearchEmails`: searching email messages in Office 365
- `O365SearchEvents`: searching calendar events in Office 365
- `O365SendEvent`: sending calendar events in Office 365
- `O365SendMessage`: sending an email in Office 365

from langchain_community.tools.office365 import O365CreateDraftMessage
from langchain_community.tools.office365 import O365SearchEmails
from langchain_community.tools.office365 import O365SearchEvents
from langchain_community.tools.office365 import O365SendEvent
from langchain_community.tools.office365 import O365SendMessage

### ​ Microsoft Azure PowerBI

We need to install `azure-identity` python package.

pip install azure-identity

from langchain_community.agent_toolkits import PowerBIToolkit
from langchain_community.utilities.powerbi import PowerBIDataset

#### ​ PowerBI individual tools

You can use individual tools from the Azure PowerBI Toolkit:

- `InfoPowerBITool`: getting metadata about a PowerBI Dataset
- `ListPowerBITool`: getting tables names
- `QueryPowerBITool`: querying a PowerBI Dataset

from langchain_community.tools.powerbi.tool import InfoPowerBITool
from langchain_community.tools.powerbi.tool import ListPowerBITool
from langchain_community.tools.powerbi.tool import QueryPowerBITool

pip install playwright lxml

from langchain_community.agent_toolkits import PlayWrightBrowserToolkit

#### ​ PlayWright Browser individual tools

You can use individual tools from the PlayWright Browser Toolkit.

from langchain_community.tools.playwright import ClickTool
from langchain_community.tools.playwright import CurrentWebPageTool
from langchain_community.tools.playwright import ExtractHyperlinksTool
from langchain_community.tools.playwright import ExtractTextTool
from langchain_community.tools.playwright import GetElementsTool
from langchain_community.tools.playwright import NavigateTool
from langchain_community.tools.playwright import NavigateBackTool

## ​ Graphs

### ​ Azure Cosmos DB for Apache Gremlin

We need to install a python package.

pip install gremlinpython

from langchain_community.graphs import GremlinGraph
from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship

## ​ Utilities

from langchain_community.utilities import BingSearchAPIWrapper

## ​ More

First, you need to install several python packages and download a `SpaCy` model.

pip install langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker
python -m spacy download en_core_web_lg

See usage examples.

from langchain_experimental.data_anonymizer import PresidioAnonymizer, PresidioReversibleAnonymizer

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Hugging Face\\
\\
Previous Ollama\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/ollama

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Popular Providers

Ollama

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Model interfaces
- Other

This page covers all LangChain integrations with Ollama.Ollama allows you to run open-source models (like `gpt-oss`) locally.For a complete list of supported models and variants, see the Ollama model library.

## ​ Model interfaces

**ChatOllama** \\
\\
Ollama chat models.\\
\\
Get started **OllamaEmbeddings** \\
\\
Ollama embedding models.\\
\\
Get started

## ​ Other

**OllamaLLM**\\
\\
(Legacy) Ollama text completion models.\\
\\
Get started

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Microsoft\\
\\
Previous ChatGroq\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/groq

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Popular Providers

ChatGroq

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Integration details
- Model features
- Setup
- Credentials
- Installation
- Instantiation
- Invocation
- API reference

This page makes reference to Groq, an AI hardware and software company. For information on how to use Grok models (provided by xAI), see the xAI provider page.

**API Reference**For detailed documentation of all features and configuration options, head to the `ChatGroq` API reference.

For a list of all Groq models, visit their docs.

## ​ Overview

### ​ Integration details

| Class | Package | Local | Serializable | JS support | Downloads | Version |
| --- | --- | --- | --- | --- | --- | --- |
| `ChatGroq` | `langchain-groq` | ❌ | beta | ✅ | !PyPI - Downloads | !PyPI - Version |

### ​ Model features

| Tool calling | Structured output | JSON mode | Image input | Audio input | Video input | Token-level streaming | Native async | Token usage | Logprobs |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | ✅ | ✅ | ✅ |

## ​ Setup

To access Groq models you’ll need to create a Groq account, get an API key, and install the `langchain-groq` integration package.

### ​ Credentials

Head to the Groq console to sign up to Groq and generate an API key. Once you’ve done this set the GROQ\_API\_KEY environment variable:

Copy

import getpass
import os

if "GROQ_API_KEY" not in os.environ:
os.environ["GROQ_API_KEY"] = getpass.getpass("Enter your Groq API key: ")

To enable automated tracing of your model calls, set your LangSmith API key:

os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
os.environ["LANGSMITH_TRACING"] = "true"

### ​ Installation

The LangChain Groq integration lives in the `langchain-groq` package:

pip install -qU langchain-groq

## ​ Instantiation

Now we can instantiate our model object and generate chat completions.

**Reasoning Format**If you choose to set a `reasoning_format`, you must ensure that the model you are using supports it. You can find a list of supported models in the Groq documentation.

from langchain_groq import ChatGroq

llm = ChatGroq(
model="qwen/qwen3-32b",
temperature=0,
max_tokens=None,
reasoning_format="parsed",
timeout=None,
max_retries=2,
# other params...
)

## ​ Invocation

messages = [\
(\
"system",\
"You are a helpful assistant that translates English to French. Translate the user sentence.",\
),\
("human", "I love programming."),\
]
ai_msg = llm.invoke(messages)
ai_msg

AIMessage(content="J'aime la programmation.", additional_kwargs={'reasoning_content': 'Okay, so I need to translate the sentence "I love programming." into French. Let me think about how to approach this. \n\nFirst, I know that "I" in French is "Je." That\'s straightforward. Now, the verb "love" in French is "aime" when referring to oneself. So, "I love" would be "J\'aime." \n\nNext, the word "programming." In French, programming is "la programmation." But wait, in French, when you talk about loving an activity, you often use the definite article. So, it would be "la programmation." \n\nPutting it all together, "I love programming" becomes "J\'aime la programmation." That sounds right. I think that\'s the correct translation. \n\nI should double-check to make sure I\'m not missing anything. Maybe I can think of similar phrases. For example, "I love reading" is "J\'aime lire," but when it\'s a noun, like "I love music," it\'s "J\'aime la musique." So, yes, using "la programmation" makes sense here. \n\nI don\'t think I need to change anything else. The sentence structure in French is Subject-Verb-Object, just like in English, so "J\'aime la programmation" should be correct. \n\nI guess another way to say it could be "J\'adore la programmation," using "adore" instead of "aime," but "aime" is more commonly used in this context. So, sticking with "J\'aime la programmation" is probably the best choice.\n'}, response_metadata={'token_usage': {'completion_tokens': 346, 'prompt_tokens': 23, 'total_tokens': 369, 'completion_time': 1.447541218, 'prompt_time': 0.000983386, 'queue_time': 0.009673684, 'total_time': 1.448524604}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_e98d30d035', 'finish_reason': 'stop', 'logprobs': None}, id='run--5679ae4f-f4e8-4931-bcd5-7304223832c0-0', usage_metadata={'input_tokens': 23, 'output_tokens': 346, 'total_tokens': 369})

print(ai_msg.content)

J'aime la programmation.

* * *

## ​ API reference

For detailed documentation of all ChatGroq features and configurations head to the API reference.

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ollama\\
\\
Previous Chat models\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/tools

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integrations by component

Tools and toolkits

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Search
- Code Interpreter
- Productivity
- Web Browsing
- Database
- Finance
- Integration Platforms
- All tools and toolkits

Tools are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed Search

The following table shows tools that execute online searches in some shape or form:

| Tool/Toolkit | Free/Paid | Return Data |
| --- | --- | --- |
| Bing Search | Paid | URL, Snippet, Title |
| Brave Search | Free | URL, Snippet, Title |
| DuckDuckgoSearch | Free | URL, Snippet, Title |
| Exa Search | 1000 free searches/month | URL, Author, Title, Published Date |
| Google Search | Paid | URL, Snippet, Title |
| Google Serper | Free | URL, Snippet, Title, Search Rank, Site Links |
| Jina Search | 1M Response Tokens Free | URL, Snippet, Title, Page Content |
| Mojeek Search | Paid | URL, Snippet, Title |
| Parallel Search | Paid | URL, Title, Excerpts |
| SearchApi | 100 Free Searches on Sign Up | URL, Snippet, Title, Search Rank, Site Links, Authors |
| SearxNG Search | Free | URL, Snippet, Title, Category |
| SerpApi | 250 Free Searches/Month | Answer |
| Tavily Search | 1000 free searches/month | URL, Content, Title, Images, Answer |
| You.com Search | Free for 60 days | URL, Title, Page Content |

## ​ Code Interpreter

The following table shows tools that can be used as code interpreters:

| Tool/Toolkit | Supported Languages | Sandbox Lifetime | Supports File Uploads | Return Types | Supports Self-Hosting |
| --- | --- | --- | --- | --- | --- |
| Azure Container Apps dynamic sessions | Python | 1 Hour | ✅ | Text, Images | ❌ |
| Bearly Code Interpreter | Python | Resets on Execution | ✅ | Text | ❌ |
| Riza Code Interpreter | Python, JavaScript, PHP, Ruby | Resets on Execution | ✅ | Text | ✅ |

## ​ Productivity

The following table shows tools that can be used to automate tasks in productivity tools:

| Tool/Toolkit | Pricing |
| --- | --- |
| GitHub Toolkit | Free |
| GitLab Toolkit | Free for personal project |
| Gmail Toolkit | Free, with limit of 250 quota units per user per second |
| Infobip Tool | Free trial, with variable pricing after |
| Jira Toolkit | Free, with rate limits |
| Office365 Toolkit | Free with Office365, includes rate limits |
| Slack Toolkit | Free |
| Twilio Tool | Free trial, with pay-as-you-go pricing after |

## ​ Web Browsing

The following table shows tools that can be used to automate tasks in web browsers:

| Tool/Toolkit | Pricing | Supports Interacting with the Browser |
| --- | --- | --- |
| AgentQL Toolkit | Free trial, with pay-as-you-go and flat rate plans after | ✅ |
| Hyperbrowser Browser Agent Tools | Free trial, with flat rate plans and pre-paid credits after | ✅ |
| Hyperbrowser Web Scraping Tools | Free trial, with flat rate plans and pre-paid credits after | ❌ |
| MultiOn Toolkit | 40 free requests/day | ✅ |
| Oxylabs Web Scraper API | Free trial, with flat rate plans and pre-paid credits after | ❌ |
| PlayWright Browser Toolkit | Free | ✅ |
| Requests Toolkit | Free | ❌ |

## ​ Database

The following table shows tools that can be used to automate tasks in databases:

| Tool/Toolkit | Allowed Operations |
| --- | --- |
| Cassandra Database Toolkit | SELECT and schema introspection |
| MCP Toolbox | Any SQL operation |
| SQLDatabase Toolkit | Any SQL operation |
| Spark SQL Toolkit | Any SQL operation |
| Drasi Toolkit | Real-time database change detection |

## ​ Finance

The following table shows tools that can be used to execute financial transactions such as payments, purchases, and more:

| Tool/Toolkit | Pricing | Capabilities |
| --- | --- | --- |
| GOAT | Free | Create and receive payments, purchase physical goods, make investments, and more. |
| Privy | Free | Create wallets with configurable permissions and execute transactions with speed. |

## ​ Integration Platforms

The following platforms provide access to multiple tools and services through a unified interface:

| Tool/Toolkit | Number of Integrations | Pricing | Key Features |
| --- | --- | --- | --- |
| Composio | 500+ | Free tier available | OAuth handling, event-driven workflows, multi-user support |

## ​ All tools and toolkits

**ADS4GPTs** \\
\\
View guide **AgentQL** \\
\\
View guide **AINetwork Toolkit** \\
\\
View guide **Alpha Vantage** \\
\\
View guide **Amadeus Toolkit** \\
\\
View guide **Anchor Browser** \\
\\
View guide **Apify Actor** \\
\\
View guide **ArXiv** \\
\\
View guide **AskNews** \\
\\
View guide **AWS Lambda** \\
\\
View guide **Azure AI Services Toolkit** \\
\\
View guide **Azure Cognitive Services Toolkit** \\
\\
View guide **Azure Container Apps Dynamic Sessions** \\
\\
View guide **Shell (bash)** \\
\\
View guide **Bearly Code Interpreter** \\
\\
View guide **Bing Search** \\
\\
View guide **Bodo DataFrames** \\
\\
View guide **Brave Search** \\
\\
View guide **BrightData Web Scraper API** \\
\\
View guide **BrightData SERP** \\
\\
View guide **BrightData Unlocker** \\
\\
View guide **Cassandra Database Toolkit** \\
\\
View guide **CDP** \\
\\
View guide **ChatGPT Plugins** \\
\\
View guide **ClickUp Toolkit** \\
\\
View guide **Cogniswitch Toolkit** \\
\\
View guide **Compass DeFi Toolkit** \\
\\
View guide **Composio** \\
\\
View guide **Connery Toolkit** \\
\\
View guide **Dall-E Image Generator** \\
\\
View guide **Dappier** \\
\\
View guide **Databricks Unity Catalog** \\
\\
View guide **DataForSEO** \\
\\
View guide **Dataherald** \\
\\
View guide **Daytona Data Analysis** \\
\\
View guide **DuckDuckGo Search** \\
\\
View guide **Discord** \\
\\
View guide **Drasi** \\
\\
View guide **E2B Data Analysis** \\
\\
View guide **Eden AI** \\
\\
View guide **ElevenLabs Text2Speech** \\
\\
View guide **Exa Search** \\
\\
View guide **File System** \\
\\
View guide **Financial Datasets Toolkit** \\
\\
View guide **FMP Data** \\
\\
View guide **GitHub Toolkit** \\
\\
View guide **GitLab Toolkit** \\
\\
View guide **Gmail Toolkit** \\
\\
View guide **GOAT** \\
\\
View guide **Privy** \\
\\
View guide **Golden Query** \\
\\
View guide **Google Books** \\
\\
View guide **Google Calendar Toolkit** \\
\\
View guide **Google Cloud Text-to-Speech** \\
\\
View guide **Google Drive** \\
\\
View guide **Google Finance** \\
\\
View guide **Google Imagen** \\
\\
View guide **Google Jobs** \\
\\
View guide **Google Lens** \\
\\
View guide **Google Places** \\
\\
View guide **Google Scholar** \\
\\
View guide **Google Search** \\
\\
View guide **Google Serper** \\
\\
View guide **Google Trends** \\
\\
View guide **Gradio** \\
\\
View guide **GraphQL** \\
\\
View guide **HuggingFace Hub Tools** \\
\\
View guide **Human as a Tool** \\
\\
View guide **Hyperbrowser Browser Agent Tools** \\
\\
View guide **Hyperbrowser Web Scraping Tools** \\
\\
View guide **IBM watsonx.ai** \\
\\
View guide **IBM watsonx.ai (SQL)** \\
\\
View guide **IFTTT WebHooks** \\
\\
View guide **Infobip** \\
\\
View guide **Ionic Shopping Tool** \\
\\
View guide **Jenkins** \\
\\
View guide **Jina Search** \\
\\
View guide **Jira Toolkit** \\
\\
View guide **JSON Toolkit** \\
\\
View guide **Lemon Agent** \\
\\
View guide **Linkup Search Tool** \\
\\
View guide **Memgraph** \\
\\
View guide **Memorize** \\
\\
View guide **Mojeek Search** \\
\\
View guide **MultiOn Toolkit** \\
\\
View guide **NASA Toolkit** \\
\\
View guide **Naver Search** \\
\\
View guide **Nuclia Understanding** \\
\\
View guide **NVIDIA Riva** \\
\\
View guide **Office365 Toolkit** \\
\\
View guide **OpenAPI Toolkit** \\
\\
View guide **Natural Language API Toolkits** \\
\\
View guide **OpenGradient** \\
\\
View guide **OpenWeatherMap** \\
\\
View guide **Oracle AI Vector Search** \\
\\
View guide **Oxylabs** \\
\\
View guide **Pandas Dataframe** \\
\\
View guide **Passio NutritionAI** \\
\\
View guide **Parallel Extract** \\
\\
View guide **Parallel Search** \\
\\
View guide **Permit** \\
\\
View guide **PlayWright Browser Toolkit** \\
\\
View guide **Polygon IO Toolkit** \\
\\
View guide **PowerBI Toolkit** \\
\\
View guide **Prolog** \\
\\
View guide **PubMed** \\
\\
View guide **Python REPL** \\
\\
View guide **Reddit Search** \\
\\
View guide **Requests Toolkit** \\
\\
View guide **Riza Code Interpreter** \\
\\
View guide **Robocorp Toolkit** \\
\\
View guide **Salesforce** \\
\\
View guide **SceneXplain** \\
\\
View guide **ScrapeGraph** \\
\\
View guide **Scrapeless Crawl** \\
\\
View guide **Scrapeless Scraping API** \\
\\
View guide **Scrapeless Universal Scraping** \\
\\
View guide **SearchApi** \\
\\
View guide **SearxNG Search** \\
\\
View guide **Semantic Scholar API** \\
\\
View guide **SerpApi** \\
\\
View guide **Slack Toolkit** \\
\\
View guide **Spark SQL Toolkit** \\
\\
View guide **SQLDatabase Toolkit** \\
\\
View guide **StackExchange** \\
\\
View guide **Steam Toolkit** \\
\\
View guide **Stripe** \\
\\
View guide **Tableau** \\
\\
View guide **Taiga** \\
\\
View guide **Tavily Extract** \\
\\
View guide **Tavily Search** \\
\\
View guide **Tilores** \\
\\
View guide **MCP Toolbox** \\
\\
View guide **Twilio** \\
\\
View guide **Upstage** \\
\\
View guide **Valthera** \\
\\
View guide **ValyuContext** \\
\\
View guide **Vectara** \\
\\
View guide **Wikidata** \\
\\
View guide **Wikipedia** \\
\\
View guide **Wolfram Alpha** \\
\\
View guide **WRITER Tools** \\
\\
View guide **Yahoo Finance News** \\
\\
View guide **You.com Search** \\
\\
View guide **YouTube** \\
\\
View guide **Zapier Natural Language Actions** \\
\\
View guide **ZenGuard AI** \\
\\
View guide

If you’d like to contribute an integration, see Contributing integrations.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Chat models\\
\\
Previous Provider-specific middleware\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/middleware

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integrations by component

Provider-specific middleware

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

Middleware designed for specific providers. Learn more about middleware.

| Provider | Middleware available |
| --- | --- |
| Anthropic | Prompt caching, bash tool, text editor, memory, and file search |
| OpenAI | Content moderation |

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Tools and toolkits\\
\\
Previous Retrievers\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/retrievers

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integrations by component

Retrievers

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Bring-your-own documents
- External index
- All retrievers

A retriever is an interface that returns documents given an unstructured query.
It is more general than a vector store.
A retriever does not need to be able to store documents, only to return (or retrieve) them.
Retrievers can be created from vector stores, but are also broad enough to include Wikipedia search and Amazon Kendra.Retrievers accept a string query as input and return a list of `Document` objects as output.Note that all vector stores can be cast to retrievers. Refer to the vector store integration docs for available vector stores.
This page lists custom retrievers, implemented via subclassing BaseRetriever.

## ​ Bring-your-own documents

The below retrievers allow you to index and search a custom corpus of documents.

| Retriever | Self-host | Cloud offering | Package |
| --- | --- | --- | --- |
| `AmazonKnowledgeBasesRetriever` | ❌ | ✅ | `langchain-aws` |
| `AzureAISearchRetriever` | ❌ | ✅ | `langchain-community` |
| `ElasticsearchRetriever` | ✅ | ✅ | `langchain-elasticsearch` |
| `VertexAISearchRetriever` | ❌ | ✅ | `langchain-google-community` |

## ​ External index

The below retrievers will search over an external index (e.g., constructed from Internet data or similar).

| Retriever | Source | Package |
| --- | --- | --- |
| `ArxivRetriever` | Scholarly articles on arxiv.org | `langchain-community` |
| `TavilySearchAPIRetriever` | Internet search | `langchain-community` |
| `WikipediaRetriever` | Wikipedia articles | `langchain-community` |

## ​ All retrievers

**Activeloop Deep Memory** \\
\\
View guide **Amazon Kendra** \\
\\
View guide **Arcee** \\
\\
View guide **Arxiv** \\
\\
View guide **AskNews** \\
\\
View guide **Azure AI Search** \\
\\
View guide **Bedrock (Knowledge Bases)** \\
\\
View guide **BM25** \\
\\
View guide **Box** \\
\\
View guide **BREEBS (Open Knowledge)** \\
\\
View guide **Chaindesk** \\
\\
View guide **ChatGPT plugin** \\
\\
View guide **Cognee** \\
\\
View guide **Cohere reranker** \\
\\
View guide **Cohere RAG** \\
\\
View guide **Contextual AI Reranker** \\
\\
View guide **Dappier** \\
\\
View guide **DocArray** \\
\\
View guide **Dria** \\
\\
View guide **ElasticSearch BM25** \\
\\
View guide **Elasticsearch** \\
\\
View guide **Embedchain** \\
\\
View guide **FlashRank reranker** \\
\\
View guide **Fleet AI Context** \\
\\
View guide **Galaxia** \\
\\
View guide **Google Drive** \\
\\
View guide **Google Vertex AI Search** \\
\\
View guide **Graph RAG** \\
\\
View guide **GreenNode** \\
\\
View guide **IBM watsonx.ai** \\
\\
View guide **JaguarDB Vector Database** \\
\\
View guide **Kay.ai** \\
\\
View guide **Kinetica Vectorstore** \\
\\
View guide **kNN** \\
\\
View guide **LinkupSearchRetriever** \\
\\
View guide **LLMLingua Document Compressor** \\
\\
View guide **LOTR (Merger Retriever)** \\
\\
View guide **Metal** \\
\\
View guide **NanoPQ (Product Quantization)** \\
\\
View guide **Nebius** \\
\\
View guide **needle** \\
\\
View guide **Nimble** \\
\\
View guide **Outline** \\
\\
View guide **Permit** \\
\\
View guide **Pinecone Hybrid Search** \\
\\
View guide **Pinecone Rerank** \\
\\
View guide **PubMed** \\
\\
View guide **Qdrant Sparse Vector** \\
\\
View guide **RAGatouille** \\
\\
View guide **RePhraseQuery** \\
\\
View guide **Rememberizer** \\
\\
View guide **SEC filing** \\
\\
View guide **SVM** \\
\\
View guide **TavilySearchAPI** \\
\\
View guide **TF-IDF** \\
\\
View guide **NeuralDB** \\
\\
View guide **ValyuContext** \\
\\
View guide **Vectorize** \\
\\
View guide **Vespa** \\
\\
View guide **Wikipedia** \\
\\
View guide **You.com** \\
\\
View guide **Zep Cloud** \\
\\
View guide **Zep Open Source** \\
\\
View guide **Zotero** \\
\\
View guide

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Provider-specific middleware\\
\\
Previous Text splitters\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/splitters

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integrations by component

Text splitters

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Text structure-based
- Length-based
- Document structure-based

pip

uv

Copy

pip install -U langchain-text-splitters

**Text splitters** break large docs into smaller chunks that will be retrievable individually and fit within model context window limit.There are several strategies for splitting documents, each with its own advantages.

For most use cases, start with the `RecursiveCharacterTextSplitter`. It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application.

## ​ Text structure-based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain’s `RecursiveCharacterTextSplitter` implements this concept:

- The `RecursiveCharacterTextSplitter` attempts to keep larger units (e.g., paragraphs) intact.
- If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).
- This process continues down to the word level if necessary.

Example usage:

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)
texts = text_splitter.split_text(document)

**Available text splitters**:

- Recursively split text

## ​ Length-based

An intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn’t exceed a specified size limit. Key benefits of length-based splitting:

- Straightforward implementation
- Consistent chunk sizes
- Easily adaptable to different model requirements

Types of length-based splitting:

- Token-based: Splits text based on the number of tokens, which is useful when working with language models.
- Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.

Example implementation using LangChain’s `CharacterTextSplitter` with token-based splitting:

from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
encoding_name="cl100k_base", chunk_size=100, chunk_overlap=0
)
texts = text_splitter.split_text(document)

- Split by tokens
- Split by characters

## ​ Document structure-based

Some documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, it’s beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting:

- Preserves the logical organization of the document
- Maintains context within each chunk
- Can be more effective for downstream tasks like retrieval or summarization

Examples of structure-based splitting:

- Markdown: Split based on headers (e.g., `#`, `##`, `###`)
- HTML: Split using tags
- JSON: Split by object or array elements
- Code: Split by functions, classes, or logical blocks

- Split Markdown
- Split JSON
- Split code
- Split HTML

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Retrievers\\
\\
Previous Embedding models\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/text_embedding

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integrations by component

Embedding models

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- How it works
- Similarity metrics
- Interface
- Top integrations
- Caching
- All embedding models

## ​ Overview

This overview covers **text-based embedding models**. LangChain does not currently support multimodal embeddings.See top embedding models.

Embedding models transform raw text—such as a sentence, paragraph, or tweet—into a fixed-length vector of numbers that captures its **semantic meaning**. These vectors allow machines to compare and search text based on meaning rather than exact words.In practice, this means that texts with similar ideas are placed close together in the vector space. For example, instead of matching only the phrase _“machine learning”_, embeddings can surface documents that discuss related concepts even when different wording is used.

### ​ How it works

1. **Vectorization** — The model encodes each input string as a high-dimensional vector.
2. **Similarity scoring** — Vectors are compared using mathematical metrics to measure how closely related the underlying texts are.

### ​ Similarity metrics

Several metrics are commonly used to compare embeddings:

- **Cosine similarity** — measures the angle between two vectors.
- **Euclidean distance** — measures the straight-line distance between points.
- **Dot product** — measures how much one vector projects onto another.

Here’s an example of computing cosine similarity between two vectors:

Copy

import numpy as np

def cosine_similarity(vec1, vec2):
dot = np.dot(vec1, vec2)
return dot / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

similarity = cosine_similarity(query_embedding, document_embedding)
print("Cosine Similarity:", similarity)

## ​ Interface

LangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the Embeddings interface.Two main methods are available:

- `embed_documents(texts: List[str]) → List[List[float]]`: Embeds a list of documents.
- `embed_query(text: str) → List[float]`: Embeds a single query.

The interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.

## ​ Top integrations

| Model | Package |
| --- | --- |
| `OpenAIEmbeddings` | `langchain-openai` |
| `AzureOpenAIEmbeddings` | `langchain-openai` |
| `GoogleGenerativeAIEmbeddings` | `langchain-google-genai` |
| `OllamaEmbeddings` | `langchain-ollama` |
| `TogetherEmbeddings` | `langchain-together` |
| `FireworksEmbeddings` | `langchain-fireworks` |
| `MistralAIEmbeddings` | `langchain-mistralai` |
| `CohereEmbeddings` | `langchain-cohere` |
| `NomicEmbeddings` | `langchain-nomic` |
| `FakeEmbeddings` | `langchain-core` |
| `DatabricksEmbeddings` | `databricks-langchain` |
| `WatsonxEmbeddings` | `langchain-ibm` |
| `NVIDIAEmbeddings` | `langchain-nvidia` |
| `AimlapiEmbeddings` | `langchain-aimlapi` |

## ​ Caching

Embeddings can be stored or temporarily cached to avoid needing to recompute them.Caching embeddings can be done using a `CacheBackedEmbeddings`. This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache.The main supported way to initialize a `CacheBackedEmbeddings` is `from_bytes_store`. It takes the following parameters:

- **`underlying_embedder`**: The embedder to use for embedding.
- **`document_embedding_cache`**: Any `ByteStore` for caching document embeddings.
- **`batch_size`**: (optional, defaults to `None`) The number of documents to embed between store updates.
- **`namespace`**: (optional, defaults to `""`) The namespace to use for the document cache. Helps avoid collisions (e.g., set it to the embedding model name).
- **`query_embedding_cache`**: (optional, defaults to `None`) A `ByteStore` for caching query embeddings, or `True` to reuse the same store as `document_embedding_cache`.

import time
from langchain_classic.embeddings import CacheBackedEmbeddings
from langchain_classic.storage import LocalFileStore
from langchain_core.vectorstores import InMemoryVectorStore

# Create your underlying embeddings model
underlying_embeddings = ... # e.g., OpenAIEmbeddings(), HuggingFaceEmbeddings(), etc.

# Store persists embeddings to the local filesystem
# This isn't for production use, but is useful for local
store = LocalFileStore("./cache/")

cached_embedder = CacheBackedEmbeddings.from_bytes_store(
underlying_embeddings,
store,
namespace=underlying_embeddings.model
)

# Example: caching a query embedding
tic = time.time()
print(cached_embedder.embed_query("Hello, world!"))
print(f"First call took: {time.time() - tic:.2f} seconds")

# Subsequent calls use the cache
tic = time.time()
print(cached_embedder.embed_query("Hello, world!"))
print(f"Second call took: {time.time() - tic:.2f} seconds")

In production, you would typically use a more robust persistent store, such as a database or cloud storage. Please see stores integrations for options.

## ​ All embedding models

**Aleph Alpha** \\
\\
View guide **Anyscale** \\
\\
View guide **Ascend** \\
\\
View guide **AI/ML API** \\
\\
View guide **AwaDB** \\
\\
View guide **AzureOpenAI** \\
\\
View guide **Baichuan Text Embeddings** \\
\\
View guide **Baidu Qianfan** \\
\\
View guide **Baseten** \\
\\
View guide **Bedrock** \\
\\
View guide **BGE on Hugging Face** \\
\\
View guide **Bookend AI** \\
\\
View guide **Clarifai** \\
\\
View guide **Cloudflare Workers AI** \\
\\
View guide **Clova Embeddings** \\
\\
View guide **Cohere** \\
\\
View guide **DashScope** \\
\\
View guide **Databricks** \\
\\
View guide **DeepInfra** \\
\\
View guide **EDEN AI** \\
\\
View guide **Elasticsearch** \\
\\
View guide **Embaas** \\
\\
View guide **Fake Embeddings** \\
\\
View guide **FastEmbed by Qdrant** \\
\\
View guide **Fireworks** \\
\\
View guide **Google Gemini** \\
\\
View guide **Google Vertex AI** \\
\\
View guide **GPT4All** \\
\\
View guide **Gradient** \\
\\
View guide **GreenNode** \\
\\
View guide **Hugging Face** \\
\\
View guide **IBM watsonx.ai** \\
\\
View guide **Infinity** \\
\\
View guide **Instruct Embeddings** \\
\\
View guide **IPEX-LLM CPU** \\
\\
View guide **IPEX-LLM GPU** \\
\\
View guide **Isaacus** \\
\\
View guide **Intel Extension for Transformers** \\
\\
View guide **Jina** \\
\\
View guide **John Snow Labs** \\
\\
View guide **LASER** \\
\\
View guide **Lindorm** \\
\\
View guide **Llama.cpp** \\
\\
View guide **LLMRails** \\
\\
View guide **LocalAI** \\
\\
View guide **MiniMax** \\
\\
View guide **MistralAI** \\
\\
View guide **Model2Vec** \\
\\
View guide **ModelScope** \\
\\
View guide **MosaicML** \\
\\
View guide **Naver** \\
\\
View guide **Nebius** \\
\\
View guide **Netmind** \\
\\
View guide **NLP Cloud** \\
\\
View guide **Nomic** \\
\\
View guide **NVIDIA NIMs** \\
\\
View guide **Oracle Cloud Infrastructure** \\
\\
View guide **Ollama** \\
\\
View guide **OpenClip** \\
\\
View guide **OpenAI** \\
\\
View guide **OpenVINO** \\
\\
View guide **Optimum Intel** \\
\\
View guide **Oracle AI Vector Search** \\
\\
View guide **OVHcloud** \\
\\
View guide **Pinecone Embeddings** \\
\\
View guide **PredictionGuard** \\
\\
View guide **PremAI** \\
\\
View guide **SageMaker** \\
\\
View guide **SambaNova** \\
\\
View guide **Self Hosted** \\
\\
View guide **Sentence Transformers** \\
\\
View guide **Solar** \\
\\
View guide **SpaCy** \\
\\
View guide **SparkLLM** \\
\\
View guide **TensorFlow Hub** \\
\\
View guide **Text Embeddings Inference** \\
\\
View guide **TextEmbed** \\
\\
View guide **Titan Takeoff** \\
\\
View guide **Together AI** \\
\\
View guide **Upstage** \\
\\
View guide **Volc Engine** \\
\\
View guide **Voyage AI** \\
\\
View guide **Xinference** \\
\\
View guide **YandexGPT** \\
\\
View guide **ZhipuAI** \\
\\
View guide

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Text splitters\\
\\
Previous Vector stores\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/vectorstores

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integrations by component

Vector stores

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Interface
- Initialization
- Adding documents
- Deleting documents
- Similarity search
- Similarity metrics & indexing
- Metadata filtering
- Top integrations
- All vector stores

## ​ Overview

A vector store stores embedded data and performs similarity search.

📤 Query phase (retrieval)

📥 Indexing phase (store)

📄 Documents

🔢 Embedding model

🔘 Embedding vectors

Vector store

❓ Query text

🔘 Query vector

🔍 Similarity search

📄 Top-k results

### ​ Interface

LangChain provides a unified interface for vector stores, allowing you to:

- `add_documents` \- Add documents to the store.
- `delete` \- Remove stored documents by ID.
- `similarity_search` \- Query for semantically similar documents.

This abstraction lets you switch between different implementations without altering your application logic.

### ​ Initialization

To initialize a vector store, provide it with an embedding model:

Copy

from langchain_core.vectorstores import InMemoryVectorStore
vector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())

### ​ Adding documents

Add `Document` objects (holding `page_content` and optional metadata) like so:

vector_store.add_documents(documents=[doc1, doc2], ids=["id1", "id2"])

### ​ Deleting documents

Delete by specifying IDs:

vector_store.delete(ids=["id1"])

### ​ Similarity search

Issue a semantic query using `similarity_search`, which returns the closest embedded documents:

similar_docs = vector_store.similarity_search("your query here")

Many vector stores support parameters like:

- `k` — number of results to return
- `filter` — conditional filtering based on metadata

### ​ Similarity metrics & indexing

Embedding similarity may be computed using:

- **Cosine similarity**
- **Euclidean distance**
- **Dot product**

Efficient search often employs indexing methods such as HNSW (Hierarchical Navigable Small World), though specifics depend on the vector store.

### ​ Metadata filtering

Filtering by metadata (e.g., source, date) can refine search results:

vector_store.similarity_search(
"query",
k=3,
filter={"source": "tweets"}
)

## ​ Top integrations

**Select embedding model:**

OpenAI

pip

uv

pip install -qU langchain-openai

import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter API key for OpenAI: ")

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

Azure

pip install -qU langchain-azure-ai

if not os.environ.get("AZURE_OPENAI_API_KEY"):
os.environ["AZURE_OPENAI_API_KEY"] = getpass.getpass("Enter API key for Azure: ")

from langchain_openai import AzureOpenAIEmbeddings

embeddings = AzureOpenAIEmbeddings(
azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
openai_api_version=os.environ["AZURE_OPENAI_API_VERSION"],
)

Google Gemini

pip install -qU langchain-google-genai

if not os.environ.get("GOOGLE_API_KEY"):
os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter API key for Google Gemini: ")

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

Google Vertex

pip install -qU langchain-google-vertexai

from langchain_google_vertexai import VertexAIEmbeddings

embeddings = VertexAIEmbeddings(model="text-embedding-005")

AWS

pip install -qU langchain-aws

from langchain_aws import BedrockEmbeddings

embeddings = BedrockEmbeddings(model_id="amazon.titan-embed-text-v2:0")

HuggingFace

pip install -qU langchain-huggingface

from langchain_huggingface import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")

Ollama

pip install -qU langchain-ollama

from langchain_ollama import OllamaEmbeddings

embeddings = OllamaEmbeddings(model="llama3")

Cohere

pip install -qU langchain-cohere

if not os.environ.get("COHERE_API_KEY"):
os.environ["COHERE_API_KEY"] = getpass.getpass("Enter API key for Cohere: ")

from langchain_cohere import CohereEmbeddings

embeddings = CohereEmbeddings(model="embed-english-v3.0")

Mistral AI

pip install -qU langchain-mistralai

if not os.environ.get("MISTRALAI_API_KEY"):
os.environ["MISTRALAI_API_KEY"] = getpass.getpass("Enter API key for MistralAI: ")

from langchain_mistralai import MistralAIEmbeddings

embeddings = MistralAIEmbeddings(model="mistral-embed")

Nomic

pip install -qU langchain-nomic

if not os.environ.get("NOMIC_API_KEY"):
os.environ["NOMIC_API_KEY"] = getpass.getpass("Enter API key for Nomic: ")

from langchain_nomic import NomicEmbeddings

embeddings = NomicEmbeddings(model="nomic-embed-text-v1.5")

NVIDIA

pip install -qU langchain-nvidia-ai-endpoints

if not os.environ.get("NVIDIA_API_KEY"):
os.environ["NVIDIA_API_KEY"] = getpass.getpass("Enter API key for NVIDIA: ")

from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings

embeddings = NVIDIAEmbeddings(model="NV-Embed-QA")

Voyage AI

pip install -qU langchain-voyageai

if not os.environ.get("VOYAGE_API_KEY"):
os.environ["VOYAGE_API_KEY"] = getpass.getpass("Enter API key for Voyage AI: ")

from langchain-voyageai import VoyageAIEmbeddings

embeddings = VoyageAIEmbeddings(model="voyage-3")

IBM watsonx

pip install -qU langchain-ibm

if not os.environ.get("WATSONX_APIKEY"):
os.environ["WATSONX_APIKEY"] = getpass.getpass("Enter API key for IBM watsonx: ")

from langchain_ibm import WatsonxEmbeddings

embeddings = WatsonxEmbeddings(
model_id="ibm/slate-125m-english-rtrvr",
url="https://us-south.ml.cloud.ibm.com",

)

Fake

pip install -qU langchain-core

from langchain_core.embeddings import DeterministicFakeEmbedding

embeddings = DeterministicFakeEmbedding(size=4096)

xAI

pip install -qU langchain-xai

if not os.environ.get("XAI_API_KEY"):
os.environ["XAI_API_KEY"] = getpass.getpass("Enter API key for xAI: ")

from langchain.chat_models import init_chat_model

model = init_chat_model("grok-2", model_provider="xai")

Perplexity

pip install -qU langchain-perplexity

if not os.environ.get("PPLX_API_KEY"):
os.environ["PPLX_API_KEY"] = getpass.getpass("Enter API key for Perplexity: ")

model = init_chat_model("llama-3.1-sonar-small-128k-online", model_provider="perplexity")

DeepSeek

pip install -qU langchain-deepseek

if not os.environ.get("DEEPSEEK_API_KEY"):
os.environ["DEEPSEEK_API_KEY"] = getpass.getpass("Enter API key for DeepSeek: ")

model = init_chat_model("deepseek-chat", model_provider="deepseek")

**Select vector store:**

In-memory

from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)

Amazon OpenSearch

pip install -qU boto3

from opensearchpy import RequestsHttpConnection

service = "es" # must set the service as 'es'
region = "us-east-2"
credentials = boto3.Session(
aws_access_key_id="xxxxxx", aws_secret_access_key="xxxxx"
).get_credentials()
awsauth = AWS4Auth("xxxxx", "xxxxxx", region, service, session_token=credentials.token)

vector_store = OpenSearchVectorSearch.from_documents(
docs,
embeddings,
opensearch_url="host url",
http_auth=awsauth,
timeout=300,
use_ssl=True,
verify_certs=True,
connection_class=RequestsHttpConnection,
index_name="test-index",
)

Astra DB

pip install -qU langchain-astradb

from langchain_astradb import AstraDBVectorStore

vector_store = AstraDBVectorStore(
embedding=embeddings,
api_endpoint=ASTRA_DB_API_ENDPOINT,
collection_name="astra_vector_langchain",
token=ASTRA_DB_APPLICATION_TOKEN,
namespace=ASTRA_DB_NAMESPACE,
)

Azure Cosmos DB NoSQL

pip install -qU langchain-azure-ai azure-cosmos

from langchain_azure_ai.vectorstores.azure_cosmos_db_no_sql import (
AzureCosmosDBNoSqlVectorSearch,
)
vector_search = AzureCosmosDBNoSqlVectorSearch.from_documents(
documents=docs,
embedding=openai_embeddings,
cosmos_client=cosmos_client,
database_name=database_name,
container_name=container_name,
vector_embedding_policy=vector_embedding_policy,
full_text_policy=full_text_policy,
indexing_policy=indexing_policy,
cosmos_container_properties=cosmos_container_properties,
cosmos_database_properties={},
full_text_search_enabled=True,
)

Azure Cosmos DB Mongo vCore

pip install -qU langchain-azure-ai pymongo

from langchain_azure_ai.vectorstores.azure_cosmos_db_mongo_vcore import (
AzureCosmosDBMongoVCoreVectorSearch,
)

vectorstore = AzureCosmosDBMongoVCoreVectorSearch.from_documents(
docs,
openai_embeddings,
collection=collection,
index_name=INDEX_NAME,
)

Chroma

pip install -qU langchain-chroma

from langchain_chroma import Chroma

vector_store = Chroma(
collection_name="example_collection",
embedding_function=embeddings,
persist_directory="./chroma_langchain_db", # Where to save data locally, remove if not necessary
)

FAISS

pip install -qU langchain-community

import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS

embedding_dim = len(embeddings.embed_query("hello world"))
index = faiss.IndexFlatL2(embedding_dim)

vector_store = FAISS(
embedding_function=embeddings,
index=index,
docstore=InMemoryDocstore(),
index_to_docstore_id={},
)

Milvus

pip install -qU langchain-milvus

from langchain_milvus import Milvus

URI = "./milvus_example.db"

vector_store = Milvus(
embedding_function=embeddings,
connection_args={"uri": URI},
index_params={"index_type": "FLAT", "metric_type": "L2"},
)

MongoDB

pip install -qU langchain-mongodb

from langchain_mongodb import MongoDBAtlasVectorSearch

vector_store = MongoDBAtlasVectorSearch(
embedding=embeddings,
collection=MONGODB_COLLECTION,
index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,
relevance_score_fn="cosine",
)

PGVector

pip install -qU langchain-postgres

from langchain_postgres import PGVector

vector_store = PGVector(
embeddings=embeddings,
collection_name="my_docs",
connection="postgresql+psycopg://..."
)

PGVectorStore

from langchain_postgres import PGEngine, PGVectorStore

$engine = PGEngine.from_connection_string(
url="postgresql+psycopg://..."
)

vector_store = PGVectorStore.create_sync(
engine=pg_engine,
table_name='test_table',
embedding_service=embedding
)

Pinecone

pip install -qU langchain-pinecone

from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

pc = Pinecone(api_key=...)
index = pc.Index(index_name)

vector_store = PineconeVectorStore(embedding=embeddings, index=index)

Qdrant

pip install -qU langchain-qdrant

from qdrant_client.models import Distance, VectorParams
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient

client = QdrantClient(":memory:")

vector_size = len(embeddings.embed_query("sample text"))

if not client.collection_exists("test"):
client.create_collection(
collection_name="test",
vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
)
vector_store = QdrantVectorStore(
client=client,
collection_name="test",
embedding=embeddings,
)

| Vectorstore | Delete by ID | Filtering | Search by Vector | Search with score | Async | Passes Standard Tests | Multi Tenancy | IDs in add Documents |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| `AstraDBVectorStore` | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| `AzureCosmosDBNoSqlVectorStore` | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ |
| `AzureCosmosDBMongoVCoreVectorStore` | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ |
| `Chroma` | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| `Clickhouse` | ✅ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ | ✅ |
| `CouchbaseSearchVectorStore` | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ |
| `DatabricksVectorSearch` | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ |
| `ElasticsearchStore` | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ |
| `FAISS` | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ |
| `InMemoryVectorStore` | ✅ | ✅ | ❌ | ✅ | ✅ | ❌ | ❌ | ✅ |
| `Milvus` | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| `Moorcheh` | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| `MongoDBAtlasVectorSearch` | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| `openGauss` | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ❌ | ✅ |
| `PGVector` | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ |
| `PGVectorStore` | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ |
| `PineconeVectorStore` | ✅ | ✅ | ✅ | ❌ | ✅ | ❌ | ❌ | ✅ |
| `QdrantVectorStore` | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ |
| `Weaviate` | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ |
| `SQLServer` | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ |
| `ZeusDB` | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ |
| `Oracle AI Vector Search` | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ |

## ​ All vector stores

**Activeloop Deep Lake** \\
\\
View guide **Alibaba Cloud OpenSearch** \\
\\
View guide **AnalyticDB** \\
\\
View guide **Annoy** \\
\\
View guide **Apache Doris** \\
\\
View guide **ApertureDB** \\
\\
View guide **Astra DB Vector Store** \\
\\
View guide **Atlas** \\
\\
View guide **AwaDB** \\
\\
View guide **Azure Cosmos DB Mongo vCore** \\
\\
View guide **Azure Cosmos DB No SQL** \\
\\
View guide **Azure Database for PostgreSQL - Flexible Server** \\
\\
View guide **Azure AI Search** \\
\\
View guide **Bagel** \\
\\
View guide **BagelDB** \\
\\
View guide **Baidu Cloud ElasticSearch VectorSearch** \\
\\
View guide **Baidu VectorDB** \\
\\
View guide **Apache Cassandra** \\
\\
View guide **Chroma** \\
\\
View guide **Clarifai** \\
\\
View guide **ClickHouse** \\
\\
View guide **Couchbase** \\
\\
View guide **DashVector** \\
\\
View guide **Databricks** \\
\\
View guide **IBM Db2** \\
\\
View guide **DingoDB** \\
\\
View guide **DocArray HnswSearch** \\
\\
View guide **DocArray InMemorySearch** \\
\\
View guide **Amazon Document DB** \\
\\
View guide **DuckDB** \\
\\
View guide **China Mobile ECloud ElasticSearch** \\
\\
View guide **Elasticsearch** \\
\\
View guide **Epsilla** \\
\\
View guide **Faiss** \\
\\
View guide **Faiss (Async)** \\
\\
View guide **FalkorDB** \\
\\
View guide **Gel** \\
\\
View guide **Google AlloyDB** \\
\\
View guide **Google BigQuery Vector Search** \\
\\
View guide **Google Cloud SQL for MySQL** \\
\\
View guide **Google Cloud SQL for PostgreSQL** \\
\\
View guide **Firestore** \\
\\
View guide **Google Memorystore for Redis** \\
\\
View guide **Google Spanner** \\
\\
View guide **Google Vertex AI Feature Store** \\
\\
View guide **Google Vertex AI Vector Search** \\
\\
View guide **Hippo** \\
\\
View guide **Hologres** \\
\\
View guide **Jaguar Vector Database** \\
\\
View guide **Kinetica** \\
\\
View guide **LanceDB** \\
\\
View guide **Lantern** \\
\\
View guide **Lindorm** \\
\\
View guide **LLMRails** \\
\\
View guide **ManticoreSearch** \\
\\
View guide **MariaDB** \\
\\
View guide **Marqo** \\
\\
View guide **Meilisearch** \\
\\
View guide **Amazon MemoryDB** \\
\\
View guide **Milvus** \\
\\
View guide **Momento Vector Index** \\
\\
View guide **Moorcheh** \\
\\
View guide **MongoDB Atlas** \\
\\
View guide **MyScale** \\
\\
View guide **Neo4j Vector Index** \\
\\
View guide **NucliaDB** \\
\\
View guide **Oceanbase** \\
\\
View guide **openGauss** \\
\\
View guide **OpenSearch** \\
\\
View guide **Oracle AI Vector Search** \\
\\
View guide **Pathway** \\
\\
View guide **Postgres Embedding** \\
\\
View guide **PGVecto.rs** \\
\\
View guide **PGVector** \\
\\
View guide **PGVectorStore** \\
\\
View guide **Pinecone** \\
\\
View guide **Pinecone (sparse)** \\
\\
View guide **Qdrant** \\
\\
View guide **Relyt** \\
\\
View guide **Rockset** \\
\\
View guide **SAP HANA Cloud Vector Engine** \\
\\
View guide **ScaNN** \\
\\
View guide **SemaDB** \\
\\
View guide **SingleStore** \\
\\
View guide **scikit-learn** \\
\\
View guide **SQLiteVec** \\
\\
View guide **SQLite-VSS** \\
\\
View guide **SQLServer** \\
\\
View guide **StarRocks** \\
\\
View guide **Supabase** \\
\\
View guide **SurrealDB** \\
\\
View guide **Tablestore** \\
\\
View guide **Tair** \\
\\
View guide **Tencent Cloud VectorDB** \\
\\
View guide **Teradata VectorStore** \\
\\
View guide **ThirdAI NeuralDB** \\
\\
View guide **TiDB Vector** \\
\\
View guide **Tigris** \\
\\
View guide **TileDB** \\
\\
View guide **Timescale Vector** \\
\\
View guide **Typesense** \\
\\
View guide **Upstash Vector** \\
\\
View guide **USearch** \\
\\
View guide **Vald** \\
\\
View guide **VDMS** \\
\\
View guide **Vearch** \\
\\
View guide **Vectara** \\
\\
View guide **Vespa** \\
\\
View guide **viking DB** \\
\\
View guide **vlite** \\
\\
View guide **Weaviate** \\
\\
View guide **Xata** \\
\\
View guide **YDB** \\
\\
View guide **Yellowbrick** \\
\\
View guide **Zep** \\
\\
View guide **Zep Cloud** \\
\\
View guide **ZeusDB** \\
\\
View guide **Zilliz** \\
\\
View guide **Oracle AI Vector Search** \\
\\
View guide

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Embedding models\\
\\
Previous Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/document_loaders

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Interface
- By category
- Webpages
- PDFs
- Cloud Providers
- Social Platforms
- Messaging Services
- Productivity tools
- Common file types
- All document loaders

Document loaders provide a **standard interface** for reading data from different sources (such as Slack, Notion, or Google Drive) into LangChain’s Document format.
This ensures that data can be handled consistently regardless of the source.All document loaders implement the `BaseLoader` interface.

## ​ Interface

Each document loader may define its own parameters, but they share a common API:

- `load()` – Loads all documents at once.
- `lazy_load()` – Streams documents lazily, useful for large datasets.

Copy

from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(
... # Integration-specific parameters here
)

# Load all documents
documents = loader.load()

# For large datasets, lazily load documents
for document in loader.lazy_load():
print(document)

## ​ By category

### ​ Webpages

The below document loaders allow you to load webpages.

| Document Loader | Description | Package/API |
| --- | --- | --- |
| Web | Uses urllib and BeautifulSoup to load and parse HTML web pages | Package |
| Unstructured | Uses Unstructured to load and parse web pages | Package |
| RecursiveURL | Recursively scrapes all child links from a root URL | Package |
| Sitemap | Scrapes all pages on a given sitemap | Package |
| Spider | Crawler and scraper that returns LLM-ready data | API |
| Firecrawl | API service that can be deployed locally | API |
| Docling | Uses Docling to load and parse web pages | Package |
| Hyperbrowser | Platform for running and scaling headless browsers, can be used to scrape/crawl any site | API |
| AgentQL | Web interaction and structured data extraction from any web page using an AgentQL query or a Natural Language prompt | API |

### ​ PDFs

The below document loaders allow you to load PDF documents.

| Document Loader | Description | Package/API |
| --- | --- | --- |
| PyPDF | Uses `pypdf` to load and parse PDFs | Package |
| Unstructured | Uses Unstructured’s open source library to load PDFs | Package |
| Amazon Textract | Uses AWS API to load PDFs | API |
| MathPix | Uses MathPix to load PDFs | Package |
| PDFPlumber | Load PDF files using PDFPlumber | Package |
| PyPDFDirectry | Load a directory with PDF files | Package |
| PyPDFium2 | Load PDF files using PyPDFium2 | Package |
| PyMuPDF | Load PDF files using PyMuPDF | Package |
| PyMuPDF4LLM | Load PDF content to Markdown using PyMuPDF4LLM | Package |
| PDFMiner | Load PDF files using PDFMiner | Package |
| Upstage Document Parse Loader | Load PDF files using UpstageDocumentParseLoader | Package |
| Docling | Load PDF files using Docling | Package |
| UnDatasIO | Load PDF files using UnDatasIO | Package |
| OpenDataLoader PDF | Load PDF files using OpenDataLoader PDF | Package |

### ​ Cloud Providers

The below document loaders allow you to load documents from your favorite cloud providers.

| Document Loader | Description | Partner Package | API reference |
| --- | --- | --- | --- |
| AWS S3 Directory | Load documents from an AWS S3 directory | ❌ | `S3DirectoryLoader` |
| AWS S3 File | Load documents from an AWS S3 file | ❌ | `S3FileLoader` |
| Azure AI Data | Load documents from Azure AI services | ❌ | `AzureAIDataLoader` |
| Azure Blob Storage | Load documents from Azure Blob Storage | ✅ | `AzureBlobStorageLoader` |
| Dropbox | Load documents from Dropbox | ❌ | `DropboxLoader` |
| Google Cloud Storage Directory | Load documents from GCS bucket | ✅ | `GCSDirectoryLoader` |
| Google Cloud Storage File | Load documents from GCS file object | ✅ | `GCSFileLoader` |
| Google Drive | Load documents from Google Drive (Google Docs only) | ✅ | `GoogleDriveLoader` |
| Huawei OBS Directory | Load documents from Huawei Object Storage Service Directory | ❌ | `OBSDirectoryLoader` |
| Huawei OBS File | Load documents from Huawei Object Storage Service File | ❌ | `OBSFileLoader` |
| Microsoft OneDrive | Load documents from Microsoft OneDrive | ❌ | `OneDriveLoader` |
| Microsoft SharePoint | Load documents from Microsoft SharePoint | ❌ | `SharePointLoader` |
| Tencent COS Directory | Load documents from Tencent Cloud Object Storage Directory | ❌ | `TencentCOSDirectoryLoader` |
| Tencent COS File | Load documents from Tencent Cloud Object Storage File | ❌ | `TencentCOSFileLoader` |

### ​ Social Platforms

The below document loaders allow you to load documents from different social media platforms.

| Document Loader | API reference |
| --- | --- |
| Twitter | `TwitterTweetLoader` |
| Reddit | `RedditPostsLoader` |

### ​ Messaging Services

The below document loaders allow you to load data from different messaging platforms.

| Document Loader | API reference |
| --- | --- |
| Telegram | `TelegramChatFileLoader` |
| WhatsApp | `WhatsAppChatLoader` |
| Discord | `DiscordChatLoader` |
| Facebook Chat | `FacebookChatLoader` |
| Mastodon | `MastodonTootsLoader` |

### ​ Productivity tools

The below document loaders allow you to load data from commonly used productivity tools.

| Document Loader | API reference |
| --- | --- |
| Figma | `FigmaFileLoader` |
| Notion | `NotionDirectoryLoader` |
| Slack | `SlackDirectoryLoader` |
| Quip | `QuipLoader` |
| Trello | `TrelloLoader` |
| Roam | `RoamLoader` |
| GitHub | `GithubFileLoader` |

### ​ Common file types

The below document loaders allow you to load data from common data formats.

| Document Loader | Data Type |
| --- | --- |
| `CSVLoader` | CSV files |
| `Unstructured` | Many file types (see |
| `JSONLoader` | JSON files |
| `BSHTMLLoader` | HTML files |
| `DoclingLoader` | Various file types (see |
| `PolarisAIDataInsightLoader` | Various file types (see |

## ​ All document loaders

**acreom** \\
\\
View guide **AgentQLLoader** \\
\\
View guide **AirbyteLoader** \\
\\
View guide **Airtable** \\
\\
View guide **Alibaba Cloud MaxCompute** \\
\\
View guide **Amazon Textract** \\
\\
View guide **Apify Dataset** \\
\\
View guide **ArxivLoader** \\
\\
View guide **AssemblyAI Audio Transcripts** \\
\\
View guide **AstraDB** \\
\\
View guide **Async Chromium** \\
\\
View guide **AsyncHtml** \\
\\
View guide **Athena** \\
\\
View guide **AWS S3 Directory** \\
\\
View guide **AWS S3 File** \\
\\
View guide **AZLyrics** \\
\\
View guide **Azure AI Data** \\
\\
View guide **Azure Blob Storage** \\
\\
View guide **Azure AI Document Intelligence** \\
\\
View guide **BibTeX** \\
\\
View guide **BiliBili** \\
\\
View guide **Blackboard** \\
\\
View guide **Blockchain** \\
\\
View guide **Box** \\
\\
View guide **Brave Search** \\
\\
View guide **Browserbase** \\
\\
View guide **Browserless** \\
\\
View guide **BSHTMLLoader** \\
\\
View guide **Cassandra** \\
\\
View guide **ChatGPT Data** \\
\\
View guide **College Confidential** \\
\\
View guide **Concurrent Loader** \\
\\
View guide **Confluence** \\
\\
View guide **CoNLL-U** \\
\\
View guide **Copy Paste** \\
\\
View guide **Couchbase** \\
\\
View guide **CSV** \\
\\
View guide **Cube Semantic Layer** \\
\\
View guide **Datadog Logs** \\
\\
View guide **Dedoc** \\
\\
View guide **Diffbot** \\
\\
View guide **Discord** \\
\\
View guide **Docling** \\
\\
View guide **Docugami** \\
\\
View guide **Docusaurus** \\
\\
View guide **Dropbox** \\
\\
View guide **Email** \\
\\
View guide **EPub** \\
\\
View guide **Etherscan** \\
\\
View guide **EverNote** \\
\\
View guide **Facebook Chat** \\
\\
View guide **Fauna** \\
\\
View guide **Figma** \\
\\
View guide **FireCrawl** \\
\\
View guide **Geopandas** \\
\\
View guide **Git** \\
\\
View guide **GitBook** \\
\\
View guide **GitHub** \\
\\
View guide **Glue Catalog** \\
\\
View guide **Google AlloyDB for PostgreSQL** \\
\\
View guide **Google BigQuery** \\
\\
View guide **Google Bigtable** \\
\\
View guide **Google Cloud SQL for SQL Server** \\
\\
View guide **Google Cloud SQL for MySQL** \\
\\
View guide **Google Cloud SQL for PostgreSQL** \\
\\
View guide **Google Cloud Storage Directory** \\
\\
View guide **Google Cloud Storage File** \\
\\
View guide **Google Firestore in Datastore Mode** \\
\\
View guide **Google Drive** \\
\\
View guide **Google El Carro for Oracle Workloads** \\
\\
View guide **Google Firestore (Native Mode)** \\
\\
View guide **Google Memorystore for Redis** \\
\\
View guide **Google Spanner** \\
\\
View guide **Google Speech-to-Text** \\
\\
View guide **Grobid** \\
\\
View guide **Gutenberg** \\
\\
View guide **Hacker News** \\
\\
View guide **Huawei OBS Directory** \\
\\
View guide **Huawei OBS File** \\
\\
View guide **HuggingFace Dataset** \\
\\
View guide **HyperbrowserLoader** \\
\\
View guide **iFixit** \\
\\
View guide **Images** \\
\\
View guide **Image Captions** \\
\\
View guide **IMSDb** \\
\\
View guide **Iugu** \\
\\
View guide **Joplin** \\
\\
View guide **JSONLoader** \\
\\
View guide **Jupyter Notebook** \\
\\
View guide **Kinetica** \\
\\
View guide **lakeFS** \\
\\
View guide **LangSmith** \\
\\
View guide **LarkSuite (FeiShu)** \\
\\
View guide **LLM Sherpa** \\
\\
View guide **Mastodon** \\
\\
View guide **MathPixPDFLoader** \\
\\
View guide **MediaWiki Dump** \\
\\
View guide **Merge Documents Loader** \\
\\
View guide **MHTML** \\
\\
View guide **Microsoft Excel** \\
\\
View guide **Microsoft OneDrive** \\
\\
View guide **Microsoft OneNote** \\
\\
View guide **Microsoft PowerPoint** \\
\\
View guide **Microsoft SharePoint** \\
\\
View guide **Microsoft Word** \\
\\
View guide **Near Blockchain** \\
\\
View guide **Modern Treasury** \\
\\
View guide **MongoDB** \\
\\
View guide **Needle Document Loader** \\
\\
View guide **News URL** \\
\\
View guide **Notion DB** \\
\\
View guide **Nuclia** \\
\\
View guide **Obsidian** \\
\\
View guide **OpenDataLoader PDF** \\
\\
View guide **Open Document Format (ODT)** \\
\\
View guide **Open City Data** \\
\\
View guide **Oracle Autonomous Database** \\
\\
View guide **Oracle AI Vector Search** \\
\\
View guide **Org-mode** \\
\\
View guide **Outline Document Loader** \\
\\
View guide **Pandas DataFrame** \\
\\
View guide **PDFMinerLoader** \\
\\
View guide **PDFPlumber** \\
\\
View guide **Pebblo Safe DocumentLoader** \\
\\
View guide **Polaris AI DataInsight** \\
\\
View guide **Polars DataFrame** \\
\\
View guide **Dell PowerScale** \\
\\
View guide **Psychic** \\
\\
View guide **PubMed** \\
\\
View guide **PullMdLoader** \\
\\
View guide **PyMuPDFLoader** \\
\\
View guide **PyMuPDF4LLM** \\
\\
View guide **PyPDFDirectoryLoader** \\
\\
View guide **PyPDFium2Loader** \\
\\
View guide **PyPDFLoader** \\
\\
View guide **PySpark** \\
\\
View guide **Quip** \\
\\
View guide **ReadTheDocs Documentation** \\
\\
View guide **Recursive URL** \\
\\
View guide **Reddit** \\
\\
View guide **Roam** \\
\\
View guide **Rockset** \\
\\
View guide **rspace** \\
\\
View guide **RSS Feeds** \\
\\
View guide **RST** \\
\\
View guide **scrapfly** \\
\\
View guide **ScrapingAnt** \\
\\
View guide **SingleStore** \\
\\
View guide **Sitemap** \\
\\
View guide **Slack** \\
\\
View guide **Snowflake** \\
\\
View guide **Source Code** \\
\\
View guide **Spider** \\
\\
View guide **Spreedly** \\
\\
View guide **Stripe** \\
\\
View guide **Subtitle** \\
\\
View guide **SurrealDB** \\
\\
View guide **Telegram** \\
\\
View guide **Tencent COS Directory** \\
\\
View guide **Tencent COS File** \\
\\
View guide **TensorFlow Datasets** \\
\\
View guide **TiDB** \\
\\
View guide **2Markdown** \\
\\
View guide **TOML** \\
\\
View guide **Trello** \\
\\
View guide **TSV** \\
\\
View guide **Twitter** \\
\\
View guide **UnDatasIO** \\
\\
View guide **Unstructured** \\
\\
View guide **UnstructuredMarkdownLoader** \\
\\
View guide **UnstructuredPDFLoader** \\
\\
View guide **Upstage** \\
\\
View guide **URL** \\
\\
View guide **Vsdx** \\
\\
View guide **Weather** \\
\\
View guide **WebBaseLoader** \\
\\
View guide **WhatsApp Chat** \\
\\
View guide **Wikipedia** \\
\\
View guide **UnstructuredXMLLoader** \\
\\
View guide **Xorbits Pandas DataFrame** \\
\\
View guide **YouTube Audio** \\
\\
View guide **YouTube Transcripts** \\
\\
View guide **YoutubeLoaderDL** \\
\\
View guide **Yuque** \\
\\
View guide **ZeroxPDFLoader** \\
\\
View guide

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Vector stores\\
\\
Previous Key-value stores\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/stores

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integrations by component

Key-value stores

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Interface
- Built-in stores for local development
- Custom stores
- All key-value stores

## ​ Overview

LangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching embeddings.

## ​ Interface

All `BaseStores` support the following interface:

Base stores are designed to work **multiple** key-value pairs at once for efficiency. This saves on network round-trips and may allow for more efficient batch operations in the underlying store.

## ​ Built-in stores for local development

**InMemoryByteStore** \\
\\
View guide **LocalFileStore** \\
\\
View guide

## ​ Custom stores

You can also implement your own custom store by extending the `BaseStore` class. See the store interface documentation for more details.

## ​ All key-value stores

**AstraDBByteStore** \\
\\
View guide **CassandraByteStore** \\
\\
View guide **ElasticsearchEmbeddingsCache** \\
\\
View guide **RedisStore** \\
\\
View guide **UpstashRedisByteStore** \\
\\
View guide

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/chroma

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Chroma

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- VectorStore

## ​ Installation and Setup

pip

uv

Copy

pip install langchain-chroma

## ​ VectorStore

There exists a wrapper around Chroma vector databases, allowing you to use it as a vectorstore,
whether for semantic search or example selection.

from langchain_chroma import Chroma

For a more detailed walkthrough of the Chroma wrapper, see this notebook

## ​ Retriever

from langchain_classic.retrievers import SelfQueryRetriever

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/groq

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Groq

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Model interfaces

This page makes reference to Groq, an AI hardware and software company. For information on how to use Grok models (provided by xAI), see the xAI provider page.

## ​ Model interfaces

**ChatGroq** \\
\\
Interface to chat models hosted on the Groq platform.\\
\\
Get started

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/pinecone

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Pinecone

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- Vector store
- Sparse Vector store
- Sparse Embedding
- Retrievers
- Pinecone Hybrid Search

## ​ Installation and Setup

Install the Python SDK:

pip

uv

Copy

pip install langchain-pinecone

## ​ Vector store

There exists a wrapper around Pinecone indexes, allowing you to use it as a vectorstore,
whether for semantic search or example selection.

from langchain_pinecone import PineconeVectorStore

For a more detailed walkthrough of the Pinecone vectorstore, see this notebook

### ​ Sparse Vector store

LangChain’s `PineconeSparseVectorStore` enables sparse retrieval using Pinecone’s sparse English model. It maps text to sparse vectors and supports adding documents and similarity search.

from langchain_pinecone import PineconeSparseVectorStore

# Initialize sparse vector store
vector_store = PineconeSparseVectorStore(
index=my_index,
embedding_model="pinecone-sparse-english-v0"
)
# Add documents
vector_store.add_documents(documents)
# Query
results = vector_store.similarity_search("your query", k=3)

For a more detailed walkthrough, see the Pinecone Sparse Vector Store notebook.

### ​ Sparse Embedding

LangChain’s `PineconeSparseEmbeddings` provides sparse embedding generation using Pinecone’s `pinecone-sparse-english-v0` model.

from langchain_pinecone.embeddings import PineconeSparseEmbeddings

# Initialize sparse embeddings
sparse_embeddings = PineconeSparseEmbeddings(
model="pinecone-sparse-english-v0"
)
# Embed a single query (returns SparseValues)
query_embedding = sparse_embeddings.embed_query("sample text")

# Embed multiple documents (returns list of SparseValues)
docs = ["Document 1 content", "Document 2 content"]
doc_embeddings = sparse_embeddings.embed_documents(docs)

For more detailed usage, see the Pinecone Sparse Embeddings notebook.

## ​ Retrievers

### ​ Pinecone Hybrid Search

pip install pinecone pinecone-text

from langchain_community.retrievers import (
PineconeHybridSearchRetriever,
)

For more detailed information, see this notebook.

### ​ Self Query retriever

Pinecone vector store can be used as a retriever for self-querying.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/pgvector

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

PGVector

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation
- Setup
- Wrappers
- VectorStore
- Usage

This page covers how to use the Postgres PGVector ecosystem within LangChain
It is broken into two parts: installation and setup, and then references to specific PGVector wrappers.

## ​ Installation

- Install the Python package with `pip install pgvector`

## ​ Setup

1. The first step is to create a database with the `pgvector` extension installed.Follow the steps at PGVector Installation Steps to install the database and the extension. The docker image is the easiest way to get started.

## ​ Wrappers

### ​ VectorStore

There exists a wrapper around Postgres vector databases, allowing you to use it as a vectorstore,
whether for semantic search or example selection.To import this vectorstore:

Copy

from langchain_community.vectorstores.pgvector import PGVector

### ​ Usage

For a more detailed walkthrough of the PGVector Wrapper, see this notebook

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/cohere

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Cohere

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- Cohere langchain integrations
- Quick copy examples
- Chat
- LLM
- Tool calling
- ReAct Agent
- RAG Retriever
- Text Embedding

## ​ Installation and Setup

- Install the Python SDK :

pip

uv

Copy

pip install langchain-cohere

Get a Cohere api key and set it as an environment variable (`COHERE_API_KEY`)

## ​ Cohere langchain integrations

| API | description | Endpoint docs | Import | Example usage |
| --- | --- | --- | --- | --- |
| Chat | Build chat bots | chat | `from langchain_cohere import ChatCohere` | cohere.ipynb |
| LLM | Generate text | generate | `from langchain_cohere.llms import Cohere` | cohere.ipynb |
| RAG Retriever | Connect to external data sources | chat + rag | `from langchain_classic.retrievers import CohereRagRetriever` | cohere.ipynb |
| Text Embedding | Embed strings to vectors | embed | `from langchain_cohere import CohereEmbeddings` | cohere.ipynb |
| Rerank Retriever | Rank strings based on relevance | rerank | `from langchain_classic.retrievers.document_compressors import CohereRerank` | cohere.ipynb |

## ​ Quick copy examples

### ​ Chat

from langchain_cohere import ChatCohere
from langchain.messages import HumanMessage
chat = ChatCohere()
messages = [HumanMessage(content="knock knock")]
print(chat.invoke(messages))

Usage of the Cohere chat model

### ​ LLM

from langchain_cohere.llms import Cohere

llm = Cohere()
print(llm.invoke("Come up with a pet name"))

Usage of the Cohere (legacy) LLM model

### ​ Tool calling

from langchain_cohere import ChatCohere
from langchain.messages import (
HumanMessage,
ToolMessage,
)
from langchain.tools import tool

@tool

"""Applies a magic operation to an integer

Args:
number: Number to have magic operation performed on
"""
return number + 10

def invoke_tools(tool_calls, messages):
for tool_call in tool_calls:
selected_tool = {"magic_function":magic_function}[\
tool_call["name"].lower()\
]
tool_output = selected_tool.invoke(tool_call["args"])
messages.append(ToolMessage(tool_output, tool_call_id=tool_call["id"]))
return messages

tools = [magic_function]

llm = ChatCohere()
llm_with_tools = llm.bind_tools(tools=tools)
messages = [\
HumanMessage(\
content="What is the value of magic_function(2)?"\
)\
]

res = llm_with_tools.invoke(messages)
while res.tool_calls:
messages.append(res)
messages = invoke_tools(res.tool_calls, messages)
res = llm_with_tools.invoke(messages)

print(res.content)

Tool calling with Cohere LLM can be done by binding the necessary tools to the llm as seen above.
An alternative, is to support multi hop tool calling with the ReAct agent as seen below.

### ​ ReAct Agent

The agent is based on the paper
ReAct: Synergizing Reasoning and Acting in Language Models.

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_cohere import ChatCohere, create_cohere_react_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import AgentExecutor

llm = ChatCohere()

internet_search = TavilySearchResults(max_results=4)
internet_search.name = "internet_search"
internet_search.description = "Route a user query to the internet"

prompt = ChatPromptTemplate.from_template("{input}")

agent = create_cohere_react_agent(
llm,
[internet_search],
prompt
)

agent_executor = AgentExecutor(agent=agent, tools=[internet_search], verbose=True)

agent_executor.invoke({
"input": "In what year was the company that was founded as Sound of Music added to the S&P 500?",
})

The ReAct agent can be used to call multiple tools in sequence.

### ​ RAG Retriever

from langchain_cohere import ChatCohere
from langchain_classic.retrievers import CohereRagRetriever
from langchain_core.documents import Document

rag = CohereRagRetriever(llm=ChatCohere())
print(rag.invoke("What is cohere ai?"))

Usage of the Cohere RAG Retriever

### ​ Text Embedding

from langchain_cohere import CohereEmbeddings

embeddings = CohereEmbeddings(model="embed-english-light-v3.0")
print(embeddings.embed_documents(["This is a test document."]))

Usage of the Cohere Text Embeddings model

### ​ Reranker

Usage of the Cohere Reranker

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/mistralai

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

MistralAI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- Chat models
- ChatMistralAI
- Embedding models

## ​ Installation and Setup

A valid API key is needed to communicate with the API.You will also need the `langchain-mistralai` package:

pip

uv

Copy

pip install langchain-mistralai

## ​ Chat models

### ​ ChatMistralAI

See a usage example.

from langchain_mistralai.chat_models import ChatMistralAI

## ​ Embedding models

### ​ MistralAIEmbeddings

from langchain_mistralai import MistralAIEmbeddings

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/fireworks

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Fireworks AI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and setup
- Authentication
- Chat models
- LLMs

## ​ Installation and setup

- Install the Fireworks integration package.

Copy

pip install langchain-fireworks

- Get a Fireworks API key by signing up at fireworks.ai.
- Authenticate by setting the FIREWORKS\_API\_KEY environment variable.

### ​ Authentication

There are two ways to authenticate using your Fireworks API key:

1. Setting the `FIREWORKS_API_KEY` environment variable.

2. Setting `api_key` field in the Fireworks LLM module.

## ​ Chat models

See a usage example.

from langchain_fireworks import ChatFireworks

## ​ LLMs

from langchain_fireworks import Fireworks

## ​ Embedding models

from langchain_fireworks import FireworksEmbeddings

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/databricks

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Databricks

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation
- Chat Model
- LLM
- Embeddings
- Vector Search
- MLflow Integration
- SQLDatabase

Databricks embraces the LangChain ecosystem in various ways:

1. 🚀 **Model Serving** \- Access state-of-the-art LLMs, such as DBRX, Llama3, Mixtral, or your fine-tuned models on Databricks Model Serving, via a highly available and low-latency inference endpoint. LangChain provides LLM (`Databricks`), Chat Model (`ChatDatabricks`), and Embeddings (`DatabricksEmbeddings`) implementations, streamlining the integration of your models hosted on Databricks Model Serving with your LangChain applications.
2. 📃 **Vector Search** \- Databricks Vector Search is a serverless vector database seamlessly integrated within the Databricks Platform. Using `DatabricksVectorSearch`, you can incorporate the highly scalable and reliable similarity search engine into your LangChain applications.
3. 📊 **MLflow** \- MLflow is an open-source platform to manage full the ML lifecycle, including experiment management, evaluation, tracing, deployment, and more. MLflow’s LangChain Integration streamlines the process of developing and operating modern compound ML systems.
4. 🌐 **SQL Database** \- Databricks SQL is integrated with `SQLDatabase` in LangChain, allowing you to access the auto-optimizing, exceptionally performant data warehouse.
5. 💡 **Open Models** \- Databricks open sources models, such as DBRX, which are available through the Hugging Face Hub. These models can be directly utilized with LangChain, leveraging its integration with the `transformers` library.

## ​ Installation

First-party Databricks integrations are now available in the databricks-langchain partner package.

pip

uv

Copy

pip install databricks-langchain

The legacy langchain-databricks partner package is still available but will be soon deprecated.

## ​ Chat Model

`ChatDatabricks` is a Chat Model class to access chat endpoints hosted on Databricks, including state-of-the-art models such as Llama3, Mixtral, and DBRX, as well as your own fine-tuned models.

from databricks_langchain import ChatDatabricks

chat_model = ChatDatabricks(endpoint="databricks-meta-llama-3-70b-instruct")

See the usage example for more guidance on how to use it within your LangChain application.

## ​ LLM

`Databricks` is an LLM class to access completion endpoints hosted on Databricks.

Text completion models have been deprecated and the latest and most popular models are chat completion models. Use `ChatDatabricks` chat model instead to use those models and advanced features such as tool calling.

from langchain_community.llm.databricks import Databricks

llm = Databricks(endpoint="your-completion-endpoint")

## ​ Embeddings

`DatabricksEmbeddings` is an Embeddings class to access text-embedding endpoints hosted on Databricks, including state-of-the-art models such as BGE, as well as your own fine-tuned models.

from databricks_langchain import DatabricksEmbeddings

embeddings = DatabricksEmbeddings(endpoint="databricks-bge-large-en")

## ​ Vector Search

Databricks Vector Search is a serverless similarity search engine that allows you to store a vector representation of your data, including metadata, in a vector database. With Vector Search, you can create auto-updating vector search indexes from Delta tables managed by Unity Catalog and query them with a simple API to return the most similar vectors.

from databricks_langchain import DatabricksVectorSearch

dvs = DatabricksVectorSearch(

index,
text_column="text",
embedding=embeddings,
columns=["source"]
)
docs = dvs.similarity_search("What is vector search?)

See the usage example for how to set up vector indices and integrate them with LangChain.

## ​ MLflow Integration

In the context of LangChain integration, MLflow provides the following capabilities:

- **Experiment Tracking**: Tracks and stores models, artifacts, and traces from your LangChain experiments.
- **Dependency Management**: Automatically records dependency libraries, ensuring consistency among development, staging, and production environments.
- **Model Evaluation** Offers native capabilities for evaluating LangChain applications.
- **Tracing**: Visually traces data flows through your LangChain application.

See MLflow LangChain Integration to learn about the full capabilities of using MLflow with LangChain through extensive code examples and guides.

## ​ SQLDatabase

To connect to Databricks SQL or query structured data, see the Databricks structured retriever tool documentation and to create an agent using the above created SQL UDF see Databricks UC Integration.

## ​ Open Models

To directly integrate Databricks’s open models hosted on HuggingFace, you can use the HuggingFace Integration of LangChain.

from langchain_huggingface import HuggingFaceEndpoint

llm = HuggingFaceEndpoint(
repo_id="databricks/dbrx-instruct",
task="text-generation",
max_new_tokens=512,
do_sample=False,
repetition_penalty=1.03,
)
llm.invoke("What is DBRX model?")

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/deepseek

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

DeepSeek

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

DeepSeek is a Chinese artificial intelligence company that develops LLMs.

Copy

from langchain_deepseek import ChatDeepSeek

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/mongodb_atlas

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

MongoDB Atlas

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- Vector Store
- Retrievers
- Full Text Search Retriever
- Hybrid Search Retriever
- Model Caches
- MongoDBCache

## ​ Installation and Setup

See detail configuration instructions.We need to install `langchain-mongodb` python package.

pip

uv

Copy

pip install langchain-mongodb

## ​ Vector Store

See a usage example.

from langchain_mongodb import MongoDBAtlasVectorSearch

## ​ Retrievers

from langchain_mongodb.retrievers import MongoDBAtlasFullTextSearchRetriever

from langchain_mongodb.retrievers import MongoDBAtlasHybridSearchRetriever

## ​ Model Caches

### ​ MongoDBCache

An abstraction to store a simple cache in MongoDB. This does not use Semantic Caching, nor does it require an index to be made on the collection before generation.To import this cache:

from langchain_mongodb.cache import MongoDBCache

To use this cache with your LLMs:

from langchain_core.globals import set_llm_cache

# use any embedding provider...
from tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings

set_llm_cache(MongoDBCache(
connection_string=mongodb_atlas_uri,
collection_name=COLLECTION_NAME,
database_name=DATABASE_NAME,
))

### ​ MongoDBAtlasSemanticCache

Semantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends MongoDBAtlas as both a cache and a vectorstore.
The MongoDBAtlasSemanticCache inherits from `MongoDBAtlasVectorSearch` and needs an Atlas Vector Search Index defined to work. Please look at the usage example on how to set up the index.To import this cache:

from langchain_mongodb.cache import MongoDBAtlasSemanticCache

set_llm_cache(MongoDBAtlasSemanticCache(
embedding=FakeEmbeddings(),
connection_string=mongodb_atlas_uri,
collection_name=COLLECTION_NAME,
database_name=DATABASE_NAME,
))

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/xai

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

xAI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation
- Environment
- Example

This page makes reference to Grok models provided by xAI \- not to be confused with Groq, a separate AI hardware and software company. See the Groq provider page.

xAI offers an API to interact with Grok models. This example goes over how to use LangChain to interact with xAI models.

## ​ Installation

Copy

pip install -U langchain-xai

## ​ Environment

To use xAI, you’ll need to create an API key. The API key can be passed in as an init param `xai_api_key` or set as environment variable `XAI_API_KEY`.

## ​ Example

See ChatXAI docs for detail and supported features.

# Querying chat models with xAI

from langchain_xai import ChatXAI

chat = ChatXAI(
# xai_api_key="YOUR_API_KEY",
model="grok-4",
)

# stream the response back from the model
for m in chat.stream("Tell me fun things to do in NYC"):
print(m.content, end="", flush=True)

# if you don't want to do streaming, you can use the invoke method
# chat.invoke("Tell me fun things to do in NYC")

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/ibm

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

IBM

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Watsonx AI
- Model interfaces
- Tools and toolkits
- Retrievers
- DB2
- Vector stores

LangChain integrations related to IBM technologies, including the
IBM watsonx.ai platform and DB2 database.

## ​ Watsonx AI

IBM watsonx.ai AI studio is part of the IBM watsonx AI and data platform, bringing together new generative
AI capabilities powered by foundation models and traditional machine learning (ML)
into a powerful studio spanning the AI lifecycle. Tune and guide models with your enterprise data to meet your needs with easy-to-use tools for
building and refining performant prompts. With watsonx.ai, you can build AI applications in a fraction of the time and with a fraction of the data.
Watsonx.ai offers:

- **Multi-model variety and flexibility:** Choose from IBM-developed, open-source and third-party models, or build your own model.
- **Differentiated client protection:** IBM stands behind IBM-developed models and indemnifies the client against third-party IP claims.
- **End-to-end AI governance:** Enterprises can scale and accelerate the impact of AI with trusted data across the business, using data wherever it resides.
- **Hybrid, multi-cloud deployments:** IBM provides the flexibility to integrate and deploy your AI workloads into your hybrid-cloud stack of choice.

### ​ Model interfaces

**ChatWatsonx** \\
\\
IBM watsonx.ai chat models.\\
\\
Get started **WatsonxLLM**\\
\\
(Legacy) IBM watsonx.ai text completion models.\\
\\
Get started **WatsonxEmbeddings** \\
\\
IBM watsonx.ai embedding models.\\
\\
Get started

### ​ Tools and toolkits

**WatsonxToolkit** \\
\\
IBM watsonx.ai toolkit.\\
\\
Get started **WatsonxSQLDatabaseToolkit** \\
\\
IBM watsonx.ai SQL Database toolkit.\\
\\
Get started

### ​ Retrievers

**WatsonxRerank** \\
\\
IBM watsonx.ai document retriever.\\
\\
Get started

## ​ DB2

The IBM DB2 relational database v12.1.2 and above offers the abilities of vector store
and vector search. Installation of `langchain-db2` package will give LangChain users
the support of DB2 vector store and vector search.

`langchain-db2` is a separate package for Vector Store feature only, and can be run without the `langchain-ibm` package.

### ​ Vector stores

**DB2VS** \\
\\
IBM DB2 Vector Store and Vector Search\\
\\
Get started

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/perplexity

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Perplexity

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup

## ​ Installation and Setup

Install the Perplexity x LangChain integration package:

pip

uv

Copy

pip install langchain-perplexity

Get your API key from here.

## ​ Chat models

See a variety of usage examples here.

from langchain_perplexity import ChatPerplexity

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/nvidia

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

NVIDIA

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation
- Setup
- Working with NVIDIA API Catalog
- Working with NVIDIA NIMs
- Using NVIDIA AI Foundation Endpoints

The `langchain-nvidia-ai-endpoints` package contains LangChain integrations building applications with models on
NVIDIA NIM inference microservice. NIM supports models across domains like chat, embedding, and re-ranking models
from the community as well as NVIDIA. These models are optimized by NVIDIA to deliver the best performance on NVIDIA
accelerated infrastructure and deployed as a NIM, an easy-to-use, prebuilt containers that deploy anywhere using a single
command on NVIDIA accelerated infrastructure.NVIDIA hosted deployments of NIMs are available to test on the NVIDIA API catalog. After testing,
NIMs can be exported from NVIDIA’s API catalog using the NVIDIA AI Enterprise license and run on-premises or in the cloud,
giving enterprises ownership and full control of their IP and AI application.NIMs are packaged as container images on a per model basis and are distributed as NGC container images through the NVIDIA NGC Catalog.
At their core, NIMs provide easy, consistent, and familiar APIs for running inference on an AI model.Below is an example on how to use some common functionality surrounding text-generative and embedding models.

## ​ Installation

Copy

pip install -qU langchain-nvidia-ai-endpoints

## ​ Setup

**To get started:**

1. Create a free account with NVIDIA, which hosts NVIDIA AI Foundation models.
2. Click on your model of choice.
3. Under Input select the Python tab, and click `Get API Key`. Then click `Generate Key`.
4. Copy and save the generated key as NVIDIA\_API\_KEY. From there, you should have access to the endpoints.

import getpass
import os

if not os.environ.get("NVIDIA_API_KEY", "").startswith("nvapi-"):
nvidia_api_key = getpass.getpass("Enter your NVIDIA API key: ")
assert nvidia_api_key.startswith("nvapi-"), f"{nvidia_api_key[:5]}... is not a valid key"
os.environ["NVIDIA_API_KEY"] = nvidia_api_key

## ​ Working with NVIDIA API Catalog

from langchain_nvidia_ai_endpoints import ChatNVIDIA

llm = ChatNVIDIA(model="mistralai/mixtral-8x22b-instruct-v0.1")
result = llm.invoke("Write a ballad about LangChain.")
print(result.content)

Using the API, you can query live endpoints available on the NVIDIA API Catalog to get quick results from a DGX-hosted cloud compute environment. All models are source-accessible and can be deployed on your own compute cluster using NVIDIA NIM which is part of NVIDIA AI Enterprise, shown in the next section Working with NVIDIA NIMs.

## ​ Working with NVIDIA NIMs

When ready to deploy, you can self-host models with NVIDIA NIM—which is included with the NVIDIA AI Enterprise software license—and run them anywhere, giving you ownership of your customizations and full control of your intellectual property (IP) and AI applications.Learn more about NIMs

from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings, NVIDIARerank

# connect to a chat NIM running at localhost:8000, specifying a model
llm = ChatNVIDIA(base_url="http://localhost:8000/v1", model="meta/llama3-8b-instruct")

# connect to an embedding NIM running at localhost:8080
embedder = NVIDIAEmbeddings(base_url="http://localhost:8080/v1")

# connect to a reranking NIM running at localhost:2016
ranker = NVIDIARerank(base_url="http://localhost:2016/v1")

## ​ Using NVIDIA AI Foundation Endpoints

A selection of NVIDIA AI Foundation models are supported directly in LangChain with familiar APIs.The active models which are supported can be found in API Catalog.**The following may be useful examples to help you get started:**

- **`ChatNVIDIA` Model.**
- **`NVIDIAEmbeddings` Model for RAG Workflows.**

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/qdrant

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Qdrant

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- Embedding models
- FastEmbedSparse
- SparseEmbeddings

## ​ Installation and Setup

Install the Python partner package:

pip

uv

Copy

pip install langchain-qdrant

## ​ Embedding models

### ​ FastEmbedSparse

from langchain_qdrant import FastEmbedSparse

### ​ SparseEmbeddings

from langchain_qdrant import SparseEmbeddings

## ​ Vector Store

There exists a wrapper around `Qdrant` indexes, allowing you to use it as a vectorstore,
whether for semantic search or example selection.To import this vectorstore:

from langchain_qdrant import QdrantVectorStore

For a more detailed walkthrough of the Qdrant wrapper, see this notebook

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/tavily

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Tavily

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- Tools

Tavily is a search engine, specifically designed for AI agents.
Tavily provides both a search and extract API, AI developers can effortlessly integrate their
applications with realtime online information. Tavily’s primary mission is to provide factual
and reliable information from trusted sources, enhancing the accuracy and reliability of AI
generated content and reasoning.

## ​ Installation and Setup

pip

uv

Copy

pip install langchain-tavily

## ​ Tools

See detail on available tools tavily\_search and tavily\_extract.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/milvus

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Milvus

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup

## ​ Installation and Setup

Install the Python SDK:

pip

uv

Copy

pip install langchain-milvus

## ​ Vector Store

See a usage example.To import this vectorstore:

from langchain_milvus import Milvus

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/elasticsearch

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Elasticsearch

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- Setup Elasticsearch
- Install Elasticsearch on your local machine
- Deploy Elasticsearch on Elastic Cloud
- Install Client
- Embedding models
- Vector store
- Third-party integrations
- EcloudESVectorStore
- Retrievers
- ElasticsearchRetriever
- BM25
- LLM cache
- Byte Store

## ​ Installation and Setup

### ​ Setup Elasticsearch

There are two ways to get started with Elasticsearch:

#### ​ Install Elasticsearch on your local machine

The easiest way to run Elasticsearch locally for development and testing is using the start-local script. This script sets up Elasticsearch using Docker with a simple one-line command.

Copy

curl -fsSL | sh

This creates an `elastic-start-local` folder. To start Elasticsearch:

cd elastic-start-local
./start.sh

Elasticsearch will be available at `http://localhost:9200`. The password for the `elastic` user and API key are automatically generated and stored in the `.env` file in the `elastic-start-local` folder.If you only need Elasticsearch without Kibana, you can use the `--esonly` option:

curl -fsSL | sh -s -- --esonly

The start-local setup is for local testing only and should not be used in production. For production installations, refer to the official Elasticsearch documentation.

#### ​ Deploy Elasticsearch on Elastic Cloud

`Elastic Cloud` is a managed Elasticsearch service. Signup for a free trial.

### ​ Install Client

pip

uv

pip install elasticsearch
pip install langchain-elasticsearch

## ​ Embedding models

See a usage example.

from langchain_elasticsearch import ElasticsearchEmbeddings

## ​ Vector store

from langchain_elasticsearch import ElasticsearchStore

#### ​ EcloudESVectorStore

from langchain_community.vectorstores.ecloud_vector_search import EcloudESVectorStore

## ​ Retrievers

### ​ ElasticsearchRetriever

The `ElasticsearchRetriever` enables flexible access to all Elasticsearch features
through the Query DSL.See a usage example.

from langchain_elasticsearch import ElasticsearchRetriever

### ​ BM25

from langchain_community.retrievers import ElasticSearchBM25Retriever

## ​ LLM cache

from langchain_elasticsearch import ElasticsearchCache

## ​ Byte Store

from langchain_elasticsearch import ElasticsearchEmbeddingsCache

## ​ Chain

It is a chain for interacting with Elasticsearch Database.

from langchain_classic.chains.elasticsearch_database import ElasticsearchDatabaseChain

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/azure_ai

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Azure AI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Chat models
- Azure AI chat completions
- Embedding models
- Azure AI model inference for embeddings
- Vector stores
- Azure CosmosDB NoSQL Vector Search
- Azure CosmosDB Mongo vCore Vector Search

This page covers all LangChain integrations with Microsoft Azure and its related projects.Integration packages for Azure AI, Dynamic Sessions, SQL Server are maintained in
the langchain-azure repository.

## ​ Chat models

We recommend developers start with the (`langchain-azure-ai`) to access all the models available in Azure AI Foundry.

### ​ Azure AI chat completions

Access models like Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral using the `AzureAIChatCompletionsModel` class.

pip

uv

Copy

pip install -U langchain-azure-ai

Configure your API key and Endpoint.

export AZURE_AI_CREDENTIAL=your-api-key
export AZURE_AI_ENDPOINT=your-endpoint

from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel

llm = AzureAIChatCompletionsModel(
model_name="gpt-4o",
api_version="2024-05-01-preview",
)

llm.invoke('Tell me a joke and include some emojis')

## ​ Embedding models

### ​ Azure AI model inference for embeddings

from langchain_azure_ai.embeddings import AzureAIEmbeddingsModel

embed_model = AzureAIEmbeddingsModel(
model_name="text-embedding-ada-002"
)

## ​ Vector stores

We need to install the `azure-cosmos` package to use this vector store.

pip install -qU azure-cosmos

from langchain_azure_ai.vectorstores.azure_cosmos_db_no_sql import (
AzureCosmosDBNoSqlVectorSearch,
)
vector_search = AzureCosmosDBNoSqlVectorSearch.from_documents(
documents=docs,
embedding=openai_embeddings,
cosmos_client=cosmos_client,
database_name=database_name,
container_name=container_name,
vector_embedding_policy=vector_embedding_policy,
full_text_policy=full_text_policy,
indexing_policy=indexing_policy,
cosmos_container_properties=cosmos_container_properties,
cosmos_database_properties={},
full_text_search_enabled=True,
)

See a usage example.

We need to install the `pymongo` package to use this vector store.

pip install -qU pymongo

from langchain_azure_ai.vectorstores.azure_cosmos_db_mongo_vcore import (
AzureCosmosDBMongoVCoreVectorSearch,
)

vectorstore = AzureCosmosDBMongoVCoreVectorSearch.from_documents(
docs,
openai_embeddings,
collection=collection,
index_name=INDEX_NAME,
)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/litellm

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LiteLLM

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and setup
- Chat Models

## ​ Installation and setup

pip

uv

Copy

pip install langchain-litellm

## ​ Chat Models

from langchain_litellm import ChatLiteLLM

from langchain_litellm import ChatLiteLLMRouter

See more detail in the guide here.

* * *

## ​ API reference

For detailed documentation of all `ChatLiteLLM` and `ChatLiteLLMRouter` features and configurations head to the API reference:

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/astradb

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Astra DB

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- Vector Store
- LLM Cache
- Semantic LLM Cache
- Document loader
- Self-querying retriever
- Store

See a tutorial provided by DataStax.

## ​ Installation and Setup

Install the following Python package:

pip

uv

Copy

Create a database (if needed) and get the connection secrets.
Set the following variables:

ASTRA_DB_API_ENDPOINT="API_ENDPOINT"
ASTRA_DB_APPLICATION_TOKEN="TOKEN"

## ​ Vector Store

A few typical initialization patterns are shown here:

from langchain_astradb import AstraDBVectorStore

vector_store = AstraDBVectorStore(
embedding=my_embedding,
collection_name="my_store",
api_endpoint=ASTRA_DB_API_ENDPOINT,
token=ASTRA_DB_APPLICATION_TOKEN,
)

from astrapy.info import VectorServiceOptions

vector_store_vectorize = AstraDBVectorStore(
collection_name="my_vectorize_store",
api_endpoint=ASTRA_DB_API_ENDPOINT,
token=ASTRA_DB_APPLICATION_TOKEN,
collection_vector_service_options=VectorServiceOptions(
provider="nvidia",
model_name="NV-Embed-QA",
),
)

from astrapy.info import (
CollectionLexicalOptions,
CollectionRerankOptions,
RerankServiceOptions,
VectorServiceOptions,
)

vector_store_hybrid = AstraDBVectorStore(
collection_name="my_hybrid_store",
api_endpoint=ASTRA_DB_API_ENDPOINT,
token=ASTRA_DB_APPLICATION_TOKEN,
collection_vector_service_options=VectorServiceOptions(
provider="nvidia",
model_name="NV-Embed-QA",
),
collection_lexical=CollectionLexicalOptions(analyzer="standard"),
collection_rerank=CollectionRerankOptions(
service=RerankServiceOptions(
provider="nvidia",
model_name="nvidia/llama-3.2-nv-rerankqa-1b-v2",
),
),
)

Notable features of class `AstraDBVectorStore`:

- native async API;
- metadata filtering in search;
- MMR (maximum marginal relevance) search;
- server-side embedding computation ( “vectorize” in Astra DB parlance);
- auto-detect its settings from an existing, pre-populated Astra DB collection;
- hybrid search (vector + BM25 and then a rerank step);
- support for non-Astra Data API (e.g. self-hosted HCD deployments);

Learn more in the example notebook.See the example provided by DataStax.

## ​ LLM Cache

from langchain.globals import set_llm_cache
from langchain_astradb import AstraDBCache

set_llm_cache(AstraDBCache(
api_endpoint=ASTRA_DB_API_ENDPOINT,
token=ASTRA_DB_APPLICATION_TOKEN,
))

## ​ Semantic LLM Cache

from langchain.globals import set_llm_cache
from langchain_astradb import AstraDBSemanticCache

set_llm_cache(AstraDBSemanticCache(
embedding=my_embedding,
api_endpoint=ASTRA_DB_API_ENDPOINT,
token=ASTRA_DB_APPLICATION_TOKEN,
))

## ​ Document loader

from langchain_astradb import AstraDBLoader

loader = AstraDBLoader(
collection_name="my_collection",
api_endpoint=ASTRA_DB_API_ENDPOINT,
token=ASTRA_DB_APPLICATION_TOKEN,
)

Learn more in the example notebook.

## ​ Self-querying retriever

from langchain_astradb import AstraDBVectorStore
from langchain_classic.retrievers.self_query.base import SelfQueryRetriever

retriever = SelfQueryRetriever.from_llm(
my_llm,
vector_store,
document_content_description,
metadata_field_info
)

## ​ Store

from langchain_astradb import AstraDBStore

store = AstraDBStore(
collection_name="my_kv_store",
api_endpoint=ASTRA_DB_API_ENDPOINT,
token=ASTRA_DB_APPLICATION_TOKEN,
)

See the API Reference for the AstraDBStore.

## ​ Byte Store

from langchain_astradb import AstraDBByteStore

store = AstraDBByteStore(
collection_name="my_kv_store",
api_endpoint=ASTRA_DB_API_ENDPOINT,
token=ASTRA_DB_APPLICATION_TOKEN,
)

See the API reference for the AstraDBByteStore.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/redis

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Redis

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- Connections
- Redis Standalone connection url
- Redis Sentinel connection url
- Redis Cluster connection url
- Cache
- Standard Cache
- Semantic Cache
- VectorStore

This page covers how to use the Redis ecosystem within LangChain.
It is broken into two parts: installation and setup, and then references to specific Redis wrappers.

## ​ Installation and Setup

Install the Python SDK and LangChain Redis integration:

pip

uv

Copy

pip install redis langchain-redis

To run Redis locally, you can use Docker:

docker run --name langchain-redis -d -p 6379:6379 redis redis-server --save 60 1 --loglevel warning

To stop the container:

docker stop langchain-redis

And to start it again:

docker start langchain-redis

### ​ Connections

We need a redis url connection string to connect to the database support either a stand alone Redis server
or a High-Availability setup with Replication and Redis Sentinels.

#### ​ Redis Standalone connection url

For standalone `Redis` server, the official redis connection url formats can be used as describe in the python redis modules
“from\_url()” method Redis.from\_urlExample: `redis_url = "redis://:secret-pass@localhost:6379/0"`

#### ​ Redis Sentinel connection url

For Redis sentinel setups the connection scheme is “redis+sentinel”.
This is an unofficial extensions to the official IANA registered protocol schemes as long as there is no connection url
for Sentinels available.Example: `redis_url = "redis+sentinel://:secret-pass@sentinel-host:26379/mymaster/0"`The format is `redis+sentinel://$USERNAME:$PASSWORD@$HOST_OR_IP:$PORT/$SERVICE_NAME/$DB_NUMBER`
with the default values of “service-name = mymaster” and “db-number = 0” if not set explicit.
The service-name is the redis server monitoring group name as configured within the Sentinel.The current url format limits the connection string to one sentinel host only (no list can be given) and
both Redis server and sentinel must have the same password set (if used).

#### ​ Redis Cluster connection url

Redis cluster is not supported right now for all methods requiring a “redis\_url” parameter.
The only way to use a Redis Cluster is with LangChain classes accepting a preconfigured Redis client like `RedisCache`
(example below).

## ​ Cache

The Cache wrapper allows for Redis to be used as a remote, low-latency, in-memory cache for LLM prompts and responses.

### ​ Standard Cache

The standard cache is the Redis bread & butter of use case in production for both open-source and enterprise users globally.

from langchain_redis import RedisCache

To use this cache with your LLMs:

from langchain.globals import set_llm_cache
import redis

redis_client = redis.Redis.from_url(...)
set_llm_cache(RedisCache(redis_client))

### ​ Semantic Cache

Semantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends Redis as both a cache and a vectorstore.

from langchain_redis import RedisSemanticCache

# use any embedding provider...
from tests.integration_tests.vectorstores.fake_embeddings import FakeEmbeddings

redis_url = "redis://localhost:6379"

set_llm_cache(RedisSemanticCache(
embedding=FakeEmbeddings(),
redis_url=redis_url
))

## ​ VectorStore

The vectorstore wrapper turns Redis into a low-latency vector database for semantic search or LLM content retrieval.

from langchain_community.vectorstores import Redis

## ​ Retriever

The Redis vector store retriever wrapper generalizes the vectorstore class to perform
low-latency document retrieval. To create the retriever, simply
call `.as_retriever()` on the base vectorstore class.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/together

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Together AI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation
- Environment
- Example

Together AI offers an API to query 50+ leading open-source models in a couple lines of code.This example goes over how to use LangChain to interact with Together AI models.

## ​ Installation

Copy

pip install -U langchain-together

## ​ Environment

To use Together AI, you’ll need an API key which you can find here:
api.together.ai/settings/api-keys. This can be passed in as an init param
`together_api_key` or set as environment variable `TOGETHER_API_KEY`.

## ​ Example

# Querying chat models with Together AI

from langchain_together import ChatTogether

# choose from our 50+ models here:
chat = ChatTogether(
# together_api_key="YOUR_API_KEY",
model="meta-llama/Llama-3-70b-chat-hf",
)

# stream the response back from the model
for m in chat.stream("Tell me fun things to do in NYC"):
print(m.content, end="", flush=True)

# if you don't want to do streaming, you can use the invoke method
# chat.invoke("Tell me fun things to do in NYC")

# Querying code and language models with Together AI

from langchain_together import Together

llm = Together(
model="codellama/CodeLlama-70b-Python-hf",
# together_api_key="..."
)

print(llm.invoke("def bubble_sort(): "))

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/toolbox

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

MCP Toolbox

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- What is it?
- Installation
- Tutorial

The MCP Toolbox in LangChain allows you to equip an agent with a set of tools. When the agent receives a query, it can intelligently select and use the most appropriate tool provided by MCP Toolbox to fulfill the request.

## ​ What is it?

MCP Toolbox is essentially a container for your tools. Think of it as a multi-tool device for your agent; it can hold any tools you create. The agent then decides which specific tool to use based on the user’s input.This is particularly useful when you have an agent that needs to perform a variety of tasks that require different capabilities.

## ​ Installation

To get started, you’ll need to install the necessary package:

pip

uv

Copy

pip install toolbox-langchain

## ​ Tutorial

For a complete, step-by-step guide on how to create, configure, and use MCP Toolbox with your agents, please refer to our detailed Jupyter notebook tutorial.**➡️ View the full tutorial here**.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/unstructured

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Unstructured

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- Data Loaders
- UnstructuredLoader
- UnstructuredCHMLoader
- UnstructuredCSVLoader
- UnstructuredEmailLoader
- UnstructuredEPubLoader
- UnstructuredExcelLoader
- UnstructuredFileIOLoader
- UnstructuredHTMLLoader
- UnstructuredImageLoader
- UnstructuredMarkdownLoader
- UnstructuredODTLoader
- UnstructuredOrgModeLoader
- UnstructuredPDFLoader
- UnstructuredPowerPointLoader
- UnstructuredRSTLoader
- UnstructuredRTFLoader
- UnstructuredTSVLoader
- UnstructuredURLLoader
- UnstructuredWordDocumentLoader

## ​ Installation and Setup

If you are using a loader that runs locally, use the following steps to get `unstructured` and its
dependencies running.

- For the smallest installation footprint and to take advantage of features not available in the
open-source `unstructured` package, install the Python SDK with `pip install unstructured-client`
along with `pip install langchain-unstructured` to use the `UnstructuredLoader` and partition
remotely against the Unstructured API. This loader lives
in a LangChain partner repo instead of the `langchain-community` repo and you will need an
`api_key`, which you can generate a free key here. - Unstructured’s documentation for the sdk can be found here:

- To run everything locally, install the open-source python package with `pip install unstructured`
along with `pip install langchain-community` and use the same `UnstructuredLoader` as mentioned above. - You can install document specific dependencies with extras, e.g. `pip install "unstructured[docx]"`. Learn more about extras here.
- To install the dependencies for all document types, use `pip install "unstructured[all-docs]"`.
- Install the following system dependencies if they are not already available on your system with e.g. `brew install` for Mac.
Depending on what document types you’re parsing, you may not need all of these. - `libmagic-dev` (filetype detection)
- `poppler-utils` (images and PDFs)
- `tesseract-ocr`(images and PDFs)
- `qpdf` (PDFs)
- `libreoffice` (MS Office docs)
- `pandoc` (EPUBs)
- When running locally, Unstructured also recommends using Docker by following this\\
guide to ensure all
system dependencies are installed correctly.

The Unstructured API requires API keys to make requests.
You can request an API key here and start using it today!
Checkout the README here here to get started making API calls.
We’d love to hear your feedback, let us know how it goes in our community slack.
And stay tuned for improvements to both quality and performance!
Check out the instructions
here if you’d like to self-host the Unstructured API or run it locally.

## ​ Data Loaders

The primary usage of `Unstructured` is in data loaders.

### ​ UnstructuredLoader

See a usage example to see how you can use
this loader for both partitioning locally and remotely with the serverless Unstructured API.

Copy

from langchain_unstructured import UnstructuredLoader

### ​ UnstructuredCHMLoader

`CHM` means `Microsoft Compiled HTML Help`.

from langchain_community.document_loaders import UnstructuredCHMLoader

### ​ UnstructuredCSVLoader

A `comma-separated values` (`CSV`) file is a delimited text file that uses
a comma to separate values. Each line of the file is a data record.
Each record consists of one or more fields, separated by commas.See a usage example.

from langchain_community.document_loaders import UnstructuredCSVLoader

### ​ UnstructuredEmailLoader

See a usage example.

from langchain_community.document_loaders import UnstructuredEmailLoader

### ​ UnstructuredEPubLoader

EPUB is an `e-book file format` that uses
the “.epub” file extension. The term is short for electronic publication and
is sometimes styled `ePub`. `EPUB` is supported by many e-readers, and compatible
software is available for most smartphones, tablets, and computers.See a usage example.

from langchain_community.document_loaders import UnstructuredEPubLoader

### ​ UnstructuredExcelLoader

from langchain_community.document_loaders import UnstructuredExcelLoader

### ​ UnstructuredFileIOLoader

from langchain_community.document_loaders import UnstructuredFileIOLoader

### ​ UnstructuredHTMLLoader

from langchain_community.document_loaders import UnstructuredHTMLLoader

### ​ UnstructuredImageLoader

from langchain_community.document_loaders import UnstructuredImageLoader

### ​ UnstructuredMarkdownLoader

from langchain_community.document_loaders import UnstructuredMarkdownLoader

### ​ UnstructuredODTLoader

The `Open Document Format for Office Applications (ODF)`, also known as `OpenDocument`,
is an open file format for word processing documents, spreadsheets, presentations
and graphics and using ZIP-compressed XML files. It was developed with the aim of
providing an open, XML-based file format specification for office applications.See a usage example.

from langchain_community.document_loaders import UnstructuredODTLoader

### ​ UnstructuredOrgModeLoader

An Org Mode document is a document editing, formatting, and organizing mode, designed for notes, planning, and authoring within the free software text editor Emacs.See a usage example.

from langchain_community.document_loaders import UnstructuredOrgModeLoader

### ​ UnstructuredPDFLoader

from langchain_community.document_loaders import UnstructuredPDFLoader

### ​ UnstructuredPowerPointLoader

from langchain_community.document_loaders import UnstructuredPowerPointLoader

### ​ UnstructuredRSTLoader

A `reStructured Text` (`RST`) file is a file format for textual data
used primarily in the Python programming language community for technical documentation.See a usage example.

from langchain_community.document_loaders import UnstructuredRSTLoader

### ​ UnstructuredRTFLoader

See a usage example in the API documentation.

from langchain_community.document_loaders import UnstructuredRTFLoader

### ​ UnstructuredTSVLoader

A `tab-separated values` (`TSV`) file is a simple, text-based file format for storing tabular data.
Records are separated by newlines, and values within a record are separated by tab characters.See a usage example.

from langchain_community.document_loaders import UnstructuredTSVLoader

### ​ UnstructuredURLLoader

from langchain_community.document_loaders import UnstructuredURLLoader

### ​ UnstructuredWordDocumentLoader

from langchain_community.document_loaders import UnstructuredWordDocumentLoader

### ​ UnstructuredXMLLoader

from langchain_community.document_loaders import UnstructuredXMLLoader

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/neo4j

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Neo4j

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- VectorStore
- GraphCypherQAChain

## ​ Installation and Setup

- Install the Python SDK with `pip install neo4j langchain-neo4j`

## ​ VectorStore

The Neo4j vector index is used as a vectorstore,
whether for semantic search or example selection.

Copy

from langchain_neo4j import Neo4jVector

See a usage example

## ​ GraphCypherQAChain

There exists a wrapper around Neo4j graph database that allows you to generate Cypher statements based on the user input
and use them to retrieve relevant information from the database.

from langchain_neo4j import GraphCypherQAChain, Neo4jGraph

## ​ Constructing a knowledge graph from text

Text data often contain rich relationships and insights that can be useful for various analytics, recommendation engines, or knowledge management applications.
Diffbot’s NLP API allows for the extraction of entities, relationships, and semantic meaning from unstructured text data.
By coupling Diffbot’s NLP API with Neo4j, a graph database, you can create powerful, dynamic graph structures based on the information extracted from text.
These graph structures are fully queryable and can be integrated into various applications.

from langchain_neo4j import Neo4jGraph
from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/graph_rag

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Graph RAG

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Installation and setup
- Retrievers

## ​ Overview

Graph RAG provides a retriever interface
that combines **unstructured** similarity search on vectors with **structured**
traversal of metadata properties. This enables graph-based retrieval over **existing**
vector stores.

## ​ Installation and setup

pip

uv

Copy

pip install langchain-graph-retriever

## ​ Retrievers

from langchain_graph_retriever import GraphRetriever

For more information, see the Graph RAG Integration Guide.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/sambanova

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

SambaNova

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Installation and Setup
- API Key
- Chat models
- Embedding Models

Customers are turning to SambaNova to quickly deploy state-of-the-art AI capabilities to gain competitive advantage. Our purpose-built enterprise-scale AI platform is the technology backbone for the next generation of AI computing. We power the foundation models that unlock the valuable business insights trapped in data.Designed for AI, the SambaNova RDU was built with a revolutionary dataflow architecture. This design makes the RDU significantly more efficient for these workloads than GPUs as it eliminates redundant calls to memory, which are an inherent limitation of how GPUs function. This built-in efficiency is one of the features that makes the RDU capable of much higher performance than GPUs in a fraction of the footprint.On top of our architecture We have developed some platforms that allow companies and developers to get full advantage of the RDU processors and open source models.

## ​ Installation and Setup

Install the integration package:

uv

Copy

pip install langchain-sambanova

## ​ API Key

Set your API key it as an environment variable:If you are a SambaCloud user request an API key and set it as an environment variable:

export SAMBANOVA_API_KEY="your-sambacloud-api-key-here"

Or if you are SambaStack user set your base URL and API key as environment variables:

export SAMBANOVA_API_BASE="your-sambastack-envirronment-base-url-here"
export SAMBANOVA_API_KEY="your-sambastack-api-key-here"

## ​ Chat models

For a detailed walkthrough of the `ChatSambaNova` component, see the usage example

from langchain_sambanova import ChatSambaNova

## ​ Embedding Models

For a detailed walkthrough of the `SambaNovaEmbeddings` component, see the usage example

from langchain_sambanova import SambaNovaEmbeddings

SambaNova API Reference

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/contributing

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Contributing

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Contribute

- Documentation
- Code
- Integrations

On this page

- Ways to Contribute
- Acceptable uses of LLMs

**Welcome! Thank you for your interest in contributing.**LangChain has helped form the largest developer community in generative AI, and we’re always open to new contributors. Whether you’re fixing bugs, adding features, improving documentation, or sharing feedback, your involvement helps make LangChain and LangGraph better for everyone 🦜❤️

## ​ Ways to Contribute

Report bugs

Found a bug? Please help us fix it by following these steps:

1

Search

Check if the issue already exists in our GitHub Issues for the respective repo:

**LangChain** \\
\\
Issues **LangGraph** \\
\\
Issues

2

Create issue

If no issue exists, create a new one. When writing, be sure to follow the template provided and to include a minimal, reproducible, example. Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.

3

Wait

A project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.

If you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please link them rather than combining them. For example,

Copy

This issue is blocked by #123 and related to #456.

Suggest features

Have an idea for a new feature or enhancement?

Search the issues for the respective repository for existing feature requests:

Discuss

If no requests exist, start a new discussion under the relevant category so that project maintainers and the community can provide feedback.

Describe

Be sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.

Improve documentation

Documentation improvements are always welcome! We strive to keep our docs clear and comprehensive, and your perspective can make a big difference. **How to propose changes to the documentation** \\
\\
Guide

Contribute code

With a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help! **How to make your first Pull Request** \\
\\
Guide If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work.If you are looking for something to work on, check out the issues labeled “good first issue” or “help wanted” in our repos:

**LangChain** \\
\\
Labels **LangGraph** \\
\\
Labels

Add a new integration

**LangChain** \\
\\
Guide to adding a new LangChain integration

## ​ Acceptable uses of LLMs

Generative AI can be a useful tool for contributors, but like any tool should be used with critical thinking and good judgement.We struggle when contributors’ entire work (code changes, documentation update, pull request descriptions) are LLM-generated. These drive-by contributions often mean well but often miss the mark in terms of contextual relevance, accuracy, and quality.We will close those pull requests and issues that are unproductive, so we can focus our maintainer capacity elsewhere.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Contributing to documentation\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/openai)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers AzureOpenAIEmbeddings Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/anthropic)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/google)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/aws)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/huggingface)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Build a SQL agent Models

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/microsoft)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Trace with LangChain (Python and JS/TS) All integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/ollama)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Ollama Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/groq)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

ChatGroq Implement a LangChain integration Contributing integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/tools)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration Trace with LangChain (Python and JS/TS) All integration providers

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/middleware)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

How to add custom middleware Trace with LangChain (Python and JS/TS) Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/retrievers)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration Contributing integrations All integration providers

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/splitters)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Contributing integrations Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/text_embedding)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration Contributing integrations Embedding models

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/vectorstores)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration Trace with LangChain (Python and JS/TS) All integration providers

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/document_loaders)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Contributing integrations All integration providers All integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/stores)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Contributing integrations Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/chroma)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/groq)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Contributing integrations Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/pinecone)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/pgvector)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/cohere)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/mistralai)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Vector stores

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/fireworks)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/databricks)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers All integrations Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/deepseek)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Vector stores

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/mongodb_atlas)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Vector stores Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/xai)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/ibm)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/perplexity)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/nvidia)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/qdrant)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/tavily)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/milvus)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/elasticsearch)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/azure_ai)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Microsoft Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/litellm)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Trace with LangChain (Python and JS/TS) Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/astradb)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/redis)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/together)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/toolbox)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/unstructured)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/neo4j)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/graph_rag)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/sambanova)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/contributing).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Contribute

- Documentation
- Code
- Integrations

404

# Page not found

We couldn’t find the page you were looking for.

Contributing to documentation Trace with LangChain (Python and JS/TS) Contributing integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/bedrock

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

ChatBedrock

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Integration details
- Model features
- Setup
- Credentials
- Installation
- Instantiation
- Invocation
- Streaming
- Extended Thinking
- Supported Models
- How extended thinking works
- Prompt caching
- Citations
- API reference

This doc will help you get started with AWS Bedrock chat models. Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serverless, you don’t have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with.AWS Bedrock maintains a Converse API which provides a unified conversational interface for Bedrock models. This API does not yet support custom models. You can see a list of all models that are supported here.

**We recommend the Converse API for users who do not need to use custom models. It can be accessed using ChatBedrockConverse.**

For detailed documentation of all Bedrock features and configurations head to the API reference.

## ​ Overview

### ​ Integration details

| Class | Package | Local | Serializable | JS support | Downloads | Version |
| --- | --- | --- | --- | --- | --- | --- |
| ChatBedrock | langchain-aws | ❌ | beta | ✅ | !PyPI - Downloads | !PyPI - Version |
| ChatBedrockConverse | langchain-aws | ❌ | beta | ✅ | !PyPI - Downloads | !PyPI - Version |

### ​ Model features

The below apply to both `ChatBedrock` and `ChatBedrockConverse`.

| Tool calling | Structured output | JSON mode | Image input | Audio input | Video input | Token-level streaming | Native async | Token usage | Logprobs |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | ✅ | ❌ | ✅ | ❌ | ❌ | ✅ | ❌ | ✅ | ❌ |

## ​ Setup

To access Bedrock models you’ll need to create an AWS account, set up the Bedrock API service, get an access key ID and secret key, and install the `langchain-aws` integration package.

### ​ Credentials

Head to the AWS docs to sign up to AWS and setup your credentials.Alternatively, `ChatBedrockConverse` will read from the following environment variables by default:

Copy

# os.environ["AWS_ACCESS_KEY_ID"] = "..."
# os.environ["AWS_SECRET_ACCESS_KEY"] = "..."

# Not required unless using temporary credentials.
# os.environ["AWS_SESSION_TOKEN"] = "..."

You’ll also need to turn on model access for your account, which you can do by following these instructions.To enable automated tracing of your model calls, set your LangSmith API key:

os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
os.environ["LANGSMITH_TRACING"] = "true"

### ​ Installation

The LangChain Bedrock integration lives in the `langchain-aws` package:

pip install -qU langchain-aws

## ​ Instantiation

Now we can instantiate our model object and generate chat completions:

from langchain_aws import ChatBedrockConverse

llm = ChatBedrockConverse(
model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",
# region_name=...,
# aws_access_key_id=...,
# aws_secret_access_key=...,
# aws_session_token=...,
# temperature=...,
# max_tokens=...,
# other params...
)

## ​ Invocation

messages = [\
(\
"system",\
"You are a helpful assistant that translates English to French. Translate the user sentence.",\
),\
("human", "I love programming."),\
]
ai_msg = llm.invoke(messages)
ai_msg

AIMessage(content="J'adore la programmation.", additional_kwargs={}, response_metadata={'ResponseMetadata': {'RequestId': 'b07d1630-06f2-44b1-82bf-e82538dd2215', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 16 Apr 2025 19:35:34 GMT', 'content-type': 'application/json', 'content-length': '206', 'connection': 'keep-alive', 'x-amzn-requestid': 'b07d1630-06f2-44b1-82bf-e82538dd2215'}, 'RetryAttempts': 0}, 'stopReason': 'end_turn', 'metrics': {'latencyMs': [488]}, 'model_name': 'anthropic.claude-3-5-sonnet-20240620-v1:0'}, id='run-d09ed928-146a-4336-b1fd-b63c9e623494-0', usage_metadata={'input_tokens': 29, 'output_tokens': 11, 'total_tokens': 40, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})

print(ai_msg.content)

J'adore la programmation.

### ​ Streaming

Note that `ChatBedrockConverse` emits content blocks while streaming:

for chunk in llm.stream(messages):
print(chunk)

content=[] additional_kwargs={} response_metadata={} id='run-d0e0836e-7146-4c3d-97c7-ad23dac6febd'
content=[{'type': 'text', 'text': 'J', 'index': 0}] additional_kwargs={} response_metadata={} id='run-d0e0836e-7146-4c3d-97c7-ad23dac6febd'
content=[{'type': 'text', 'text': "'adore la", 'index': 0}] additional_kwargs={} response_metadata={} id='run-d0e0836e-7146-4c3d-97c7-ad23dac6febd'
content=[{'type': 'text', 'text': ' programmation.', 'index': 0}] additional_kwargs={} response_metadata={} id='run-d0e0836e-7146-4c3d-97c7-ad23dac6febd'
content=[{'index': 0}] additional_kwargs={} response_metadata={} id='run-d0e0836e-7146-4c3d-97c7-ad23dac6febd'
content=[] additional_kwargs={} response_metadata={'stopReason': 'end_turn'} id='run-d0e0836e-7146-4c3d-97c7-ad23dac6febd'
content=[] additional_kwargs={} response_metadata={'metrics': {'latencyMs': 600}, 'model_name': 'anthropic.claude-3-5-sonnet-20240620-v1:0'} id='run-d0e0836e-7146-4c3d-97c7-ad23dac6febd' usage_metadata={'input_tokens': 29, 'output_tokens': 11, 'total_tokens': 40, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}

You can filter to text using the .text property on the output:

for chunk in llm.stream(messages):
print(chunk.text, end="|")

|J|'adore la| programmation.||||

## ​ Extended Thinking

This guide focuses on implementing Extended Thinking using AWS Bedrock with LangChain’s `ChatBedrockConverse` integration.

### ​ Supported Models

Extended Thinking is available for the following Claude models on AWS Bedrock:

| Model | Model ID |
| --- | --- |
| **Claude Opus 4** | `anthropic.claude-opus-4-20250514-v1:0` |
| **Claude Sonnet 4** | `anthropic.claude-sonnet-4-20250514-v1:0` |
| **Claude 3.7 Sonnet** | `us.anthropic.claude-3-7-sonnet-20250219-v1:0` |

llm = ChatBedrockConverse(
model_id="us.anthropic.claude-sonnet-4-20250514-v1:0",
region_name="us-west-2",
max_tokens=4096,
additional_model_request_fields={
"thinking": {"type": "enabled", "budget_tokens": 1024},
},
)

ai_msg = llm.invoke(messages)
ai_msg.content_blocks

[{'type': 'reasoning',\
'reasoning': 'The user wants me to translate "I love programming" from English to French.\n\n"I love" translates to "J\'aime" in French.\n"Programming" translates to "la programmation" in French.\n\nSo the full translation would be "J\'aime la programmation."',\
'extras': {'signature': 'EpkDCkgIBxABGAIqQGI0KGz8LoVaFwqSAYPN7N+FecI1ZGtb0zpfPr5F8Sb1yxtQHQlmbKUS8JByenWCFGpRKigNaQh1+rLZ59GEX/sSDB+6gxZAT24DJrq4pxoMySVhzwALI6FEC+1UIjDcozOIznjRTYlDWPcYUNYvpt8rwF9IHE38Ha2uqVY8ROJa1tjOMk3OEnbSoV13Pa8q/gETsz+1UwxNX5tgxOa+38jLEryhdFyyAk2JDLrmluZBM6TMrtyzALQvVbZqjpkKAXdtcVCrsz8zUo/LZT1B/92Ukux2dE0O1ZOdcW3tORK+NFLSBaWuqigcFUTDH9XNQoHd2WpQNhl+ypnCItbL2wDRscN/tEBkgGMQugvPmL0LAuLKBmsRKStKRi/RMYGJb3Ft2yEDsRnYNJBJ6TtgxXFvjDwqc/UaI9cIcTxdoVVlsPFsYccpVwirzwAOiz6CSQ1oOQTYJVT90eQ71QW74n1ubbFIZAvDBKk0KG8jK1FGx4FpuuZyFhBpXtfrgOCdrlVSAO/EE9fKCbP9FlhPbRgB'}},\
{'type': 'text', 'text': "J'aime la programmation."}]

### ​ How extended thinking works

When extended thinking is turned on, Claude creates thinking content blocks where it outputs its internal reasoning. Claude incorporates insights from this reasoning before crafting a final response. The API response will include thinking content blocks, followed by text content blocks.

next_messages = messages + [("ai", ai_msg.content), ("human", "I love AI")]

ai_msg = llm.invoke(next_messages)
ai_msg.content_blocks

[{'type': 'reasoning',\
'reasoning': 'The user wants me to translate "I love AI" from English to French. \n\n"I love" translates to "J\'aime" in French.\n"AI" stands for "Artificial Intelligence" which in French is "Intelligence Artificielle" or "IA" (the French abbreviation).\n\nSo the translation would be "J\'aime l\'IA" or "J\'aime l\'intelligence artificielle".\n\nI think using the abbreviation "IA" would be more natural and concise, similar to how the user used "AI" in English.',\
'extras': {'signature': 'EuAECkgIBxABGAIqQLWbkzJ8RzfxhVN1BhfRj5+On8/M9Utt0yH9kvj9P2zlQkO5xloq6I/AiEeArwwdJeqJVcLRjqLtinh6HIBbSDwSDFwt0GL409TqjSZNBhoMPQtJdZmx/uiPrLHUIjCJXyyjgSK3vzbcSEnsvo7pdpoo+waUFrAPDCGL/CIN5u7c8ueLCuCn8W0qGGc+BNgqxQO6UbV11RnMdnUyFmVgTPJErfzBr6U6KyUHd5dJmFWIUVpbbxT2C9vawpbKMPThaRW3BhItEafWGUpPqztzFhqJpSegXtXehIn5iY4yHzTUZ5FPdkNIuAmTsFNNGxiKr9H/gqknvQ2B7I4ushRHLg+drU4cH18EGZlAo5Tu1O9yH5GbweIEew4Uv7oWje+R8TIku0OFVhrbnQqqqukBicMV2JRifUYuz6dYM1UDYS8SfxQ1MmcVY5t1L9LDpoL4F/CtpL8/6YDsB/FosU37Qc1qm+D+pKEPTYnyxaP5tRXqTBfqUIiNJGqr9Egl17Akoy6NIv234rPfuf8HjTcu5scZoPGhOreG5rWxJ7AbTCIXgGWqpcf2TqDtniOac3jW4OtnlID9fsloKNq6Y5twgXHDR47c4Jh6vWmucZiIlL6hkklQzt5To6vOnqcTOGUtuCis8Y2wRzlNGeR2d8A+ocYm7mBvR/Y5DvDgstJwB/vCLoQlIL+jm6+h8k6EX/24GqOsh5hxsS5IsNIob/p8tr4TBbc9noCoUSYkMhbQPi2xpRrNML9GUIo7Skbh1ni67uqeShj1xuUrFG+cN6x4yzDaRb59LCAYAQ=='}},\
{'type': 'text', 'text': "J'aime l'IA."}]

## ​ Prompt caching

Bedrock supports caching of elements of your prompts, including messages and tools. This allows you to re-use large documents, instructions, few-shot documents, and other data to reduce latency and costs.

**Not all models support prompt caching. See supported models here.**

To enable caching on an element of a prompt, mark its associated content block using the `cachePoint` key. See example below:

import requests
from langchain_aws import ChatBedrockConverse

llm = ChatBedrockConverse(model="us.anthropic.claude-sonnet-4-5-20250929-v1:0")

# Pull LangChain readme
get_response = requests.get(
"https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md"
)
readme = get_response.text

messages = [\
{\
"role": "user",\
"content": [\
{\
"type": "text",\
"text": "What's LangChain, according to its README?",\
},\
{\
"type": "text",\
"text": f"{readme}",\
},\
{\
"cachePoint": {"type": "default"},\
},\
],\
},\
]

response_1 = llm.invoke(messages)
response_2 = llm.invoke(messages)

usage_1 = response_1.usage_metadata["input_token_details"]
usage_2 = response_2.usage_metadata["input_token_details"]

print(f"First invocation:\n{usage_1}")
print(f"\nSecond:\n{usage_2}")

First invocation:
{'cache_creation': 1528, 'cache_read': 0}

Second:
{'cache_creation': 0, 'cache_read': 1528}

## ​ Citations

Citations can be generated if they are enabled on input documents. Documents can be
specified in Bedrock’s
native format
or LangChain’s standard types:

Bedrock format

LangChain standard format

llm = ChatBedrockConverse(model="us.anthropic.claude-sonnet-4-20250514-v1:0")

pdf_path = "path/to/your/file.pdf"

with open(pdf_path, "rb") as f:
pdf_bytes = f.read()

document = {
"document": {
"format": "pdf",
"source": {"bytes": pdf_bytes},
"name": "my-pdf",
"citations": {"enabled": True},
},
}

response = llm.invoke(
[\
{\
"role": "user",\
"content": [\
{"type": "text", "text": "Describe this document."},\
document,\
]\
},\
]
)
response.content_blocks

* * *

## ​ API reference

For detailed documentation of all ChatBedrock features and configurations head to the API reference: python.langchain.com/api\_reference/aws/chat\_models/langchain\_aws.chat\_models.bedrock.ChatBedrock.htmlFor detailed documentation of all ChatBedrockConverse features and configurations head to the API reference: python.langchain.com/api\_reference/aws/chat\_models/langchain\_aws.chat\_models.bedrock\_converse.ChatBedrockConverse.html

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/anthropic)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration All integration providers Contributing to code

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/openai)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

ChatOpenAI Build customer support with handoffs Build a multi-source knowledge base with routing

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/bedrock),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration All integration providers Contributing integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/azure_chat_openai

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

AzureChatOpenAI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Integration details
- Model features
- Setup
- Installation
- Credentials
- Instantiation
- Invocation
- Streaming usage metadata
- Specifying model version
- API reference

You can find information about Azure OpenAI’s latest models and their costs, context windows, and supported input types in the Azure docs.

**Azure OpenAI vs OpenAI**Azure OpenAI refers to OpenAI models hosted on the Microsoft Azure platform. OpenAI also provides its own model APIs. To access OpenAI services directly, use the `ChatOpenAI` integration.

**Azure OpenAI v1 API**Azure OpenAI’s v1 API (Generally Available as of August 2025) allows you to use `ChatOpenAI` directly with Azure endpoints. This provides a unified interface and native support for Microsoft Entra ID authentication with automatic token refresh.See the ChatOpenAI Azure section for details on using `ChatOpenAI` with Azure’s v1 API.`AzureChatOpenAI` is still currently supported for traditional Azure OpenAI API versions and scenarios requiring Azure-specific configurations, but we recommend using `ChatOpenAI` or the `AzureAIChatCompletionsModel` in LangChain Azure AI going forward.

`AzureChatOpenAI` shares the same underlying base implementation as `ChatOpenAI`,
which interfaces with OpenAI services directly.This page serves as a quickstart for authenticating and connecting your Azure OpenAI service to a LangChain chat model.Visit the `ChatOpenAI` docs for details on available
features, or head to the `AzureChatOpenAI` API reference.

**API Reference**For detailed documentation of all features and configuration options, head to the `AzureChatOpenAI` API reference.

## ​ Overview

### ​ Integration details

| Class | Package | Local | Serializable | JS/TS Support | Downloads | Latest Version |
| --- | --- | --- | --- | --- | --- | --- |
| `AzureChatOpenAI` | `langchain-openai` | ❌ | beta | ✅ (npm) | ![Downloads per month](https://pypi.org/project/langchain-openai/) | ![PyPI - Latest version](https://pypi.org/project/langchain-openai/) |

### ​ Model features

| Tool calling | Structured output | JSON mode | Image input | Audio input | Video input | Token-level streaming | Native async | Token usage | Logprobs |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ✅ | ✅ |

## ​ Setup

To access `AzureChatOpenAI` models you’ll need to create an Azure account, create a deployment of an Azure OpenAI model, get the name and endpoint for your deployment, get an Azure OpenAI API key, and install the `langchain-openai` integration package.

### ​ Installation

pip

uv

Copy

pip install -U langchain-openai

### ​ Credentials

Head to the Azure docs to create your deployment and generate an API key. Once you’ve done this set the `AZURE_OPENAI_API_KEY` and `AZURE_OPENAI_ENDPOINT` environment variables:

import getpass
import os

if "AZURE_OPENAI_API_KEY" not in os.environ:
os.environ["AZURE_OPENAI_API_KEY"] = getpass.getpass(
"Enter your AzureOpenAI API key: "
)
os.environ["AZURE_OPENAI_ENDPOINT"] = "https://YOUR-ENDPOINT.openai.azure.com/"

To enable automated tracing of your model calls, set your LangSmith API key:

os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
os.environ["LANGSMITH_TRACING"] = "true"

## ​ Instantiation

Now we can instantiate our model object and generate chat completions.

- Replace `azure_deployment` with the name of your deployment,
- You can find the latest supported `api_version` here: learn.microsoft.com/en-us/azure/ai-services/openai/reference.

from langchain_openai import AzureChatOpenAI

llm = AzureChatOpenAI(
azure_deployment="gpt-35-turbo", # or your deployment
api_version="2023-06-01-preview", # or your api version
temperature=0,
max_tokens=None,
timeout=None,
max_retries=2,
# other params...
)

## ​ Invocation

messages = [\
(\
"system",\
"You are a helpful assistant that translates English to French. Translate the user sentence.",\
),\
("human", "I love programming."),\
]
ai_msg = llm.invoke(messages)
ai_msg

AIMessage(content="J'adore la programmation.", response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 31, 'total_tokens': 39}, 'model_name': 'gpt-35-turbo', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-bea4b46c-e3e1-4495-9d3a-698370ad963d-0', usage_metadata={'input_tokens': 31, 'output_tokens': 8, 'total_tokens': 39})

print(ai_msg.content)

J'adore la programmation.

## ​ Streaming usage metadata

OpenAI’s Chat Completions API does not stream token usage statistics by default (see API reference here).To recover token counts when streaming with `ChatOpenAI` or `AzureChatOpenAI`, set `stream_usage=True` as an initialization parameter or on invocation:

llm = AzureChatOpenAI(model="gpt-4.1-mini", stream_usage=True)

## ​ Specifying model version

Azure OpenAI responses contain `model_name` response metadata property, which is name of the model used to generate the response. However unlike native OpenAI responses, it does not contain the specific version of the model, which is set on the deployment in Azure. e.g. it does not distinguish between `gpt-35-turbo-0125` and `gpt-35-turbo-0301`. This makes it tricky to know which version of the model was used to generate the response, which as result can lead to e.g. wrong total cost calculation with `OpenAICallbackHandler`.To solve this problem, you can pass `model_version` parameter to `AzureChatOpenAI` class, which will be added to the model name in the llm output. This way you can easily distinguish between different versions of the model.

pip install -qU langchain-community

from langchain_community.callbacks import get_openai_callback

with get_openai_callback() as cb:
llm.invoke(messages)
print(
f"Total Cost (USD): ${format(cb.total_cost, '.6f')}"
) # without specifying the model version, flat-rate 0.002 USD per 1k input and output tokens is used

Total Cost (USD): $0.000063

llm_0301 = AzureChatOpenAI(
azure_deployment="gpt-35-turbo", # or your deployment
api_version="2023-06-01-preview", # or your api version
model_version="0301",
)
with get_openai_callback() as cb:
llm_0301.invoke(messages)
print(f"Total Cost (USD): ${format(cb.total_cost, '.6f')}")

Total Cost (USD): $0.000074

* * *

## ​ API reference

For detailed documentation of all features and configuration options, head to the `AzureChatOpenAI` API reference.

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

ChatGoogleGenerativeAI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Integration details
- Model features
- Setup
- Installation
- Credentials
- Backend selection
- Environment variables
- Instantiation
- Proxy configuration
- Invocation
- Multimodal usage
- Supported input methods
- File upload
- Image input
- PDF input
- Audio input
- Video input
- Image generation
- Audio generation
- Tool calling
- Structured output
- Structured output methods
- Token usage tracking
- Thinking support
- Thinking level
- Gemini 2.5 models: thinking\_budget
- Viewing model thoughts
- Thought signatures
- Built-in tools
- Google search
- Google Maps
- URL context
- Code execution
- Computer use
- Safety settings
- Context caching
- Response metadata
- API reference

Access Google’s Generative AI models, including the Gemini family, via the **Gemini Developer API** or **Vertex AI**. The Gemini Developer API offers quick setup with API keys, ideal for individual developers. Vertex AI provides enterprise features and integrates with Google Cloud Platform.For information on the latest models, model IDs, their features, context windows, etc. head to the Google AI docs.

**Vertex AI consolidation & compatibility**As of `langchain-google-genai` 4.0.0, this package uses the consolidated `google-genai` SDK instead of the legacy `google-ai-generativelanguage` SDK.This migration brings support for Gemini models both via the Gemini Developer API and Gemini API in Vertex AI, superseding certain classes in `langchain-google-vertexai`, such as `ChatVertexAI`.Read the full announcement and migration guide.

**API Reference**For detailed documentation of all features and configuration options, head to the `ChatGoogleGenerativeAI` API reference.

## ​ Overview

### ​ Integration details

| Class | Package | Local | Serializable | JS support | Downloads | Version |
| --- | --- | --- | --- | --- | --- | --- |
| `ChatGoogleGenerativeAI` | `langchain-google-genai` | ❌ | beta | ✅ | !PyPI - Downloads | !PyPI - Version |

### ​ Model features

| Tool calling | Structured output | JSON mode | Image input | Audio input | Video input | Token-level streaming | Native async | Token usage | Logprobs |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ⚠️ |

## ​ Setup

To access Google AI models you’ll need to create a Google Account, get a Google AI API key, and install the `langchain-google-genai` integration package.

### ​ Installation

Copy

pip install -U langchain-google-genai

### ​ Credentials

This integration supports two backends: **Gemini Developer API** and **Vertex AI**. The backend is selected automatically based on your configuration.

#### ​ Backend selection

The backend is determined as follows:

1. If `GOOGLE_GENAI_USE_VERTEXAI` env var is set, uses that value
2. If `credentials` parameter is provided, uses Vertex AI
3. If `project` parameter is provided, uses Vertex AI
4. Otherwise, uses Gemini Developer API

You can also explicitly set `vertexai=True` or `vertexai=False` to override auto-detection.

- Gemini Developer API

- Vertex AI with API key

- Vertex AI with credentials

**Quick setup with API key**Recommended for individual developers / new users.Head to Google AI Studio to generate an API key:

import getpass
import os

if "GOOGLE_API_KEY" not in os.environ:
os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google AI API key: ")

The integration checks for `GOOGLE_API_KEY` first, then `GEMINI_API_KEY` as a fallback.

**Vertex AI using API key authentication**You can use Vertex AI with API key authentication for simpler setup:

export GEMINI_API_KEY='your-api-key'
export GOOGLE_GENAI_USE_VERTEXAI=true
export GOOGLE_CLOUD_PROJECT='your-project-id'

Or programmatically:

from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
model="gemini-2.5-flash",
api_key="your-api-key",
project="your-project-id",
vertexai=True,
)

**Vertex AI using service account or ADC**Set up Application Default Credentials (ADC):

gcloud auth application-default login

Set your Google Cloud project:

export GOOGLE_CLOUD_PROJECT='your-project-id'
# Optional: set region (defaults to us-central1)
export GOOGLE_CLOUD_LOCATION='us-central1'

Or use service account credentials:

from google.oauth2 import service_account
from langchain_google_genai import ChatGoogleGenerativeAI

credentials = service_account.Credentials.from_service_account_file(
"path/to/service-account.json",
scopes=["https://www.googleapis.com/auth/cloud-platform"],
)

llm = ChatGoogleGenerativeAI(
model="gemini-2.5-flash",
credentials=credentials,
project="your-project-id",
)

#### ​ Environment variables

| Variable | Purpose | Backend |
| --- | --- | --- |
| `GOOGLE_API_KEY` | API key (primary) | Both (see `GOOGLE_GENAI_USE_VERTEXAI`) |
| `GEMINI_API_KEY` | API key (fallback) | Both (see `GOOGLE_GENAI_USE_VERTEXAI`) |
| `GOOGLE_GENAI_USE_VERTEXAI` | Force Vertex AI backend (`true`/`false`) | Vertex AI |
| `GOOGLE_CLOUD_PROJECT` | GCP project ID | Vertex AI |
| `GOOGLE_CLOUD_LOCATION` | GCP region (default: `us-central1`) | Vertex AI |

To enable automated tracing of your model calls, set your LangSmith API key:

os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
os.environ["LANGSMITH_TRACING"] = "true"

## ​ Instantiation

Now we can instantiate our model object and generate responses:

- Vertex AI

model = ChatGoogleGenerativeAI(
model="gemini-3-pro-preview",
temperature=1.0, # Gemini 3.0+ defaults to 1.0
max_tokens=None,
timeout=None,
max_retries=2,
# other params...
)

model = ChatGoogleGenerativeAI(
model="gemini-3-pro-preview",
project="your-project-id",
location="us-central1", # Optional, defaults to us-central1
temperature=1.0, # Gemini 3.0+ defaults to 1.0
max_tokens=None,
timeout=None,
max_retries=2,

Providing `project` automatically selects the Vertex AI backend unless you explicitly set `vertexai=False`.

**Temperature for Gemini 3.0+ models**If `temperature` is not explicitly set and the model is Gemini 3.0 or later, it will be automatically set to `1.0` instead of the default `0.7` per Google GenAI API best practices. Using `0.7` with Gemini 3.0+ can cause infinite loops, degraded reasoning performance, and failure on complex tasks.

See the `ChatGoogleGenerativeAI` API Reference for the full set of available model parameters.

### ​ Proxy configuration

If you need to use a proxy, set these environment variables before initializing:

export HTTPS_PROXY='http://username:password@proxy_uri:port'
export SSL_CERT_FILE='path/to/cert.pem' # Optional: custom SSL certificate

For SOCKS5 proxies or advanced proxy configuration, use the `client_args` parameter:

model = ChatGoogleGenerativeAI(
model="gemini-3-pro-preview",
client_args={"proxy": "socks5://user:pass@host:port"},
)

## ​ Invocation

messages = [\
(\
"system",\
"You are a helpful assistant that translates English to French. Translate the user sentence.",\
),\
("human", "I love programming."),\
]
ai_msg = model.invoke(messages)
ai_msg

Gemini 3

Gemini 2.5

AIMessage(content=[{'type': 'text', 'text': "J'adore la programmation.", 'extras': {'signature': 'EpoWCpc...'}}], additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-3-pro-preview', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--fb732b64-1ab4-4a28-b93b-dcfb2a164a3d-0', usage_metadata={'input_tokens': 21, 'output_tokens': 779, 'total_tokens': 800, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 772}})

**Message content shape**Gemini 3 series models will always return a list of content blocks to capture thought signatures. Use the `.text` property to recover string content.

print(ai_msg.content)

[{'type': 'text',\
'text': "J'adore la programmation.",\
'extras': {'signature': '...'}}]

print(ai_msg.text)

J'adore la programmation.

## ​ Multimodal usage

Gemini models can accept multimodal inputs (text, images, audio, video) and, for some models, generate multimodal outputs.

### ​ Supported input methods

| Method | Image | Video | Audio | PDF |
| --- | --- | --- | --- | --- |
| File upload (Files API) | ✅ | ✅ | ✅ | ✅ |
| Base64 inline data | ✅ | ✅ | ✅ | ✅ |
| HTTP/HTTPS URLs\* | ✅ | ✅ | ✅ | ✅ |
| GCS URIs (`gs://...`) | ✅ | ✅ | ✅ | ✅ |

\*YouTube URLs are supported for video input in preview.

### ​ File upload

You can upload files to Google’s servers and reference them by URI. This works for PDFs, images, videos, and audio files.

import time
from google import genai
from langchain.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI

client = genai.Client()
model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

# Upload file to Google's servers
myfile = client.files.upload(file="/path/to/your/file.pdf")
while myfile.state.name == "PROCESSING":
time.sleep(2)
myfile = client.files.get(name=myfile.name)

# Reference by file_id in FileContentBlock
message = HumanMessage(
content=[\
{"type": "text", "text": "What is in the document?"},\
{\
"type": "file",\
"file_id": myfile.uri, # or myfile.name\
"mime_type": "application/pdf",\
},\
]
)
response = model.invoke([message])

Once uploaded, you can reference the file in any of the media-specific sections below using the `file_id` pattern.

### ​ Image input

Provide image inputs along with text using a `HumanMessage` with a list content format.

Image URL

Chat Completions image\_url format

Base64 encoded

Uploaded file

from langchain.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")

message = HumanMessage(
content=[\
{"type": "text", "text": "Describe the image at the URL."},\
{\
"type": "image",\
"url": "https://picsum.photos/seed/picsum/200/300",\
},\
]
)
response = model.invoke([message])

Other supported image formats:

- A Google Cloud Storage URI (`gs://...`). Ensure the service account has access.

### ​ PDF input

Provide PDF file inputs along with text.

URL

message = HumanMessage(
content=[\
{"type": "text", "text": "Describe the document in a sentence."},\
{\
"type": "image_url", # (PDFs are treated as images)\
"image_url": "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf",\
},\
]
)
response = model.invoke([message])

### ​ Audio input

Provide audio file inputs along with text.

message = HumanMessage(
content=[\
{"type": "text", "text": "Summarize this audio in a sentence."},\
{\
"type": "image_url",\
"image_url": "https://example.com/audio.mp3",\
},\
]
)
response = model.invoke([message])

### ​ Video input

Provide video file inputs along with text.

YouTube URL

import base64
from langchain.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI

video_bytes = open("path/to/your/video.mp4", "rb").read()
video_base64 = base64.b64encode(video_bytes).decode("utf-8")
mime_type = "video/mp4"

message = HumanMessage(
content=[\
{"type": "text", "text": "Describe what's in this video in a sentence."},\
{\
"type": "video",\
"base64": video_base64,\
"mime_type": mime_type,\
},\
]
)
response = model.invoke([message])

**YouTube limitations**

- Only public videos are supported (not private or unlisted)
- Free tier: max 8 hours of YouTube video per day
- Feature is currently in preview

### ​ Image generation

Certain models can generate text and images inline. See more information on the Gemini API docs for details.This example demonstrates how to generate and display an image in a Jupyter notebook:

import base64

from IPython.display import Image, display
from langchain.messages import AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-image")

response = model.invoke("Generate a photorealistic image of a cuddly cat wearing a hat.")

image_block = next(
block
for block in response.content
if isinstance(block, dict) and block.get("image_url")
)
return image_block["image_url"].get("url").split(",")[-1]

image_base64 = _get_image_base64(response)
display(Image(data=base64.b64decode(image_base64), width=300))

Use `image_config` to control resulting image dimensions and quality. See `genai.types.ImageConfig`
for a list of supported fields and their values.Setting `image_config` during instantiation applies the configuration to all invocations, while setting it during invocation overrides the default for that call only; this allows for flexible control over generation parameters on a per-request basis.

Instantiation

Invocation

model = ChatGoogleGenerativeAI(
model="gemini-2.5-flash-image",
image_config={
"aspect_ratio": "16:9",
},
)

Supported parameters vary by model and backend (Gemini Developer API and Vertex AI each support different subsets of parameters and models).

By default, image generation models may return both text and images (e.g. _“Ok! Here’s an image of a…”_).You can request that the model only return images by setting the `response_modalities` parameter:

from langchain_google_genai import ChatGoogleGenerativeAI, Modality

model = ChatGoogleGenerativeAI(
model="gemini-2.5-flash-image",
response_modalities=[Modality.IMAGE],
)

# All invocations will return only images

### ​ Audio generation

Certain models can generate audio files. See more information on the Gemini API docs for details.

**Vertex AI Limitation**Audio generation models are currently in limited preview on Vertex AI and may require allowlist access. If you encounter an `INVALID_ARGUMENT` error when using TTS models with `vertexai=True`, your GCP project may need to be allowlisted.For more details, see this Google AI forum discussion.

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-preview-tts")

response = model.invoke("Please say The quick brown fox jumps over the lazy dog")

# Base64 encoded binary data of the audio
wav_data = response.additional_kwargs.get("audio")
with open("output.wav", "wb") as f:
f.write(wav_data)

## ​ Tool calling

You can equip the model with tools to call.

from langchain.tools import tool
from langchain.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI

# Define the tool
@tool(description="Get the current weather in a given location")

return "It's sunny."

# Initialize and bind (potentially multiple) tools to the model
model_with_tools = ChatGoogleGenerativeAI(model="gemini-3-pro-preview").bind_tools([get_weather])

# Step 1: Model generates tool calls
messages = [HumanMessage("What's the weather in Boston?")]
ai_msg = model_with_tools.invoke(messages)
messages.append(ai_msg)

# Check the tool calls in the response
print(ai_msg.tool_calls)

# Step 2: Execute tools and collect results
for tool_call in ai_msg.tool_calls:
# Execute the tool with the generated arguments
tool_result = get_weather.invoke(tool_call)
messages.append(tool_result)

# Step 3: Pass results
final_response

[{'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': '879b4233-901b-4bbb-af56-3771ca8d3a75', 'type': 'tool_call'}]

AIMessage(content=[{'type': 'text', 'text': 'The weather in Boston is sunny.'}], additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-3-pro-preview', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--190be543-c974-460b-a708-7257892c3121-0', usage_metadata={'input_tokens': 143, 'output_tokens': 7, 'total_tokens': 150, 'input_token_details': {'cache_read': 0}})

## ​ Structured output

Force the model to respond with a specific structure. See the Gemini API docs for more info.

from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel
from typing import Literal

class Feedback(BaseModel):
sentiment: Literal["positive", "neutral", "negative"]
summary: str

model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")
structured_model = model.with_structured_output(
schema=Feedback.model_json_schema(), method="json_schema"
)

response = structured_model.invoke("The new UI is great!")
response["sentiment"] # "positive"
response["summary"] # "The user expresses positive..."

For streaming structured output, merge dictionaries instead of using `+=`:

stream = structured_model.stream("The interface is intuitive and beautiful!")
full = next(stream)
for chunk in stream:
full.update(chunk) # Merge dictionaries
print(full) # Complete structured response

### ​ Structured output methods

Two methods are supported for structured output:

- **`method="json_schema"` (default)**: Uses Gemini’s native structured output. Recommended for better reliability, as it constrains the model’s generation process directly rather than relying on post-processing tool calls.
- **`method="function_calling"`**: Uses tool calling to extract structured data.

## ​ Token usage tracking

Access token usage information from the response metadata.

result = model.invoke("Explain the concept of prompt engineering in one sentence.")

print(result.content)
print("\nUsage Metadata:")
print(result.usage_metadata)

Prompt engineering is the art and science of crafting effective text prompts to elicit desired and accurate responses from large language models.

Usage Metadata:
{'input_tokens': 10, 'output_tokens': 24, 'total_tokens': 34, 'input_token_details': {'cache_read': 0}}

## ​ Thinking support

Certain Gemini models support configurable thinking depth. Depending on the model version used, you can control this via either `thinking_level` (Gemini 3+) or `thinking_budget` (Gemini 2.5).

### ​ Thinking level

For Gemini 3+ models, use `thinking_level` to control reasoning depth. Some model providers call this “reasoning effort”.

| Value | Models | Description |
| --- | --- | --- |
| `'minimal'` | Flash | Matches the “no thinking” setting for most queries |
| `'low'` | Flash, Pro | Minimizes latency and cost |
| `'medium'` | Flash | Balances latency/cost with reasoning depth |
| `'high'` | Pro | Maximizes reasoning depth (default) |

Note that `minimal` does not guarantee that thinking is off.

llm = ChatGoogleGenerativeAI(
model="gemini-3-pro-preview",
thinking_level="low",
)

response = llm.invoke("How many O's are in Google?")

### ​ Gemini 2.5 models: `thinking_budget`

For Gemini 2.5 models, use `thinking_budget` (an integer token count) instead:

- Set to `0` to disable thinking (where supported)
- Set to `-1` for dynamic thinking (model decides)
- Set to a positive integer to constrain token usage

llm = ChatGoogleGenerativeAI(
model="gemini-2.5-flash",
thinking_budget=1024,
)

Not all models allow disabling thinking. See the Gemini models documentation for details.

### ​ Viewing model thoughts

To see a thinking model’s reasoning, set `include_thoughts=True`:

llm = ChatGoogleGenerativeAI(
model="gemini-3-pro-preview",
include_thoughts=True,
)

response = llm.invoke("How many O's are in Google? How did you verify your answer?")
reasoning_tokens = response.usage_metadata["output_token_details"]["reasoning"]

print("Response:", response.content)
print("Reasoning tokens used:", reasoning_tokens)

Response: [{'type': 'thinking', 'thinking': '**Analyzing and Cou...'}, {'type': 'text', 'text': 'There a...', 'extras': {'signature': 'EroR...'}}]
Reasoning tokens used: 672

See the Gemini API docs for more information on thinking.

### ​ Thought signatures

Thought signatures are encrypted representations of the model’s reasoning processes. They enable Gemini to maintain thought context across multi-turn conversations, since the API is stateless and treats each request independently.

Signatures appear in two places in `AIMessage` responses:

- **Text blocks**: Stored in `extras.signature` within the content block
- **Tool calls**: Stored in `additional_kwargs["__gemini_function_call_thought_signatures__"]`

response = llm.invoke("How many O's are in Google? How did you verify your answer?")

response.content_blocks[-1]

{'type': 'text',
'text': 'There are **2** O\'s in the word "Google...',
'extras': {'signature': 'EsUSCsIS...'}}

For multi-turn conversations with tool calls, you must pass the full `AIMessage`
messages = [HumanMessage("What's the weather in Tokyo?")]

# Step 1: Model returns tool call with thought signature attached
ai_msg = model.invoke(messages)
messages.append(ai_msg) # Preserves thought signature

# Step 2: Execute tool and add result
for tool_call in ai_msg.tool_calls:
result = get_weather.invoke(tool_call)
messages.append(result)

# Step 3: Model receives signature back, continues reasoning coherently
final_response = model.invoke(messages)

**Don’t reconstruct messages manually.** If you create a new `AIMessage` instead of passing the original object, the signatures will be lost and the API may reject the request.

## ​ Built-in tools

Google Gemini supports a variety of built-in tools, which can be bound to the model in the usual way.

### ​ Google search

See Gemini docs for detail.

Bind to model

Use on invocation

model_with_search = model.bind_tools([{"google_search": {}}])
response = model_with_search.invoke("When is the next total solar eclipse in US?")

response.content_blocks

[{'type': 'text',\
'text': 'The next total solar eclipse visible in the contiguous United States will occur on...',\
'annotations': [{'type': 'citation',\
'id': 'abc123',\

'start_index': 0,\
'end_index': 99,\
'cited_text': 'The next total solar eclipse...',\
'extras': {'google_ai_metadata': {'web_search_queries': ['next total solar eclipse in US'],\
'grounding_chunk_index': 0,\
'confidence_scores': []}}},\
...\
```\
\
### ​ Google Maps\
\
Certain models support grounding using Google Maps. Maps grounding connects Gemini’s generative capabilities with Google Maps’ current, factual location data. This enables location-aware applications that provide accurate, geographically specific responses. See Gemini docs for detail.\
\
Bind to model\
\
Use on invocation\
\
Copy\
\
```\
from langchain_google_genai import ChatGoogleGenerativeAI\
\
model = ChatGoogleGenerativeAI(model="gemini-2.5-pro")\
\
model_with_maps = model.bind_tools([{"google_maps": {}}])\
response = model_with_maps.invoke(\
"What are some good Italian restaurants near the Eiffel Tower in Paris?"\
)\
```\
\
The response will include grounding metadata with location information from Google Maps.You can optionally provide a specific location context using `tool_config` with `lat_lng`. This is useful when you want to ground queries relative to a specific geographic point.\
\
Bind to model\
\
Use on invocation\
\
Copy\
\
```\
from langchain_google_genai import ChatGoogleGenerativeAI\
\
model = ChatGoogleGenerativeAI(model="gemini-2.5-pro")\
\
# Provide location context (latitude and longitude)\
model_with_maps = model.bind_tools(\
[{"google_maps": {}}],\
tool_config={\
"retrieval_config": { # Eiffel Tower\
"lat_lng": {\
"latitude": 48.858844,\
"longitude": 2.294351,\
}\
}\
},\
)\
\
response = model_with_maps.invoke(\
"What Italian restaurants are within a 5 minute walk from here?"\
)\
```\
\
### ​ URL context\
\
The URL context tool enables the model to access and analyze content from URLs you provide in your prompt. This is useful for tasks like summarizing web pages, extracting data from multiple sources, or answering questions about online content. See Gemini docs for detail and limitations.\
\
Bind to model\
\
Use on invocation\
\
Copy\
\
```\
from langchain_google_genai import ChatGoogleGenerativeAI\
\
model = ChatGoogleGenerativeAI(model="gemini-2.5-flash")\
\
model_with_url_context = model.bind_tools([{"url_context": {}}])\
response = model_with_url_context.invoke(\
"Summarize the content at
)\
```\
\
### ​ Code execution\
\
See Gemini docs for detail.\
\
Bind to model\
\
Use on invocation\
\
Copy\
\
```\
from langchain_google_genai import ChatGoogleGenerativeAI\
\
model = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")\
\
model_with_code_interpreter = model.bind_tools([{"code_execution": {}}])\
response = model_with_code_interpreter.invoke("Use Python to calculate 3^3.")\
\
response.content_blocks\
```\
\
Copy\
\
```\
[{'type': 'server_tool_call',\
'name': 'code_interpreter',\

'id': '...'},\
{'type': 'server_tool_result',\
'tool_call_id': '',\
'status': 'success',\
'output': '27\n',\
'extras': {'block_type': 'code_execution_result',\

{'type': 'text', 'text': 'The calculation of 3 to the power of 3 is 27.'}]\
```\
\
### ​ Computer use\
\
The Gemini 2.5 Computer Use model (`gemini-2.5-computer-use-preview-10-2025`) can interact with browser environments to automate web tasks like clicking, typing, and scrolling.\
\
**Preview model limitations**The Computer Use model is in preview and may produce unexpected behavior. Always supervise automated tasks and avoid use with sensitive data or critical operations. See the Gemini API docs for safety best practices.\
\
Bind to model\
\
Use on invocation\
\
Copy\
\
```\
from langchain_google_genai import ChatGoogleGenerativeAI\
\
model = ChatGoogleGenerativeAI(model="gemini-2.5-computer-use-preview-10-2025")\
model_with_computer = model.bind_tools([{"computer_use": {}}])\
\
response = model_with_computer.invoke("Please navigate to example.com")\
\
response.content_blocks\
```\
\
Copy\
\
```\
[{'type': 'tool_call',\
'id': '08a8b175-16ab-4861-8965-b736d5d4dd7e',\
'name': 'open_web_browser',\
'args': {}}]\
```\
\
You can configure the environment and exclude specific UI actions:\
\
Advanced configuration\
\
Copy\
\
```\
from langchain_google_genai import ChatGoogleGenerativeAI, Environment\
\
model = ChatGoogleGenerativeAI(model="gemini-2.5-computer-use-preview-10-2025")\
\
# Specify the environment (browser is default)\
model_with_computer = model.bind_tools(\
[{"computer_use": {"environment": Environment.ENVIRONMENT_BROWSER}}]\
)\
\
# Exclude specific UI actions\
model_with_computer = model.bind_tools(\
[\
{\
"computer_use": {\
"environment": Environment.ENVIRONMENT_BROWSER,\
"excludedPredefinedFunctions": [\
"drag_and_drop",\
"key_combination",\
],\
}\
}\
]\
)\
\
response = model_with_computer.invoke("Search for Python tutorials")\
```\
\
The model returns function calls for UI actions (like `click_at`, `type_text_at`, `scroll`) with normalized coordinates. You’ll need to implement the actual execution of these actions in your browser automation framework.\
\
## ​ Safety settings\
\
Gemini models have default safety settings that can be overridden. If you are receiving lots of `'Safety Warnings'` from your models, you can try tweaking the `safety_settings` attribute of the model. For example, to turn off safety blocking for dangerous content, you can construct your LLM as follows:\
\
Copy\
\
```\
from langchain_google_genai import (\
ChatGoogleGenerativeAI,\
HarmBlockThreshold,\
HarmCategory,\
)\
\
llm = ChatGoogleGenerativeAI(\
model="gemini-3-pro-preview",\
safety_settings={\
HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\
},\
)\
```\
\
For an enumeration of the categories and thresholds available, see Google’s safety setting types.\
\
## ​ Context caching\
\
Context caching allows you to store and reuse content (e.g., PDFs, images) for faster processing. The `cached_content` parameter accepts a cache name created via the Google Generative AI API.\
\
Single file example\
\
This caches a single file and queries it.\
\
Copy\
\
```\
import time\
from google import genai\
from google.genai import types\
from langchain.messages import HumanMessage\
from langchain_google_genai import ChatGoogleGenerativeAI\
\
client = genai.Client()\
\
# Upload file\
file = client.files.upload(file="path/to/your/file")\
while file.state.name == "PROCESSING":\
time.sleep(2)\
file = client.files.get(name=file.name)\
\
# Create cache\
model = "gemini-3-pro-preview"\
cache = client.caches.create(\
model=model,\
config=types.CreateCachedContentConfig(\
display_name="Cached Content",\
system_instruction=(\
"You are an expert content analyzer, and your job is to answer "\
"the user's query based on the file you have access to."\
),\
contents=[file],\
ttl="300s",\
),\
)\
\
# Query with LangChain\
llm = ChatGoogleGenerativeAI(\
model=model,\
cached_content=cache.name,\
)\
message = HumanMessage(content="Summarize the main points of the content.")\
llm.invoke([message])\
```\
\
Multiple files example\
\
This caches two files using `Part` and queries them together.\
\
Copy\
\
```\
import time\
from google import genai\
from google.genai.types import CreateCachedContentConfig, Content, Part\
from langchain.messages import HumanMessage\
from langchain_google_genai import ChatGoogleGenerativeAI\
\
client = genai.Client()\
\
# Upload files\
file_1 = client.files.upload(file="./file1")\
while file_1.state.name == "PROCESSING":\
time.sleep(2)\
file_1 = client.files.get(name=file_1.name)\
\
file_2 = client.files.upload(file="./file2")\
while file_2.state.name == "PROCESSING":\
time.sleep(2)\
file_2 = client.files.get(name=file_2.name)\
\
# Create cache with multiple files\
contents = [\
Content(\
role="user",\
parts=[\
Part.from_uri(file_uri=file_1.uri, mime_type=file_1.mime_type),\
Part.from_uri(file_uri=file_2.uri, mime_type=file_2.mime_type),\
],\
)\
]\
model = "gemini-3-pro-preview"\
cache = client.caches.create(\
model=model,\
config=CreateCachedContentConfig(\
display_name="Cached Contents",\
system_instruction=(\
"You are an expert content analyzer, and your job is to answer "\
"the user's query based on the files you have access to."\
),\
contents=contents,\
ttl="300s",\
),\
)\
\
llm = ChatGoogleGenerativeAI(\
model=model,\
cached_content=cache.name,\
)\
message = HumanMessage(\
content="Provide a summary of the key information across both files."\
)\
llm.invoke([message])\
```\
\
See the Gemini API docs on context caching for more information.\
\
## ​ Response metadata\
\
Access response metadata from the model response.\
\
Copy\
\
```\
from langchain_google_genai import ChatGoogleGenerativeAI\
\
llm = ChatGoogleGenerativeAI(model="gemini-3-pro-preview")\
\
response = llm.invoke("Hello!")\
response.response_metadata\
```\
\
Copy\
\
```\
{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\
'finish_reason': 'STOP',\
'model_name': 'gemini-3-pro-preview',\
'safety_ratings': [],\
'model_provider': 'google_genai'}\
```\
\
* * *\
\
## ​ API reference\
\
For detailed documentation of all features and configuration options, head to the `ChatGoogleGenerativeAI` API reference.\
\
* * *\
\
Edit this page on GitHub or file an issue.\
\
Connect these docs to Claude, VSCode, and more via MCP for real-time answers.\
\
Was this page helpful?\
\
YesNo\
\
Ctrl+I\
\
Assistant\
\
Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/huggingface

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

ChatHuggingFace

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Integration details
- Model features
- Setup
- Credentials
- Installation
- Model features
- Setup
- Credentials
- Instantiation
- HuggingFaceEndpoint
- HuggingFacePipeline
- Instatiating with Quantization
- Invocation
- API reference

This will help you get started with `langchain_huggingface` chat models. For detailed documentation of all `ChatHuggingFace` features and configurations head to the API reference. For a list of models supported by Hugging Face check out this page.

## ​ Overview

### ​ Integration details

| Class | Package | Local | Serializable | JS support | Downloads | Version |
| --- | --- | --- | --- | --- | --- | --- |
| ChatHuggingFace | langchain-huggingface | ✅ | beta | ❌ | !PyPI - Downloads | !PyPI - Version |

### ​ Model features

| Tool calling | Structured output | JSON mode | Image input | Audio input | Video input | Token-level streaming | Native async | Token usage | Logprobs |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ❌ |

## ​ Setup

To access Hugging Face models you’ll need to create a Hugging Face account, get an API key, and install the `langchain-huggingface` integration package.

### ​ Credentials

Generate a Hugging Face Access Token and store it as an environment variable: `HUGGINGFACEHUB_API_TOKEN`.

Copy

import getpass
import os

if not os.getenv("HUGGINGFACEHUB_API_TOKEN"):
os.environ["HUGGINGFACEHUB_API_TOKEN"] = getpass.getpass("Enter your token: ")

### ​ Installation

| Class | Package | Local | Serializable | JS support | Downloads | Version |
| --- | --- | --- | --- | --- | --- | --- |
| ChatHuggingFace | langchain-huggingface | ✅ | ❌ | ❌ | !PyPI - Downloads | !PyPI - Version |

### ​ Model features

| Tool calling | Structured output | JSON mode | Image input | Audio input | Video input | Token-level streaming | Native async | Token usage | Logprobs |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ |

To access `langchain_huggingface` models you’ll need to create a `Hugging Face` account, get an API key, and install the `langchain-huggingface` integration package.

### ​ Credentials

You’ll need to have a Hugging Face Access Token saved as an environment variable: `HUGGINGFACEHUB_API_TOKEN`.

os.environ["HUGGINGFACEHUB_API_TOKEN"] = getpass.getpass(
"Enter your Hugging Face API key: "
)

pip install -qU langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate

## ​ Instantiation

You can instantiate a `ChatHuggingFace` model in two different ways, either from a `HuggingFaceEndpoint` or from a `HuggingFacePipeline`.

### ​ `HuggingFaceEndpoint`

from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint

llm = HuggingFaceEndpoint(
repo_id="deepseek-ai/DeepSeek-R1-0528",
task="text-generation",
max_new_tokens=512,
do_sample=False,
repetition_penalty=1.03,
provider="auto", # let Hugging Face choose the best provider for you
)

chat_model = ChatHuggingFace(llm=llm)

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /Users/isaachershenson/.cache/huggingface/token
Login successful

Now let’s take advantage of Inference Providers to run the model on specific third-party providers

llm = HuggingFaceEndpoint(
repo_id="deepseek-ai/DeepSeek-R1-0528",
task="text-generation",
provider="hyperbolic", # set your provider here
# provider="nebius",
# provider="together",
)

### ​ `HuggingFacePipeline`

from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline

llm = HuggingFacePipeline.from_model_id(
model_id="HuggingFaceH4/zephyr-7b-beta",
task="text-generation",
pipeline_kwargs=dict(
max_new_tokens=512,
do_sample=False,
repetition_penalty=1.03,
),
)

config.json: 0%| | 0.00/638 [00:00<?, ?B/s]

model.safetensors.index.json: 0%| | 0.00/23.9k [00:00<?, ?B/s]

Downloading shards: 0%| | 0/8 [00:00<?, ?it/s]

model-00001-of-00008.safetensors: 0%| | 0.00/1.89G [00:00<?, ?B/s]

model-00002-of-00008.safetensors: 0%| | 0.00/1.95G [00:00<?, ?B/s]

model-00003-of-00008.safetensors: 0%| | 0.00/1.98G [00:00<?, ?B/s]

model-00004-of-00008.safetensors: 0%| | 0.00/1.95G [00:00<?, ?B/s]

model-00005-of-00008.safetensors: 0%| | 0.00/1.98G [00:00<?, ?B/s]

model-00006-of-00008.safetensors: 0%| | 0.00/1.95G [00:00<?, ?B/s]

model-00007-of-00008.safetensors: 0%| | 0.00/1.98G [00:00<?, ?B/s]

model-00008-of-00008.safetensors: 0%| | 0.00/816M [00:00<?, ?B/s]

Loading checkpoint shards: 0%| | 0/8 [00:00<?, ?it/s]

generation_config.json: 0%| | 0.00/111 [00:00<?, ?B/s]

### ​ Instatiating with Quantization

To run a quantized version of your model, you can specify a `bitsandbytes` quantization config as follows:

from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_quant_type="nf4",
bnb_4bit_compute_dtype="float16",
bnb_4bit_use_double_quant=True,
)

and pass it to the `HuggingFacePipeline` as a part of its `model_kwargs`:

llm = HuggingFacePipeline.from_model_id(
model_id="HuggingFaceH4/zephyr-7b-beta",
task="text-generation",
pipeline_kwargs=dict(
max_new_tokens=512,
do_sample=False,
repetition_penalty=1.03,
return_full_text=False,
),
model_kwargs={"quantization_config": quantization_config},
)

## ​ Invocation

from langchain.messages import (
HumanMessage,
SystemMessage,
)

messages = [\
SystemMessage(content="You're a helpful assistant"),\
HumanMessage(\
content="What happens when an unstoppable force meets an immovable object?"\
),\
]

ai_msg = chat_model.invoke(messages)

print(ai_msg.content)

According to the popular phrase and hypothetical scenario, when an unstoppable force meets an immovable object, a paradoxical situation arises as both forces are seemingly contradictory. On one hand, an unstoppable force is an entity that cannot be stopped or prevented from moving forward, while on the other hand, an immovable object is something that cannot be moved or displaced from its position.

In this scenario, it is un

* * *

## ​ API reference

For detailed documentation of all `ChatHuggingFace` features and configurations head to the API reference: python.langchain.com/api\_reference/huggingface/chat\_models/langchain\_huggingface.chat\_models.huggingface.ChatHuggingFace.html

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/ollama

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

ChatOllama

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Integration details
- Model features
- Setup
- Installation
- Instantiation
- Invocation
- Tool calling
- Multi-modal
- Reasoning models and custom message roles
- API reference

Ollama allows you to run open-source Large Language Models (LLMs), such as `gpt-oss`, locally.Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including GPU usage.For a complete list of supported models and model variants, see the Ollama model library.

**API Reference**For detailed documentation of all features and configuration options, head to the `ChatOllama` API reference.

## ​ Overview

### ​ Integration details

| Class | Package | Local | Serializable | JS support | Downloads | Version |
| --- | --- | --- | --- | --- | --- | --- |
| `ChatOllama` | `langchain-ollama` | ✅ | ❌ | ✅ | !PyPI - Downloads | !PyPI - Version |

### ​ Model features

| Tool calling | Structured output | JSON mode | Image input | Audio input | Video input | Token-level streaming | Native async | Token usage | Logprobs |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | ❌ |

## ​ Setup

First, follow these instructions to set up and run a local Ollama instance:

- Downloadand install Ollama onto the available supported platforms (including Windows Subsystem for Linux aka WSL, macOS, and Linux)

- macOS users can install via Homebrew with `brew install ollama` and start with `brew services start ollama`

- View a list of available models via the model library
- e.g., `ollama pull gpt-oss:20b`

- Specify the exact version of the model of interest as such `ollama pull gpt-oss:20b` (View the various tags for the `Vicuna` model in this instance)
- To view all pulled models, use `ollama list`

- View the Ollama documentation for more commands. You can run `ollama help` in the terminal to see available commands.

To enable automated tracing of your model calls, set your LangSmith API key:

Copy

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")

### ​ Installation

The LangChain Ollama integration lives in the `langchain-ollama` package:

pip install -qU langchain-ollama

Make sure you’re using the latest Ollama version!

Update by running:

pip install -U ollama

## ​ Instantiation

Now we can instantiate our model object and generate chat completions:

from langchain_ollama import ChatOllama

llm = ChatOllama(
model="llama3.1",
temperature=0,
# other params...
)

## ​ Invocation

messages = [\
(\
"system",\
"You are a helpful assistant that translates English to French. Translate the user sentence.",\
),\
("human", "I love programming."),\
]
ai_msg = llm.invoke(messages)
ai_msg

AIMessage(content='The translation of "I love programming" in French is:\n\n"J\'adore le programmation."', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-06-25T18:43:00.483666Z', 'done': True, 'done_reason': 'stop', 'total_duration': 619971208, 'load_duration': 27793125, 'prompt_eval_count': 35, 'prompt_eval_duration': 36354583, 'eval_count': 22, 'eval_duration': 555182667, 'model_name': 'llama3.1'}, id='run--348bb5ef-9dd9-4271-bc7e-a9ddb54c28c1-0', usage_metadata={'input_tokens': 35, 'output_tokens': 22, 'total_tokens': 57})

print(ai_msg.content)

The translation of "I love programming" in French is:

"J'adore le programmation."

## ​ Tool calling

Ollama tool calling uses the OpenAI compatible web server specification, and can be used with the default `BaseChatModel.bind_tools()` methods as described here.Make sure to select an ollama model that supports tool calling.We can use tool calling with an LLM that has been fine-tuned for tool use such as `gpt-oss`:

ollama pull gpt-oss:20b

Details on creating custom tools are available in this guide. Below, we demonstrate how to create a tool using the `@tool` decorator on a normal python function.

from typing import List

from langchain.messages import AIMessage
from langchain.tools import tool
from langchain_ollama import ChatOllama

@tool

"""Validate user using historical addresses.

Args:
user_id (int): the user ID.
addresses (List[str]): Previous addresses as a list of strings.
"""
return True

llm = ChatOllama(
model="gpt-oss:20b",
validate_model_on_init=True,
temperature=0,
).bind_tools([validate_user])

result = llm.invoke(
"Could you validate user 123? They previously lived at "
"123 Fake St in Boston MA and 234 Pretend Boulevard in "
"Houston TX."
)

if isinstance(result, AIMessage) and result.tool_calls:
print(result.tool_calls)

[{'name': 'validate_user', 'args': {'addresses': ['123 Fake St, Boston, MA', '234 Pretend Boulevard, Houston, TX'], 'user_id': '123'}, 'id': 'aef33a32-a34b-4b37-b054-e0d85584772f', 'type': 'tool_call'}]

## ​ Multi-modal

Ollama has limited support for multi-modal LLMs, such as gemma3Be sure to update Ollama so that you have the most recent version to support multi-modal.

pip install pillow

import base64
from io import BytesIO

from IPython.display import HTML, display
from PIL import Image

def convert_to_base64(pil_image):
"""
Convert PIL images to Base64 encoded strings

:param pil_image: PIL image
:return: Re-sized Base64 string
"""

buffered = BytesIO()
pil_image.save(buffered, format="JPEG") # You can change the format if needed
img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
return img_str

def plt_img_base64(img_base64):
"""
Disply base64 encoded string as image

:param img_base64: Base64 string
"""
# Create an HTML img tag with the base64 string as the source

# Display the image by rendering the HTML
display(HTML(image_html))

file_path = "../../../static/img/ollama_example_img.jpg"
pil_image = Image.open(file_path)

image_b64 = convert_to_base64(pil_image)
plt_img_base64(image_b64)

from langchain.messages import HumanMessage
from langchain_ollama import ChatOllama

llm = ChatOllama(model="bakllava", temperature=0)

def prompt_func(data):
text = data["text"]
image = data["image"]

image_part = {
"type": "image_url",
"image_url": f"data:image/jpeg;base64,{image}",
}

content_parts = []

text_part = {"type": "text", "text": text}

content_parts.append(image_part)
content_parts.append(text_part)

return [HumanMessage(content=content_parts)]

from langchain_core.output_parsers import StrOutputParser

chain = prompt_func | llm | StrOutputParser()

query_chain = chain.invoke(
{"text": "What is the Dollar-based gross retention rate?", "image": image_b64}
)

print(query_chain)

90%

## ​ Reasoning models and custom message roles

Some models, such as IBM’s Granite 3.2, support custom message roles to enable thinking processes.To access Granite 3.2’s thinking features, pass a message with a `"control"` role with content set to `"thinking"`. Because `"control"` is a non-standard message role, we can use a ChatMessage object to implement it:

from langchain.messages import HumanMessage
from langchain_core.messages import ChatMessage
from langchain_ollama import ChatOllama

llm = ChatOllama(model="granite3.2:8b")

messages = [\
ChatMessage(role="control", content="thinking"),\
HumanMessage("What is 3^3?"),\
]

response = llm.invoke(messages)
print(response.content)

Here is my thought process:
The user is asking for the value of 3 raised to the power of 3, which is a basic exponentiation operation.

Here is my response:

3^3 (read as "3 to the power of 3") equals 27.

This calculation is performed by multiplying 3 by itself three times: 3*3*3 = 27.

Note that the model exposes its thought process in addition to its final response.

* * *

## ​ API reference

For detailed documentation of all `ChatOllama` features and configurations head to the API reference.

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/agents).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview Build a voice agent with LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration All integration providers Contributing integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/azure_chat_openai)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

AzureOpenAIEmbeddings ChatOpenAI AzureChatOpenAI

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration Google All integration providers

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/bedrock)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration All integration providers BedrockChat

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/huggingface)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Build a SQL agent Models Build a multi-source knowledge base with routing

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/agents),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview Build a voice agent with LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/openai),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

ChatOpenAI Build customer support with handoffs Build a SQL agent

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/anthropic)).

Skip to main content).#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration All integration providers Contributing integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/anthropic))

Skip to main content)#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration All integration providers Contributing integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/messages))

Skip to main content)#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/ollama)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Ollama Implement a LangChain integration All integration providers

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration Google All integration providers

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/messages),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/xai

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

ChatXAI

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Overview
- Integration details
- Model features
- Setup
- Credentials
- Installation
- Instantiation
- Invocation
- Tool calling
- ChatXAI.bind\_tools()
- Live Search
- API reference

This page makes reference to Grok models provided by xAI \- not to be confused with Groq, a separate AI hardware and software company. See the Groq provider page.

xAI offers an API to interact with Grok models.

**API Reference**For detailed documentation of all features and configuration options, head to the `ChatXAI` API reference.

## ​ Overview

### ​ Integration details

| Class | Package | Local | Serializable | JS support | Downloads | Version |
| --- | --- | --- | --- | --- | --- | --- |
| `ChatXAI` | `langchain-xai` | ❌ | beta | ✅ | !PyPI - Downloads | !PyPI - Version |

### ​ Model features

| Tool calling | Structured output | JSON mode | Image input | Audio input | Video input | Token-level streaming | Native async | Token usage | Logprobs |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ❌ | ✅ | ✅ |

## ​ Setup

To access xAI models, you’ll need to create an xAI account, get an API key, and install the `langchain-xai` integration package.

### ​ Credentials

Head to this page to sign up for xAI and generate an API key. Once you’ve done this, set the `XAI_API_KEY` environment variable:

Copy

import getpass
import os

if "XAI_API_KEY" not in os.environ:
os.environ["XAI_API_KEY"] = getpass.getpass("Enter your xAI API key: ")

To enable automated tracing of your model calls, set your LangSmith API key:

os.environ["LANGSMITH_API_KEY"] = getpass.getpass("Enter your LangSmith API key: ")
os.environ["LANGSMITH_TRACING"] = "true"

### ​ Installation

The LangChain xAI integration lives in the `langchain-xai` package:

pip install -qU langchain-xai

## ​ Instantiation

Now we can instantiate our model object and generate chat completions:

from langchain_xai import ChatXAI

llm = ChatXAI(
model="grok-beta",
temperature=0,
max_tokens=None,
timeout=None,
max_retries=2,
# other params...
)

## ​ Invocation

messages = [\
(\
"system",\
"You are a helpful assistant that translates English to French. Translate the user sentence.",\
),\
("human", "I love programming."),\
]
ai_msg = llm.invoke(messages)
ai_msg

AIMessage(content="J'adore programmer.", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 30, 'total_tokens': 36, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'grok-beta', 'system_fingerprint': 'fp_14b89b2dfc', 'finish_reason': 'stop', 'logprobs': None}, id='run-adffb7a3-e48a-4f52-b694-340d85abe5c3-0', usage_metadata={'input_tokens': 30, 'output_tokens': 6, 'total_tokens': 36, 'input_token_details': {}, 'output_token_details': {}})

print(ai_msg.content)

J'adore programmer.

## ​ Tool calling

ChatXAI has a tool calling (we use “tool calling” and “function calling” interchangeably here) API that lets you describe tools and their arguments, and have the model return a JSON object with a tool to invoke and the inputs to that tool. Tool-calling is extremely useful for building tool-using chains and agents, and for getting structured outputs from models more generally.

### ​ ChatXAI.bind\_tools()

With `ChatXAI.bind_tools`, we can easily pass in Pydantic classes, dict schemas, LangChain tools, or even functions as tools to the model. Under the hood, these are converted to an OpenAI tool schema, which looks like:

{
"name": "...",
"description": "...",
"parameters": {...} # JSONSchema
}

and passed in every model invocation.

from pydantic import BaseModel, Field

class GetWeather(BaseModel):
"""Get the current weather in a given location"""

location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

llm_with_tools = llm.bind_tools([GetWeather])

ai_msg = llm_with_tools.invoke(
"what is the weather like in San Francisco",
)
ai_msg

AIMessage(content='I am retrieving the current weather for San Francisco.', additional_kwargs={'tool_calls': [{'id': '0', 'function': {'arguments': '{"location":"San Francisco, CA"}', 'name': 'GetWeather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 151, 'total_tokens': 162, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'grok-beta', 'system_fingerprint': 'fp_14b89b2dfc', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-73707da7-afec-4a52-bee1-a176b0ab8585-0', tool_calls=[{'name': 'GetWeather', 'args': {'location': 'San Francisco, CA'}, 'id': '0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 11, 'total_tokens': 162, 'input_token_details': {}, 'output_token_details': {}})

## ​ Live Search

xAI supports a Live Search feature that enables Grok to ground its answers using results from web searches:

llm = ChatXAI(
model="grok-3-latest",
search_parameters={
"mode": "auto",
# Example optional parameters below:
"max_search_results": 3,
"from_date": "2025-05-26",
"to_date": "2025-05-27",
},
)

llm.invoke("Provide me a digest of world news in the last 24 hours.")

See xAI docs for the full set of web search options.

* * *

## ​ API reference

For detailed documentation of all `ChatXAI` features and configurations, head to the API reference.

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/openai),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers AzureOpenAIEmbeddings Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/anthropic),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/providers/xai)).

Skip to main content).#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

All integration providers Implement a LangChain integration Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/xai)).

Skip to main content).#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration All integration providers Contributing integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/anthropic),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration All integration providers Contributing integrations

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/chat/google_generative_ai))

Skip to main content)#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Implement a LangChain integration Google All integration providers

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/agents)),

Skip to main content),#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) LangChain overview Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/human-in-the-loop),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Build a voice agent with LangChain LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/middleware/anthropic

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Anthropic middleware

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Middleware vs tools
- When to use which
- Feature comparison
- Prompt caching
- Bash tool
- Text editor
- Memory
- File search

Middleware specifically designed for Anthropic’s Claude models. Learn more about middleware.

| Middleware | Description |
| --- | --- |
| Prompt caching | Reduce costs by caching repetitive prompt prefixes |
| Bash tool | Execute Claude’s native bash tool with local command execution |
| Text editor | Provide Claude’s text editor tool for file editing |
| Memory | Provide Claude’s memory tool for persistent agent memory |
| File search | Search tools for state-based file systems |

## ​ Middleware vs tools

`langchain-anthropic` provides two ways to use Claude’s native tools:

- **Middleware** (this page): Production-ready implementations with built-in execution, state management, and security policies
- **Tools** (via `bind_tools`): Low-level building blocks where you provide your own execution logic

### ​ When to use which

| Use case | Recommended | Why |
| --- | --- | --- |
| Production agents with bash | Middleware | Persistent sessions, Docker isolation, output redaction |
| State-based file editing | Middleware | Built-in LangGraph state persistence |
| Filesystem file editing | Middleware | Writes to disk with path validation |
| Custom execution logic | Tools | Full control over execution |
| Quick prototype | Tools | Simpler, bring your own callback |
| Non-agent use with `bind_tools` | Tools | Middleware requires `create_agent` |

### ​ Feature comparison

| Feature | Middleware | Tools |
| --- | --- | --- |
| Works with `create_agent` | ✅ | ✅ |
| Works with `bind_tools` | ❌ | ✅ |
| Built-in state management | ✅ | ❌ |
| Custom execute callback | ❌ | ✅ |

Example: Middleware vs tools comparison

**Using middleware** (turnkey solution):

Copy

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import ClaudeBashToolMiddleware
from langchain.agents import create_agent
from langchain.agents.middleware import DockerExecutionPolicy

# Production-ready with Docker isolation, session management, etc.
agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
middleware=[\
ClaudeBashToolMiddleware(\
workspace_root="/workspace",\
execution_policy=DockerExecutionPolicy(image="python:3.11"),\
startup_commands=["pip install pandas"],\
),\
],
)

**Using tools** (bring your own execution):

import subprocess

from anthropic.types.beta import BetaToolBash20250124Param
from langchain_anthropic import ChatAnthropic
from langchain.agents import create_agent
from langchain.tools import tool

tool_spec = BetaToolBash20250124Param(
name="bash",
type="bash_20250124",
strict=True,
)

@tool(extras={"provider_tool_definition": tool_spec})
def bash(*, command: str, restart: bool = False, **kw):
"""Execute a bash command."""
if restart:
return "Bash session restarted"
try:
result = subprocess.run(
command,
shell=True,
capture_output=True,
text=True,
timeout=30,
)
return result.stdout + result.stderr
except Exception as e:
return f"Error: {e}"

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[bash],
)

result = agent.invoke(
{"messages": [{"role": "user", "content": "List files in this directory"}]}
)
print(result["messages"][-1].content)

* * *

## ​ Prompt caching

Reduce costs and latency by caching static or repetitive prompt content (like system prompts, tool definitions, and conversation history) on Anthropic’s servers. This middleware implements a **conversational caching strategy** that places cache breakpoints after the most recent message, allowing the entire conversation history (including the latest user message) to be cached and reused in subsequent API calls.Prompt caching is useful for the following:

- Applications with long, static system prompts that don’t change between requests
- Agents with many tool definitions that remain constant across invocations
- Conversations where early message history is reused across multiple turns
- High-volume deployments where reducing API costs and latency is critical

Learn more about Anthropic prompt caching strategies and limitations.

**API reference:** `AnthropicPromptCachingMiddleware`

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import AnthropicPromptCachingMiddleware
from langchain.agents import create_agent

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),

middleware=[AnthropicPromptCachingMiddleware(ttl="5m")],
)

Configuration options

​

type

string

default:"ephemeral"

Cache type. Only `'ephemeral'` is currently supported.

ttl

default:"5m"

Time to live for cached content. Valid values: `'5m'` or `'1h'`

min\_messages\_to\_cache

number

default:"0"

Minimum number of messages before caching starts

unsupported\_model\_behavior

default:"warn"

Behavior when using non-Anthropic models. Options: `'ignore'`, `'warn'`, or `'raise'`

Full example

The middleware caches content up to and including the latest message in each request. On subsequent requests within the TTL window (5 minutes or 1 hour), previously seen content is retrieved from cache rather than reprocessed, significantly reducing costs and latency.**How it works:**

1. First request: System prompt, tools, and the user message _“Hi, my name is Bob”_ are sent to the API and cached
2. Second request: The cached content (system prompt, tools, and first message) is retrieved from cache. Only the new message _“What’s my name?”_ needs to be processed, plus the model’s response from the first request
3. This pattern continues for each turn, with each request reusing the cached conversation history

Prompt caching reduces API costs by caching tokens, but does **not** provide conversation memory. To persist conversation history across invocations, use a checkpointer like `MemorySaver`.

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import AnthropicPromptCachingMiddleware
from langchain.agents import create_agent
from langchain.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver

LONG_PROMPT = """
Please be a helpful assistant.

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
system_prompt=LONG_PROMPT,
middleware=[AnthropicPromptCachingMiddleware(ttl="5m")],
checkpointer=MemorySaver(), # Persists conversation history
)

# Use a thread_id to maintain conversation state
config: RunnableConfig = {"configurable": {"thread_id": "user-123"}}

# First invocation: Creates cache with system prompt, tools, and "Hi, my name is Bob"
agent.invoke({"messages": [HumanMessage("Hi, my name is Bob")]}, config=config)

# Second invocation: Reuses cached system prompt, tools, and previous messages
# The checkpointer maintains conversation history, so the agent remembers "Bob"
result = agent.invoke({"messages": [HumanMessage("What's my name?")]}, config=config)
print(result["messages"][-1].content)

Your name is Bob! You told me that when you introduced yourself at the start of our conversation.

## ​ Bash tool

Execute Claude’s native `bash_20250124` tool with local command execution.The bash tool middleware is useful for the following:

- Using Claude’s built-in bash tool with local execution
- Leveraging Claude’s optimized bash tool interface
- Agents that need persistent shell sessions with Anthropic models

This middleware wraps `ShellToolMiddleware` and exposes it as Claude’s native bash tool.

**API reference:** `ClaudeBashToolMiddleware`

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import ClaudeBashToolMiddleware
from langchain.agents import create_agent

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[\
ClaudeBashToolMiddleware(\
workspace_root="/workspace",\
),\
],
)

`ClaudeBashToolMiddleware` accepts all parameters from `ShellToolMiddleware`, including:

workspace\_root

str \| Path \| None

Base directory for the shell session

startup\_commands

tuple\[str, ...\] \| list\[str\] \| str \| None

Commands to run when the session starts

execution\_policy

BaseExecutionPolicy \| None

Execution policy (`HostExecutionPolicy`, `DockerExecutionPolicy`, or `CodexSandboxExecutionPolicy`)

redaction\_rules

tuple\[RedactionRule, ...\] \| list\[RedactionRule\] \| None

Rules for sanitizing command output

See Shell tool for full configuration details.

import tempfile

# Create a temporary workspace directory for this demo.
# In production, use a persistent directory path.
workspace = tempfile.mkdtemp(prefix="agent-workspace-")

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[\
ClaudeBashToolMiddleware(\
workspace_root=workspace,\
startup_commands=["echo 'Session initialized'"],\
execution_policy=DockerExecutionPolicy(\
image="python:3.11-slim",\
),\
),\
],
)

# Claude can now use its native bash tool
result = agent.invoke(
{"messages": [{"role": "user", "content": "What version of Python is installed?"}]}
)
print(result["messages"][-1].content)

Python 3.11.14 is installed.

## ​ Text editor

Provide Claude’s text editor tool (`text_editor_20250728`) for file creation and editing.The text editor middleware is useful for the following:

- File-based agent workflows
- Code editing and refactoring tasks
- Multi-file project work
- Agents that need persistent file storage

Available in two variants: **State-based** (files in LangGraph state) and **Filesystem-based** (files on disk).

**API references:**

- `StateClaudeTextEditorMiddleware`
- `FilesystemClaudeTextEditorMiddleware`

State-based text editor

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import StateClaudeTextEditorMiddleware
from langchain.agents import create_agent

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[StateClaudeTextEditorMiddleware()],
)

Filesystem-based text editor

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import FilesystemClaudeTextEditorMiddleware
from langchain.agents import create_agent

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[\
FilesystemClaudeTextEditorMiddleware(\
root_path="/workspace",\
),\
],
)

Claude’s text editor tool supports the following commands:

- `view` \- View file contents or list directory
- `create` \- Create a new file
- `str_replace` \- Replace string in file
- `insert` \- Insert text at line number
- `delete` \- Delete a file
- `rename` \- Rename/move a file

**`StateClaudeTextEditorMiddleware` (state-based)**

allowed\_path\_prefixes

Sequence\[str\] \| None

Optional list of allowed path prefixes. If specified, only paths starting with these prefixes are allowed.

**`FilesystemClaudeTextEditorMiddleware` (filesystem-based)**

root\_path

str

required

Root directory for file operations

allowed\_prefixes

list\[str\] \| None

Optional list of allowed virtual path prefixes (default: `["/"]`)

max\_file\_size\_mb

int

default:"10"

Maximum file size in MB

Full example: State-based text editor

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import StateClaudeTextEditorMiddleware
from langchain.agents import create_agent
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[\
StateClaudeTextEditorMiddleware(\
allowed_path_prefixes=["/project"],\
),\
],
checkpointer=MemorySaver(),
)

# Use a thread_id to persist state across invocations
config: RunnableConfig = {"configurable": {"thread_id": "my-session"}}

# Claude can now create and edit files (stored in LangGraph state)
result = agent.invoke(
{"messages": [{"role": "user", "content": "Create a file at /project/hello.py with a simple hello world program"}]},
config=config,
)
print(result["messages"][-1].content)

I've created a simple "Hello, World!" program at `/project/hello.py`. The program uses Python's `print()` function to display "Hello, World!" to the console when executed.

Full example: Filesystem-based text editor

workspace = tempfile.mkdtemp(prefix="editor-workspace-")

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[\
FilesystemClaudeTextEditorMiddleware(\
root_path=workspace,\
allowed_prefixes=["/src"],\
max_file_size_mb=10,\
),\
],
)

# Claude can now create and edit files (stored on disk)
result = agent.invoke(
{"messages": [{"role": "user", "content": "Create a file at /src/hello.py with a simple hello world program"}]}
)
print(result["messages"][-1].content)

I've created a simple "Hello, World!" program at `/src/hello.py`. The program uses Python's `print()` function to display "Hello, World!" to the console when executed.

## ​ Memory

Provide Claude’s memory tool (`memory_20250818`) for persistent agent memory across conversation turns.The memory middleware is useful for the following:

- Long-running agent conversations
- Maintaining context across interruptions
- Task progress tracking
- Persistent agent state management

Claude’s memory tool uses a `/memories` directory and automatically injects a system prompt encouraging the agent to check and update memory.

**API reference:** `StateClaudeMemoryMiddleware`, `FilesystemClaudeMemoryMiddleware`

State-based memory

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import StateClaudeMemoryMiddleware
from langchain.agents import create_agent

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[StateClaudeMemoryMiddleware()],
)

Filesystem-based memory

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import FilesystemClaudeMemoryMiddleware
from langchain.agents import create_agent

agent_fs = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[\
FilesystemClaudeMemoryMiddleware(\
root_path="/workspace",\
),\
],
)

**`StateClaudeMemoryMiddleware` (state-based)**

Optional list of allowed path prefixes. Defaults to `["/memories"]`.

system\_prompt

System prompt to inject. Defaults to Anthropic’s recommended memory prompt that encourages the agent to check and update memory.

**`FilesystemClaudeMemoryMiddleware` (filesystem-based)**

Optional list of allowed virtual path prefixes. Defaults to `["/memories"]`.

System prompt to inject

Full example: State-based memory

The agent will automatically:

1. Check `/memories` directory at start
2. Record progress and thoughts during execution
3. Update memory files as work progresses

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import StateClaudeMemoryMiddleware
from langchain.agents import create_agent
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[StateClaudeMemoryMiddleware()],
checkpointer=MemorySaver(),
)

# Claude can now use memory to track progress (stored in LangGraph state)
result = agent.invoke(
{"messages": [{"role": "user", "content": "Remember that my favorite color is blue, then confirm what you stored."}]},
config=config,
)
print(result["messages"][-1].content)

Perfect! I've stored your favorite color as **blue** in my memory system. The information is saved in my user preferences file where I can access it in future conversations.

Full example: Filesystem-based memory

workspace = tempfile.mkdtemp(prefix="memory-workspace-")

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[\
FilesystemClaudeMemoryMiddleware(\
root_path=workspace,\
),\
],
)

# Claude can now use memory to track progress (stored on disk)
result = agent.invoke(
{"messages": [{"role": "user", "content": "Remember that my favorite color is blue, then confirm what you stored."}]}
)
print(result["messages"][-1].content)

## ​ File search

Provide Glob and Grep search tools for files stored in LangGraph state. File search middleware is useful for the following:

- Searching through state-based virtual file systems
- Works with text editor and memory tools
- Finding files by patterns
- Content search with regex

**API reference:** `StateFileSearchMiddleware`

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import (
StateClaudeTextEditorMiddleware,
StateFileSearchMiddleware,
)
from langchain.agents import create_agent

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[\
StateClaudeTextEditorMiddleware(),\
StateFileSearchMiddleware(), # Search text editor files\
],
)

state\_key

default:"text\_editor\_files"

State key containing files to search. Use `"text_editor_files"` for text editor files or `"memory_files"` for memory files.

Full example: Search text editor files

The middleware adds Glob and Grep search tools that work with state-based files.

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import (
StateClaudeTextEditorMiddleware,
StateFileSearchMiddleware,
)
from langchain.agents import create_agent
from langchain.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[\
StateClaudeTextEditorMiddleware(),\
StateFileSearchMiddleware(state_key="text_editor_files"),\
],
checkpointer=MemorySaver(),
)

# First invocation: Create some files using the text editor tool
result = agent.invoke(
{"messages": [HumanMessage("Create a Python project with main.py, utils/helpers.py, and tests/test_main.py")]},
config=config,
)

# The agent creates files, which are stored in state
print("Files created:", list(result["text_editor_files"].keys()))

# Second invocation: Search the files we just created
# State is automatically persisted via the checkpointer
result = agent.invoke(
{"messages": [HumanMessage("Find all Python files in the project")]},
config=config,
)
print(result["messages"][-1].content)

Files created: ['/project/main.py', '/project/utils/helpers.py', '/project/utils/__init__.py', '/project/tests/test_main.py', '/project/tests/__init__.py', '/project/README.md']

I found 5 Python files in the project:

1. `/project/main.py` - Main application file
2. `/project/utils/__init__.py` - Utils package initialization
3. `/project/utils/helpers.py` - Helper utilities
4. `/project/tests/__init__.py` - Tests package initialization
5. `/project/tests/test_main.py` - Main test file

Would you like me to view the contents of any of these files?

Full example: Search memory files

from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import (
StateClaudeMemoryMiddleware,
StateFileSearchMiddleware,
)
from langchain.agents import create_agent
from langchain.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver

agent = create_agent(
model=ChatAnthropic(model="claude-sonnet-4-5-20250929"),
tools=[],
middleware=[\
StateClaudeMemoryMiddleware(),\
StateFileSearchMiddleware(state_key="memory_files"),\
],
checkpointer=MemorySaver(),
)

# First invocation: Record some memories
result = agent.invoke(
{"messages": [HumanMessage("Remember that the project deadline is March 15th and code review deadline is March 10th")]},
config=config,
)

# The agent creates memory files, which are stored in state
print("Memory files created:", list(result["memory_files"].keys()))

# Second invocation: Search the memories we just recorded
result = agent.invoke(
{"messages": [HumanMessage("Search my memories for project deadlines")]},
config=config,
)
print(result["messages"][-1].content)

Memory files created: ['/memories/project_info.md']

I found your project deadlines in my memory! Here's what I have recorded:

## Important Deadlines
- **Code Review Deadline:** March 10th
- **Project Deadline:** March 15th

## Notes
- Code review must be completed 5 days before final project deadline
- Need to ensure all code is ready for review by March 10th

Is there anything specific about these deadlines you'd like to know or update?

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/middleware/openai

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

OpenAI middleware

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

On this page

- Content moderation

Middleware specifically designed for OpenAI models. Learn more about middleware.

| Middleware | Description |
| --- | --- |
| Content moderation | Moderate agent traffic using OpenAI’s moderation endpoint |

## ​ Content moderation

Moderate agent traffic (user input, model output, and tool results) using OpenAI’s moderation endpoint to detect and handle unsafe content. Content moderation is useful for the following:

- Applications requiring content safety and compliance
- Filtering harmful, hateful, or inappropriate content
- Customer-facing agents that need safety guardrails
- Meeting platform moderation requirements

Learn more about OpenAI’s moderation models and categories.

**API reference:** `OpenAIModerationMiddleware`

Copy

from langchain_openai import ChatOpenAI
from langchain_openai.middleware import OpenAIModerationMiddleware
from langchain.agents import create_agent

agent = create_agent(
model=ChatOpenAI(model="gpt-4o"),
tools=[search_tool, database_tool],
middleware=[\
OpenAIModerationMiddleware(\
model="omni-moderation-latest",\
check_input=True,\
check_output=True,\
exit_behavior="end",\
),\
],
)

Configuration options

​

model

ModerationModel

default:"omni-moderation-latest"

OpenAI moderation model to use. Options: `'omni-moderation-latest'`, `'omni-moderation-2024-09-26'`, `'text-moderation-latest'`, `'text-moderation-stable'`

check\_input

bool

default:"True"

Whether to check user input messages before the model is called

check\_output

Whether to check model output messages after the model is called

check\_tool\_results

default:"False"

Whether to check tool result messages before the model is called

exit\_behavior

string

default:"end"

How to handle violations when content is flagged. Options:

- `'end'` \- End agent execution immediately with a violation message
- `'error'` \- Raise `OpenAIModerationError` exception
- `'replace'` \- Replace the flagged content with the violation message and continue

violation\_message

str \| None

Custom template for violation messages. Supports template variables:

- `{categories}` \- Comma-separated list of flagged categories
- `{category_scores}` \- JSON string of category scores
- `{original_content}` \- The original flagged content

Default: `"I'm sorry, but I can't comply with that request. It was flagged for {categories}."`

client

OpenAI \| None

Optional pre-configured OpenAI client to reuse. If not provided, a new client will be created.

async\_client

AsyncOpenAI \| None

Optional pre-configured AsyncOpenAI client to reuse. If not provided, a new async client will be created.

Full example

The middleware integrates OpenAI’s moderation endpoint to check content at different stages:**Moderation stages:**

- `check_input` \- User messages before model call
- `check_output` \- AI messages after model call
- `check_tool_results` \- Tool outputs before model call

**Exit behaviors:**

- `'end'` (default) - Stop execution with violation message
- `'error'` \- Raise exception for application handling
- `'replace'` \- Replace flagged content and continue

# Basic moderation
agent = create_agent(
model=ChatOpenAI(model="gpt-4o"),
tools=[search_tool, customer_data_tool],
middleware=[\
OpenAIModerationMiddleware(\
model="omni-moderation-latest",\
check_input=True,\
check_output=True,\
),\
],
)

# Strict moderation with custom message
agent_strict = create_agent(
model=ChatOpenAI(model="gpt-4o"),
tools=[search_tool, customer_data_tool],
middleware=[\
OpenAIModerationMiddleware(\
model="omni-moderation-latest",\
check_input=True,\
check_output=True,\
check_tool_results=True,\
exit_behavior="error",\
violation_message=(\
"Content policy violation detected: {categories}. "\
"Please rephrase your request."\
),\
),\
],
)

# Moderation with replacement behavior
agent_replace = create_agent(
model=ChatOpenAI(model="gpt-4o"),
tools=[search_tool],
middleware=[\
OpenAIModerationMiddleware(\
check_input=True,\
exit_behavior="replace",\
violation_message="[Content removed due to safety policies]",\
),\
],
)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/human-in-the-loop).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/middleware/anthropic)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

Provider-specific middleware How to add custom middleware All integration providers

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/integrations/middleware/openai)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- LangChain integrations

- All providers

##### Popular Providers

- OpenAI
- Anthropic (Claude)
- Google
- AWS (Amazon)
- Hugging Face
- Microsoft
- Ollama
- Groq

##### Integrations by component

- Chat models
- Tools and toolkits
- Middleware
- Retrievers
- Text splitters
- Embedding models
- Vector stores
- Document loaders
- Key-value stores

404

# Page not found

We couldn’t find the page you were looking for.

AzureOpenAIEmbeddings Provider-specific middleware Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langgraph/persistence),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Local server
- Changelog
- Thinking in LangGraph
- Workflows + agents

##### Capabilities

- Persistence
- Durable execution
- Streaming
- Interrupts
- Time travel
- Memory
- Subgraphs

##### Production

- Application structure
- Test
- LangSmith Studio
- Agent Chat UI
- LangSmith Deployment
- LangSmith Observability

##### LangGraph APIs

- Graph API

- Functional API

- Runtime

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph Python SDK LangSmith reference Install LangGraph

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/platform-setup).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/generative-ui-react

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Tutorials

How to implement generative user interfaces with LangGraph

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

- Deploy other frameworks (e.g., Strands, CrewAI)
- Integrate LangGraph into your React application
- Implement generative user interfaces with LangGraph

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

On this page

- Tutorial
- 1\. Define and configure UI components
- 2\. Send the UI components in your graph
- 3\. Handle UI elements in your React application
- How-to guides
- Provide custom components on the client side
- Show loading UI when components are loading
- Customise the namespace of UI components.
- Access and interact with the thread state from the UI component
- Pass additional context to the client components
- Streaming UI messages from the server
- Remove UI messages from state
- Learn more

**Prerequisites**

- LangSmith
- Agent Server
- `useStream()` React Hook

Generative user interfaces (Generative UI) allows agents to go beyond text and generate rich user interfaces. This enables creating more interactive and context-aware applications where the UI adapts based on the conversation flow and AI responses.!Agent Chat showing a prompt about booking/lodging and a generated set of hotel listing cards (images, titles, prices, locations) rendered inline as UI components.LangSmith supports colocating your React components with your graph code. This allows you to focus on building specific UI components for your graph while easily plugging into existing chat interfaces such as Agent Chat and loading the code only when actually needed.

## ​ Tutorial

### ​ 1\. Define and configure UI components

First, create your first UI component. For each component you need to provide an unique identifier that will be used to reference the component in your graph code.

src/agent/ui.tsx

Copy

};

export default {
weather: WeatherComponent,
};

Next, define your UI components in your `langgraph.json` configuration:

{
"node_version": "20",
"graphs": {
"agent": "./src/agent/index.ts:graph"
},
"ui": {
"agent": "./src/agent/ui.tsx"
}
}

The `ui` section points to the UI components that will be used by graphs. By default, we recommend using the same key as the graph name, but you can split out the components however you like, see Customise the namespace of UI components for more details.LangSmith will automatically bundle your UI components code and styles and serve them as external assets that can be loaded by the `LoadExternalComponent` component. Some dependencies such as `react` and `react-dom` will be automatically excluded from the bundle.CSS and Tailwind 4.x is also supported out of the box, so you can freely use Tailwind classes as well as `shadcn/ui` in your UI components.

- src/agent/ui.tsx

- src/agent/styles.css

import "./styles.css";

@import "tailwindcss";

### ​ 2\. Send the UI components in your graph

- Python

- JS

src/agent.py

import uuid
from typing import Annotated, Sequence, TypedDict

from langchain.messages import AIMessage
from langchain_core.messages import BaseMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.graph.ui import AnyUIMessage, ui_message_reducer, push_ui_message

class AgentState(TypedDict): # noqa: D101
messages: Annotated[Sequence[BaseMessage], add_messages]
ui: Annotated[Sequence[AnyUIMessage], ui_message_reducer]

async def weather(state: AgentState):
class WeatherOutput(TypedDict):
city: str

weather: WeatherOutput = (
await ChatOpenAI(model="gpt-4o-mini")
.with_structured_output(WeatherOutput)
.with_config({"tags": ["nostream"]})
.ainvoke(state["messages"])
)

message = AIMessage(
id=str(uuid.uuid4()),
content=f"Here's the weather for {weather['city']}",
)

# Emit UI elements associated with the message
push_ui_message("weather", weather, message=message)
return {"messages": [message]}

workflow = StateGraph(AgentState)
workflow.add_node(weather)
workflow.add_edge("__start__", "weather")
graph = workflow.compile()

Use the `typedUi` utility to emit UI elements from your agent nodes:

src/agent/index.ts

import {
typedUi,
uiMessageReducer,
} from "@langchain/langgraph-sdk/react-ui/server";

import { ChatOpenAI } from "@langchain/openai";
import { v4 as uuidv4 } from "uuid";
import { z } from "zod";

import type ComponentMap from "./ui.js";

import {
Annotation,
MessagesAnnotation,
StateGraph,
type LangGraphRunnableConfig,
} from "@langchain/langgraph";

const AgentState = Annotation.Root({
...MessagesAnnotation.spec,

});

export const graph = new StateGraph(AgentState)

// Provide the type of the component map to ensure
// type safety of `ui.push()` calls as well as
// pushing the messages to the `ui` and sending a custom event as well.

const weather = await new ChatOpenAI({ model: "gpt-4o-mini" })
.withStructuredOutput(z.object({ city: z.string() }))
.withConfig({ tags: ["nostream"] })
.invoke(state.messages);

const response = {
id: uuidv4(),
type: "ai",
content: `Here's the weather for ${weather.city}`,
};

// Emit UI elements associated with the AI message
ui.push({ name: "weather", props: weather }, { message: response });

return { messages: [response] };
})
.addEdge("__start__", "weather")
.compile();

### ​ 3\. Handle UI elements in your React application

On the client side, you can use `useStream()` and `LoadExternalComponent` to display the UI elements.

src/app/page.tsx

"use client";

import { useStream } from "@langchain/langgraph-sdk/react";
import { LoadExternalComponent } from "@langchain/langgraph-sdk/react-ui";

export default function Page() {
const { thread, values } = useStream({
apiUrl: "http://localhost:2024",
assistantId: "agent",
});

return (

{values.ui

}

Behind the scenes, `LoadExternalComponent` will fetch the JS and CSS for the UI components from LangSmith and render them in a shadow DOM, thus ensuring style isolation from the rest of your application.

## ​ How-to guides

### ​ Provide custom components on the client side

If you already have the components loaded in your client application, you can provide a map of such components to be rendered directly without fetching the UI code from LangSmith.

const clientComponents = {
weather: WeatherComponent,
};

<LoadExternalComponent
stream={thread}
message={ui}
components={clientComponents}

### ​ Show loading UI when components are loading

You can provide a fallback UI to be rendered when the components are loading.

<LoadExternalComponent
stream={thread}
message={ui}

### ​ Customise the namespace of UI components.

By default `LoadExternalComponent` will use the `assistantId` from `useStream()` hook to fetch the code for UI components. You can customise this by providing a `namespace` prop to the `LoadExternalComponent` component.

- src/app/page.tsx

- langgraph.json

<LoadExternalComponent
stream={thread}
message={ui}
namespace="custom-namespace"

{
"ui": {
"custom-namespace": "./src/agent/ui.tsx"
}
}

### ​ Access and interact with the thread state from the UI component

You can access the thread state inside the UI component by using the `useStreamContext` hook.

import { useStreamContext } from "@langchain/langgraph-sdk/react-ui";

const { thread, submit } = useStreamContext();
return (

const newMessage = {
type: "human",
content: `What's the weather in ${props.city}?`,
};

submit({ messages: [newMessage] });

### ​ Pass additional context to the client components

You can pass additional context to the client components by providing a `meta` prop to the `LoadExternalComponent` component.

Then, you can access the `meta` prop in the UI component by using the `useStreamContext` hook.

const { meta } = useStreamContext<
{ city: string },

### ​ Streaming UI messages from the server

You can stream UI messages before the node execution is finished by using the `onCustomEvent` callback of the `useStream()` hook. This is especially useful when updating the UI component as the LLM is generating the response.

import { uiMessageReducer } from "@langchain/langgraph-sdk/react-ui";

const { thread, submit } = useStream({
apiUrl: "http://localhost:2024",
assistantId: "agent",

const ui = uiMessageReducer(prev.ui ?? [], event);
return { ...prev, ui };
});
},
});

Then you can push updates to the UI component by calling `ui.push()` / `push_ui_message()` with the same ID as the UI message you wish to update.

- ui.tsx

from typing import Annotated, Sequence, TypedDict

from langchain_anthropic import ChatAnthropic
from langchain.messages import AIMessage, AIMessageChunk, BaseMessage
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langgraph.graph.ui import AnyUIMessage, push_ui_message, ui_message_reducer

class CreateTextDocument(TypedDict):
"""Prepare a document heading for the user."""

title: str

async def writer_node(state: AgentState):
model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
message: AIMessage = await model.bind_tools(
tools=[CreateTextDocument],
tool_choice={"type": "tool", "name": "CreateTextDocument"},
).ainvoke(state["messages"])

tool_call = next(
(x["args"] for x in message.tool_calls if x["name"] == "CreateTextDocument"),
None,
)

if tool_call:
ui_message = push_ui_message("writer", tool_call, message=message)
ui_message_id = ui_message["id"]

# We're already streaming the LLM response to the client through UI messages
# so we don't need to stream it again to the `messages` stream mode.
content_stream = model.with_config({"tags": ["nostream"]}).astream(
f"Create a document with the title: {tool_call['title']}"
)

content: AIMessageChunk | None = None
async for chunk in content_stream:
content = content + chunk if content else chunk

push_ui_message(
"writer",
{"content": content.text()},
id=ui_message_id,
message=message,
# Use `merge=rue` to merge props with the existing UI message
merge=True,
)

return {"messages": [message]}

import {
Annotation,
MessagesAnnotation,
type LangGraphRunnableConfig,
} from "@langchain/langgraph";
import { z } from "zod";
import { ChatAnthropic } from "@langchain/anthropic";
import {
typedUi,
uiMessageReducer,
} from "@langchain/langgraph-sdk/react-ui/server";
import type { AIMessageChunk } from "@langchain/core/messages";

import type ComponentMap from "./ui";

async function writerNode(
state: typeof AgentState.State,
config: LangGraphRunnableConfig

const model = new ChatAnthropic({ model: "claude-sonnet-4-5-20250929" });
const message = await model
.bindTools(
[\
{\
name: "create_text_document",\
description: "Prepare a document heading for the user.",\
schema: z.object({ title: z.string() }),\
},\
],
{ tool_choice: { type: "tool", name: "create_text_document" } }
)
.invoke(state.messages);

type ToolCall = { name: "create_text_document"; args: { title: string } };
const toolCall = message.tool_calls?.find(

);

if (toolCall) {
const { id, name } = ui.push(
{ name: "writer", props: { title: toolCall.args.title } },
{ message }
);

const contentStream = await model
// We're already streaming the LLM response to the client through UI messages
// so we don't need to stream it again to the `messages` stream mode.
.withConfig({ tags: ["nostream"] })
.stream(`Create a short poem with the topic: ${message.text}`);

let content: AIMessageChunk | undefined;
for await (const chunk of contentStream) {
content = content?.concat(chunk) ?? chunk;

ui.push(
{ id, name, props: { content: content?.text } },
// Use `merge: true` to merge props with the existing UI message
{ message, merge: true }
);
}
}

return { messages: [message] };
}

function WriterComponent(props: { title: string; content?: string }) {
return (

export default {
weather: WriterComponent,
};

### ​ Remove UI messages from state

Similar to how messages can be removed from the state by appending a RemoveMessage you can remove an UI message from the state by calling `remove_ui_message` / `ui.delete` with the ID of the UI message.

from langgraph.graph.ui import push_ui_message, delete_ui_message

# push message
message = push_ui_message("weather", {"city": "London"})

# remove said message
delete_ui_message(message["id"])

// push message
const message = ui.push({ name: "weather", props: { city: "London" } });

// remove said message
ui.delete(message.id);

## ​ Learn more

- JS/TS SDK Reference

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to integrate LangGraph into your React application\\
\\
Previous LangSmith Studio\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/home)).Agent

Skip to main content).Agent#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Plans

- Create an account and API key

##### Account administration

- Overview
- Set up hierarchy
- Manage organizations using the API
- Manage billing
- Set up resource tags
- User management

##### Additional resources

- Polly (Beta)
- Data management

- Access control & Authentication

- Scalability & resilience
- FAQs
- Regions FAQ
- Pricing FAQ
- LangSmith status

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/generative-ui-react).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

How to implement generative user interfaces with LangGraph App development in LangSmith Deployment How to integrate LangGraph into your React application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/deploy).After

Skip to main content.After#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/pytest

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Frameworks & integrations

How to run evaluations with pytest (beta)

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Run an evaluation asynchronously
- Run evaluations with pytest
- Run evals with Vitest/Jest
- With the API
- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

On this page

- Installation
- Define and run tests
- Log inputs, outputs, and reference outputs
- Log feedback
- Trace intermediate calls
- Grouping tests into a test suite
- Naming experiments
- Caching
- pytest features
- Parametrize with pytest.mark.parametrize
- Parallelize with pytest-xdist
- Async tests with pytest-asyncio
- Watch mode with pytest-watch
- Rich outputs
- Dry-run mode
- Expectations
- Legacy
- @test / @unit decorator

The LangSmith pytest plugin lets Python developers define their datasets and evaluations as pytest test cases.Compared to the standard evaluation flow, this is useful when:

- **Each example requires different evaluation logic**: Standard evaluation flows assume consistent application and evaluator execution across all dataset examples. For more complex systems or comprehensive evaluations, specific system subsets may require evaluation with particular input types and metrics. These heterogeneous evaluations are simpler to write as distinct test case suites that track together.
- **You want to assert binary expectations**: Track assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines). Testing tools help when both evaluating system outputs and asserting basic properties about them.
- **You want pytest-like terminal outputs**: Get familiar pytest output formatting
- **You already use pytest to test your app**: Add LangSmith tracking to existing pytest workflows

The pytest integration is in beta and is subject to change in upcoming releases.

The JS/TS SDK has an analogous Vitest/Jest integration.

## ​ Installation

pip

uv

Copy

pip install -U "langsmith[pytest]"

## ​ Define and run tests

The pytest integration lets you define datasets and evaluators as test cases.To track a test in LangSmith add the `@pytest.mark.langsmith` decorator. Every decorated test case will be synced to a dataset example. When you run the test suite, the dataset will be updated and a new experiment will be created with one result for each test case.

Python

###################### my_app/main.py ######################
import openai
from langsmith import traceable, wrappers

oai_client = wrappers.wrap_openai(openai.OpenAI())

@traceable

result = oai_client.chat.completions.create(
model="gpt-4o-mini",
messages=[\
{"role": "system", "content": "Convert the user query to a SQL query."},\
{"role": "user", "content": user_query},\
],
)
return result.choices[0].message.content

###################### tests/test_my_app.py ######################
import pytest
from langsmith import testing as t

"""Return True if the query is valid SQL."""
return True # Dummy implementation

@pytest.mark.langsmith # <-- Mark as a LangSmith test case

user_query = "Get all users from the customers table"
t.log_inputs({"user_query": user_query}) # <-- Log example inputs, optional
expected = "SELECT * FROM customers;"
t.log_reference_outputs({"sql": expected}) # <-- Log example reference outputs, optional

sql = generate_sql(user_query)
t.log_outputs({"sql": sql}) # <-- Log run outputs, optional

t.log_feedback(key="valid_sql", score=is_valid_sql(sql)) # <-- Log feedback, optional
assert sql == expected # <-- Test pass/fail status automatically logged to LangSmith under 'pass' feedback key

When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any inputs, outputs, and reference (expected) outputs that you log.Use `pytest` as you normally would to run the tests:

pytest tests/

In most cases we recommend setting a test suite name:

LANGSMITH_TEST_SUITE='SQL app tests' pytest tests/

Each time you run this test suite, LangSmith:

- creates a dataset for each test file. If a dataset for this test file already exists it will be updated
- creates an experiment in each created/updated dataset
- creates an experiment row for each test case, with the inputs, outputs, reference outputs and feedback you’ve logged
- collects the pass/fail rate under the `pass` feedback key for each test case

Here’s what a test suite dataset looks like:!DatasetAnd what an experiment against that test suite looks like:!Experiment

## ​ Log inputs, outputs, and reference outputs

Every time we run a test we’re syncing it to a dataset example and tracing it as a run. There’s a few different ways that we can trace the example inputs and reference outputs and the run outputs. The simplest is to use the `log_inputs`, `log_outputs`, and `log_reference_outputs` methods. You can run these any time in a test to update the example and run for that test:

@pytest.mark.langsmith

t.log_inputs({"a": 1, "b": 2})
t.log_reference_outputs({"foo": "bar"})
t.log_outputs({"foo": "baz"})
assert True

Running this test will create/update an example with name “test\_foo”, inputs `{"a": 1, "b": 2}`, reference outputs `{"foo": "bar"}` and trace a run with outputs `{"foo": "baz"}`.**NOTE**: If you run `log_inputs`, `log_outputs`, or `log_reference_outputs` twice, the previous values will be overwritten.Another way to define example inputs and reference outputs is via pytest fixtures/parametrizations. By default any arguments to your test function will be logged as inputs on the corresponding example. If certain arguments are meant to represet reference outputs, you can specify that they should be logged as such using `@pytest.mark.langsmith(output_keys=["name_of_ref_output_arg"])`:

import pytest

@pytest.fixture

return 5

return 6

@pytest.mark.langsmith(output_keys=["d"])

result = 2 * c
t.log_outputs({"d": result}) # Log run outputs
assert result == d

This will create/sync an example with name “test\_cd”, inputs `{"c": 5}` and reference outputs `{"d": 6}`, and run output `{"d": 10}`.

## ​ Log feedback

By default LangSmith collects the pass/fail rate under the `pass` feedback key for each test case. You can add additional feedback with `log_feedback`.

import openai
import pytest
from langsmith import wrappers
from langsmith import testing as t

user_query = "whats up"
t.log_inputs({"user_query": user_query})

sql = generate_sql(user_query)
t.log_outputs({"sql": sql})

expected = "Sorry that is not a valid query."
t.log_reference_outputs({"sql": expected})

# Use this context manager to trace any steps used for generating evaluation
# feedback separately from the main application logic
with t.trace_feedback():
instructions = (
"Return 1 if the ACTUAL and EXPECTED answers are semantically equivalent, "
"otherwise return 0. Return only 0 or 1 and nothing else."
)

grade = oai_client.chat.completions.create(
model="gpt-4o-mini",
messages=[\
{"role": "system", "content": instructions},\
{"role": "user", "content": f"ACTUAL: {sql}\nEXPECTED: {expected}"},\
],
)
score = float(grade.choices[0].message.content)
t.log_feedback(key="correct", score=score)

assert score

Note the use of the `trace_feedback()` context manager. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case. Instead of showing up in the main test case run it will instead show up in the trace for the `correct` feedback key.**NOTE**: Make sure that the `log_feedback` call associated with the feedback trace occurs inside the `trace_feedback` context. This way we’ll be able to associate the feedback with the trace, and when seeing the feedback in the UI you’ll be able to click on it to see the trace that generated it.

## ​ Trace intermediate calls

LangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.

## ​ Grouping tests into a test suite

By default, all tests within a given file will be grouped as a single “test suite” with a corresponding dataset. You can configure which test suite a test belongs to by passing the `test_suite_name` parameter to `@pytest.mark.langsmith` for case-by-case grouping, or you can set the `LANGSMITH_TEST_SUITE` env var to group all tests from an execution into a single test suite:

LANGSMITH_TEST_SUITE="SQL app tests" pytest tests/

We generally recommend setting `LANGSMITH_TEST_SUITE` to get a consolidated view of all of your results.

## ​ Naming experiments

You can name an experiment using the `LANGSMITH_EXPERIMENT` env var:

LANGSMITH_TEST_SUITE="SQL app tests" LANGSMITH_EXPERIMENT="baseline" pytest tests/

## ​ Caching

LLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache HTTP requests to disk. To enable caching, install with `langsmith[pytest]` and set an env var: `LANGSMITH_TEST_CACHE=/my/cache/path`:

pip install -U "langsmith[pytest]"
LANGSMITH_TEST_CACHE=tests/cassettes pytest tests/my_llm_tests

@pytest.mark.langsmith(cached_hosts=["api.openai.com", "https://api.anthropic.com"])
def my_test():
...

## ​ pytest features

`@pytest.mark.langsmith` is designed to stay out of your way and works well with familiar `pytest` features.

### ​ Parametrize with `pytest.mark.parametrize`

You can use the `parametrize` decorator as before. This will create a new test case for each parametrized instance of the test.

@pytest.mark.langsmith(output_keys=["expected_sql"])
@pytest.mark.parametrize(
"user_query, expected_sql",
[\
("Get all users from the customers table", "SELECT * FROM customers"),\
("Get all users from the orders table", "SELECT * FROM orders"),\
],
)
def test_sql_generation_parametrized(user_query, expected_sql):
sql = generate_sql(user_query)
assert sql == expected_sql

**Note:** as the parametrized list grows, you may consider using `evaluate()` instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset.

### ​ Parallelize with `pytest-xdist`

You can use pytest-xdist as you normally would to parallelize test execution:

pip install -U pytest-xdist
pytest -n auto tests

### ​ Async tests with `pytest-asyncio`

`@pytest.mark.langsmith` works with sync or async tests, so you can run async tests exactly as before.

### ​ Watch mode with `pytest-watch`

Use watch mode to quickly iterate on your tests. We _highly_ recommend ony using this with test caching (see below) enabled to avoid unnecessary LLM calls:

pip install pytest-watch
LANGSMITH_TEST_CACHE=tests/cassettes ptw tests/my_llm_tests

## ​ Rich outputs

If you’d like to see a rich display of the LangSmith results of your test run you can specify `--langsmith-output`:

pytest --langsmith-output tests

**Note:** This flag used to be `--output=langsmith` in `langsmith<=0.3.3` but was updated to avoid collisions with other pytest plugins.You’ll get a nice table per test suite that updates live as the results are uploaded to LangSmith:!Rich pytest outputsSome important notes for using this feature:

- Make sure you’ve installed `pip install -U "langsmith[pytest]"`
- Rich outputs do not currently work with `pytest-xdist`

**NOTE**: The custom output removes all the standard pytest outputs. If you’re trying to debug some unexpected behavior it’s often better to show the regular pytest outputs so to get full error traces.

## ​ Dry-run mode

If you want to run the tests without syncing the results to LangSmith, you can set `LANGSMITH_TEST_TRACKING=false` in your environment.

LANGSMITH_TEST_TRACKING=false pytest tests/

The tests will run as normal, but the experiment logs will not be sent to LangSmith.

## ​ Expectations

LangSmith provides an expect utility to help define expectations about your LLM output. For example:

from langsmith import expect

@pytest.mark.langsmith
def test_sql_generation_select_all():
user_query = "Get all users from the customers table"
sql = generate_sql(user_query)
expect(sql).to_contain("customers")

This will log the binary “expectation” score to the experiment results, additionally `assert`ing that the expectation is met possibly triggering a test failure.`expect` also provides “fuzzy match” methods. For example:

@pytest.mark.langsmith(output_keys=["expectation"])
@pytest.mark.parametrize(
"query, expectation",
[\
("what's the capital of France?", "Paris"),\
],
)
def test_embedding_similarity(query, expectation):
prediction = my_chatbot(query)
expect.embedding_distance(
# This step logs the distance as feedback for this run
prediction=prediction, expectation=expectation
# Adding a matcher (in this case, 'to_be_*"), logs 'expectation' feedback
).to_be_less_than(0.5) # Optional predicate to assert against

expect.edit_distance(
# This computes the normalized Damerau-Levenshtein distance between the two strings
# If no predicate is provided below, 'assert' isn't called, but the score is still logged
prediction=prediction, expectation=expectation
)

This test case will be assigned 4 scores:

1. The `embedding_distance` between the prediction and the expectation
2. The binary `expectation` score (1 if cosine distance is less than 0.5, 0 if not)
3. The `edit_distance` between the prediction and the expectation
4. The overall test pass/fail score (binary)

The `expect` utility is modeled off of Jest’s expect API, with some off-the-shelf functionality to make it easier to grade your LLMs.

## ​ Legacy

#### ​ `@test` / `@unit` decorator

The legacy method for marking test cases is using the `@test` or `@unit` decorators:

from langsmith import test

@test

pass

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

How to run an evaluation asynchronously\\
\\
Previous How to run evaluations with Vitest/Jest (beta)\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/pytest)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Test a ReAct agent with Pytest/Vitest and LangSmith How to run evaluations with pytest (beta) Test

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/pytest).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Test a ReAct agent with Pytest/Vitest and LangSmith How to run evaluations with pytest (beta) Test

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent/custom-workflow

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Multi-agent

Custom workflow

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Overview
- Subagents
- Handoffs
- Skills
- Router
- Custom workflow
- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

On this page

- Key characteristics
- When to use
- Basic implementation
- Example: RAG pipeline

In the **custom workflow** architecture, you define your own bespoke execution flow using LangGraph. You have complete control over the graph structure—including sequential steps, conditional branches, loops, and parallel execution.

path\_a

path\_b

Input

Conditional

Deterministic step

Agentic step

Output

## ​ Key characteristics

- Complete control over graph structure
- Mix deterministic logic with agentic behavior
- Support for sequential steps, conditional branches, loops, and parallel execution
- Embed other patterns as nodes in your workflow

## ​ When to use

Use custom workflows when standard patterns (subagents, skills, etc.) don’t fit your requirements, you need to mix deterministic logic with agentic behavior, or your use case requires complex routing or multi-stage processing.Each node in your workflow can be a simple function, an LLM call, or an entire agent with tools. You can also compose other architectures within a custom workflow—for example, embedding a multi-agent system as a single node.For a complete example of a custom workflow, see the tutorial below.

**Tutorial: Build a multi-source knowledge base with routing**

The router pattern is an example of a custom workflow. This tutorial walks through building a router that queries GitHub, Notion, and Slack in parallel, then synthesizes results.

Learn more

## ​ Basic implementation

The core insight is that you can call a LangChain agent directly inside any LangGraph node, combining the flexibility of custom workflows with the convenience of pre-built agents:

Copy

from langchain.agents import create_agent
from langgraph.graph import StateGraph, START, END

agent = create_agent(model="openai:gpt-4o", tools=[...])

"""A LangGraph node that invokes a LangChain agent."""
result = agent.invoke({
"messages": [{"role": "user", "content": state["query"]}]
})
return {"answer": result["messages"][-1].content}

# Build a simple workflow
workflow = (
StateGraph(State)
.add_node("agent", agent_node)
.add_edge(START, "agent")
.add_edge("agent", END)
.compile()
)

## ​ Example: RAG pipeline

A common use case is combining retrieval with an agent. This example builds a WNBA stats assistant that retrieves from a knowledge base and can fetch live news.

Custom RAG workflow

The workflow demonstrates three types of nodes:

- **Model node** (Rewrite): Rewrites the user query for better retrieval using structured output.
- **Deterministic node** (Retrieve): Performs vector similarity search — no LLM involved.
- **Agent node** (Agent): Reasons over retrieved context and can fetch additional information via tools.

Query

Rewrite

Retrieve

Agent

Response

You can use LangGraph state to pass information between workflow steps. This allows each part of your workflow to read and update structured fields, making it easy to share data and context across nodes.

from typing import TypedDict
from pydantic import BaseModel
from langgraph.graph import StateGraph, START, END
from langchain.agents import create_agent
from langchain.tools import tool
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore

class State(TypedDict):
question: str
rewritten_query: str
documents: list[str]
answer: str

# WNBA knowledge base with rosters, game results, and player stats
embeddings = OpenAIEmbeddings()
vector_store = InMemoryVectorStore(embeddings)
vector_store.add_texts([\
# Rosters\
"New York Liberty 2024 roster: Breanna Stewart, Sabrina Ionescu, Jonquel Jones, Courtney Vandersloot.",\
"Las Vegas Aces 2024 roster: A'ja Wilson, Kelsey Plum, Jackie Young, Chelsea Gray.",\
"Indiana Fever 2024 roster: Caitlin Clark, Aliyah Boston, Kelsey Mitchell, NaLyssa Smith.",\
# Game results\
"2024 WNBA Finals: New York Liberty defeated Minnesota Lynx 3-2 to win the championship.",\
"June 15, 2024: Indiana Fever 85, Chicago Sky 79. Caitlin Clark had 23 points and 8 assists.",\
"August 20, 2024: Las Vegas Aces 92, Phoenix Mercury 84. A'ja Wilson scored 35 points.",\
# Player stats\
"A'ja Wilson 2024 season stats: 26.9 PPG, 11.9 RPG, 2.6 BPG. Won MVP award.",\
"Caitlin Clark 2024 rookie stats: 19.2 PPG, 8.4 APG, 5.7 RPG. Won Rookie of the Year.",\
"Breanna Stewart 2024 stats: 20.4 PPG, 8.5 RPG, 3.5 APG.",\
])
retriever = vector_store.as_retriever(search_kwargs={"k": 5})

@tool

"""Get the latest WNBA news and updates."""
# Your news API here
return "Latest: The WNBA announced expanded playoff format for 2025..."

agent = create_agent(
model="openai:gpt-4o",
tools=[get_latest_news],
)

model = ChatOpenAI(model="gpt-4o")

class RewrittenQuery(BaseModel):
query: str

"""Rewrite the user query for better retrieval."""
system_prompt = """Rewrite this query to retrieve relevant WNBA information.
The knowledge base contains: team rosters, game results with scores, and player statistics (PPG, RPG, APG).
Focus on specific player names, team names, or stat categories mentioned."""
response = model.with_structured_output(RewrittenQuery).invoke([\
{"role": "system", "content": system_prompt},\
{"role": "user", "content": state["question"]}\
])
return {"rewritten_query": response.query}

"""Retrieve documents based on the rewritten query."""
docs = retriever.invoke(state["rewritten_query"])
return {"documents": [doc.page_content for doc in docs]}

"""Generate answer using retrieved context."""
context = "\n\n".join(state["documents"])
prompt = f"Context:\n{context}\n\nQuestion: {state['question']}"
response = agent.invoke({"messages": [{"role": "user", "content": prompt}]})
return {"answer": response["messages"][-1].content_blocks}

workflow = (
StateGraph(State)
.add_node("rewrite", rewrite_query)
.add_node("retrieve", retrieve)
.add_node("agent", call_agent)
.add_edge(START, "rewrite")
.add_edge("rewrite", "retrieve")
.add_edge("retrieve", "agent")
.add_edge("agent", END)
.compile()
)

result = workflow.invoke({"question": "Who won the 2024 WNBA Championship?"})
print(result["answer"])

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Router\\
\\
Previous Retrieval\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/multi-agent/custom-workflow)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Build a custom RAG agent with LangGraph LangChain overview

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/contributing/implement-langchain

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integrations

Implement a LangChain integration

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Contribute

- Documentation
- Code
- Integrations

- Guide
- Implement
- Standard tests
- Publish
- Co-marketing

Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.LangChain components are subclasses of base classes in `langchain-core`. Examples include chat models, tools, retrievers, and more.Your integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.

- Chat Models

- Tools

- Retrievers

- Vector Stores

- Embeddings

Chat models are subclasses of the `BaseChatModel` class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.

The chat model integration guide is currently WIP. In the meantime, read the chat model conceptual guide for details on how LangChain chat models function.

Tools are used in 2 main ways:

1. To define an “input schema” or “args schema” to pass to a chat model’s tool calling feature along with a text request, such that the chat model can generate a “tool call”, or parameters to call the tool with.
2. To take a “tool call” as generated above, and take some action and return a response that can be passed base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.

The tools integration guide is currently WIP. In the meantime, read the tools conceptual guide for details on how LangChain tools function.

Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.

The retriever integration guide is currently WIP. In the meantime, read the retriever conceptual guide for details on how LangChain retrievers function.

All vector stores must inherit from the `VectorStore` base class. This interface consists of methods for writing, deleting and searching for documents in the vector store.See the vector store integration guide for details on implementing a vector store integration.

The vector store integration guide is currently WIP. In the meantime, read the vector store conceptual guide for details on how LangChain vector stores function.

Embedding models are subclasses of the `Embeddings` class.

The embedding model integration guide is currently WIP. In the meantime, read the embedding model conceptual guide for details on how LangChain embedding models function.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Contributing integrations\\
\\
Previous Using standard tests\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/docs/reference/langchain-mcp-adapters

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Model Context Protocol (MCP) Reference Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/docs/reference/langchain-core/documents

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith docs Publish an integration Build a semantic search engine with LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/docs/concepts/messages

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Messages Build a voice agent with LangChain ChatGoogleGenerativeAI

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/tools),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration LangChain integrations packages

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/docs/concepts/messages),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Messages Build a voice agent with LangChain LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/context-engineering),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Implement a LangChain integration Install LangChain

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/cli)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Fetch LangGraph CLI LangSmith Studio

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-server))

Skip to main content)#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Test locally

- App development

- Cloud quickstart

##### Configure app for deployment

- Application structure
- Setup

- Deployment components

- Rebuild graph at runtime
- Interact with a deployment using RemoteGraph
- Add semantic search to your agent deployment
- Add TTLs to your application
- Configure Agent Server for scale
- Implement a CI/CD pipeline

##### Deployment guides

- Cloud
- With control plane
- Standalone servers

##### App development

- Data models

- Core capabilities

- Tutorials

##### Studio

- Overview
- Quickstart
- Runs, assistants, threads
- Traces, datasets, prompts
- Troubleshooting

##### Auth & access control

- Overview
- Add custom authentication
- Set up custom authentication
- Make conversations private
- Connect an authentication provider
- Document API authentication in OpenAPI
- Set up Agent Auth (Beta)

##### Server customization

- Add custom lifespan events
- Add custom middleware
- Add custom routes
- Configurable headers
- Logging Headers

404

# Page not found

We couldn’t find the page you were looking for.

Configure LangSmith Agent Server for scale Agent Server API reference for LangSmith Deployment LangSmith Tool Server

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/python/langchain/deploy).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Page Not Found

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

Python

- Overview

##### Get started

- Install
- Quickstart
- Changelog
- Philosophy

##### Core components

- Agents
- Models
- Messages
- Tools
- Short-term memory
- Streaming
- Structured output

##### Middleware

- Overview
- Built-in middleware
- Custom middleware

##### Advanced usage

- Guardrails
- Runtime
- Context engineering
- Model Context Protocol (MCP)
- Human-in-the-loop
- Multi-agent

- Retrieval
- Long-term memory

##### Agent development

- LangSmith Studio
- Test
- Agent Chat UI

##### Deploy with LangSmith

- Deployment
- Observability

404

# Page not found

We couldn’t find the page you were looking for.

Trace with LangChain (Python and JS/TS) Install LangChain Implement a LangChain integration

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/contributing/implement-langchain

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Integrations

Implement a LangChain integration

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Contribute

- Documentation
- Code
- Integrations

- Guide
- Implement
- Standard tests
- Publish
- Co-marketing

Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.LangChain components are subclasses of base classes in `langchain-core`. Examples include chat models, tools, retrievers, and more.Your integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.

- Chat Models

- Tools

- Retrievers

- Vector Stores

- Embeddings

Chat models are subclasses of the `BaseChatModel` class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.

The chat model integration guide is currently WIP. In the meantime, read the chat model conceptual guide for details on how LangChain chat models function.

Tools are used in 2 main ways:

1. To define an “input schema” or “args schema” to pass to a chat model’s tool calling feature along with a text request, such that the chat model can generate a “tool call”, or parameters to call the tool with.
2. To take a “tool call” as generated above, and take some action and return a response that can be passed base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.

The tools integration guide is currently WIP. In the meantime, read the tools conceptual guide for details on how LangChain tools function.

Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.

The retriever integration guide is currently WIP. In the meantime, read the retriever conceptual guide for details on how LangChain retrievers function.

All vector stores must inherit from the `VectorStore` base class. This interface consists of methods for writing, deleting and searching for documents in the vector store.See the vector store integration guide for details on implementing a vector store integration.

The vector store integration guide is currently WIP. In the meantime, read the vector store conceptual guide for details on how LangChain vector stores function.

Embedding models are subclasses of the `Embeddings` class.

The embedding model integration guide is currently WIP. In the meantime, read the embedding model conceptual guide for details on how LangChain embedding models function.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Contributing integrations\\
\\
Previous Using standard tests\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/agent-server-changelog

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Releases

Agent Server changelog

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

FiltersClear

agent-server

**Subscribe**: Our changelog includes an RSS feed that can integrate with Slack, email, Discord bots like Readybot or RSS Feeds to Discord Bot, and other subscription tools.

Agent Server is an API platform for creating and managing agent-based applications. It provides built-in persistence, a task queue, and supports deploying, configuring, and running assistants (agentic workflows) at scale. This changelog documents all notable updates, features, and fixes to Agent Server releases.

​

2025-12-23

## ​ v0.6.12

- Improved resolve\_embeddings to be more robust, enabling multiple calls without errors.
- Updated `@langchain/langgraph` from version 1.0.4 to 1.0.7, adding support for resumableStreams on remote graphs and undeprecating toolsCondition.
- Implemented `RemoteCheckpointer` to enable subgraph checkpointing, enhancing task execution reliability.

2025-12-20

## ​ v0.6.11

- Made the maximum number of retries configurable for enhanced customization.

## ​ v0.6.10

- Ensured run cancellation only processes ‘message’ type Redis events, improving pubsub client reliability.
- Added custom encryption for the Store API `value` field, allowing users to choose which keys to encrypt for enhanced security.
- Enabled streaming for subgraph custom events by updating TeeStream to handle event types separately.

2025-12-18

## ​ v0.6.9

- Enforced stable JSON keys for custom encryption, removed model-type-specific custom JSON functions, and improved error handling for double-encryption scenarios.

## ​ v0.6.8

- Added profiling feature to enhance performance analysis and monitoring.

## ​ v0.6.7

- Logged server startup time for improved monitoring and diagnostics.

2025-12-17

## ​ v0.6.5

- Added a warning log that triggers during import time for improved visibility.

2025-12-16

## ​ v0.6.4

- Enhanced custom encryption by parallelizing metadata and config processes, added encryption for thread.config and some checkpoints, improved tests and schema consistency.
- Ensured the Go server starts as `core-api` in the queue entrypoint for consistent runtime behavior.

2025-12-15

## ​ v0.6.2

- Resolved an issue that caused duplicate calls to middleware when `mount_prefix` was specified.

## ​ v0.6.0

This minor version updates the streaming APIs `/join-stream` and `/stream` behavior with respect to the `last-event-id` parameter to align with the SSE spec. Previously, passing a last-event-id would return that message in addition to any following messages. Going forward, these APIs will only return new messages following the provided last-event-id. For example, with the following stream, previously passing a last-event-id of `2` would return the messages with ids `2` and `3`, but will now only return the message with id `3`:

Copy

{
"id": 1,
"event": "message",
"data": {
"content": "Excluded"
}
},
{
"id": 2,
"event": "message",
"data": {
"content": "Passed last-event-id"
}
},
{
"id": 3,
"event": "message",
"data": {
"content": "Included"
}
}

This bump also includes some fixes, including a bug exposing unintended internal events in run streams.

2025-12-12

## ​ v0.5.42

- Modified the Go server to rely solely on the CLI `-service` flag for determining service mode, ignoring the globally set `FF_USE_CORE_API` for better deployment specificity.

2025-12-11

## ​ v0.5.41

Fixed an issue with cron jobs in hybrid mode by ensuring proper initialization of the ENTERPRISE\_SAAS global flag.

2025-12-10

## ​ v0.5.39

- Completed the implementation of custom encryptions for runs and crons, along with simplifying encryption processes.
- Introduced support for streaming subgraph events in both `values` and `updates` stream modes.

## ​ v0.5.38

- Implemented complete custom encryption for threads, ensuring all thread data is properly secured and encrypted.
- Ensured Redis attempt flags are consistently expired to prevent stale data.
- Added core authentication and support for OR/AND filters, enhancing security and flexibility.

2025-12-09

## ​ v0.5.37

Added a `name` parameter to the assistants count API for improved search flexibility.

## ​ v0.5.36

- Introduced configurable webhook support, allowing users to customize submitted webhooks and headers.
- Added an `/ok` endpoint at the root for easier health checks and simplified configuration.

2025-12-08

## ​ v0.5.34

Introduced custom encryption middleware, allowing users to define their own encryption methods for enhanced data protection.

## ​ v0.5.33

Set Uvicorn’s keep-alive timeout to 75 seconds to prevent occasional 502 errors and improve connection handling.

2025-12-06

## ​ v0.5.32

Introduced OpenTelemetry telemetry agent with support for New Relic integration.

2025-12-05

## ​ v0.5.31

Added Py-Spy profiling for improved analysis of deployment performance, with some limitations on coverage.

## ​ v0.5.30

- Always configure loopback transport clients to enhance reliability.
- Ensured authentication headers are passed for remote non-stream methods in JS.

2025-12-04

## ​ v0.5.28

- Introduced a faster, Rust-based implementation of uuid7 to improve performance, now used in langsmith and langchain-core.
- Added support for `$or` and `$and` in PostgreSQL auth filters to enable complex logic in authentication checks.
- Capped psycopg and psycopg-pool versions to prevent infinite waiting on startup.

2025-11-26

## ​ v0.5.27

- Ensured `runs.list` with filters returns only run fields, preventing incorrect status data from being included.
- (JS) Updated `uuid` from version 10.0.0 to 13.0.0. and `exit-hook` from version 4.0.0 to 5.0.1.

2025-11-24

## ​ v0.5.26

Resolved issues with `store.put` when used without AsyncBatchedStore in the JavaScript environment.

2025-11-22

## ​ v0.5.25

- Introduced the ability to search assistants by their `name` using a new endpoint.
- Casted store\_get return types to tuple in JavaScript to ensure type consistency.

2025-11-21

## ​ v0.5.24

- Added executor metrics for Datadog and enhanced core stream API metrics for better performance tracking.
- Disabled Redis Go maintenance notifications to prevent startup errors with unsupported commands in Redis versions below 8.

2025-11-20

## ​ v0.5.20

Resolved an error in the executor service that occurred when handling large messages.

2025-11-19

## ​ v0.5.19

Upgraded built-in langchain-core to version 1.0.7 to address a prompt formatting vulnerability.

## ​ v0.5.18

Introduced persistent cron threads with `on_run_completed: {keep,delete}` for enhanced cron management and retrieval options.

## ​ v0.5.17

Enhanced task handling to support multiple interrupts, aligning with open-source functionality.

2025-11-18

## ​ v0.5.15

Added custom JSON unmarshalling for `Resume` and `Goto` commands to fix map-style null resume interpretation issues.

2025-11-14

## ​ v0.5.14

Ensured `pg make start` command functions correctly with core-api enabled.

2025-11-13

## ​ v0.5.13

Support `include` and `exclude` (plural form key for `includes` and `excludes`) since a doc incorrectly claimed support for that. Now the server accepts either.

2025-11-10

## ​ v0.5.11

- Ensured auth handlers are applied consistently when streaming threads, aligning with recent security practices.
- Bumped `undici` dependency from version 6.21.3 to 7.16.0, introducing various performance improvements and bug fixes.
- Updated `p-queue` from version 8.0.1 to 9.0.0, introducing new features and breaking changes, including the removal of the `throwOnTimeout` option.

## ​ v0.5.10

Implemented healthcheck calls in the queue /ok handler to improve Kubernetes liveness and readiness probe compatibility.

2025-11-09

## ​ v0.5.9

- Resolved an issue causing an “unbound local error” for the `elapsed` variable during a SIGINT interruption.
- Mapped the “interrupted” status to A2A’s “input-required” status for better task status alignment.

2025-11-07

## ​ v0.5.8

- Ensured environment variables are passed as a dictionary when starting langgraph-ui for compatibility with `uvloop`.
- Implemented CRUD operations for runs in Go, simplifying JSON merges and improving transaction readability, with PostgreSQL as a reference.

## ​ v0.5.7

Replaced no-retry Redis client with a retry client to handle connection errors more effectively and reduced corresponding logging severity.

2025-11-06

## ​ v0.5.6

- Added pending time metrics to provide better insights into task waiting times.
- Replaced `pb.Value` with `ChannelValue` to streamline code structure.

2025-11-05

## ​ v0.5.5

Made the Redis `health_check_interval` more frequent and configurable for better handling of idle connections.

## ​ v0.5.4

Implemented `ormsgpack` with `OPT_REPLACE_SURROGATES` and updated for compatibility with the latest FastAPI release affecting custom authentication dependencies.

2025-11-03

## ​ v0.5.2

Added retry logic for PostgreSQL connections during startup to enhance deployment reliability and improved error logging for easier debugging.

## ​ v0.5.1

- Resolved an issue where persistence was not functioning correctly with LangChain.js’s createAgent feature.
- Optimized assistants CRUD performance by improving database connection pooling and gRPC client reuse, reducing latency for large payloads.

2025-10-31

## ​ v0.5.0

This minor version now requires langgraph-checkpoint versions later than 3.0 to prevent a deserialization vulnerability in earlier versions of the langgraph-checkpoint library.
The `langgraph-checkpoint` library is compatible with `langgraph` minor versions 0.4, 0.5, 0.6, and 1.0.This version removes default support for deserialization of payloads saved using the “json” type, which has never been the default.
By default, objects are serialized using msgpack. Under certain uncommon situations, payloads were serialized using an older “json” mode. If those payloads contained custom python objects, those will no longer be deserializable unless you provide a `serde` config:

{
"checkpointer": {
"serde": {
"allowed_json_modules": [\
["my_agent", "my_file", "SomeType"],\
]
}
}
}

2025-10-29

## ​ v0.4.47

- Validated and auto-corrected environment configuration types using TypeAdapter.
- Added support for LangChain.js and LangGraph.js version 1.x, ensuring compatibility.
- Updated hono library from version 4.9.7 to 4.10.3, addressing a CORS middleware security issue and enhancing JWT audience validation.
- Introduced a modular benchmark framework, adding support for assistants and streams, with improvements to the existing ramp benchmark methodology.
- Introduced a gRPC API for core threads CRUD operations, with updated Python and TypeScript clients.
- Updated `hono` package from version 4.9.7 to 4.10.2, including security improvements for JWT audience validation.
- Updated `hono` dependency from version 4.9.7 to 4.10.3 to fix a security issue and improve CORS middleware handling.
- Introduced basic CRUD operations for threads, including create, get, patch, delete, search, count, and copy, with support for Go, gRPC server, and Python and TypeScript clients.

2025-10-21

## ​ v0.4.46

Added an option to enable message streaming from subgraph events, giving users more control over event notifications.

## ​ v0.4.45

- Implemented support for authorization on custom routes, controlled by the `enable_custom_route_auth` flag.
- Set default tracing to off for improved performance and simplified debugging.

2025-10-18

## ​ v0.4.44

Used Redis key prefix for license-related keys to prevent conflicts with existing setups.

2025-10-16

## ​ v0.4.43

Implemented a health check for Redis connections to prevent them from idling out.

2025-10-15

## ​ v0.4.40

- Prevented duplicate messages in resumable run and thread streams by addressing a race condition and adding tests to ensure consistent behavior.
- Ensured that runs don’t start until the pubsub subscription is confirmed to prevent message drops on startup.
- Renamed platform from langgraph to improve clarity and branding.
- Reset PostgreSQL connections after use to prevent lock holding and improved error reporting for transaction issues.

2025-10-10

## ​ v0.4.39

- Upgraded `hono` from version 4.7.6 to 4.9.7, addressing a security issue related to the `bodyLimit` middleware.
- Allowed customization of the base authentication URL to enhance flexibility.
- Pinned the ‘ty’ dependency to a stable version using ‘uv’ to prevent unexpected linting failures.

2025-10-08

## ​ v0.4.38

- Replaced `LANGSMITH_API_KEY` with `LANGSMITH_CONTROL_PLANE_API_KEY` to support hybrid deployments requiring license verification.
- Introduced self-hosted log ingestion support, configurable via `SELF_HOSTED_LOGS_ENABLED` and `SELF_HOSTED_LOGS_ENDPOINT` environment variables.

2025-10-06

## ​ v0.4.37

Required create permissions for copying threads to ensure proper authorization.

2025-10-03

## ​ v0.4.36

- Improved error handling and added a delay to the sweep loop for smoother operation during Redis downtime or cancellation errors.
- Updated the queue entrypoint to start the core-api gRPC server when `FF_USE_CORE_API` is enabled.
- Introduced checks for invalid configurations in assistant endpoints to ensure consistency with other endpoints.

2025-10-02

## ​ v0.4.35

- Resolved a timezone issue in the core API, ensuring accurate time data retrieval.
- Introduced a new `middleware_order` setting to apply authentication middleware before custom middleware, allowing finer control over protected route configurations.
- Logged the Redis URL when errors occur during Redis client creation.
- Improved Go engine/runtime context propagation to ensure consistent execution flow.
- Removed the unnecessary `assistants.put` call from the executor entrypoint to streamline the process.

2025-10-01

## ​ v0.4.34

Blocked unauthorized users from updating thread TTL settings to enhance security.

## ​ v0.4.33

- Improved error handling for Redis locks by logging `LockNotOwnedError` and extending initial pool migration lock timeout to 60 seconds.
- Updated the BaseMessage schema to align with the latest langchain-core version and synchronized build dependencies for consistent local development.

2025-09-30

## ​ v0.4.32

- Added a GO persistence layer to the API image, enabling GRPC server operation with PostgreSQL support and enhancing configurability.
- Set the status to error when a timeout occurs to improve error handling.

## ​ v0.4.30

- Added support for context when using `stream_mode="events"` and included new tests for this functionality.
- Added support for overriding the server port using `$LANGGRAPH_SERVER_PORT` and removed an unnecessary Dockerfile `ARG` for cleaner configuration.
- Applied authorization filters to all table references in thread delete CTE to enhance security.
- Introduced self-hosted metrics ingestion capability, allowing metrics to be sent to an OTLP collector every minute when the corresponding environment variables are set.
- Ensured that the `set_latest` function properly updates the name and description of the version.

2025-09-26

## ​ v0.4.29

Ensured proper cleanup of redis pubsub connections in all scenarios.

2025-09-25

## ​ v0.4.28

- Added a format parameter to the queue metrics server for enhanced customization.
- Corrected `MOUNT_PREFIX` environment variable usage in CLI for consistency with documentation and to prevent confusion.
- Added a feature to log warnings when messages are dropped due to no subscribers, controllable via a feature flag.
- Added support for Bookworm and Bullseye distributions in Node images.
- Consolidated executor definitions by moving them from the `langgraph-go` repository, improving manageability and updating the checkpointer setup method for server migrations.
- Ensured correct response headers are sent for a2a, improving compatibility and communication.
- Consolidated PostgreSQL checkpoint implementation, added CI testing for the `/core` directory, fixed RemoteStore test errors, and enhanced the Store implementation with transactions.
- Added PostgreSQL migrations to the queue server to prevent errors from graphs being added before migrations are performed.

2025-09-23

## ​ v0.4.27

Replaced `coredis` with `redis-py` to improve connection handling and reliability under high traffic loads.

2025-09-22

## ​ v0.4.24

- Added functionality to return full message history for A2A calls in accordance with the A2A spec.
- Added a `LANGGRAPH_SERVER_HOST` environment variable to Dockerfiles to support custom host settings for dual stack mode.

## ​ v0.4.23

Use a faster message codec for redis streaming.

2025-09-19

## ​ v0.4.22

Ported long-stream handling to the run stream, join, and cancel endpoints for improved stream management.

2025-09-18

## ​ v0.4.21

- Added A2A streaming functionality and enhanced testing with the A2A SDK.
- Added Prometheus metrics to track language usage in graphs, middleware, and authentication for improved insights.
- Fixed bugs in Open Source Software related to message conversion for chunks.
- Removed await from pubsub subscribes to reduce flakiness in cluster tests and added retries in the shutdown suite to enhance API stability.

2025-09-11

## ​ v0.4.20

Optimized Pubsub initialization to prevent overhead and address subscription timing issues, ensuring smoother run execution.

## ​ v0.4.19

Removed warnings from psycopg by addressing function checks introduced in version 3.2.10.

## ​ v0.4.17

Filtered out logs with mount prefix to reduce noise in logging output.

2025-09-10

## ​ v0.4.16

- Added support for implicit thread creation in a2a to streamline operations.
- Improved error serialization and emission in distributed runtime streams, enabling more comprehensive testing.

2025-09-09

## ​ v0.4.13

- Monitored queue status in the health endpoint to ensure correct behavior when PostgreSQL fails to initialize.
- Addressed an issue with unequal swept ID lengths to improve log clarity.
- Enhanced streaming outputs by avoiding re-serialization of DR payloads, using msgpack byte inspection for json-like parsing.

2025-09-04

## ​ v0.4.12

- Ensured metrics are returned even when experiencing database connection issues.
- Optimized update streams to prevent unnecessary data transmission.
- Upgraded `hono` from version 4.9.2 to 4.9.6 in the `storage_postgres/langgraph-api-server` for improved URL path parsing security.
- Added retries and an in-memory cache for LangSmith access calls to improve resilience against single failures.

## ​ v0.4.11

Added support for TTL (time-to-live) in thread updates.

## ​ v0.4.10

2025-09-02

## ​ v0.4.9

- Added support for filtering search results by IDs in the search endpoint for more precise queries.
- Included configurable headers for assistant endpoints to enhance request customization.
- Implemented a simple A2A endpoint with support for agent card retrieval, task creation, and task management.

2025-08-30

## ​ v0.4.7

Stopped the inclusion of x-api-key to enhance security.

2025-08-29

## ​ v0.4.6

Fixed a race condition when joining streams, preventing duplicate start events.

## ​ v0.4.5

- Ensured the checkpointer starts and stops correctly before and after the queue to improve shutdown and startup efficiency.
- Resolved an issue where workers were being prematurely cancelled when the queue was cancelled.
- Prevented queue termination by adding a fallback for cases when Redis fails to wake a worker.

2025-08-28

## ​ v0.4.4

- Set the custom auth thread\_id to None for stateless runs to prevent conflicts.
- Improved Redis signaling in the Go runtime by adding a wakeup worker and Redis lock implementation, and updated sweep logic.

2025-08-27

## ​ v0.4.3

- Added stream mode to thread stream for improved data processing.
- Added a durability parameter to runs for improved data persistence.

## ​ v0.4.2

Ensured pubsub is initialized before creating a run to prevent errors from missing messages.

2025-08-25

## ​ v0.4.0

Minor version 0.4 comes with a number of improvements as well as some breaking changes.

- Emitted attempt messages correctly within the thread stream.
- Reduced cluster conflicts by using only the thread ID for hashing in cluster mapping, prioritizing efficiency with stream\_thread\_cache.
- Introduced a stream endpoint for threads to track all outputs across sequentially executed runs.
- Made the filter query builder in PostgreSQL more robust against malformed expressions and improved validation to prevent potential security risks.

This minor version also includes a couple of breaking changes to improve the usability and security of the service:

- In this minor version, we stop the practice of automatically including headers as configurable values in your runs. You can opt-in to specific patterns by setting **configurable\_headers** in your agent server config.
- Run stream event IDs (for resumable streams) are now in the format of `ms-seq` instead of the previous format. We retain backwards compatibility for the old format, but we recommend using the new format for new code.

## ​ v0.3.4

- Added custom Prometheus metrics for Redis/PG connection pools and switched the queue server to Uvicorn/Starlette for improved monitoring.
- Restored Wolfi image build by correcting shell command formatting and added a Makefile target for testing with nginx.

2025-08-22

## ​ v0.3.3

- Added timeouts to specific Redis calls to prevent workers from being left active.
- Updated the Golang runtime and added pytest skips for unsupported functionalities, including initial support for passing store to node and message streaming.
- Introduced a reverse proxy setup for serving combined Python and Node.js graphs, with nginx handling server routing, to facilitate a Postgres/Redis backend for the Node.js API server.

2025-08-21

## ​ v0.3.1

Added a statement timeout to the pool to prevent long-running queries.

## ​ v0.3.0

- Set a default 15-minute statement timeout and implemented monitoring for long-running queries to ensure system efficiency.
- Stop propagating run configurable values to the thread configuration, because this can cause issues on subsequent runs if you are specifying a checkpoint\_id. This is a **slight breaking change** in behavior, since the thread value will no longer automatically reflect the unioned configuration of the most recent run. We believe this behavior is more intuitive, however.
- Enhanced compatibility with older worker versions by handling event data in channel names within ops.py.

2025-08-20

## ​ v0.2.137

Fixed an unbound local error and improved logging for thread interruptions or errors, along with type updates.

## ​ v0.2.136

- Added enhanced logging to aid in debugging metaview issues.
- Upgraded executor and runtime to the latest version for improved performance and stability.

2025-08-19

## ​ v0.2.135

Ensured async coroutines are properly awaited to prevent potential runtime errors.

2025-08-18

## ​ v0.2.134

Enhanced search functionality to improve performance by allowing users to select specific columns for query results.

## ​ v0.2.133

- Added count endpoints for crons, threads, and assistants to enhance data tracking (#1132).
- Improved SSH functionality for better reliability and stability.
- Updated @langchain/langgraph-api to version 0.0.59 to fix an invalid state schema issue.

2025-08-15

## ​ v0.2.132

- Added Go language images to enhance project compatibility and functionality.
- Printed internal PIDs for JS workers to facilitate process inspection via SIGUSR1 signal.
- Resolved a `run_pkey` error that occurred when attempting to insert duplicate runs.
- Added `ty run` command and switched to using uuid7 for generating run IDs.
- Implemented the initial Golang runtime to expand language support.

2025-08-14

## ​ v0.2.131

Added support for `object agent spec` with descriptions in JS.

2025-08-13

## ​ v0.2.130

- Added a feature flag (FF\_RICH\_THREADS=false) to disable thread updates on run creation, reducing lock contention and simplifying thread status handling.
- Utilized existing connections for `aput` and `apwrite` operations to improve performance.
- Improved error handling for decoding issues to enhance data processing reliability.
- Excluded headers from logs to improve security while maintaining runtime functionality.
- Fixed an error that prevented mapping slots to a single node.
- Added debug logs to track node execution in JS deployments for improved issue diagnosis.
- Changed the default multitask strategy to enqueue, improving throughput by eliminating the need to fetch inflight runs during new run insertions.
- Optimized database operations for `Runs.next` and `Runs.sweep` to reduce redundant queries and improve efficiency.
- Improved run creation speed by skipping unnecessary inflight runs queries.

2025-08-11

## ​ v0.2.129

- Stopped passing internal LGP fields to context to prevent breaking type checks.
- Exposed content-location headers to ensure correct resumability behavior in the API.

2025-08-08

## ​ v0.2.128

Ensured synchronized updates between `configurable` and `context` in assistants, preventing setup errors and supporting smoother version transitions.

## ​ v0.2.127

Excluded unrequested stream modes from the resumable stream to optimize functionality.

## ​ v0.2.126

- Made access logger headers configurable to enhance logging flexibility.
- Debounced the Runs.stats function to reduce the frequency of expensive calls and improve performance.
- Introduced debouncing for sweepers to enhance performance and efficiency (#1147).
- Acquired a lock for TTL sweeping to prevent database spamming during scale-out operations.

2025-08-06

## ​ v0.2.125

Updated tracing context replicas to use the new format, ensuring compatibility.

## ​ v0.2.123

Added an entrypoint to the queue replica for improved deployment management.

## ​ v0.2.122

Utilized persisted interrupt status in `join` to ensure correct handling of user’s interrupt state after completion.

## ​ v0.2.121

- Consolidated events to a single channel to prevent race conditions and optimize startup performance.
- Ensured custom lifespans are invoked on queue workers for proper setup, and added tests.

2025-08-04

## ​ v0.2.120

- Restored the original streaming behavior of runs, ensuring consistent inclusion of interrupt events based on `stream_mode` settings.
- Optimized `Runs.next` query to reduce average execution time from ~14.43ms to ~2.42ms, improving performance.
- Added support for stream mode “tasks” and “checkpoints”, normalized the UI namespace, and upgraded `@langchain/langgraph-api` for enhanced functionality.

2025-07-31

## ​ v0.2.117

Added a composite index on threads for faster searches with owner-based authentication and updated the default sort order to `updated_at` for improved query performance.

## ​ v0.2.116

Reduced the default number of history checkpoints from 10 to 1 to optimize performance.

## ​ v0.2.115

Optimized cache re-use to enhance application performance and efficiency.

2025-07-30

## ​ v0.2.113

Improved thread search pagination by updating response headers with `X-Pagination-Total` and `X-Pagination-Next` for better navigation.

## ​ v0.2.112

- Ensured sync logging methods are awaited and added a linter to prevent future occurrences.
- Fixed an issue where JavaScript tasks were not being populated correctly for JS graphs.

2025-07-29

## ​ v0.2.111

Fixed JS graph streaming failure by starting the heartbeat as soon as the connection opens.

## ​ v0.2.110

Added interrupts as default values for join operations while preserving stream behavior.

2025-07-28

## ​ v0.2.109

Fixed an issue where config schema was missing when `config_type` was not set, ensuring more reliable configurations.

## ​ v0.2.108

Prepared for LangGraph v0.6 compatibility with new context API support and bug fixes.

2025-07-27

## ​ v0.2.107

- Implemented caching for authentication processes to enhance performance and efficiency.
- Optimized database performance by merging count and select queries.

## ​ v0.2.106

Made log streams resumable, enhancing reliability and improving user experience when reconnecting.

## ​ v0.2.105

Added a heapdump endpoint to save memory heap information to a file.

2025-07-25

## ​ v0.2.103

Used the correct metadata endpoint to resolve issues with data retrieval.

2025-07-24

## ​ v0.2.102

- Captured interrupt events in the wait method to preserve previous behavior from langgraph 0.5.0.
- Added support for SDK structlog in the JavaScript environment for enhanced logging capabilities.

## ​ v0.2.101

Corrected the metadata endpoint for self-hosted deployments.

2025-07-22

## ​ v0.2.99

- Improved license check by adding an in-memory cache and handling Redis connection errors more effectively.
- Reloaded assistants to preserve manually created ones while discarding those removed from the configuration file.
- Reverted changes to ensure the UI namespace for gen UI is a valid JavaScript property name.
- Ensured that the UI namespace for generated UI is a valid JavaScript property name, improving API compliance.
- Enhanced error handling to return a 422 status code for unprocessable entity requests.

2025-07-19

## ​ v0.2.98

Added context to langgraph nodes to improve log filtering and trace visibility.

## ​ v0.2.97

- Improved interoperability with the ckpt ingestion worker on the main loop to prevent task scheduling issues.
- Delayed queue worker startup until after migrations are completed to prevent premature execution.
- Enhanced thread state error handling by adding specific metadata and improved response codes for better clarity when state updates fail during creation.
- Exposed the interrupt ID when retrieving the thread state to improve API transparency.

2025-07-17

## ​ v0.2.96

Added a fallback mechanism for configurable header patterns to handle exclude/include settings more effectively.

## ​ v0.2.95

- Avoided setting the future if it is already done to prevent redundant operations.
- Resolved compatibility errors in CI by switching from `typing.TypedDict` to `typing_extensions.TypedDict` for Python versions below 3.12.

2025-07-16

## ​ v0.2.94

- Improved performance by omitting pending sends for langgraph versions 0.5 and above.
- Improved server startup logs to provide clearer warnings when the DD\_API\_KEY environment variable is set.

## ​ v0.2.93

Removed the GIN index for run metadata to improve performance.

## ​ v0.2.92

Enabled copying functionality for blobs and checkpoints, improving data management flexibility.

## ​ v0.2.91

Reduced writes to the `checkpoint_blobs` table by inlining small values (null, numeric, str, etc.). This means we don’t need to store extra values for channels that haven’t been updated.

## ​ v0.2.90

Improve checkpoint writes via node-local background queueing.

2025-07-15

## ​ v0.2.89

Decoupled checkpoint writing from thread/run state by removing foreign keys and updated logger to prevent timeout-related failures.

2025-07-14

## ​ v0.2.88

Removed the foreign key constraint for `thread` in the `run` table to simplify database schema.

## ​ v0.2.87

Added more detailed logs for Redis worker signaling to improve debugging.

2025-07-11

## ​ v0.2.86

Honored tool descriptions in the `/mcp` endpoint to align with expected functionality.

2025-07-10

## ​ v0.2.85

Added support for the `on_disconnect` field to `runs/wait` and included disconnect logs for better debugging.

2025-07-09

## ​ v0.2.84

Removed unnecessary status updates to streamline thread handling and updated version to 0.2.84.

## ​ v0.2.83

- Reduced the default time-to-live for resumable streams to 2 minutes.
- Enhanced data submission logic to send data to both Beacon and LangSmith instance based on license configuration.
- Enabled submission of self-hosted data to a LangSmith instance when the endpoint is configured.

2025-07-03

## ​ v0.2.82

Addressed a race condition in background runs by implementing a lock using join, ensuring reliable execution across CTEs.

## ​ v0.2.81

Optimized run streams by reducing initial wait time to improve responsiveness for older or non-existent runs.

## ​ v0.2.80

Corrected parameter passing in the `logger.ainfo()` API call to resolve a TypeError.

2025-07-02

## ​ v0.2.79

- Fixed a JsonDecodeError in checkpointing with remote graph by correcting JSON serialization to handle trailing slashes properly.
- Introduced a configuration flag to disable webhooks globally across all routes.

## ​ v0.2.78

- Added timeout retries to webhook calls to improve reliability.
- Added HTTP request metrics, including a request count and latency histogram, for enhanced monitoring capabilities.

## ​ v0.2.77

- Added HTTP metrics to improve performance monitoring.
- Changed the Redis cache delimiter to reduce conflicts with subgraph message names and updated caching behavior.

2025-07-01

## ​ v0.2.76

Updated Redis cache delimiter to prevent conflicts with subgraph messages.

2025-06-30

## ​ v0.2.74

Scheduled webhooks in an isolated loop to ensure thread-safe operations and prevent errors with PYTHONASYNCIODEBUG=1.

2025-06-27

## ​ v0.2.73

- Fixed an infinite frame loop issue and removed the dict\_parser due to structlog’s unexpected behavior.
- Throw a 409 error on deadlock occurrence during run cancellations to handle lock conflicts gracefully.

## ​ v0.2.72

- Ensured compatibility with future langgraph versions.
- Implemented a 409 response status to handle deadlock issues during cancellation.

2025-06-26

## ​ v0.2.71

Improved logging for better clarity and detail regarding log types.

## ​ v0.2.70

Improved error handling to better distinguish and log TimeoutErrors caused by users from internal run timeouts.

## ​ v0.2.69

Added sorting and pagination to the crons API and updated schema definitions for improved accuracy.

## ​ v0.2.66

Fixed a 404 error when creating multiple runs with the same thread\_id using `on_not_exist="create"`.

2025-06-25

## ​ v0.2.65

- Ensured that only fields from `assistant_versions` are returned when necessary.
- Ensured consistent data types for in-memory and PostgreSQL users, improving internal authentication handling.

2025-06-24

## ​ v0.2.64

Added descriptions to version entries for better clarity.

2025-06-23

## ​ v0.2.62

- Improved user handling for custom authentication in the JS Studio.
- Added Prometheus-format run statistics to the metrics endpoint for better monitoring.
- Added run statistics in Prometheus format to the metrics endpoint.

2025-06-20

## ​ v0.2.61

Set a maximum idle time for Redis connections to prevent unnecessary open connections.

## ​ v0.2.60

- Enhanced error logging to include traceback details for dictionary operations.
- Added a `/metrics` endpoint to expose queue worker metrics for monitoring.

2025-06-18

## ​ v0.2.57

- Removed CancelledError from retriable exceptions to allow local interrupts while maintaining retriability for workers.
- Introduced middleware to gracefully shut down the server after completing in-flight requests upon receiving a SIGINT.
- Reduced metadata stored in checkpoint to only include necessary information.
- Improved error handling in join runs to return error details when present.

2025-06-17

## ​ v0.2.56

Improved application stability by adding a handler for SIGTERM signals.

## ​ v0.2.55

- Improved the handling of cancellations in the queue entrypoint.
- Improved cancellation handling in the queue entry point.

2025-06-16

## ​ v0.2.54

- Enhanced error message for LuaLock timeout during license validation.
- Fixed the $contains filter in custom auth by requiring an explicit ::text cast and updated tests accordingly.
- Ensured project and tenant IDs are formatted as UUIDs for consistency.

2025-06-13

## ​ v0.2.53

- Resolved a timing issue to ensure the queue starts only after the graph is registered.
- Improved performance by setting thread and run status in a single query and enhanced error handling during checkpoint writes.
- Reduced the default background grace period to 3 minutes.

2025-06-12

## ​ v0.2.52

- Now logging expected graphs when one is omitted to improve traceability.
- Implemented a time-to-live (TTL) feature for resumable streams.
- Improved query efficiency and consistency by adding a unique index and optimizing row locking.

## ​ v0.2.51

- Handled `CancelledError` by marking tasks as ready to retry, improving error management in worker processes.
- Added LG API version and request ID to metadata and logs for better tracking.
- Added LG API version and request ID to metadata and logs to improve traceability.
- Improved database performance by creating indexes concurrently.
- Ensured postgres write is committed only after the Redis running marker is set to prevent race conditions.
- Enhanced query efficiency and reliability by adding a unique index on thread\_id/running, optimizing row locks, and ensuring deterministic run selection.
- Resolved a race condition by ensuring Postgres updates only occur after the Redis running marker is set.

2025-06-07

## ​ v0.2.46

Introduced a new connection for each operation while preserving transaction characteristics in Threads state `update()` and `bulk()` commands.

2025-06-05

## ​ v0.2.45

- Enhanced streaming feature by incorporating tracing contexts.
- Removed an unnecessary query from the Crons.search function.
- Resolved connection reuse issue when scheduling next run for multiple cron jobs.
- Removed an unnecessary query in the Crons.search function to improve efficiency.
- Resolved an issue with scheduling the next cron run by improving connection reuse.

2025-06-04

## ​ v0.2.44

- Enhanced the worker logic to exit the pipeline before continuing when the Redis message limit is reached.
- Introduced a ceiling for Redis message size with an option to skip messages larger than 128 MB for improved performance.
- Ensured the pipeline always closes properly to prevent resource leaks.

## ​ v0.2.43

- Improved performance by omitting logs in metadata calls and ensuring output schema compliance in value streaming.
- Ensured the connection is properly closed after use.
- Aligned output format to strictly adhere to the specified schema.
- Stopped sending internal logs in metadata requests to improve privacy.

## ​ v0.2.42

- Added timestamps to track the start and end of a request’s run.
- Added tracer information to the configuration settings.
- Added support for streaming with tracing contexts.

2025-06-03

## ​ v0.2.41

Added locking mechanism to prevent errors in pipelined executions.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Environment variables\\
\\
Previous Self-hosted LangSmith changelog\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/langchain/voice-agent

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

LangChain

Build a voice agent with LangChain

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Learn

##### Tutorials

- LangChain

- Semantic search
- RAG agent
- SQL agent
- Voice agent
- Multi-agent

- Subagents: Personal assistant
- Router: Knowledge base
- LangGraph

- Custom RAG agent

##### Conceptual overviews

- Component architecture
- Memory
- Context
- Graph API
- Functional API

##### LangChain Academy

- LangChain Academy

##### Additional resources

- Case studies
- Get help

On this page

- Overview
- What are voice agents?
- How do voice agents work?

- 2\. Speech-to-Speech Architecture (S2S)
- Demo Application Overview
- Architecture
- Setup
- 1\. Speech-to-text
- Key Concepts
- Implementation
- 2\. LangChain agent
- Key Concepts
- Implementation
- 3\. Text-to-speech
- Key Concepts
- Implementation
- Putting It All Together

## ​ Overview

Chat interfaces have dominated how we interact with AI, but recent breakthroughs in multimodal AI are opening up exciting new possibilities. High-quality generative models and expressive text-to-speech (TTS) systems now make it possible to build agents that feel less like tools and more like conversational partners.Voice agents are one example of this. Instead of relying on a keyboard and mouse to type inputs into an agent, you can use spoken words to interact with it. This can be a more natural and engaging way to interact with AI, and can be especially useful for certain contexts.

### ​ What are voice agents?

Voice agents are agents that can engage in natural spoken conversations with users. These agents combine speech recognition, natural language processing, generative AI, and text-to-speech technologies to create seamless, natural conversations.They’re suited for a variety of use cases, including:

- Customer support
- Personal assistants
- Hands-free interfaces
- Coaching and training

### ​ How do voice agents work?

At a high level, every voice agent needs to handle three tasks:

1. **Listen** \- capture audio and transcribe it
2. **Think** \- interpret intent, reason, plan

The Sandwich architecture composes three distinct components: speech-to-text (STT), a text-based LangChain agent, and text-to-speech (TTS).

User Audio

Speech-to-Text

LangChain Agent

Text-to-Speech

Audio Output

**Pros:**

- Full control over each component (swap STT/TTS providers as needed)
- Access to latest capabilities from modern text-modality models
- Transparent behavior with clear boundaries between components

**Cons:**

- Requires orchestrating multiple services
- Additional complexity in managing the pipeline
- Conversion from speech to text loses information (e.g., tone, emotion)

#### ​ 2\. Speech-to-Speech Architecture (S2S)

Speech-to-speech uses a multimodal model that processes audio input and generates audio output natively.

Multimodal Model

- Simpler architecture with fewer moving parts
- Typically lower latency for simple interactions
- Direct audio processing captures tone and other nuances of speech

- Limited model options, greater risk of provider lock-in
- Features may lag behind text-modality models
- Less transparency in how audio is processed
- Reduced controllability and customization options

This guide demonstrates the **sandwich architecture** to balance performance, controllability, and access to modern model capabilities. The sandwich can achieve sub-700ms latency with some STT and TTS providers while maintaining control over modular components.

### ​ Demo Application Overview

We’ll walk through building a voice-based agent using the sandwich architecture. The agent will manage orders for a sandwich shop. The application will demonstrate all three components of the sandwich architecture, using AssemblyAI for STT and Cartesia for TTS (although adapters can be built for most providers).An end-to-end reference application is available in the voice-sandwich-demo repository. We will walk through that application here.The demo uses WebSockets for real-time bidirectional communication between the browser and server. The same architecture can be adapted for other transports like telephony systems (Twilio, Vonage) or WebRTC connections.

### ​ Architecture

The demo implements a streaming pipeline where each stage processes data asynchronously:**Client (Browser)**

- Captures microphone audio and encodes it as PCM
- Establishes WebSocket connection to the backend server
- Streams audio chunks to the server in real-time
- Receives and plays back synthesized speech audio

**Server (Node.js)**

- Accepts WebSocket connections from clients
- Orchestrates the three-step pipeline: - Speech-to-text (STT): Forwards audio to the STT provider (e.g., AssemblyAI), receives transcript events
- Agent: Processes transcripts with LangChain agent, streams response tokens
- Text-to-speech (TTS): Sends agent responses to the TTS provider (e.g., Cartesia), receives audio chunks
- Returns synthesized audio to the client for playback

The pipeline uses async iterators to enable streaming at each stage. This allows downstream components to begin processing before upstream stages complete, minimizing end-to-end latency.

## ​ Setup

For detailed installation instructions and setup, see the repository README.

## ​ 1\. Speech-to-text

The STT stage transforms an incoming audio stream into text transcripts. The implementation uses a producer-consumer pattern to handle audio streaming and transcript reception concurrently.

### ​ Key Concepts

**Producer-Consumer Pattern**: Audio chunks are sent to the STT service concurrently with receiving transcript events. This allows transcription to begin before all audio has arrived.**Event Types**:

- `stt_chunk`: Partial transcripts provided as the STT service processes audio
- `stt_output`: Final, formatted transcripts that trigger agent processing

**WebSocket Connection**: Maintains a persistent connection to AssemblyAI’s real-time STT API, configured for 16kHz PCM audio with automatic turn formatting.

### ​ Implementation

Copy

import { AssemblyAISTT } from "./assemblyai";
import type { VoiceAgentEvent } from "./types";

async function* sttStream(

const stt = new AssemblyAISTT({ sampleRate: 16000 });

// Producer: pump audio chunks to AssemblyAI

try {
for await (const audioChunk of audioStream) {
await stt.sendAudio(audioChunk);
}
} finally {
await stt.close();
}
})();

// Consumer: receive transcription events

for await (const event of stt.receiveEvents()) {
passthrough.push(event);
}
})();

try {
// Yield events as they arrive
yield* passthrough;
} finally {
// Wait for producer and consumer to complete
await Promise.all([producer, consumer]);
}
}

The application implements an AssemblyAI client to manage the WebSocket connection and message parsing. See below for implementations; similar adapters can be constructed for other STT providers.

AssemblyAI Client

export class AssemblyAISTT {

const conn = await this._connection;
conn.send(buffer);
}

yield* this._bufferIterator;
}

if (this._connectionPromise) return this._connectionPromise;

const params = new URLSearchParams({
sample_rate: this.sampleRate.toString(),
format_turns: "true",
});
const url = `wss://streaming.assemblyai.com/v3/ws?${params}`;
const ws = new WebSocket(url, {
headers: { Authorization: this.apiKey },
});

const message = JSON.parse(data.toString());
if (message.type === "Turn") {
if (message.turn_is_formatted) {
this._bufferIterator.push({
type: "stt_output",
transcript: message.transcript,
ts: Date.now()
});
} else {
this._bufferIterator.push({
type: "stt_chunk",
transcript: message.transcript,
ts: Date.now()
});
}
}
});
});

return this._connectionPromise;
}
}

## ​ 2\. LangChain agent

The agent stage processes text transcripts through a LangChain agent and streams the response tokens. In this case, we stream all text content blocks generated by the agent.

### ​ Key Concepts

**Streaming Responses**: The agent uses `stream_mode="messages"` to emit response tokens as they’re generated, rather than waiting for the complete response. This enables the TTS stage to begin synthesis immediately.**Conversation Memory**: A checkpointer maintains conversation state across turns using a unique thread ID. This allows the agent to reference previous exchanges in the conversation.

### ​ Implementation

import { createAgent } from "langchain";
import { HumanMessage } from "@langchain/core/messages";
import { MemorySaver } from "@langchain/langgraph";
import { tool } from "@langchain/core/tools";
import { z } from "zod";
import { v4 as uuidv4 } from "uuid";

// Define agent tools
const addToOrder = tool(

return `Added ${quantity} x ${item} to the order.`;
},
{
name: "add_to_order",
description: "Add an item to the customer's sandwich order.",
schema: z.object({
item: z.string(),
quantity: z.number(),
}),
}
);

const confirmOrder = tool(

return `Order confirmed: ${orderSummary}. Sending to kitchen.`;
},
{
name: "confirm_order",
description: "Confirm the final order with the customer.",
schema: z.object({
orderSummary: z.string().describe("Summary of the order"),
}),
}
);

// Create agent with tools and memory
const agent = createAgent({
model: "claude-haiku-4-5",
tools: [addToOrder, confirmOrder],
checkpointer: new MemorySaver(),
systemPrompt: `You are a helpful sandwich shop assistant.
Your goal is to take the user's order. Be concise and friendly.
Do NOT use emojis, special characters, or markdown.
Your responses will be read by a text-to-speech engine.`,
});

async function* agentStream(

// Generate unique thread ID for conversation memory
const threadId = uuidv4();

for await (const event of eventStream) {
// Pass through all upstream events
yield event;

// Process final transcripts through the agent
if (event.type === "stt_output") {
const stream = await agent.stream(
{ messages: [new HumanMessage(event.transcript)] },
{
configurable: { thread_id: threadId },
streamMode: "messages",
}
);

// Yield agent response chunks as they arrive
for await (const [message] of stream) {
yield { type: "agent_chunk", text: message.text, ts: Date.now() };
}
}
}
}

## ​ 3\. Text-to-speech

The TTS stage synthesizes agent response text into audio and streams it Key Concepts

**Concurrent Processing**: The implementation merges two async streams:

- **Upstream processing**: Passes through all events and sends agent text chunks to the TTS provider
- **Audio reception**: Receives synthesized audio chunks from the TTS provider

**Streaming TTS**: Some providers (such as Cartesia) begin synthesizing audio as soon as it receives text, enabling audio play Implementation

import { CartesiaTTS } from "./cartesia";

async function* ttsStream(

const tts = new CartesiaTTS();

// Producer: read upstream events and send text to Cartesia

try {
for await (const event of eventStream) {
passthrough.push(event);
if (event.type === "agent_chunk") {
await tts.sendText(event.text);
}
}
} finally {
await tts.close();
}
})();

// Consumer: receive audio from Cartesia

for await (const event of tts.receiveEvents()) {
passthrough.push(event);
}
})();

try {
// Yield events from both producer and consumer
yield* passthrough;
} finally {
await Promise.all([producer, consumer]);
}
}

The application implements an Cartesia client to manage the WebSocket connection and audio streaming. See below for implementations; similar adapters can be constructed for other TTS providers.

Cartesia Client

export class CartesiaTTS {

if (!text || !text.trim()) return;

const conn = await this._connection;
const payload = { text, try_trigger_generation: false };
conn.send(JSON.stringify(payload));
}

protected _generateContextId(): string {
const timestamp = Date.now();
const counter = this._contextCounter++;
return `ctx_${timestamp}_${counter}`;
}

const params = new URLSearchParams({
api_key: this.apiKey,
cartesia_version: this.cartesiaVersion,
});
const url = `wss://api.cartesia.ai/tts/websocket?${params.toString()}`;
const ws = new WebSocket(url);

resolve(ws);
});

const message: CartesiaTTSResponse = JSON.parse(data.toString());
if (message.data) {
this._bufferIterator.push({
type: "tts_chunk",
audio: message.data,
ts: Date.now(),
});
} else if (message.error) {
throw new Error(`Cartesia error: ${message.error}`);
}
});
});

## ​ Putting It All Together

The complete pipeline chains the three stages together:

// using

// Chain the three stages
const transcriptEventStream = sttStream(inputStream);
const agentEventStream = agentStream(transcriptEventStream);
const outputEventStream = ttsStream(agentEventStream);

// Process pipeline and send TTS audio to client

for await (const event of outputEventStream) {
if (event.type === "tts_chunk") {
currentSocket?.send(event.audio);
}
}
})();

return {
onMessage(event) {
// Push incoming audio into pipeline
const data = event.data;
if (Buffer.isBuffer(data)) {
inputStream.push(new Uint8Array(data));
}
},
async onClose() {
inputStream.cancel();
await flushPromise;
},
};
}));

Each stage processes events independently and concurrently: audio transcription begins as soon as audio arrives, the agent starts reasoning as soon as a transcript is available, and speech synthesis begins as soon as agent text is generated. This architecture can achieve sub-700ms latency to support natural conversation.For more on building agents with LangChain, see the Agents guide.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Build a SQL agent\\
\\
Previous Build a personal assistant with subagents\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/oss/javascript/contributing/documentation

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangChain + LangGraph

Search...

Ctrl K

Navigation

Contribute

Contributing to documentation

LangChain LangGraph Deep Agents Integrations Learn Reference Contribute

TypeScript

- Overview

##### Contribute

- Documentation
- Code
- Integrations

On this page

- Contribute
- Quick edits
- Larger edits and additions
- Set up local environment
- Edit documentation
- Create a sharable preview build
- Run quality checks
- Publish to prod
- Documentation types
- How-to guides
- Conceptual guides
- Reference
- Tutorials
- Writing standards
- Mintlify components
- Page structure
- Co-locate Python and JavaScript/TypeScript content
- Quality standards
- General guidelines
- Accessibility requirements
- Cross-referencing
- Get help

Accessible documentation is a vital part of LangChain. We welcome both documentation for new features and integrations, as well as community improvements to existing docs.

These are contribution guidelines for our open source projects, but they also apply to the LangSmith documentation.

## ​ Contribute

### ​ Quick edits

For quick changes like fixing typos or changing a link, you can edit directly on GitHub without setting up a local development environment:

**Prerequisites:**

- A GitHub account
- Basic familiarity of the fork-and-pull workflow for contributing

1. At the bottom of the page you want to edit, click the link **Edit this page on GitHub**.
2. GitHub will prompt you to fork the repository to your account. Make sure to fork into your personal account.
3. Make the changes directly in GitHub’s web editor.
4. Click **Commit changes…** and give your commit a descriptive title like `fix(docs): summary of change`. If applicable, add an extended description.
5. GitHub will redirect you to create a pull request. Give it a title (often the same as the commit) and follow the PR template checklist.

Docs PRs are typically reviewed within a few days. Keep an eye on your PR to address any feedback from maintainers.Do not bump the PR unless you have new information to provide – maintainers will address it as their availability permits.

### ​ Larger edits and additions

For larger changes, additions, or ongoing contributions, it’s necessary to set up a local development environment on your machine. Our documentation build pipeline offers local preview, important for ensuring your changes appear as intended before submitting.

#### ​ Set up local environment

Before you can work on this project, ensure you have the following installed:**Required:**

- **`uv`** \- Python package manager (used for dependency management)
- **Node.js** and **`npm`** \- For Mintlify CLI and reference documentation builds
- **Make** \- For running build commands
- **Git** \- For version control

**Optional but recommended:**

- **`markdownlint-cli`** \- For linting markdown files

Copy

npm install -g markdownlint-cli

- **`pnpm`** \- Required only if you’re working on reference documentation

npm install -g pnpm@10.14.0

- **Mintlify MDX VSCode extension**

**Setup steps:**

1. Clone the `langchain-ai/docs` repo. Follow the steps outlined in `IDE_SETUP.md`.
2. Install dependencies:

make install

This command will: - Install Python dependencies using `uv sync --all-groups`
- Install Mintlify CLI globally via npm
3. Verify your setup:

make build

This should build the documentation without errors.

After install, you’ll have access to the `docs` command:

docs --help

Common commands:

- `docs dev` \- Start development mode with file watching and hot reload
- `docs build` \- Build documentation

See Available commands for more details.

#### ​ Edit documentation

**Only edit files in `src/`** – The `build/` directory is automatically generated.

1. Ensure your environment is set up and that you have followed the steps in `IDE_SETUP.md` to configure your IDE/editor to automatically apply the correct settings.
2. Edit files in `src/` - Make changes to markdown files and the build system will automatically detect changes and rebuild affected files.
- If OSS content varies between Python and JavaScript/TypeScript, add content for both in the same file. Otherwise, content will be identical for both languages.
- Use Mintlify syntax for formatting.
3. Start development mode to preview changes locally:

docs dev

This starts a development server with hot reload at `http://localhost:3000`.
4. Iterate - Continue editing and see changes reflected immediately.
- The development server rebuilds only changed files for faster feedback.
5. Run the quality checks to ensure your changes are valid.
6. Get approval from the relevant reviewers.LangChain team members can generate a sharable preview build
7. Publish to production (team members only).

#### ​ Create a sharable preview build

Only LangChain team members can create sharable preview builds.

Instructions

Previews are useful for sharing work-in-progress changes with others.When you create or update a PR, a preview branch/ID is automatically generated for you. A comment will be left on the PR with the ID, which you can then use to generate a preview. (You can also run this workflow manually if needed.)

1. Copy the preview branch’s ID from the comment.
2. In the Mintlify dashboard, click **Create preview deployment**.
3. Enter the preview branch’s ID.
4. Click **Create deployment**.
A **Manual update** will display in the **Previews** table.
5. Select the preview and click **Visit** to view the preview build.

To redeploy the preview build with the latest changes, click **Redeploy** on the Mintlify dashboard.

#### ​ Run quality checks

Before submitting changes, ensure your code passes formatting and linting checks:

# Check broken links
make mint-broken-links

# Format code automatically
make format

# Check for linting issues
make lint

# Fix markdown issues
make lint_md_fix

# Run tests to ensure your changes don't break existing functionality
make test

For more details, see the available commands section in the `README`.

#### ​ Publish to prod

Only internal team members can publish to production.

Once your branch has been merged into `main`, you need to push the changes to `prod` for them to render on the live docs site. Use the Publish documentation GH action:

1. Go to Publish documentation.
2. Click the **Run workflow** button.
3. Select the **main** branch to deploy.
4. Click **Run workflow**.

## ​ Documentation types

All documentation falls under one of four categories:

**How-to guides** \\
\\
Task-oriented instructions for users who know what they want to accomplish. **Conceptual guides** \\
\\
Explanations that provide deeper understanding and insights. **Reference** \\
\\
Technical descriptions of APIs and implementation details. **Tutorials** \\
\\
Lessons that guide users through practical activities to build understanding.

Where applicable, all documentation must have both Python and JavaScript/TypeScript content. For more details, see the co-locate Python and JavaScript/TypeScript content section.

### ​ How-to guides

How-to guides are task-oriented instructions for users who know what they want to accomplish. Examples of how-to guides are on the LangChain and LangGraph tabs.

Characteristics

- **Task-focused**: Focus on a specific task or problem
- **Step-by-step**: Break down the task into smaller steps
- **Hands-on**: Provide concrete examples and code snippets

Tips

- Focus on the **how** rather than the **why**
- Use concrete examples and code snippets
- Break down the task into smaller steps
- Link to related conceptual guides and references

Examples

- Messages
- Tools
- Streaming

### ​ Conceptual guides

Conceptual guide cover core concepts abstractly, providing deep understanding.

- **Understanding-focused**: Explain why things work as they do
- **Broad perspective**: Higher and wider view than other types
- **Design-oriented**: Explain decisions and trade-offs
- **Context-rich**: Use analogies and comparisons

- Focus on the **“why”** rather than the “how”
- Provides supplementary information not necessarily required for feature usage
- Can use analogies and reference alternatives
- Avoid blending in too much reference content
- Link to related tutorials and how-to guides

- Memory
- Context
- Graph API
- Functional API

### ​ Reference

Reference documentation contains detailed, low-level information describing exactly what functionality exists and how to use it.

**Python reference** **JavaScript/TypeScript reference**

A good reference should:

- Describe what exists (all parameters, options, return values)
- Be comprehensive and structured for easy lookup
- Serve as the authoritative source for technical details

Contributing to references

See the contributing guide for JavaScript/TypeScript reference docs.

LangChain reference best practices

- **Be consistent**; follow existing patterns for provider-specific documentation
- Include both basic usage (code snippets) and common edge cases/failure modes
- Note when features require specific versions

When to create new reference documentation

- New integrations or providers need dedicated reference pages
- Complex configuration options require detailed explanation
- API changes introduce new parameters or behavior
- Community frequently asks questions about specific functionality

### ​ Tutorials

Tutorials are longer form step-by-step guides that builds upon itself and takes users through a specific practical activity to build understanding. Tutorials are typically found on the Learn tab.

We generally do not merge new tutorials from outside contributors without an acute need. If you feel that a certain topic is missing from docs or is not sufficiently covered, please open a new issue.

- **Practical**: Focus on practical activities to build understanding.
- **Step-by-step**: Break down the activity into smaller steps.
- **Hands-on**: Provide sequential, working code snippets.
- **Supplementary**: Provide additional context and information not necessarily required for feature usage.

- Code snippets should be sequential and working if the user follows the steps in order.
- Provide some context for the activity, but link to related conceptual guides and references for more detailed information.

- Semantic search
- RAG agent

## ​ Writing standards

Reference documentation has different standards - see the reference docs contributing guide for details.

### ​ Mintlify components

Use Mintlify components to enhance readability:

- Callouts

- Structure

- Code

- Always specify language tags on code blocks (e.g., ``` ```python```, ``` ```javascript```).
- Titles for code blocks (e.g. `Success`, `Error Response`)

### ​ Page structure

Every documentation page must begin with YAML frontmatter:

title: "Clear, specific title"
sidebarTitle: "Short title for the sidebar (optional)"

### ​ Co-locate Python and JavaScript/TypeScript content

All documentation must be written in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages:

:::python
Python-specific content. In real docs, the preceding backslash (before `python`) is omitted.
:::

:::js
JavaScript/TypeScript-specific content. In real docs, the preceding backslash (before `js`) is omitted.
:::

Content for both languages (not wrapped)

This will generate two outputs (one for each language) at `/oss/python/concepts/foo.mdx` and `/oss/javascript/concepts/foo.mdx`. Each outputted page will need to be added to the `/src/docs.json` file to be included in the navigation.

We don’t want a lack of parity to block contributions. If a feature is only available in one language, it’s okay to have documentation only in that language until the other language catches up. In such cases, please include a note indicating that the feature is not yet available in the other language.If you need help translating content between Python and JavaScript/TypeScript, please ask in the community slack or tag a maintainer in your PR.

## ​ Quality standards

### ​ General guidelines

Avoid duplication

Multiple pages covering the same material are difficult to maintain and cause confusion. There should be only one canonical page for each concept or feature. Link to other guides instead of re-explaining.

Link frequently

Documentation sections don’t exist in a vacuum. Link to other sections frequently to allow users to learn about unfamiliar topics. This includes linking to API references and conceptual sections.

Be concise

Take a less-is-more approach. If another section with a good explanation exists, link to it rather than re-explain, unless your content presents a new angle.

### ​ Accessibility requirements

Ensure documentation is accessible to all users:

- Structure content for easy scanning with headers and lists
- Use specific, actionable link text instead of “click here”
- Include descriptive alt text for all images and diagrams

### ​ Cross-referencing

Use consistent cross-references to connect docs with API reference documentation.**From docs to API reference:**Use the `@[]` syntax to link to API reference pages:

See @[`ChatAnthropic`] for all configuration options.

The @[`bind_tools`][ChatAnthropic.bind_tools] method accepts...

The build pipeline transforms these into proper markdown links based on the current language scope (Python or JavaScript). For example, `@[ChatAnthropic]` becomes a link to the Python or JS API reference page depending on which version of the docs is being built, **but only if an entry exists in the `link_map.py` file!** See below for details.

How autolinks work

The `@[]` syntax is processed by `handle_auto_links.py`. It looks up link keys in `link_map.py`, which contains dictionary mappings for both Python and JavaScript scopes.**Supported formats:**

| Syntax | Result |
| --- | --- |
| `@[ChatAnthropic]` | Link with “ChatAnthropic” as the displayed text |
| ``@[`ChatAnthropic`]`` | Link with ```ChatAnthropic``` (code formatted) as text |
| `@[text][ChatAnthropic]` | Link with “text” as text and `ChatAnthropic` as the key in the link map |
| `\@[ChatAnthropic]` | Escaped: renders as literal `@[ChatAnthropic]` (no link – what’s being used on this page!) |

**Adding new links:**If a link isn’t found in the map, it will be left unchanged in the output. To add a new autolink:

1. Open `pipeline/preprocessors/link_map.py`
2. Add an entry to the appropriate scope (`python` or `js`) in `LINK_MAPS`
3. The key is the link name used in `@[key]` or `@[text][key]`, the value is the path relative to the reference host

## ​ Get help

Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please ask in the community slack or open a forum post. Internal team members can reach out in the #documentation Slack channel.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Contributing\\
\\
Previous Contributing to code\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/smith-python-sdk

Skip to content

Edit this page

# LangSmith SDK reference

![PyPI - Version](https://pypi.org/project/langsmith/#history)![PyPI - License](https://opensource.org/licenses/MIT)![PyPI - Downloads](https://pypistats.org/packages/langsmith)

Welcome to the LangSmith Python SDK reference docs! These pages detail the core interfaces you will use when building with LangSmith's Observability and Evaluations tools.

For user guides, tutorials, and conceptual overviews, please visit the LangSmith documentation.

## Quick Reference ¶

| Class/function | Description |
| --- | --- |
| `Client` | Synchronous client for interacting with the LangSmith API. |
| `AsyncClient` | Asynchronous client for interacting with the LangSmith API. |
| `traceable` | Wrapper/decorator for tracing any function. |
| `@pytest.mark.langsmith` | LangSmith `pytest` integration. |
| `wrap_openai` | Wrapper for OpenAI client, adds LangSmith tracing. |
| `wrap_anthropic` | Wrapper for Anthropic client, adds LangSmith tracing. |

## Core APIs ¶

The primary interfaces for the LangSmith SDK.

- `Client`: Synchronous client for the LangSmith API.
- `AsyncClient`: Asynchronous client for the LangSmith API.
- Run Helpers: Functions like `traceable`, `trace`, and tracing context management.
- Run Trees: Tree structure for representing runs and nested runs.
- Evaluation: Tools for evaluating functions and models on datasets.

## Additional APIs ¶

- Schemas: Data schemas and type definitions.
- Utilities: Utility classes including error types and thread pool executors.
- Wrappers: Tracing wrappers for popular LLM providers.
- Anonymizer: Tools for anonymizing sensitive data.
- Testing: Testing utilities and pytest integration.
- Expect API: Assertions and expectations for testing.

---

# https://docs.langchain.com/langsmith/cloud

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Overview

Cloud

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

On this page

- Get started
- Cloud architecture and scalability
- Architecture
- Regional storage
- Region-independent storage
- GCP services
- Allowlisting IP addresses
- Egress from LangChain SaaS
- Ingress into LangChain SaaS
- API rate limits
- Rate limit categories

If you’re ready to deploy your app to Cloud, follow the Cloud deployment quickstart or the full setup guide. This page explains the Cloud managed architecture for reference.

The **Cloud** option is a fully managed model where LangChain hosts and operates all LangSmith infrastructure and services:

- **Fully managed infrastructure**: LangChain handles all infrastructure, updates, scaling, and maintenance.
- **Deploy from GitHub**: Connect your repositories and deploy with a few clicks.
- **Automated CI/CD**: Build process is handled automatically by the platform.
- **LangSmith UI**: Full access to observability, evaluation, deployment management, and Studio.

| | **Who manages it** | **Where it runs** |
| --- | --- | --- |
| **LangSmith platform (UI, APIs, datastores)** | LangChain | LangChain’s cloud |
| **Your Agent Servers** | LangChain | LangChain’s cloud |
| **CI/CD for your apps** | LangChain | LangChain’s cloud |

## ​ Get started

To deploy your first application to Cloud, follow the Cloud deployment quickstart or refer to the comprehensive setup guide.

## ​ Cloud architecture and scalability

This section is only relevant for the cloud-managed LangSmith services available at and information on the self-hosted LangSmith solution, please refer to the self-hosted documentation.

LangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for LLM application observability, evaluation, and agent deployment

### ​ Architecture

The US-based LangSmith service is deployed in the `us-central1` (Iowa) region of GCP.

The EU-based LangSmith service is now available (as of mid-July 2024) and is deployed in the `europe-west4` (Netherlands) region of GCP. If you are interested in an enterprise plan in this region, contact our sales team.

#### ​ Regional storage

The resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses Supabase for authentication/authorization and ClickHouse Cloud for data warehouse.

| | US | EU |
| --- | --- | --- |
| URL | | |
| API URL | | |
| GCP | us-central1 (Iowa) | europe-west4 (Netherlands) |
| Supabase | AWS us-east-1 (N. Virginia) | AWS eu-central-1 (Germany) |
| ClickHouse Cloud | us-central1 (Iowa) | europe-west4 (Netherlands) |
| LangSmith deployment | us-central1 (Iowa) | europe-west4 (Netherlands) |

See the Regions FAQ for more information.

#### ​ Region-independent storage

Data listed here is stored exclusively in the US:

- Payment and billing information with Stripe and Metronome

#### ​ GCP services

LangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):

- LangSmith Frontend: serves the LangSmith UI.
- LangSmith Backend: serves the LangSmith API.
- LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)
- LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.
- LangSmith Queue: handles processing of asynchronous tasks. (Internal service)

LangSmith uses the following GCP storage services:

- Google Cloud Storage (GCS) for runs inputs and outputs.
- Google Cloud SQL PostgreSQL for transactional workloads.
- Google Cloud Memorystore for Redis for queuing and caching.
- Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.

Some additional GCP services we use include:

- Google Cloud Load Balancer for routing traffic to the LangSmith services.
- Google Cloud CDN for caching static assets.
- Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to this guide.

#### ​ Egress from LangChain SaaS

All traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:

| US | EU |
| --- | --- |
| 34.59.65.97 | 34.13.192.67 |
| 34.67.51.221 | 34.147.105.64 |
| 34.46.212.37 | 34.90.22.166 |
| 34.132.150.88 | 34.147.36.213 |
| 35.188.222.201 | 34.32.137.113 |
| 34.58.194.127 | 34.91.238.184 |
| 34.59.97.173 | 35.204.101.241 |
| 104.198.162.55 | 35.204.48.32 |

It may be helpful to allowlist these IP addresses if connecting to your own AzureOpenAI service or other endpoints that may be required by the Playground or Online Evaluation.

#### ​ Ingress into LangChain SaaS

The langchain endpoints map to the following static IP addresses:

| US | EU |
| --- | --- |
| 34.8.121.39 | 34.95.92.214 |
| 34.107.251.234 | 34.13.73.122 |

You may need to allowlist these to enable traffic from your private network to LangSmith SaaS endpoints (`api.smith.langchain.com`, `smith.langchain.com`, `beacon.langchain.com`, `eu.api.smith.langchain.com`, `eu.smith.langchain.com`, `eu.beacon.langchain.com`).

### ​ API rate limits

LangSmith enforces rate limits on API endpoints to ensure service stability and fair usage. The following table shows the rate limits for different endpoints in both US and EU regions. Note that:

- Rate limits are expressed as `count / interval` where count is the number of requests allowed within the interval (in seconds). For example, `2000 / 10` means 2000 requests per 10 seconds.
- When no HTTP method is specified in the endpoint column, the rate limit applies to all HTTP methods for that endpoint.
- When a specific method is listed (e.g., `POST`, `GET`), the rate limit applies only to that method.

| Match / Endpoint (method) | Identity key | US prod limit | EU prod limit | Category |
| --- | --- | --- | --- | --- |
| OPTIONS, `/info`, `*/v1/metadata/submit` | IP | 2000 / 10 | 2000 / 10 | High throughput |
| `/auth` | `x-api-key` | 2000 / 10 | 2000 / 10 | High throughput |
| `/auth` | `x-user-id` \+ IP | 2000 / 10 | 2000 / 10 | High throughput |
| `/v1/beacon` | IP | 2000 / 10 | 2000 / 10 | High throughput |
| `/repos` | `x-api-key` | 100 / 60 | 100 / 60 | Repository |
| `/repos` | `x-user-id` \+ IP | 100 / 60 | 100 / 60 | Repository |
| `POST /runs/batch` | `x-api-key` | 2000 / 10 | 2000 / 10 | High throughput |
| `POST /otel/v1/traces` | `x-api-key` | 2000 / 10 | 2000 / 10 | Run ingest |
| `POST` containing `/charts` | `x-api-key` | 750 / 600 | 750 / 600 | Charts |
| `POST` containing `/charts` | `x-user-id` \+ IP | 750 / 600 | 750 / 600 | Charts |
| `POST /runs/multipart` | `x-api-key` | 6000 / 10 | 6000 / 10 | Multipart ingest |
| `POST /runs/query` | `x-api-key` | 15 / 10 | 15 / 10 | Run query (API) |
| `POST /runs/query` | `x-user-id` \+ IP | 300 / 10 | 300 / 10 | Run query (User) |
| `/generate` | `x-api-key` | 30 / 3600 | 30 / 3600 | Generation |
| `/generate` | `x-user-id` \+ IP | 30 / 3600 | 30 / 3600 | Generation |
| `/commits` | `x-api-key` | 10000 / 60 | 2000 / 60 | Commits |
| `/commits` | `x-user-id` \+ IP | 10000 / 60 | 2000 / 60 | Commits |
| `DELETE /sessions` or `*/trigger` | `x-api-key` | 10 / 60 | 10 / 60 | Deletion |
| `DELETE /sessions` or `*/trigger` | `x-user-id` \+ IP | 30 / 60 | 30 / 60 | Deletion |
| `POST /runs` (single run ingest) | `x-api-key` | 2000 / 10 | 2000 / 10 | Run ingest |
| `PATCH` containing `/runs` | `x-api-key` | 2000 / 10 | 2000 / 10 | Run ingest |
| `POST /feedback` | `x-api-key` | 2000 / 10 | 2000 / 10 | High throughput |
| `GET /runs/{uuid}` or `/api/v1/runs/{uuid}` | `x-api-key` | 30 / 60 | 30 / 60 | Run lookup |
| `GET` containing `/examples` | `x-api-key` | 5000 / 60 | 5000 / 60 | Examples |
| Any request with `x-api-key` | `x-api-key` | 1000 / 10 | 1000 / 10 | Default (API key) |
| Any request with `x-user-id` | `x-user-id` \+ IP | 1000 / 10 | 1000 / 10 | Default (User) |
| `/public/download` | IP | 5000 / 60 | 5000 / 60 | Public download |
| `/runs/stats` | `x-api-key` | 1 / 10 | 20 / 10 | Stats |
| All other IPs (catch-all) | IP | 100 / 60 | 100 / 60 | Public (catch-all) |

#### ​ Rate limit categories

- **High throughput**: General high-volume endpoints for core operations like authentication, metadata, and feedback.
- **Repository**: Repository and prompt management operations.
- **Run ingest**: Individual trace/run ingestion endpoints for observability.
- **Charts**: Chart generation and visualization endpoints.
- **Multipart ingest**: Bulk run ingestion via multipart upload for high-volume tracing.
- **Run query (API)**: API key-based run query operations with stricter limits for complex queries.
- **Run query (User)**: User-based run query operations with higher limits for interactive use.
- **Generation**: AI-powered code and content generation endpoints (limited to prevent abuse).
- **Commits**: Prompt versioning and commit operations.
- **Deletion**: Session deletion and workflow trigger operations.
- **Run lookup**: Retrieving specific runs by UUID.
- **Examples**: Fetching dataset examples for few-shot prompting.
- **Default (API key)**: Fallback rate limit for authenticated API requests not matching specific patterns.
- **Default (User)**: Fallback rate limit for authenticated user requests not matching specific patterns.
- **Public download**: High-volume public download endpoints for shared resources.
- **Stats**: Run statistics and analytics endpoints (region-specific limits apply).
- **Public (catch-all)**: Default rate limit for unauthenticated public access.

For more information on rate limits and other service limits, refer to the Administration overview.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Set up LangSmith\\
\\
Previous Self-hosted on AWS\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/aws-self-hosted

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Self-hosted cloud architecture

Self-hosted on AWS

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

On this page

- Reference architecture
- Compute options
- AWS Well-Architected best practices
- Operational excellence
- Security
- Reliability
- Performance efficiency
- Cost optimization
- Sustainability
- Security and compliance
- Monitoring and evals

When running LangSmith on Amazon Web Services (AWS), you can set up in either full self-hosted or hybrid mode. Full self-hosted mode deploys a complete LangSmith platform with observability functionality as well as the option to create agent deployments. Hybrid mode entails just the infrastructure to run agents in a data plane within your cloud, while our SaaS provides the control plane and observability functionality.This page provides AWS-specific architecture patterns, service recommendations, and best practices for deploying and operating LangSmith on AWS.

LangChain provides Terraform modules specifically for AWS to help provision infrastructure for LangSmith. These modules can quickly set up EKS clusters, RDS, ElastiCache, S3, and networking resources.View the AWS Terraform modules for documentation and examples.

## ​ Reference architecture

We recommend leveraging AWS’s managed services to provide a scalable, secure, and resilient platform. The following architecture applies to both self-hosted and hybrid and aligns with the AWS Well-Architected Framework:!Architecture diagram showing AWS relations to LangSmith services

- **Ingress & networking**: Requests enter via Amazon Application Load Balancer (ALB) within your VPC, secured using AWS WAF and IAM-based authentication.
- **Frontend & backend services:** Containers run on Amazon EKS, orchestrated behind the ALB. routes requests to other services within the cluster as necessary.
- **Storage & databases:** - Amazon RDS for PostgreSQL or Aurora: metadata, projects, users, and short-term and long-term memory for deployed agents. LangSmith supports PostgreSQL version 14 or higher.
- Amazon ElastiCache (Redis): caching and job queues. ElastiCache can be in single-instance or cluster mode, running Redis OSS version 5 or higher.
- ClickHouse + Amazon EBS: analytics and trace storage.

- We recommend using an externally managed ClickHouse solution unless security or compliance reasons
prevent you from doing so.
- ClickHouse is not required for hybrid deployments.
- Amazon S3: object storage for trace artifacts and telemetry.
- **LLM integration:** Optionally proxy requests to Amazon Bedrock or Amazon SageMaker for LLM inference.
- **Monitoring & observability:** Integrate with Amazon CloudWatch

## ​ Compute options

LangSmith supports multiple compute options depending on your requirements:

| Compute option | Description | Suitable for |
| --- | --- | --- |
| **Elastic Kubernetes Service (preferred)** | Advanced scaling and multi-tenant support | Large enterprises |
| **EC2-based** | Full control, BYO-infra | Regulated or air-gapped environments |

## ​ AWS Well-Architected best practices

This reference is designed to align with the six pillars of the AWS Well-Architected Framework:

### ​ Operational excellence

- Automate deployments with IaC ( CloudFormation / Terraform).
- Use AWS Systems Manager Parameter Store for configuration.
- Configure your LangSmith instance to export telemetry data and continuously monitor via CloudWatch Logs.
- The preferred method to manage LangSmith deployments is to create a CI process that builds Agent Server images and pushes them to ECR. Create a test deployment for pull requests before deploying a new revision to staging or production upon PR merge.

### ​ Security

- Use IAM roles with least-privilege policies.
- Enable encryption at rest ( RDS, S3, ClickHouse volumes) and in transit (TLS 1.2+).
- Integrate with AWS Secrets Manager for credentials.
- Use Amazon Cognito as an IDP in conjunction with LangSmith’s built-in authentication and authorization features to secure access to agents and their tools.

### ​ Reliability

- Replicate the LangSmith data plane across regions: Deploy identical data planes to Kubernetes clusters in different regions for LangSmith Deployment. Deploy RDS and ECS services across Multi-AZ.
- Implement auto-scaling for backend workers.
- Use Amazon Route 53 health checks and failover policies.

### ​ Performance efficiency

- Leverage EC2 instances for optimized compute.
- Use S3 Intelligent-Tiering for infrequently accessed trace data.

### ​ Cost optimization

- Right-size EKS clusters using Compute Savings Plans.
- Monitor cost KPIs using AWS Cost Explorer dashboards.

### ​ Sustainability

- Minimize idle workloads with on-demand compute.
- Store telemetry in low-latency, low-cost tiers.
- Enable auto-shutdown for non-prod environments.

## ​ Security and compliance

LangSmith can be configured for:

- PrivateLink-only access (no public internet exposure, besides egress necessary for billing).
- KMS-based encryption keys for S3, RDS, and EBS.
- Audit logging to CloudWatch and AWS CloudTrail.

Customers can deploy in GovCloud, ISO, or HIPAA regions as needed.

## ​ Monitoring and evals

Use LangSmith to:

- Capture traces from LLM apps running on Bedrock or SageMaker.
- Evaluate model outputs via LangSmith datasets.
- Track latency, token usage, and success rates.

Integrate with:

- AWS CloudWatch dashboards.
- OpenTelemetry and Prometheus exporters.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Cloud\\
\\
Previous Self-hosted on Azure\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/azure-self-hosted

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Self-hosted cloud architecture

Self-hosted on Azure

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

On this page

- Reference architecture
- Compute and networking on Azure
- Azure Kubernetes Service (AKS)
- Network model
- Ingress and load balancing
- Web Application Firewall (WAF)
- Network policies
- High availability
- Networking and identity
- Virtual network integration
- Authentication
- Storage and data services
- Azure Database for PostgreSQL
- High availability
- Backups and disaster recovery
- Scaling
- Azure Managed Redis
- Persistence and redundancy
- ClickHouse on Azure
- Azure Blob Storage
- Redundancy
- Naming and partitioning
- Networking
- Security and access control
- Azure Key Vault
- Separate vaults per application and environment
- Access control
- Data protection and logging
- Network security
- Ingress isolation
- RBAC and pod security
- Secrets management
- Observability and monitoring
- Azure Monitor
- Managed Prometheus and Grafana
- Container Insights
- Application logging
- Continuous integration

When running LangSmith on Microsoft Azure, you can set up in either full self-hosted or hybrid mode. Full self-hosted mode deploys a complete LangSmith platform with observability functionality as well as the option to create agent deployments. Hybrid mode entails just the infrastructure to run agents in a data plane within your cloud, while our SaaS provides the control plane and observability functionality.This page provides Azure-specific architecture patterns, service recommendations, and best practices for deploying and operating LangSmith on Azure.

LangChain provides Terraform modules specifically for Azure to help provision infrastructure for LangSmith. These modules can quickly set up AKS clusters, Azure Database for PostgreSQL, Azure Managed Redis, Blob Storage, and networking resources.View the Azure Terraform modules for documentation and examples.

## ​ Reference architecture

We recommend using Azure’s managed services to provide a scalable, secure, and resilient platform. The following architecture applies to both self-hosted and hybrid deployments:!Architecture diagram showing Azure relations to LangSmith services

- **Client interfaces**: Users interact with LangSmith via a web browser or the LangChain SDK. All traffic terminates at an Azure Load Balancer and is routed to the frontend (NGINX) within the AKS cluster before being routed to another service within the cluster if necessary.
- **Storage services**: The platform requires persistent storage for traces, metadata and caching. On Azure the recommended services are:

- **Azure Database for PostgreSQL (Flexible Server)** for transactional data (e.g., runs, projects). Azure’s high-availability options provision a standby replica in another zone; data is synchronously committed to both primary and standby servers. LangSmith requires PostgreSQL version 14 or higher.
- **Azure Managed Redis** for queues and caching. Best practices include storing small values and breaking large objects into multiple keys, using pipelining to maximize throughput and ensuring the client and server reside in the same region. You can also use Azure Cache for Redis, running either in single-instance or cluster mode. LangSmith requires Redis OSS version 5 or higher.
- **ClickHouse** for high-volume analytics of traces. We recommend using an externally managed ClickHouse solution. If, for security or compliance reasons, that is not an option, deploy a ClickHouse cluster on AKS using the open-source operator. Ensure replication across availability zones for durability. Clickhouse is not required for a hybrid deployment.
- **Azure Blob Storage** for large artifacts. Use redundant storage configurations such as read-access geo-redundant (RA-GRS) or geo-zone-redundant (RA-GZRS) storage and design applications to read from the secondary region during an outage.

## ​ Compute and networking on Azure

### ​ Azure Kubernetes Service (AKS)

AKS is the recommended compute platform for production deployments. This section outlines the key considerations for planning your setup.

#### ​ Network model

Use Azure CNI networking for production clusters. This model integrates the cluster into an existing virtual network, assigns IP addresses to each pod and node, and allows direct connectivity to on-premises or other Azure services. Ensure the subnet has enough IPs for nodes and pods, avoid overlapping address ranges and allocate additional IP space for scale-out events.

#### ​ Ingress and load balancing

Use Kubernetes Ingress resources and controllers to distribute HTTP/HTTPS traffic. Ingress controllers operate at layer 7 and can route traffic based on URL paths and handle TLS termination. They reduce the number of public IP addresses compared to layer-4 load balancers. Use the application routing add-on for managed NGINX ingress controllers integrated with Azure DNS and Key Vault for SSL certificates.

#### ​ Web Application Firewall (WAF)

For additional protection against attacks, deploy a WAF such as Azure Application Gateway. A WAF filters traffic using OWASP rules and can terminate TLS before the traffic reaches your AKS cluster.

#### ​ Network policies

Apply Kubernetes network policies to restrict pod-to-pod traffic and reduce the impact of compromised workloads. Enable network policy support when creating the cluster and design rules based on application connectivity.

#### ​ High availability

Configure node pools across availability zones and use Pod Disruption Budgets (PDB) and multiple replicas for all deployments. Set pod resource requests and limits; the AKS resource management best practices recommend setting CPU and memory limits to prevent pods from consuming all resources. Use Cluster Autoscaler and Vertical Pod Autoscaler to scale node pools and adjust pod resources automatically.

#### ​ Virtual network integration

Deploy AKS into its own virtual network and create separate subnets for the cluster, database, Redis, and storage endpoints. Use Private Link and service endpoints to keep traffic within your virtual network and avoid exposure to the public internet.

#### ​ Authentication

Integrate LangSmith with Microsoft Entra ID (Azure AD) for single sign-on. Use Azure AD OAuth2 for bearer tokens and assign roles to control access to the UI and API.

## ​ Storage and data services

#### ​ High availability

Use Flexible Server with high-availability mode. Azure provisions a standby replica either within the same availability zone (zonal) or across zones (zone-redundant). Data is synchronously committed to both the primary and standby servers, ensuring that committed data is not lost. Zone-redundant configurations place the standby in a different zone to protect against zone outages but may add write latency.

#### ​ Backups and disaster recovery

Enable automatic backups and configure geo-redundant backup storage to protect against region-wide outages. For critical applications, create read replicas in a secondary region.

#### ​ Scaling

Choose an appropriate SKU that matches your workload; Flexible Server allows scaling compute and storage independently. Monitor metrics and configure alerts through Azure Monitor.

#### ​ Persistence and redundancy

Choose a tier that provides replication and persistence. Configure Redis persistence or data backup for durability. For high-availability, use active geo-replication or zone-redundant caches depending on the tier.

### ​ ClickHouse on Azure

ClickHouse is used for analytical workloads (traces and feedback). If you cannot use an externally managed solution, deploy a ClickHouse cluster on AKS using Helm or the official operator. For resilience, replicate data across nodes and availability zones. Consider using Azure Disks for local storage and mount them as StatefulSets.

#### ​ Redundancy

Choose a redundancy configuration based on your recovery objectives. Use read-access geo-redundant (RA-GRS) or geo-zone-redundant (RA-GZRS) storage and design applications to switch reads to the secondary region during a primary region outage.

#### ​ Naming and partitioning

Use naming conventions that improve load balancing across partitions and plan for the maximum number of concurrent clients. Stay within Azure’s scalability and capacity targets and partition data across multiple storage accounts if necessary.

#### ​ Networking

Access blob storage through private endpoints or by using SAS tokens and CORS rules to enable direct client access.

## ​ Security and access control

#### ​ Separate vaults per application and environment

Store secrets such as database connection strings and API keys in Azure Key Vault. Use a dedicated vault for each application and environment (dev, test, prod) to limit the impact of a security breach.

#### ​ Access control

Use the RBAC permission model to assign roles at the vault scope and restrict access to required principals. Restrict network access using Private Link and firewalls.

#### ​ Data protection and logging

Enable soft delete and purge protection to prevent accidental deletion. Turn on logging and configure alerts for Key Vault access events.

#### ​ Ingress isolation

Expose only the frontend service through the ingress controller or WAF. Other services should be internal and communicate through cluster networking.

#### ​ RBAC and pod security

Use Kubernetes RBAC to control who can deploy, modify, or read resources. Enable pod security admission to enforce baseline, restricted, or privileged profiles.

#### ​ Secrets management

Mount secrets from Key Vault into pods using CSI Secret Store. Avoid storing secrets in environment variables or configuration files.

## ​ Observability and monitoring

Configure your LangSmith instance to export telemetry data so you can use Azure’s services to monitor it.

### ​ Azure Monitor

Use Azure Monitor for metrics, logs, and alerting. Proactive monitoring involves configuring alerts on key signals like node CPU/memory utilization, pod status, and service latency. Azure Monitor alerts notify you when predefined thresholds are exceeded.

### ​ Managed Prometheus and Grafana

Enable Azure Monitor managed Prometheus to collect Kubernetes metrics. Combine it with Grafana dashboards for visualization. Define service-level objectives (SLOs) and configure alerts accordingly.

### ​ Container Insights

Install Container Insights to capture logs and metrics from AKS nodes and pods. Use Azure Log Analytics workspaces to query and analyze logs.

### ​ Application logging

Ensure LangSmith services emit logs to stdout/stderr and forward them via Fluent Bit or the Azure Monitor agent.

## ​ Continuous integration

- The preferred method to manage LangSmith deployments is to create a CI process that builds Agent Server images and pushes them to Azure Container Registry. Create a test deployment for pull requests before deploying a new revision to staging or production upon PR merge.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Self-hosted on AWS\\
\\
Previous Hybrid\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/hybrid

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Hybrid

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

On this page

- Workflow
- Architecture
- Compute Platforms
- Egress to LangSmith and the control plane
- Listeners
- Kubernetes cluster organization
- LangSmith workspace organization
- Use Cases
- Each LangSmith workspace → separate Kubernetes cluster
- One cluster, one namespace per workspace
- Separate clusters, with shared “dev” cluster

**Important**
The hybrid option requires an Enterprise plan.

The **hybrid** model splits LangSmith infrastructure between LangChain’s cloud and yours:

- **Control plane** (LangSmith UI, APIs, and orchestration) runs in LangChain’s cloud, managed by LangChain.
- **Data plane** (your Agent Servers and agent workloads) runs in your cloud, managed by you.

This combines the convenience of a managed interface with the flexibility of running workloads in your own environment.

Learn more about the control plane, data plane, and Agent Server architecture concepts.

| Component | Responsibilities | Where it runs | Who manages it |
| --- | --- | --- | --- |

When running LangSmith in a hybrid model, you authenticate with a LangSmith API key.

### ​ Workflow

1. Use the `langgraph-cli` or Studio to test your graph locally.
2. Build a Docker image using the `langgraph build` command.
3. Deploy your Agent Server from the control plane UI.

Supported Compute Platforms: Kubernetes.

For setup, refer to the Hybrid setup guide.

### ​ Compute Platforms

- **Kubernetes**: Hybrid supports running the data plane on any Kubernetes cluster.

For setup in Kubernetes, refer to the Hybrid setup guide

### ​ Egress to LangSmith and the control plane

In the hybrid deployment model, your self-hosted data plane will send network requests to the control plane to poll for changes that need to be implemented in the data plane. Traces from data plane deployments also get sent to the LangSmith instance integrated with the control plane. This traffic to the control plane is encrypted, over HTTPS. The data plane authenticates with the control plane with a LangSmith API key.In order to enable this egress, you may need to update internal firewall rules or cloud resources (such as Security Groups) to allow certain IP addresses.

AWS/Azure PrivateLink or GCP Private Service Connect is currently not supported. This traffic will go over the internet.

## ​ Listeners

In the hybrid option, one or more “listener” applications can run depending on how your LangSmith workspaces and Kubernetes clusters are organized.

### ​ Kubernetes cluster organization

- One or more listeners can run in a Kubernetes cluster.
- A listener can deploy into one or more namespaces in that cluster.
- Multiple listeners cannot deploy to the same namespace.
- Cluster owners are responsible for planning listener layout and Agent Server deployments.

### ​ LangSmith workspace organization

- A workspace can be associated with one or more listeners.
- A listener can only be associated with one workspace. LangSmith workspace to listener is a one-to-many relationship.
- A workspace can only deploy to Kubernetes clusters where all of its listeners are deployed.

## ​ Use Cases

Here are some common listener configurations (not strict requirements):

### ​ Each LangSmith workspace → separate Kubernetes cluster

- Cluster `alpha` runs workspace `A`
- Cluster `beta` runs workspace `B`

### ​ One cluster, one namespace per workspace

- Cluster `alpha`, namespace `1` runs workspace `A`
- Cluster `alpha`, namespace `2` runs workspace `B`

### ​ Separate clusters, with shared “dev” cluster

- Cluster `alpha` runs workspace `A`
- Cluster `beta` runs workspace `B`
- Cluster `dev` runs workspaces `A` and `B`
- Both workspaces have two listeners; cluster `dev` has two listener deployments

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Self-hosted on Azure\\
\\
Previous Set up hybrid LangSmith\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deploy-hybrid

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Hybrid

Set up hybrid LangSmith

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

On this page

- Kubernetes
- Prerequisites
- Setup
- Configuring additional data planes in the same cluster
- Next steps

**Important**
The Hybrid deployment option requires an Enterprise plan.

The **hybrid** model lets you run the data plane—your Agent Server deployments and agent workloads—in your own cloud, while LangChain hosts and manages the control plane (the LangSmith UI and orchestration). This setup gives you the flexibility of self-hosting your runtime environments with the convenience of a managed LangSmith instance.The following steps describe how to connect your self-hosted data plane to the managed LangSmith control plane.

## ​ Kubernetes

### ​ Prerequisites

1. `KEDA` is installed on your cluster.

Copy

helm repo add kedacore
helm install keda kedacore/keda --namespace keda --create-namespace

2. A valid `Ingress` controller is installed on your cluster. For more information about configuring ingress for your deployment, refer to Create an ingress for installations. We highly recommend using the modern Gateway API in a production setup.
3. If you plan to have the listener watch multiple namespaces, you **MUST** use the Gateway API or an Istio Gateway instead of the standard ingress resource. A standard ingress resource can only route traffic to services in the same namespace, whereas a Gateway or Istio Gateway can route traffic to services across multiple namespaces.
4. You have slack space in your cluster for multiple deployments. `Cluster-Autoscaler` is recommended to automatically provision new nodes.
5. You will need to enable egress to two control plane URLs. The listener polls these endpoints for deployments:
-
-

### ​ Setup

1. Provide your LangSmith organization ID to us. Your LangSmith organization will be configured to deploy the data plane in your cloud.
2. Create a listener from the LangSmith UI. The `Listener` data model is configured for the actual “listener” application.

2. In the top-right of the page, select `+ Create Listener`.
3. Enter a unique `Compute ID` for the listener. The `Compute ID` is a user-defined identifier that should be unique across all listeners in the current LangSmith workspace. The `Compute ID` is displayed to end users when they are creating a new deployment. Ensure that the `Compute ID` provides context to the end user about where their Agent Server deployments will be deployed to. For example, a `Compute ID` can be set to `k8s-cluster-name-dev-01`. In this example, the name of the Kubernetes cluster is `k8s-cluster-name`, `dev` denotes that the cluster is reserved for “development” workloads, and `01` is a numerical suffix to reduce naming collisions.
4. Enter one or more Kubernetes namespaces. Later, the “listener” application will be configured to deploy to each of these namespaces.
5. In the top-right on the page, select `Submit`.
6. After the listener is created, copy the listener ID. You will use it later when installing the actual “listener” application in the Kubernetes cluster (step 5).

**Important**
Creating a listener from the LangSmith UI does not install the “listener” application in the Kubernetes cluster.

3. A Helm chart is provided to install the necesssary components in your Kubernetes cluster. - `langgraph-dataplane-listener`: This is a service that listens to LangChain’s control plane for changes to your deployments and creates/updates downstream CRDs. This is the “listener” application.
- `LangGraphPlatform CRD`: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith Deployment.
- `langgraph-dataplane-operator`: This operator handles changes to your LangSmith CRDs.
- `langgraph-dataplane-redis`: A Redis instance is used by the `langgraph-dataplane-listener` to manage various tasks (mainly creating and deleting deployments).
4. Configure your `langgraph-dataplane-values.yaml` file.

config:
langsmithApiKey: "" # API Key of your Workspace
langsmithWorkspaceId: "" # Workspace ID
hostBackendUrl: "https://api.host.langchain.com" # Only override this if on EU
smithBackendUrl: "https://api.smith.langchain.com" # Only override this if on EU
langgraphListenerId: "" # Listener ID from Step 2f
watchNamespaces: "" # comma-separated list of Kubernetes namespaces that the listener and operator will deploy to
enableLGPDeploymentHealthCheck: true # enable/disable health check step for deployments

ingress:
hostname: "" # specify a hostname that will be configured for all deployments

operator:
enabled: true
createCRDs: true # set this to `false` if the CRD has been previously installed in the current Kubernetes cluster

- `config.langsmithApiKey`: The `langgraph-listener` deployment authenticates with LangChain’s LangGraph control plane API with the `langsmithApiKey`.
- `config.langsmithWorkspaceId`: The `langgraph-listener` deployment is coupled to Agent Server deployments in the LangSmith workspace. In other words, the `langgraph-listener` deployment can only manage Agent Server deployments in the specified LangSmith workspace ID.
- `config.langgraphListenerId`: In addition to being coupled with a LangSmith workspace, the `langgraph-listener` deployment is also coupled to a listener. When a new Agent Server deployment is created, it is automatically coupled to a `langgraphListenerId`. Specifying `langgraphListenerId` ensures that the `langgraph-listener` deployment can only manage Agent Server deployments that are coupled to `langgraphListenerId`.
- `config.watchNamespaces`: A comma-separated list of Kubernetes namespaces that the `langgraph-listener` deployment will deploy to. This list should match the list of namespaces specified in step 2d.
- `config.enableLGPDeploymentHealthCheck`: To disable the Agent Server health check, set this to `false`.
- `ingress.hostname`: As part of the deployment workflow, the `langgraph-listener` deployment attempts to call the Agent Server health check endpoint (`GET /ok`) to verify that the application has started up correctly. A typical setup involves creating a shared DNS record or domain for Agent Server deployments. This is not managed by LangSmith. Once created, set `ingress.hostname` to the domain, which will be used to complete the health check.
- `operator.createCRDs`: Set this value to `false` if the Kubernetes cluster already has the `LangGraphPlatform CRD` installed. During installation, an error will occur if the CRD is already installed. This situation may occur if multiple listeners are deployed on the same Kubernetes cluster.
5. Deploy `langgraph-dataplane` Helm chart.

helm repo add langchain
helm repo update
helm upgrade -i langgraph-dataplane langchain/langgraph-dataplane --values langgraph-dataplane-values.yaml --wait --debug

6. If successful, you will see three services start up in your namespace.

NAME READY STATUS RESTARTS AGE
langgraph-dataplane-listener-6dd4749445-zjmr4 0/1 ContainerCreating 0 26s
langgraph-dataplane-operator-6b88879f9b-t76gk 1/1 Running 0 26s
langgraph-dataplane-redis-0 1/1 Running 0 25s

Your hybrid infrastructure is now ready to create deployments.

### ​ Configuring additional data planes in the same cluster

To create a data plane in a different namespace in the same cluster, repeat the above steps and pass a `-n` option to `helm upgrade` to specify a different namespace.**When installing multiple data planes in the same cluster, it is very important to follow the rules below:**

1. The `config.watchNamespaces` list should never intersect with other installations `config.watchNamespaces`. For example, if installation A is watching namespaces `foo,bar`, installation B cannot watch either `foo` or `bar`. Multiple operators or listeners watching the same namespace will lead to unexpected behavior. This means that multiple LangSmith workspaces cannot deploy to the same namespace! Please review the cluster organization section to understand this better.
2. It is required to use the Gateway API or an Istio Gateway. Relying on the standard ingress resource can cause conflicts with Ingress objects created by other data planes in the same cluster. Because behavior in these cases depends on the specific ingress controller, this may result in unpredictable or undesired outcomes.

## ​ Next steps

Once your infrastructure is set up, you’re ready to deploy applications. See the deployment guides in the Deployment tab for instructions on building and deploying your applications.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Hybrid\\
\\
Previous Self-hosted LangSmith\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/cloud)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment Self-hosted LangSmith Self-host LangSmith with Docker

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/aws-self-hosted)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

404

# Page not found

We couldn’t find the page you were looking for.

Self-host LangSmith with Docker Self-hosted on AWS Self-host LangSmith on Kubernetes

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/azure-self-hosted)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

404

# Page not found

We couldn’t find the page you were looking for.

Self-host LangSmith with Docker Self-hosted on Azure Self-host LangSmith on Kubernetes

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/hybrid)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

404

# Page not found

We couldn’t find the page you were looking for.

Set up hybrid LangSmith LangSmith Deployment LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/deploy-hybrid)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

404

# Page not found

We couldn’t find the page you were looking for.

Set up hybrid LangSmith LangSmith Deployment

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/self-hosted)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

404

# Page not found

We couldn’t find the page you were looking for.

Self-hosted LangSmith changelog Self-hosted LangSmith Self-host LangSmith with Docker

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/observability),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluation),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Evaluation LangSmith docs Implement a CI/CD pipeline using LangSmith Deployment and Evaluation

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/prompt-engineering),

Skip to main content,#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

##### Create and update prompts

- Create a prompt
- Manage prompts
- Manage prompts programmatically
- Configure prompt settings
- Use tools in a prompt
- Include multimodal content in a prompt
- Write your prompt with AI
- Connect to models

##### Tutorials

- Optimize a classifier
- Sync prompts with GitHub
- Test multi-turn conversations

404

# Page not found

We couldn’t find the page you were looking for.

Prompt engineering quickstart Prompt engineering Prompt engineering concepts

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/cloud):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Deployment Self-hosted LangSmith Self-host LangSmith with Docker

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/hybrid):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

404

# Page not found

We couldn’t find the page you were looking for.

Set up hybrid LangSmith LangSmith Deployment LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/self-hosted):

Skip to main content:#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

##### Overview

- Set up LangSmith
- Cloud (SaaS)

##### Self-hosted cloud architecture

- AWS
- Azure

##### Hybrid

- Overview
- Setup guide

##### Self-hosted

- Overview
- Setup guides

- Configuration

- Connect external services

- Platform auth & access control

- Self-hosted observability

- Scripts for management tasks

404

# Page not found

We couldn’t find the page you were looking for.

Self-hosted LangSmith changelog Self-hosted LangSmith Self-host LangSmith with Docker

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/env-var

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

LangSmith Deployment

Environment variables

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

On this page

- BG\_JOB\_ISOLATED\_LOOPS
- BG\_JOB\_SHUTDOWN\_GRACE\_PERIOD\_SECS
- BG\_JOB\_TIMEOUT\_SECS
- OTEL\_EXPORTER\_OTLP\_TRACES\_ENDPOINT
- DD\_API\_KEY
- LANGCHAIN\_TRACING\_SAMPLING\_RATE
- LANGGRAPH\_AUTH\_TYPE
- LANGGRAPH\_POSTGRES\_POOL\_MAX\_SIZE
- LANGSMITH\_API\_KEY
- LANGSMITH\_ENDPOINT
- LANGSMITH\_TRACING
- LOG\_COLOR
- LOG\_LEVEL
- LOG\_JSON
- MOUNT\_PREFIX
- N\_JOBS\_PER\_WORKER
- POSTGRES\_URI\_CUSTOM
- REDIS\_CLUSTER
- REDIS\_KEY\_PREFIX
- REDIS\_URI\_CUSTOM
- REDIS\_MAX\_CONNECTIONS
- RESUMABLE\_STREAM\_TTL\_SECONDS

The Agent Server supports specific environment variables for configuring a deployment.

## ​ `BG_JOB_ISOLATED_LOOPS`

Set `BG_JOB_ISOLATED_LOOPS` to `True` to execute background runs in an isolated event loop separate from the serving API event loop.This environment variable should be set to `True` if the implementation of a graph/node contains synchronous code. In this situation, the synchronous code will block the serving API event loop, which may cause the API to be unavailable. A symptom of an unavailable API is continuous application restarts due to failing health checks.Defaults to `False`.

## ​ `BG_JOB_SHUTDOWN_GRACE_PERIOD_SECS`

Specifies, in seconds, how long the server will wait for background jobs to finish after the queue receives a shutdown signal. After this period, the server will force termination. Defaults to `180` seconds. The maximum value is `3600` seconds. Set this to ensure jobs have enough time to complete cleanly during shutdown. Added in `langgraph-api==0.2.16`.

## ​ `BG_JOB_TIMEOUT_SECS`

The timeout of a background run can be increased. However, the infrastructure for a Cloud deployment enforces a 1 hour timeout limit for API requests. This means the connection between client and server will timeout after 1 hour. This is not configurable.A background run can execute for longer than 1 hour, but a client must reconnect to the server (e.g. join stream via `POST /threads/{thread_id}/runs/{run_id}/stream`) to retrieve output from the run if the run is taking longer than 1 hour.Defaults to `3600`.

## ​ `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT`

Specify `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT` to configure OpenTelemetry APM tracing for the deployment. Specify other `OTEL_*` environment variables to configure tracing, logging, and other instrumentation.

Copy

# If you set OTEL_EXPORTER_OTLP_TRACES_ENDPOINT or OTEL_EXPORTER_OTLP_ENDPOINT,
# the server starts with OpenTelemetry instrumentation enabled.

OTEL_SERVICE_NAME=MY_LANGSMITH_DEPLOYMENT

OTEL_ATTRIBUTE_VALUE_LENGTH_LIMIT=4095
OTEL_EXPORTER_OTLP_COMPRESSION=gzip
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE=delta
# Optional: OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true

For example, to submit OpenTelemetry traces to New Relic’s US region, set the following:

OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://otlp.nr-data.net/v1/traces
OTEL_EXPORTER_OTLP_ENDPOINT=https://otlp.nr-data.net

OTel tracing was added in Agent Server version `0.5.32` and is currently in Alpha.

## ​ `DD_API_KEY`

Specify `DD_API_KEY` (your Datadog API Key) to automatically enable Datadog tracing for the deployment. Specify other `DD_*` environment variables to configure the tracing instrumentation.If `DD_API_KEY` is specified, the application process is wrapped in the `ddtrace-run` command. Other `DD_*` environment variables (e.g. `DD_SITE`, `DD_ENV`, `DD_SERVICE`, `DD_TRACE_ENABLED`) are typically needed to properly configure the tracing instrumentation. See `DD_*` environment variables for more details. You can enable `DD_TRACE_DEBUG=true` and set `DD_LOG_LEVEL=debug` to troubleshoot.

Enabling `DD_API_KEY` (and thus `ddtrace-run`) can override or interfere with other auto-instrumentation solutions (such as OpenTelemetry) that you may have instrumented into your application code.

## ​ `LANGCHAIN_TRACING_SAMPLING_RATE`

Sampling rate for traces sent to LangSmith. Valid values: Any float between `0` and `1`.For more details, refer to Set a sampling rate for traces.

## ​ `LANGGRAPH_AUTH_TYPE`

Type of authentication for the Agent Server deployment. Valid values: `langsmith`, `noop`.For deployments to LangSmith, this environment variable is set automatically. For local development or deployments where authentication is handled externally (e.g. self-hosted), set this environment variable to `noop`.

## ​ `LANGGRAPH_POSTGRES_POOL_MAX_SIZE`

Beginning with langgraph-api version `0.2.12`, the maximum size of the Postgres connection pool (per replica) can be controlled using the `LANGGRAPH_POSTGRES_POOL_MAX_SIZE` environment variable. By setting this variable, you can determine the upper bound on the number of simultaneous connections the server will establish with the Postgres database.For example, if a deployment is scaled up to 10 replicas and `LANGGRAPH_POSTGRES_POOL_MAX_SIZE` is configured to `150`, then up to `1500` connections to Postgres can be established. This is particularly useful for deployments where database resources are limited (or more available) or where you need to tune connection behavior for performance or scaling reasons.Defaults to `150` connections.

## ​ `LANGSMITH_API_KEY`

For deployments with self-hosted LangSmith only.To send traces to a self-hosted LangSmith instance, set `LANGSMITH_API_KEY` to an API key created from the self-hosted instance.

## ​ `LANGSMITH_ENDPOINT`

For deployments with self-hosted LangSmith only.To send traces to a self-hosted LangSmith instance, set `LANGSMITH_ENDPOINT` to the hostname of the self-hosted instance.

## ​ `LANGSMITH_TRACING`

Set `LANGSMITH_TRACING` to `false` to disable tracing to LangSmith.Defaults to `true`.

## ​ `LOG_COLOR`

This is mainly relevant in the context of using the dev server via the `langgraph dev` command. Set `LOG_COLOR` to `true` to enable ANSI-colored console output when using the default console renderer. Disabling color output by setting this variable to `false` produces monochrome logs. Defaults to `true`.

## ​ `LOG_LEVEL`

Configure log level. Defaults to `INFO`.

## ​ `LOG_JSON`

Set `LOG_JSON` to `true` to render all log messages as JSON objects using the configured `JSONRenderer`. This produces structured logs that can be easily parsed or ingested by log management systems. Defaults to `false`.

## ​ `MOUNT_PREFIX`

**Only Allowed in Self-Hosted Deployments**
The `MOUNT_PREFIX` environment variable is only allowed in Self-Hosted Deployment models, LangSmith SaaS will not allow this environment variable.

Set `MOUNT_PREFIX` to serve the Agent Server under a specific path prefix. This is useful for deployments where the server is behind a reverse proxy or load balancer that requires a specific path prefix.For example, if the server is to be served under `https://example.com/langgraph`, set `MOUNT_PREFIX` to `/langgraph`.

## ​ `N_JOBS_PER_WORKER`

Number of jobs per worker for the Agent Server task queue. Defaults to `10`.

## ​ `POSTGRES_URI_CUSTOM`

**Only for Hybrid and Self-Hosted**
Custom Postgres instances are only available for Hybrid and Self-Hosted deployments.

Specify `POSTGRES_URI_CUSTOM` to use a custom Postgres instance. The value of `POSTGRES_URI_CUSTOM` must be a valid Postgres connection URI.Postgres:

- Version 15.8 or higher.
- An initial database must be present and the connection URI must reference the database.

Control Plane Functionality:

- If `POSTGRES_URI_CUSTOM` is specified, the control plane will not provision a database for the server.
- If `POSTGRES_URI_CUSTOM` is removed, the control plane will not provision a database for the server and will not delete the externally managed Postgres instance.
- If `POSTGRES_URI_CUSTOM` is removed, deployment of the revision will not succeed. Once `POSTGRES_URI_CUSTOM` is specified, it must always be set for the lifecycle of the deployment.
- If the deployment is deleted, the control plane will not delete the externally managed Postgres instance.
- The value of `POSTGRES_URI_CUSTOM` can be updated. For example, a password in the URI can be updated.

Database Connectivity:

- The custom Postgres instance must be accessible by the Agent Server. The user is responsible for ensuring connectivity.

## ​ `REDIS_CLUSTER`

This feature is in Alpha.

**Only Allowed in Self-Hosted Deployments**
Redis Cluster mode is only available in Self-Hosted Deployment models, LangSmith SaaS will provision a redis instance for you by default.

Set `REDIS_CLUSTER` to `True` to enable Redis Cluster mode. When enabled, the system will connect to Redis using cluster mode. This is useful when connecting to a Redis Cluster deployment.Defaults to `False`.

## ​ `REDIS_KEY_PREFIX`

**Available in API Server version 0.1.9+**
This environment variable is supported in API Server version 0.1.9 and above.

Specify a prefix for Redis keys. This allows multiple Agent Server instances to share the same Redis instance by using different key prefixes.Defaults to `''`.

## ​ `REDIS_URI_CUSTOM`

**Only for Hybrid and Self-Hosted**
Custom Redis instances are only available for Hybrid and Self-Hosted deployments.

Specify `REDIS_URI_CUSTOM` to use a custom Redis instance. The value of `REDIS_URI_CUSTOM` must be a valid Redis connection URI.

## ​ `REDIS_MAX_CONNECTIONS`

The maximum size of the Redis connection pool (per replica) can be controlled using the `REDIS_MAX_CONNECTIONS` environment variable. By setting this variable, you can determine the upper bound on the number of simultaneous connections the server will establish with the Redis instance.For example, if a deployment is scaled up to 10 replicas and `REDIS_MAX_CONNECTIONS` is configured to `150`, then up to `1500` connections to Redis can be established.Defaults to `2000`.

## ​ `RESUMABLE_STREAM_TTL_SECONDS`

Time-to-live in seconds for resumable stream data in Redis.When a run is created and the output is streamed, the stream can be configured to be resumable (e.g. `stream_resumable=True`). If a stream is resumable, output from the stream is temporarily stored in Redis. The TTL for this data can be configured by setting `RESUMABLE_STREAM_TTL_SECONDS`.See the Python and JS/TS SDKs for more details on how to implement resumable streams.Defaults to `120` seconds.

Setting a very high value for `RESUMABLE_STREAM_TTL_SECONDS` can result in substantial Redis memory usage when there are many concurrent runs with large or frequent streaming output. Set this value to the minimum value to enable recovery during network interruptions and prefer checkpointing for long term durability and execution snapshotting.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

RemoteGraph\\
\\
Previous Agent Server changelog\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/self-hosted-changelog

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Releases

Self-hosted LangSmith changelog

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

FiltersClear

self-hosted

**Subscribe**: Our changelog includes an RSS feed that can integrate with Slack, email, Discord bots like Readybot or RSS Feeds to Discord Bot, and other subscription tools.

Self-hosted LangSmith is an add-on to the Enterprise plan designed for our largest, most security-conscious customers. For more details, refer to Pricing. Contact our sales team if you want to get a license key to trial LangSmith in your environment.

​

2025-12-12

## ​ langsmith-0.12.32

- Added IAM connection support for PostgreSQL (AWS only).
- Added GPT-5.2 model support to the playground.
- Added support for setting memory limits on executor pods.

**Download the Helm chart:** `langsmith-0.12.32.tgz`

2025-12-11

## ​ langsmith-0.12.31

- Improved error messages for basic authentication misconfiguration.
- Added organization operator role support.
- Fixed issues with streaming datasets endpoint.

**Download the Helm chart:** `langsmith-0.12.31.tgz`

2025-12-09

## ​ langsmith-0.12.30

- Fixed API Docs button not redirecting to the correct URL when using a sub path.
- Performance improvements and bug fixes.

**Download the Helm chart:** `langsmith-0.12.30.tgz`

2025-12-08

## ​ langsmith-0.12.29

- Added mTLS (mutual TLS) support for ClickHouse connections to enhance security for database communication.

**Download the Helm chart:** `langsmith-0.12.29.tgz`

2025-12-05

## ​ langsmith-0.12.28

- Added mTLS (mutual TLS) support for PostgreSQL connections to enhance security for database communication.
- Added mTLS support for ClickHouse clients.
- Fixed Agent Builder onboarding and side navigation visibility when disabled in self-hosted deployments.

**Download the Helm chart:** `langsmith-0.12.28.tgz`

2025-12-04

## ​ langsmith-0.12.27

- Added mTLS (mutual TLS) support for Redis connections to enhance security.
- Added support for empty trigger server configuration in self-hosted deployments.
- Improved incident banner styling and content.

**Download the Helm chart:** `langsmith-0.12.27.tgz`

2025-12-01

## ​ langsmith-0.12.25

- Enabled Agent Builder UI feature flag for self-hosted deployments.
- Added Redis Cluster support for improved scalability and high availability.

**Download the Helm chart:** `langsmith-0.12.25.tgz`

2025-11-27

## ​ langsmith-0.12.24

- Added dequeue timeouts to all SAQ (Simple Async Queue) queues to improve reliability.
- Performance improvements and bug fixes.

**Download the Helm chart:** `langsmith-0.12.24.tgz`

2025-11-26

## ​ langsmith-0.12.22

- Added Claude Opus 4.5 model support to the playground.
- Updated dataplane operator version.
- Added `LANGCHAIN_ENDPOINT` environment variable when basePath is configured.

**Download the Helm chart:** `langsmith-0.12.22.tgz`

## ​ langsmith-0.12.21

- Added explicit `revisionHistoryLimit` configuration for operator deployment template.

**Download the Helm chart:** `langsmith-0.12.21.tgz`

2025-11-24

## ​ langsmith-0.12.20

- Added support for self-hosted customers to opt into the pairwise annotation queue feature.
- Updated operator to version 0.1.21 in LangSmith and data plane charts.

**Download the Helm chart:** `langsmith-0.12.20.tgz`

## ​ langsmith-0.12.19

- Fixed playground environment configuration to use correct default settings.

**Download the Helm chart:** `langsmith-0.12.19.tgz`

2025-11-20

## ​ langsmith-0.12.18

- Internal updates and maintenance.

**Download the Helm chart:** `langsmith-0.12.18.tgz`

Additional Helm chart releases are available in the `langchain-ai/helm` GitHub repository.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Agent Server changelog\\
\\
Previous Release versions\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/release-versions

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Releases

Release versions

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

On this page

- Support levels
- Active
- Critical
- End of life (EOL)
- Deprecated
- Version support policy
- Minor version support
- Patch releases
- Recommendations
- Version compatibility
- Current version support

LangSmith provides different support levels for different versions, which may include new features, bug fixes, or security patches.

## ​ Support levels

There are four support levels:

- Active
- Critical
- End of life (EOL)
- Deprecated

### ​ Active

Where N represents the latest minor version (e.g., 0.3, 0.4, etc.).The current minor version (N) receives full support, including:

- New features and capabilities
- Bug fixes and regressions
- Security patches
- Quality-of-life improvements
- High confidence changes that are narrowly scoped

### ​ Critical

The previous minor version (N-1) receives limited support:

- Critical security fixes
- Installation fixes
- No new features or general bug fixes
- Transitioned from Active when a newer minor version is released

### ​ End of life (EOL)

Versions older than N-2 (N-2, N-3, etc.) receive no support:

- No new patch releases
- No bug fixes, including known bugs
- No security updates
- Users should upgrade to a supported version

### ​ Deprecated

Versions that are no longer maintained:

- All versions prior to the first stable release
- Versions that have been explicitly deprecated
- No support or maintenance provided

## ​ Version support policy

LangSmith follows an N-2 support policy for minor versions:

- **N (Current)**: Active support
- **N-1**: Critical support
- **N-2 and older**: End of Life

### ​ Minor version support

Minor versions include new features and capabilities and are supported according to the N-2 policy. When we refer to a minor version, such as v0.3, we always mean its latest available patch release (v0.3.x).

### ​ Patch releases

During the support window for each version:

- **Active Support**: Regular patch releases with bug fixes, regressions, and new features
- **Critical Support**: Security-only releases for critical fixes related to security and installation
- **End of Life**: No new patches released

## ​ Recommendations

- **Stay Current**: We recommend upgrading to the latest minor version to receive full support and access to new features
- **Plan Upgrades**: Monitor the changelog for upcoming version changes and plan upgrades accordingly
- **Security**: Critical security fixes are only provided for Active and Critical support versions
- **Testing**: Test your applications with newer versions before upgrading in production

## ​ Version compatibility

When upgrading between minor versions:

- Review the changelog for breaking changes
- Test your applications thoroughly
- Follow the upgrade guides provided in the documentation
- Consider the support timeline for your current version

## ​ Current version support

To check the current supported versions and their support levels, refer to the Agent Server Changelog for the latest release information.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Self-hosted LangSmith changelog\\
\\
Previous

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/smith-js-ts-sdk

- langsmith

# Module langsmith - v0.4.4

# LangSmith Client SDK Permalink

This package contains the TypeScript client for interacting with the LangSmith platform.

To install:

yarn add langsmith
Copy

A typical workflow looks like:

1. Set up an account with LangSmith.
2. Log traces.
3. Debug, Create Datasets, and Evaluate Runs.

We'll walk through these steps in more detail below.

## 1\. Connect to LangSmith Permalink

Sign up for LangSmith using your GitHub, Discord accounts, or an email address and password. If you sign up with an email, make sure to verify your email address before logging in.

Then, create a unique API key on the Settings Page.

Note

Save the API Key in a secure location. It will not be shown again.

## 2\. Log Traces Permalink

You can log traces natively in your LangChain application or using a LangSmith RunTree.

### Logging Traces with LangChain Permalink

LangSmith seamlessly integrates with the JavaScript LangChain library to record traces from your LLM applications.

yarn add langchain
Copy

1. **Copy the environment variables from the Settings Page and add them to your application.**

Tracing can be activated by setting the following environment variables or by manually specifying the LangChainTracer.

process.env.LANGSMITH_TRACING = "true";
process.env.LANGSMITH_ENDPOINT = "https://api.smith.langchain.com";
// process.env.LANGSMITH_ENDPOINT = "https://eu.api.smith.langchain.com"; // If signed up in the EU region

// process.env.LANGSMITH_PROJECT = "My Project Name"; // Optional: "default" is used if not set

Copy

2. **Run an Agent, Chain, or Language Model in LangChain**

If the environment variables are correctly set, your application will automatically connect to the LangSmith platform.

import { ChatOpenAI } from "langchain/chat_models/openai";

const chat = new ChatOpenAI({ temperature: 0 });
const response = await chat.predict(
"Translate this sentence from English to French. I love programming."
);
console.log(response);
Copy

### Logging Traces Outside LangChain Permalink

You can still use the LangSmith development platform without depending on any
LangChain code. You can connect either by setting the appropriate environment variables,
or by directly specifying the connection information in the RunTree.

export LANGSMITH_TRACING="true";

# export LANGSMITH_ENDPOINT=https://api.smith.langchain.com # or your own server

## Integrations Permalink

Langsmith's `traceable` wrapper function makes it easy to trace any function or LLM call in your own favorite framework. Below are some examples.

### OpenAI SDK Permalink

The easiest way to trace calls from the OpenAI SDK with LangSmith
is using the `wrapOpenAI` wrapper function available in LangSmith 0.1.3 and up.

In order to use, you first need to set your LangSmith API key:

Next, you will need to install the LangSmith SDK and the OpenAI SDK:

npm install langsmith openai
Copy

After that, initialize your OpenAI client and wrap the client with `wrapOpenAI` method to enable tracing for the completions and chat completions methods:

import { OpenAI } from "openai";
import { wrapOpenAI } from "langsmith/wrappers";

const openai = wrapOpenAI(new OpenAI());

await openai.chat.completions.create({
model: "gpt-3.5-turbo",
messages: [{ content: "Hi there!", role: "user" }],
});
Copy

Alternatively, you can use the `traceable` function to wrap the client methods you want to use:

import { traceable } from "langsmith/traceable";

const openai = new OpenAI();

const createCompletion = traceable(
openai.chat.completions.create.bind(openai.chat.completions),
{ name: "OpenAI Chat Completion", run_type: "llm" }
);

await createCompletion({
model: "gpt-3.5-turbo",
messages: [{ content: "Hi there!", role: "user" }],
});
Copy

Note the use of `.bind` to preserve the function's context. The `run_type` field in the
extra config object marks the function as an LLM call, and enables token usage tracking
for OpenAI.

Oftentimes, you use the OpenAI client inside of other functions or as part of a longer
sequence. You can automatically get nested traces by using this wrapped method
within other functions wrapped with `traceable`.

const completion = await openai.chat.completions.create({
model: "gpt-3.5-turbo",
messages: [{ content: text, role: "user" }],
});
return completion;
});

await nestedTrace("Why is the sky blue?");
Copy
{
"id": "chatcmpl-8sPToJQLLVepJvyeTfzZMOMVIKjMo",
"object": "chat.completion",
"created": 1707978348,
"model": "gpt-3.5-turbo-0613",
"choices": [\
{\
"index": 0,\
"message": {\
"role": "assistant",\
"content": "The sky appears blue because of a phenomenon known as Rayleigh scattering. The Earth's atmosphere is composed of tiny molecules, such as nitrogen and oxygen, which are much smaller than the wavelength of visible light. When sunlight interacts with these molecules, it gets scattered in all directions. However, shorter wavelengths of light (blue and violet) are scattered more compared to longer wavelengths (red, orange, and yellow). \n\nAs a result, when sunlight passes through the Earth's atmosphere, the blue and violet wavelengths are scattered in all directions, making the sky appear blue. This scattering of shorter wavelengths is also responsible for the vibrant colors observed during sunrise and sunset, when the sunlight has to pass through a thicker portion of the atmosphere, causing the longer wavelengths to dominate the scattered light."\
},\
"logprobs": null,\
"finish_reason": "stop"\
}\
],
"usage": {
"prompt_tokens": 13,
"completion_tokens": 154,
"total_tokens": 167
},
"system_fingerprint": null
}
Copy

:::tip
Click here to see an example LangSmith trace of the above.
:::

## Next.js Permalink

You can use the `traceable` wrapper function in Next.js apps to wrap arbitrary functions much like in the example above.

One neat trick you can use for Next.js and other similar server frameworks is to wrap the entire exported handler for a route
to group traces for the any sub-runs. Here's an example:

import { NextRequest, NextResponse } from "next/server";

import { OpenAI } from "openai";
import { traceable } from "langsmith/traceable";
import { wrapOpenAI } from "langsmith/wrappers";

export const runtime = "edge";

const handler = traceable(
async function () {
const openai = wrapOpenAI(new OpenAI());

const completion = await openai.chat.completions.create({
model: "gpt-3.5-turbo",
messages: [{ content: "Why is the sky blue?", role: "user" }],
});

const response1 = completion.choices[0].message.content;

const completion2 = await openai.chat.completions.create({
model: "gpt-3.5-turbo",
messages: [\
{ content: "Why is the sky blue?", role: "user" },\
{ content: response1, role: "assistant" },\
{ content: "Cool thank you!", role: "user" },\
],
});

const response2 = completion2.choices[0].message.content;

return {
text: response2,
};
},
{
name: "Simple Next.js handler",
}
);

export async function POST(req: NextRequest) {
const result = await handler();
return NextResponse.json(result);
}
Copy

The two OpenAI calls within the handler will be traced with appropriate inputs, outputs,
and token usage information.

## Vercel AI SDK Permalink

The Vercel AI SDK contains integrations with a variety of model providers.
Here's an example of how you can trace outputs in a Next.js handler:

import { traceable } from "langsmith/traceable";
import { OpenAIStream, StreamingTextResponse } from "ai";

// Note: There are no types for the Mistral API client yet.
import MistralClient from "@mistralai/mistralai";

const client = new MistralClient(process.env.MISTRAL_API_KEY || "");

export async function POST(req: Request) {
// Extract the `messages` from the body of the request
const { messages } = await req.json();

const mistralChatStream = traceable(client.chatStream.bind(client), {
name: "Mistral Stream",
run_type: "llm",
});

const response = await mistralChatStream({
model: "mistral-tiny",
maxTokens: 1000,
messages,
});

// Convert the response into a friendly text-stream. The Mistral client responses are
// compatible with the Vercel AI SDK OpenAIStream adapter.
const stream = OpenAIStream(response as any);

// Respond with the stream
return new StreamingTextResponse(stream);
}
Copy

See the AI SDK docs for more examples.

## Arbitrary SDKs Permalink

You can use the generic `wrapSDK` method to add tracing for arbitrary SDKs.

Do note that this will trace ALL methods in the SDK, not just chat completion endpoints.
If the SDK you are wrapping has other methods, we recommend using it for only LLM calls.

Here's an example using the Anthropic SDK:

import { wrapSDK } from "langsmith/wrappers";
import { Anthropic } from "@anthropic-ai/sdk";

const originalSDK = new Anthropic();
const sdkWithTracing = wrapSDK(originalSDK);

const response = await sdkWithTracing.messages.create({
messages: [\
{\
role: "user",\
content: `What is 1 + 1? Respond only with "2" and nothing else.`,\
},\
],
model: "claude-3-sonnet-20240229",
max_tokens: 1024,
});
Copy

#### Alternatives: **Log traces using a RunTree.** Permalink

A RunTree tracks your application. Each RunTree object is required to have a name and run\_type. These and other important attributes are as follows:

- `name`: `string` \- used to identify the component's purpose
- `run_type`: `string` \- Currently one of "llm", "chain" or "tool"; more options will be added in the future

- `outputs`: `Optional<Record<string, any>>` \- the (optional) returned values from the component

import { RunTree, RunTreeConfig } from "langsmith";

const parentRunConfig: RunTreeConfig = {
name: "My Chat Bot",
run_type: "chain",
inputs: {
text: "Summarize this morning's meetings.",
},
serialized: {}, // Serialized representation of this chain
// project_name: "Defaults to the LANGSMITH_PROJECT env var"
// apiUrl: "Defaults to the LANGSMITH_ENDPOINT env var"
// apiKey: "Defaults to the LANGSMITH_API_KEY env var"
};

const parentRun = new RunTree(parentRunConfig);

await parentRun.postRun();

const childLlmRun = await parentRun.createChild({
name: "My Proprietary LLM",
run_type: "llm",
inputs: {
prompts: [\
"You are an AI Assistant. The time is XYZ." +\
" Summarize this morning's meetings.",\
],
},
});

await childLlmRun.postRun();

await childLlmRun.end({
outputs: {
generations: [\
"I should use the transcript_loader tool" +\
" to fetch meeting_transcripts from XYZ",\
],
},
});

await childLlmRun.patchRun();

const childToolRun = await parentRun.createChild({
name: "transcript_loader",
run_type: "tool",
inputs: {
date: "XYZ",
content_type: "meeting_transcripts",
},
});
await childToolRun.postRun();

await childToolRun.end({
outputs: {
meetings: ["Meeting1 notes.."],
},
});

await childToolRun.patchRun();

const childChainRun = await parentRun.createChild({
name: "Unreliable Component",
run_type: "tool",
inputs: {
input: "Summarize these notes...",
},
});

await childChainRun.postRun();

try {
// .... the component does work
throw new Error("Something went wrong");
} catch (e) {
await childChainRun.end({
error: `I errored again ${e.message}`,
});
await childChainRun.patchRun();
throw e;
}

await childChainRun.patchRun();

await parentRun.end({
outputs: {
output: ["The meeting notes are as follows:..."],
},
});

// False directs to not exclude child runs
await parentRun.patchRun();
Copy

## Evaluation Permalink

#### Create a Dataset from Existing Runs Permalink

Once your runs are stored in LangSmith, you can convert them into a dataset.
For this example, we will do so using the Client, but you can also do this using
the web interface, as explained in the LangSmith docs.

import { Client } from "langsmith/client";
const client = new Client({
// apiUrl: "https://api.langchain.com", // Defaults to the LANGSMITH_ENDPOINT env var
// apiKey: "my_api_key", // Defaults to the LANGSMITH_API_KEY env var
/* callerOptions: {
maxConcurrency?: Infinity; // Maximum number of concurrent requests to make
maxRetries?: 6; // Maximum number of retries to make
*/
});
const datasetName = "Example Dataset";
// We will only use examples from the top level AgentExecutor run here,
// and exclude runs that errored.
const runs = await client.listRuns({
projectName: "my_project",
executionOrder: 1,
error: false,
});

const dataset = await client.createDataset(datasetName, {
description: "An example dataset",
});

for (const run of runs) {
await client.createExample(run.inputs, run.outputs ?? {}, {
datasetId: dataset.id,
});
}
Copy

## Additional Documentation Permalink

To learn more about the LangSmith platform, check out the docs.

## Modules

anonymizer Permalinkclient Permalinkevaluation Permalinkexperimental/otel/exporter Permalinkexperimental/otel/processor Permalinkexperimental/otel/setup Permalinkexperimental/vercel Permalinkjest Permalinkjest/reporter Permalinklangchain Permalinkrun\_trees Permalinkschemas Permalinksingletons/traceable Permalinktraceable Permalinkutils/jestlike Permalinkvitest Permalinkvitest/reporter Permalinkwrappers Permalinkwrappers/anthropic Permalinkwrappers/openai Permalink

### Settings

Member Visibility

- Protected
- Inherited
- External

ThemeOSLightDark

### On This Page

LangSmith Client SDK

- 1\. Connect to LangSmith
- 2\. Log Traces
- Logging Traces with LangChain
- Logging Traces Outside LangChain
- Integrations
- OpenAI SDK
- Next.js
- Vercel AI SDK
- Arbitrary SDKs
- Alternatives: Log traces using a RunTree.
- Evaluation
- Create a Dataset from Existing Runs
- Additional Documentation

Modules

anonymizer client evaluation experimental/otel/exporter experimental/otel/processor experimental/otel/setup experimental/vercel jest jest/reporter langchain run\_trees schemas singletons/traceable traceable utils/jestlike vitest vitest/reporter wrappers wrappers/anthropic wrappers/openai

MMNEPVFCICPMFPCPTTAAATR

---

# https://docs.langchain.com/langsmith/langgraph-js-ts-sdk

- @langchain/langgraph-sdk

# Module @langchain/langgraph-sdk - v1.3.1

# LangGraph JS/TS SDK Permalink

This repository contains the JS/TS SDK for interacting with the LangGraph REST API.

## Quick Start Permalink

To get started with the JS/TS SDK, install the package

yarn add @langchain/langgraph-sdk
Copy

You will need a running LangGraph API server. If you're running a server locally using `langgraph-cli`, SDK will automatically point at `http://localhost:8123`, otherwise
you would need to specify the server URL when creating a client.

import { Client } from "@langchain/langgraph-sdk";

const client = new Client();

// List all assistants
const assistants = await client.assistants.search({
metadata: null,
offset: 0,
limit: 10,
});

// We auto-create an assistant for each graph you register in config.
const agent = assistants[0];

// Start a new thread
const thread = await client.threads.create();

// Start a streaming run
const messages = [{ role: "human", content: "what's the weather in la" }];

const streamResponse = client.runs.stream(
thread["thread_id"],
agent["assistant_id"],
{
input: { messages },
}
);

for await (const chunk of streamResponse) {
console.log(chunk);
}
Copy

## Documentation Permalink

To generate documentation, run the following commands:

1. Generate docs.

yarn typedoc

2. Consolidate doc files into one markdown file.

3. Copy `js_ts_sdk_ref.md` to MkDocs directory.

cp docs/js_ts_sdk_ref.md ../../docs/docs/cloud/reference/sdk/js_ts_sdk_ref.md

## Reference Documentation Permalink

The reference documentation is available here.

More usage examples can be found here.

## Change Log Permalink

The change log for new versions can be found here.

## Modules

auth Permalinkclient Permalinkindex Permalinklogging Permalinkreact Permalinkreact-ui Permalinkreact-ui/server Permalink

### Settings

Member Visibility

- Protected
- Inherited
- External

ThemeOSLightDark

### On This Page

LangGraph JS/TS SDK

- Quick Start
- Documentation
- Reference Documentation
- Change Log

Modules

auth client index logging react react-ui react-ui/server

MMNEPVFCICPMFPCPTTAAATR

---

# https://docs.langchain.com/langsmith/smith-api-ref

- Host
- Authentication
- audit-logs
- getGet Audit Logs
- tracer-sessions
- postGet Tracing Project Prebuilt Dashboard
- getRead Tracer Session
- patchUpdate Tracer Session
- delDelete Tracer Session
- getRead Tracer Sessions
- postCreate Tracer Session
- delDelete Tracer Sessions
- getRead Tracer Sessions Runs Metadata
- getRead Filter Views
- postCreate Filter View
- getRead Filter View
- patchUpdate Filter View
- delDelete Filter View
- get\[Beta\] Get Insights Jobs
- post\[Beta\] Create Insights Job
- get\[Beta\] Get Insights Job Configs
- post\[Beta\] Create Insights Job Config
- post\[Beta\] Auto-Generate Insights Job Config
- patch\[Beta\] Update Insights Job Config
- del\[Beta\] Delete Insights Job Config
- get\[Beta\] Get Insights Job
- patch\[Beta\] Update Insights Job
- del\[Beta\] Delete Insights Job
- get\[Beta\] Get Run Cluster From Insights Job
- get\[Beta\] Get Runs From Insights Job
- orgs
- getList Organizations
- postCreate Organization
- postCreate Customers And Get Stripe Setup Intent
- getGet Organization Info
- getGet Current Organization Info
- patchUpdate Current Organization Info
- getGet Organization Billing Info
- getGet Dashboard
- postOn Payment Method Created
- getGet Company Info
- postSet Company Info
- postChange Payment Plan
- getList Organization Roles
- postCreate Organization Roles
- delDelete Organization Roles
- patchUpdate Organization Roles
- getList Permissions
- getList Pending Organization Invites
- getGet Current Org Members
- postAdd Member To Current Org
- getGet Current Active Org Members
- getGet Current Pending Org Members
- postAdd Members To Current Org Batch
- postAdd Basic Auth Members To Current Org
- delDelete Current Org Pending Member
- delDelete Pending Organization Invite
- postClaim Pending Organization Invite
- delRemove Member From Current Org
- patchUpdate Current Org Member
- patchUpdate Current User
- getList Ttl Settings
- putUpsert Ttl Settings
- getGet Current Sso Settings
- postCreate Sso Settings
- patchUpdate Sso Settings
- delDelete Sso Settings
- patchUpdate Allowed Login Methods
- getGet Org Usage
- getGet Granular Usage
- getExport Granular Usage Csv
- getGet Current User Login Methods
- postCreate Stripe Checkout Sessions Endpoint
- postConfirm Checkout Session Completion Endpoint
- postCreate Stripe Account Links Endpoint
- getList Org Service Keys
- postCreate Org Service Key
- delDelete Org Service Key
- getList Org Personal Access Tokens
- postCreate Org Personal Access Token
- delDelete Org Personal Access Token
- postSet Default Sso Provision
- auth
- postLogin
- postSend Sso Email Confirmation
- postCheck Sso Email Verification Status
- postConfirm Sso User Email
- getGet Sso Settings
- api-key
- getGet Api Keys
- postGenerate Api Key
- delDelete Api Key
- getGet Personal Access Tokens
- postGenerate Personal Access Token
- delDelete Personal Access Token
- examples
- getCount Examples
- getRead Example
- patchUpdate Example
- delDelete Example
- getRead Examples
- postCreate Example
- delDelete Examples
- postCreate Examples
- patchLegacy Update Examples
- postUpload Examples From Csv
- postValidate Example
- postValidate Examples
- postHard Delete Examples
- postUpload Examples
- patchUpdate Examples
- datasets
- getRead Datasets
- postCreate Dataset
- getRead Datasets Stream
- getRead Dataset
- delDelete Dataset
- patchUpdate Dataset
- postUpload Csv Dataset
- postUpload Experiment
- getGet Dataset Versions
- getDiff Dataset Versions
- getGet Dataset Version
- putUpdate Dataset Version
- getDownload Dataset Openai
- getDownload Dataset Openai Ft
- getDownload Dataset Csv
- getDownload Dataset Jsonl
- postRead Examples With Runs
- postRead Examples With Runs Grouped
- postRead Delta
- postRead Grouped Experiments
- getRead Dataset Share State
- putShare Dataset
- delUnshare Dataset
- getRead Comparative Experiments
- postCreate Comparative Experiment
- delDelete Comparative Experiment
- postClone Dataset
- getGet Dataset Splits
- putUpdate Dataset Splits
- postIndex
- delRemove Index
- getGet Index Info
- postSync Index
- postSearch
- postGenerate
- postDataset Handler
- postStream Dataset Handler
- postStudio Experiment
- run
- getList Rules
- postCreate Rule
- postValidate Rule
- patchUpdate Rule
- delDelete Rule
- getThread Preview
- getList Rule Logs
- getGet Last Applied Rule
- postTrigger Rule
- postTrigger Rules
- getRead Run
- patchUpdate Run
- getRead Run Share State
- putShare Run
- delUnshare Run
- postQuery Runs
- postGenerate Query For Runs
- postStats Runs
- postGroup Runs
- postStats Group Runs
- postDelete Runs
- feedback
- postCreate Feedback Formula Ep
- getList Feedback Formula Ep
- getGet Feedback Formula Ep
- putUpdate Feedback Formula Ep
- delDelete Feedback Formula Endpoint
- getRead Feedback
- patchUpdate Feedback
- delDelete Feedback
- getRead Feedbacks
- postCreate Feedback
- postEagerly Create Feedback
- postCreate Feedback Ingest Token
- getList Feedback Ingest Tokens
- getCreate Feedback With Token Get
- postCreate Feedback With Token Post
- postIngest Feedback (Batch JSON)
- public
- getGet Shared Run
- getGet Shared Run By Id
- postQuery Shared Runs
- getRead Shared Feedbacks
- getRead Shared Dataset
- getCount Shared Examples
- getRead Shared Examples
- getRead Shared Dataset Tracer Sessions
- getRead Shared Dataset Tracer Sessions Bulk
- postRead Shared Dataset Examples With Runs
- postRead Shared Delta
- postQuery Shared Dataset Runs
- postGenerate Query For Shared Dataset Runs
- postStats Shared Dataset Runs
- getRead Shared Dataset Run
- getRead Shared Dataset Feedback
- getRead Shared Comparative Experiments
- getGet Message Json Schema
- getGet Tool Def Json Schema
- annotation-queues
- getGet Annotation Queues
- postCreate Annotation Queue
- postPopulate Annotation Queue
- delDelete Annotation Queue
- patchUpdate Annotation Queue
- getGet Annotation Queue
- postAdd Runs To Annotation Queue
- getGet Runs From Annotation Queue
- postExport Annotation Queue Archived Runs
- getGet Run From Annotation Queue
- getGet Annotation Queues For Run
- patchUpdate Run In Annotation Queue
- delDelete Run From Annotation Queue
- postDelete Runs From Annotation Queue
- getGet Total Size From Annotation Queue
- getGet Total Archived From Annotation Queue
- getGet Size From Annotation Queue
- postCreate Identity Annotation Queue Run Status
- ace
- postExecute
- bulk-exports
- getGet Bulk Exports
- postCreate Bulk Export
- getGet Bulk Export Destinations
- postCreate Bulk Export Destination
- getGet Bulk Export Runs Filtered
- getGet Bulk Export
- patchCancel Bulk Export
- getGet Bulk Export Destination
- getGet Bulk Export Runs
- getGet Bulk Export Run
- tenant
- getList Tenants
- postCreate Tenant
- info
- getGet Server Info
- getGet Health Info
- feedback-configs
- getList Feedback Configs Endpoint
- postCreate Feedback Config Endpoint
- patchUpdate Feedback Config Endpoint
- delDelete Feedback Config Endpoint
- metrics
- getGet Queue Metrics
- model-price-map
- getRead Model Price Map
- postCreate New Model Price
- putUpdate Model Price
- delDelete Model Price
- usage-limits
- getList Usage Limits
- putUpsert Usage Limit
- getList Org Usage Limits
- delDelete Usage Limit
- ttl-settings
- getList Ttl Settings
- putUpsert Ttl Settings
- prompts
- postInvoke Prompt
- postPrompt Canvas
- prompt-webhooks
- getList Prompt Webhooks
- postCreate Prompt Webhook
- getGet Prompt Webhook
- patchUpdate Prompt Webhook
- delDelete Prompt Webhook
- postTest Prompt Webhook
- workspaces
- getList Workspaces
- postCreate Workspace
- patchPatch Workspace
- delDelete Workspace
- getList Pending Workspace Invites
- delDelete Pending Workspace Invite
- postClaim Pending Workspace Invite
- getGet Current Workspace Stats
- getGet Current Workspace Members
- postAdd Member To Current Workspace
- getGet Current Active Workspace Members
- getGet Current Pending Workspace Members
- postAdd Members To Current Workspace Batch
- getGet Shared Tokens
- delBulk Unshare Entities
- delDelete Current Workspace Member
- patchPatch Current Workspace Member
- delDelete Current Workspace Pending Member
- getGet Current Workspace Usage Limits Info
- getList Current Workspace Secrets
- postUpsert Current Workspace Secrets
- getGet Current Workspace Encrypted Secrets
- getList Tag Keys
- postCreate Tag Key
- patchUpdate Tag Key
- getGet Tag Key
- delDelete Tag Key
- postCreate Tag Value
- getList Tag Values
- getGet Tag Value
- patchUpdate Tag Value
- delDelete Tag Value
- postCreate Tagging
- getList Taggings
- delDelete Tagging
- getList Tags
- getList Tags For Resource
- postList Tags For Resources
- playground-settings
- getList Playground Settings
- postCreate Playground Settings
- patchUpdate Playground Settings
- delDelete Playground Settings
- me
- getGet Onboarding State
- postCreate Onboarding State
- putUpdate Onboarding State Field
- getGet Ls User Id
- service-accounts
- getGet Service Accounts
- postCreate Service Account
- delDelete Service Account
- charts
- postClone Section
- getRead Sections
- postCreate Section
- postRead Charts
- postRead Chart Preview
- postCreate Chart
- postRead Single Chart
- patchUpdate Chart
- delDelete Chart
- postRead Single Section
- patchUpdate Section
- delDelete Section
- getOrg Read Sections
- postOrg Create Section
- postOrg Read Charts
- postOrg Read Chart Preview
- postOrg Create Chart
- postOrg Read Single Chart
- patchOrg Update Chart
- delOrg Delete Chart
- postOrg Read Single Section
- patchOrg Update Section
- delOrg Delete Section
- mcp
- getProxy Get
- postProxy
- getOk
- repos
- getList Repos
- postCreate Repo
- getGet Repo
- patchUpdate Repo
- delDelete Repo
- postFork Repo
- getList Repo Tags
- postOptimize Prompt Job
- likes
- postLike Repo
- settings
- getGet Settings
- postSet Tenant Handle
- comments
- postCreate Comment
- getGet Comments
- getGet Sub Comments
- postCreate Sub Comment
- postLike Comment
- delUnlike Comment
- tags
- getGet Tags
- postCreate Tag
- getGet Tag
- patchUpdate Tag
- delDelete Tag
- optimization-jobs
- getList Jobs
- postCreate Job
- getGet Job
- patchUpdate Job
- delDelete Job
- getList Job Logs
- postCreate Log
- getGet Log
- delDelete Log
- commits
- getList commits
- postCreate a commit
- getGet a commit
- experiment-view-overrides
- getGet experiment view override configurations for a dataset
- postCreate new experiment view override configuration for a dataset
- getGet experiment view override configuration by specific ID
- delDelete experiment view override configuration
- patchUpdate existing experiment view override configuration
- runs
- postCreate a Run
- postIngest Runs (Batch JSON)
- postIngest Runs (Multipart)
- patchUpdate a Run
- alert\_rules
- postCreate an alert rule
- postTest an alert action to determine if configuration is valid
- getGet an alert rule
- delDelete an alert rule
- patchUpdate an alert rule
- access\_policies
- getList access policies
- postCreate an access policy
- postAttach access policies to a role
- getGet an access policy
- delDelete an access policy

![redocly logoAPI docs by Redocly](https://redocly.com/redoc/)

# LangSmith (0.1.0)

Download OpenAPI specification: Download

The LangSmith API is used to programmatically create and manage LangSmith resources.

## section/Host Host

## section/Authentication Authentication

To authenticate with the LangSmith API, set the `X-Api-Key` header
to a valid LangSmith API key.

## tag/audit-logs audit-logs

## tag/audit-logs/operation/get_audit_logs_api_v1_audit_logs_get Get Audit Logs

Retrieve audit log records for the authenticated user's organization in OCSF format.

Requires both start\_time and end\_time parameters to filter logs within a date range.
Supports cursor-based pagination.

Returns results in OCSF API Activity (Class UID: 6003) format,
which is compatible with security monitoring and SIEM tools.
Reference:

##### Authorizations:

_API Key__Organization ID__Bearer Auth_

##### query Parameters

| | |
| --- | --- |

### Responses

**200**

Successful Response

**422**

Validation Error

get/api/v1/audit-logs

### Response samples

- 200
- 422

Content type

application/json

Copy
Expand all Collapse all

`{"cursor": "string",

"items": [{"class_uid": 6003,\
\
"class_name": "API Activity",\
\
"category_uid": 6,\
\
"category_name": "Application Activity",\
\
"severity_id": 99,\
\
"type_uid": 600300,\
\
"activity_id": 0,\
\
"activity_name": "string",\
\
"status_id": 0,\
\
"status": "string",\
\
"time": 0,\
\
"metadata": {"uid": "07cc67f4-45d6-494b-adac-09b5cbc7e2b5",\
\
"product": {"name": "string",\
\
"vendor_name": "string"\
\
}\
\
},\
\
"api": {"operation": "create_api_key"\
\
},\
\
"http_request": {"http_method": "string",\
\
"url": {"path": "string"\
\
}\
\
},\
\
"http_response": {"code": 0\
\
},\
\
"actor": {"user": {"uid": "07cc67f4-45d6-494b-adac-09b5cbc7e2b5",\
\
"credential_uid": "fd62ce1a-2893-47c2-a856-505fac5f1859"\
\
}\
\
},\
\
"src_endpoint": {"ip": "string",\
\
"port": 0,\
\
"intermediate_ips": ["string"\
\
]\
\
},\
\
"resources": [{"uid": "07cc67f4-45d6-494b-adac-09b5cbc7e2b5"\
\
}\
\
],\
\
"unmapped": {"original_audit_log": {"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"operation_name": "string",\
\
"resource_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"operation_succeeded": true,\
\
"request_time": "2019-08-24T14:15:22Z",\
\
"request_method": "string",\
\
"request_path": "string",\
\
"client_host": "string",\
\
"client_port": 0,\
\
"x_forwarded_for": "string",\
\
"api_key_id": "b0dd218e-3bcf-4bdb-a1e3-0689d60a8afd",\
\
"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"ls_user_id": "e368397c-ab29-448f-907e-f04754df6859",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"workspace_id": "0967198e-ec7b-4c6b-b4d3-f71244cadbe9",\
\
"response_status_code": 0\
\
}\
\
}\
\
}\
\
]

}`

## tag/tracer-sessions tracer-sessions

## tag/tracer-sessions/operation/get_tracing_project_prebuilt_dashboard_api_v1_sessions__session_id__dashboard_post Get Tracing Project Prebuilt Dashboard

Get a prebuilt dashboard for a tracing project.

##### Authorizations:

_API Key__Tenant ID__Bearer Auth_

##### path Parameters

##### header Parameters

| | |
| --- | --- |
| accept | Accept (string) or Accept (null) (Accept) |

##### Request Body schema: application/json required

| start\_time | Start Time (string) or Start Time (null) (Start Time) |
| end\_time | End Time (string) or End Time (null) (End Time) |

| group\_by | RunStatsGroupBy (object) or null |

### Responses

post/api/v1/sessions/{session\_id}/dashboard

### Request samples

- Payload

`{"timezone": "UTC",

"start_time": "2019-08-24T14:15:22Z",

"end_time": "2019-08-24T14:15:22Z",

"stride": {"days": 0,

"hours": 0,

"minutes": 0

},

"omit_data": false,

"group_by": {"attribute": "name",

"path": "string",

"max_groups": 5

}

### Response samples

`{"title": "string",

"description": "string",

"index": 0,

"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",

"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",

"charts": [{"data": [{"series_id": "string",\
\
"timestamp": "2019-08-24T14:15:22Z",\
\
"value": 0,\
\
"group": "string"\
\
}\
\
],\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"title": "string",\
\
"description": "string",\
\
"metadata": { },\
\
"index": 0,\
\
"chart_type": "line",\
\
"series": [{"name": "string",\
\
"filters": {"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
},\
\
"metric": "run_count",\
\
"feedback_key": "string",\
\
"workspace_id": "0967198e-ec7b-4c6b-b4d3-f71244cadbe9",\
\
"project_metric": "memory_usage",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"group_by": {"attribute": "name",\
\
"path": "string",\
\
"max_groups": 5,\
\
"set_by": "section"\
\
}\
\
}\
\
],\
\
"common_filters": {"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
}\
\
}\
\
],

"sub_sections": [{"title": "string",\
\
"description": "string",\
\
"index": 0,\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"charts": [{"data": [{"series_id": "string",\
\
"timestamp": "2019-08-24T14:15:22Z",\
\
"value": 0,\
\
"group": "string"\
\
}\
\
],\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"title": "string",\
\
"description": "string",\
\
"metadata": { },\
\
"index": 0,\
\
"chart_type": "line",\
\
"series": [{"name": "string",\
\
"filters": {"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
},\
\
"metric": "run_count",\
\
"feedback_key": "string",\
\
"workspace_id": "0967198e-ec7b-4c6b-b4d3-f71244cadbe9",\
\
"project_metric": "memory_usage",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"group_by": {"attribute": "name",\
\
"path": "string",\
\
"max_groups": 5,\
\
"set_by": "section"\
\
}\
\
}\
\
],\
\
"common_filters": {"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
}\
\
}\
\
]\
\
}\
\
]

## tag/tracer-sessions/operation/read_tracer_session_api_v1_sessions__session_id__get Read Tracer Session

Get a specific session.

##### Authorizations:

##### path Parameters

##### query Parameters

| stats\_start\_time | Stats Start Time (string) or Stats Start Time (null) (Stats Start Time) |

##### header Parameters

### Responses

get/api/v1/sessions/{session\_id}

### Response samples

`{"start_time": "2019-08-24T14:15:22Z",

"extra": { },

"name": "string",

"default_dataset_id": "467c3141-9a68-490e-ab5c-4000a5a11fa8",

"reference_dataset_id": "4abef5d4-9fac-44e9-b393-753a8de1db88",

"trace_tier": "longlived",

"run_count": 0,

"latency_p50": 0,

"latency_p99": 0,

"first_token_p50": 0,

"first_token_p99": 0,

"total_tokens": 0,

"prompt_tokens": 0,

"completion_tokens": 0,

"total_cost": "string",

"prompt_cost": "string",

"completion_cost": "string",

"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",

"last_run_start_time": "2019-08-24T14:15:22Z",

"last_run_start_time_live": "2019-08-24T14:15:22Z",

"feedback_stats": { },

"session_feedback_stats": { },

"run_facets": [{ }\
\
],

"error_rate": 0,

"streaming_rate": 0,

"test_run_number": 0

## tag/tracer-sessions/operation/update_tracer_session_api_v1_sessions__session_id__patch Update Tracer Session

Create a new session.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| name | Name (string) or Name (null) (Name) |
| description | Description (string) or Description (null) (Description) |
| default\_dataset\_id | Default Dataset Id (string) or Default Dataset Id (null) (Default Dataset Id) |
| end\_time | End Time (string) or End Time (null) (End Time) |
| extra | Extra (object) or Extra (null) (Extra) |
| trace\_tier | TraceTier (string) or null |

### Responses

patch/api/v1/sessions/{session\_id}

### Request samples

`{"name": "string",

"trace_tier": "longlived"

### Response samples

"last_run_start_time_live": "2019-08-24T14:15:22Z"

## tag/tracer-sessions/operation/delete_tracer_session_api_v1_sessions__session_id__delete Delete Tracer Session

Delete a specific session.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/sessions/{session\_id}

### Response samples

Copy

`null`

## tag/tracer-sessions/operation/read_tracer_sessions_api_v1_sessions_get Read Tracer Sessions

Get all sessions.

##### Authorizations:

##### query Parameters

| | |
| --- | --- |
| reference\_free | Reference Free (boolean) or Reference Free (null) (Reference Free) |
| reference\_dataset | Array of Reference Dataset (strings) or Reference Dataset (null) (Reference Dataset) |
| id | Array of Id (strings) or Id (null) (Id) |
| name | Name (string) or Name (null) (Name) |
| name\_contains | Name Contains (string) or Name Contains (null) (Name Contains) |
| dataset\_version | Dataset Version (string) or Dataset Version (null) (Dataset Version) |

| metadata | Metadata (string) or Metadata (null) (Metadata) |
| sort\_by\_feedback\_key | Sort By Feedback Key (string) or Sort By Feedback Key (null) (Sort By Feedback Key) |

| tag\_value\_id | Array of Tag Value Id (strings) or Tag Value Id (null) (Tag Value Id) |

| filter | Filter (string) or Filter (null) (Filter) |

##### header Parameters

### Responses

get/api/v1/sessions

### Response samples

`[{"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"extra": { },\
\
"name": "string",\
\
"description": "string",\
\
"default_dataset_id": "467c3141-9a68-490e-ab5c-4000a5a11fa8",\
\
"reference_dataset_id": "4abef5d4-9fac-44e9-b393-753a8de1db88",\
\
"trace_tier": "longlived",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"run_count": 0,\
\
"latency_p50": 0,\
\
"latency_p99": 0,\
\
"first_token_p50": 0,\
\
"first_token_p99": 0,\
\
"total_tokens": 0,\
\
"prompt_tokens": 0,\
\
"completion_tokens": 0,\
\
"total_cost": "string",\
\
"prompt_cost": "string",\
\
"completion_cost": "string",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"last_run_start_time": "2019-08-24T14:15:22Z",\
\
"last_run_start_time_live": "2019-08-24T14:15:22Z",\
\
"feedback_stats": { },\
\
"session_feedback_stats": { },\
\
"run_facets": [{ }\
\
],\
\
"error_rate": 0,\
\
"streaming_rate": 0,\
\
"test_run_number": 0\
\
}\
\
]`

## tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_post Create Tracer Session

##### Authorizations:

##### query Parameters

##### Request Body schema: application/json required

| end\_time | End Time (string) or End Time (null) (End Time) |
| extra | Extra (object) or Extra (null) (Extra) |
| name | string (Name) |
| description | Description (string) or Description (null) (Description) |
| default\_dataset\_id | Default Dataset Id (string) or Default Dataset Id (null) (Default Dataset Id) |
| reference\_dataset\_id | Reference Dataset Id (string) or Reference Dataset Id (null) (Reference Dataset Id) |
| trace\_tier | TraceTier (string) or null |
| id | Id (string) or Id (null) (Id) |

### Responses

post/api/v1/sessions

### Request samples

"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08"

### Response samples

## tag/tracer-sessions/operation/delete_tracer_sessions_api_v1_sessions_delete Delete Tracer Sessions

##### Authorizations:

##### query Parameters

### Responses

delete/api/v1/sessions

### Response samples

## tag/tracer-sessions/operation/read_tracer_sessions_runs_metadata_api_v1_sessions__session_id__metadata_get Read Tracer Sessions Runs Metadata

Given a session, a number K, and (optionally) a list of metadata keys, return the top K values for each key.

##### Authorizations:

##### path Parameters

##### query Parameters

| | |
| --- | --- |
| metadata\_keys | Array of Metadata Keys (strings) or Metadata Keys (null) (Metadata Keys) |
| start\_time | Start Time (string) or Start Time (null) (Start Time) |

### Responses

get/api/v1/sessions/{session\_id}/metadata

### Response samples

`{"property1": ["string"\
\
],

"property2": ["string"\
\
]

## tag/tracer-sessions/operation/read_filter_views_api_v1_sessions__session_id__views_get Read Filter Views

Get all filter views for a session.

##### Authorizations:

##### path Parameters

##### query Parameters

| | |
| --- | --- |
| type | FilterViewType (string) or Type (null) (Type) |

### Responses

get/api/v1/sessions/{session\_id}/views

### Response samples

`[{"filter_string": "string",\
\
"trace_filter_string": "string",\
\
"tree_filter_string": "string",\
\
"display_name": "string",\
\
"description": "string",\
\
"type": "runs",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/tracer-sessions/operation/create_filter_view_api_v1_sessions__session_id__views_post Create Filter View

Create a new filter view.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| filter\_string | Filter String (string) or Filter String (null) (Filter String) |
| trace\_filter\_string | Trace Filter String (string) or Trace Filter String (null) (Trace Filter String) |
| tree\_filter\_string | Tree Filter String (string) or Tree Filter String (null) (Tree Filter String) |

| description | Description (string) or Description (null) (Description) |

### Responses

post/api/v1/sessions/{session\_id}/views

### Request samples

`{"filter_string": "string",

"trace_filter_string": "string",

"tree_filter_string": "string",

"display_name": "string",

"type": "runs"

### Response samples

"type": "runs",

"created_at": "2019-08-24T14:15:22Z",

"updated_at": "2019-08-24T14:15:22Z"

## tag/tracer-sessions/operation/read_filter_view_api_v1_sessions__session_id__views__view_id__get Read Filter View

Get a specific filter view.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/sessions/{session\_id}/views/{view\_id}

### Response samples

## tag/tracer-sessions/operation/update_filter_view_api_v1_sessions__session_id__views__view_id__patch Update Filter View

Update a filter view.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| filter\_string | Filter String (string) or Filter String (null) (Filter String) |
| display\_name | Display Name (string) or Display Name (null) (Display Name) |
| description | Description (string) or Description (null) (Description) |
| trace\_filter\_string | Trace Filter String (string) or Trace Filter String (null) (Trace Filter String) |
| tree\_filter\_string | Tree Filter String (string) or Tree Filter String (null) (Tree Filter String) |
| type | FilterViewType (string) or null |

### Responses

patch/api/v1/sessions/{session\_id}/views/{view\_id}

### Request samples

### Response samples

## tag/tracer-sessions/operation/delete_filter_view_api_v1_sessions__session_id__views__view_id__delete Delete Filter View

Delete a specific filter view.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/sessions/{session\_id}/views/{view\_id}

### Response samples

## tag/tracer-sessions/operation/_Beta__Get_Insights_Jobs_api_v1_sessions__session_id__insights_get\[Beta\] Get Insights Jobs

Get all clusters for a session.

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/sessions/{session\_id}/insights

### Response samples

`{"clustering_jobs": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"name": "string",\
\
"status": "string",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"metadata": { },\
\
"shape": {"property1": 0,\
\
"property2": 0\
\
},\
\
"error": "string"\
\
}\
\
]

## tag/tracer-sessions/operation/_Beta__Create_Insights_Job_api_v1_sessions__session_id__insights_post\[Beta\] Create Insights Job

Create an insights job.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| start\_time | Start Time (string) or Start Time (null) (Start Time) |
| end\_time | End Time (string) or End Time (null) (End Time) |
| last\_n\_hours | Last N Hours (integer) or Last N Hours (null) (Last N Hours) |
| hierarchy | Array of Hierarchy (integers) or Hierarchy (null) (Hierarchy) |
| partitions | Partitions (object) or Partitions (null) (Partitions) |
| sample | Sample (number) or Sample (integer) or Sample (null) (Sample) |
| summary\_prompt | Summary Prompt (string) or Summary Prompt (null) (Summary Prompt) |
| filter | Filter (string) or Filter (null) (Filter) |
| name | Name (string) or Name (null) (Name) |
| attribute\_schemas | Attribute Schemas (object) or Attribute Schemas (null) (Attribute Schemas) |
| user\_context | User Context (object) or User Context (null) (User Context) |

### Responses

post/api/v1/sessions/{session\_id}/insights

### Request samples

"last_n_hours": 0,

"hierarchy": [0\
\
],

"partitions": {"property1": "string",

"property2": "string"

"sample": 0,

"summary_prompt": "string",

"filter": "string",

"attribute_schemas": { },

"user_context": {"property1": "string",

"model": "openai",

"validate_model_secrets": true

### Response samples

`{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",

"status": "string",

"error": "string"

## tag/tracer-sessions/operation/_Beta__Get_Insights_Job_Configs_api_v1_sessions__session_id__insights_configs_get\[Beta\] Get Insights Job Configs

Get all insights job configs for a session.

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/sessions/{session\_id}/insights/configs

### Response samples

`{"configs": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"name": "string",\
\
"description": "string",\
\
"config": {"name": "string",\
\
"last_n_hours": 0,\
\
"hierarchy": [0\
\
],\
\
"partitions": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"sample": 0,\
\
"summary_prompt": "string",\
\
"filter": "string",\
\
"attribute_schemas": { },\
\
"user_context": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"model": "openai"\
\
},\
\
"prebuilt": true\
\
}\
\
]

## tag/tracer-sessions/operation/_Beta__Create_Insights_Job_Config_api_v1_sessions__session_id__insights_configs_post\[Beta\] Create Insights Job Config

Save an insights job config.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/sessions/{session\_id}/insights/configs

### Request samples

"config": {"start_time": "2019-08-24T14:15:22Z",

### Response samples

"config": {"name": "string",

"model": "openai"

## tag/tracer-sessions/operation/_Beta__Auto_Generate_Insights_Job_Config_api_v1_sessions__session_id__insights_configs_generate_post\[Beta\] Auto-Generate Insights Job Config

Auto-generate an insights job config.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/sessions/{session\_id}/insights/configs/generate

### Request samples

`{"user_context": {"property1": "string",

### Response samples

`{"summary_prompt": "string",

"attribute_schemas": {"property1": { },

"property2": { }

## tag/tracer-sessions/operation/_Beta__Update_Insights_Job_Config_api_v1_sessions__session_id__insights_configs__config_id__patch\[Beta\] Update Insights Job Config

Update an insights job config.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| name | Name (string) or Name (null) (Name) |
| description | Description (string) or Description (null) (Description) |
| config | CreateRunClusteringJobRequest (object) or null |

### Responses

patch/api/v1/sessions/{session\_id}/insights/configs/{config\_id}

### Request samples

### Response samples

## tag/tracer-sessions/operation/_Beta__Delete_Insights_Job_Config_api_v1_sessions__session_id__insights_configs__config_id__delete\[Beta\] Delete Insights Job Config

Delete an insights job config.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/sessions/{session\_id}/insights/configs/{config\_id}

### Response samples

"message": "string"

## tag/tracer-sessions/operation/_Beta__Get_Insights_Job_api_v1_sessions__session_id__insights__job_id__get\[Beta\] Get Insights Job

Get a specific cluster job for a session.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/sessions/{session\_id}/insights/{job\_id}

### Response samples

"metadata": { },

"shape": {"property1": 0,

"property2": 0

"error": "string",

"clusters": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"parent_id": "1c6ca187-e61f-4301-8dcb-0e9749e89eef",\
\
"level": 0,\
\
"name": "string",\
\
"description": "string",\
\
"parent_name": "string",\
\
"num_runs": 0,\
\
"stats": { }\
\
}\
\
],

"report": {"key_points": ["string"\
\
],

"title": "string",

"highlighted_traces": [{"run_id": "dded282c-8ebd-44cf-8ba5-9a234973d1ec",\
\
"cluster_id": "d3d1bfdf-67c4-41fa-b065-858242731616",\
\
"cluster_name": "string",\
\
"rank": 0,\
\
"highlight_reason": "string",\
\
"summary": "string"\
\
}\
\
],

"created_at": "2019-08-24T14:15:22Z"

## tag/tracer-sessions/operation/_Beta__Update_Insights_Job_api_v1_sessions__session_id__insights__job_id__patch\[Beta\] Update Insights Job

Update a session cluster job.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

patch/api/v1/sessions/{session\_id}/insights/{job\_id}

### Request samples

`{"name": "string"

### Response samples

"status": "string"

## tag/tracer-sessions/operation/_Beta__Delete_Insights_Job_api_v1_sessions__session_id__insights__job_id__delete\[Beta\] Delete Insights Job

Delete a session cluster job.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/sessions/{session\_id}/insights/{job\_id}

### Response samples

## tag/tracer-sessions/operation/_Beta__Get_Run_Cluster_from_Insights_Job_api_v1_sessions__session_id__insights__job_id__clusters__cluster_id__get\[Beta\] Get Run Cluster From Insights Job

Get a specific cluster for a session.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/sessions/{session\_id}/insights/{job\_id}/clusters/{cluster\_id}

### Response samples

"parent_id": "1c6ca187-e61f-4301-8dcb-0e9749e89eef",

"num_children": 0,

"level": 0,

"description": "string"

## tag/tracer-sessions/operation/_Beta__Get_Runs_from_Insights_Job_api_v1_sessions__session_id__insights__job_id__runs_get\[Beta\] Get Runs From Insights Job

Get all runs for a cluster job, optionally filtered by cluster.

##### Authorizations:

##### path Parameters

##### query Parameters

| | |
| --- | --- |
| cluster\_id | Cluster Id (string) or Cluster Id (null) (Cluster Id) |

| attribute\_sort\_key | Attribute Sort Key (string) or Attribute Sort Key (null) (Attribute Sort Key) |
| attribute\_sort\_order | Attribute Sort Order (string) or Attribute Sort Order (null) (Attribute Sort Order) |

### Responses

get/api/v1/sessions/{session\_id}/insights/{job\_id}/runs

### Response samples

`{"runs": [{ }\
\
],

"offset": 0

## tag/orgs orgs

## tag/orgs/operation/list_organizations_api_v1_orgs_get List Organizations

Get all orgs visible to this auth

##### Authorizations:

_Bearer Auth_

##### query Parameters

### Responses

get/api/v1/orgs

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"display_name": "string",\
\
"tier": "no_plan",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"created_by_user_id": "209f54c4-4c33-43bc-9c6a-ef4c65ad7473",\
\
"created_by_ls_user_id": "39902b7a-c152-4bc7-8175-eb184d8ec8a0",\
\
"modified_at": "2019-08-24T14:15:22Z",\
\
"is_personal": true,\
\
"disabled": true,\
\
"sso_login_slug": "string",\
\
"sso_only": false,\
\
"jit_provisioning_enabled": true,\
\
"invites_enabled": true,\
\
"public_sharing_disabled": false,\
\
"pat_creation_disabled": false,\
\
"workspace_admin_can_invite_to_org": false,\
\
"default_sso_provision": false\
\
}\
\
]`

## tag/orgs/operation/create_organization_api_v1_orgs_post Create Organization

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/orgs

### Request samples

`{"display_name": "string",

"is_personal": true

### Response samples

"tier": "no_plan",

"created_by_user_id": "209f54c4-4c33-43bc-9c6a-ef4c65ad7473",

"created_by_ls_user_id": "39902b7a-c152-4bc7-8175-eb184d8ec8a0",

"modified_at": "2019-08-24T14:15:22Z",

"is_personal": true,

"disabled": true,

"sso_login_slug": "string",

"sso_only": false,

"jit_provisioning_enabled": true,

"invites_enabled": true,

"public_sharing_disabled": false,

"pat_creation_disabled": false,

"workspace_admin_can_invite_to_org": false,

"default_sso_provision": false

## tag/orgs/operation/create_customers_and_get_stripe_setup_intent_api_v1_orgs_current_setup_post Create Customers And Get Stripe Setup Intent

##### Authorizations:

### Responses

post/api/v1/orgs/current/setup

### Response samples

- 200

`{"client_secret": "string"

## tag/orgs/operation/get_organization_info_api_v1_orgs_current_get Get Organization Info

##### Authorizations:

### Responses

get/api/v1/orgs/current

### Response samples

"config": {"plan_tier": "string",

"max_identities": 5,

"max_workspaces": 1,

"can_use_rbac": false,

"can_use_abac": false,

"can_use_audit_logs": false,

"can_add_seats": true,

"startup_plan_approval_date": "string",

"partner_plan_approval_date": "string",

"premier_plan_approval_date": "string",

"can_disable_public_sharing": false,

"can_serve_datasets": false,

"can_use_langgraph_cloud": false,

"max_langgraph_cloud_deployments": 3,

"max_free_langgraph_cloud_deployments": 0,

"can_use_saml_sso": false,

"can_use_bulk_export": false,

"show_updated_sidenav": false,

"show_updated_resource_tags": false,

"kv_dataset_message_support": true,

"show_playground_prompt_canvas": false,

"allow_custom_iframes": false,

"enable_langgraph_pricing": false,

"enable_thread_view_playground": false,

"enable_org_usage_charts": false,

"use_exact_search_for_prompts": false,

"langgraph_deploy_own_cloud_enabled": false,

"prompt_optimization_jobs_enabled": false,

"demo_lgp_new_graph_enabled": false,

"datadog_rum_session_sample_rate": 20,

"langgraph_remote_reconciler_enabled": false,

"langgraph_enterprise_enabled": false,

"langsmith_alerts_poc_enabled": true,

"tenant_skip_topk_facets": false,

"lgp_templates_enabled": false,

"enable_align_evaluators": false,

"enable_run_tree_streaming": false,

"enable_querying_v2_endpoints": false,

"enable_threads_improvements": false,

"max_prompt_webhooks": 1,

"playground_evaluator_strategy": "sync",

"enable_monthly_usage_charts": false,

"new_rule_evaluator_creation_version": 2,

"enable_lgp_listeners_page": false,

"clio_enabled": false,

"enable_include_extended_stats": false,

"enable_markdown_in_tracing": false,

"enable_pricing_redesign": false,

"arbitrary_cost_tracking_enabled": false,

"langsmith_deployment_distributed_runtime_enabled": false,

"pairwise_annotation_queues_enabled": false,

"max_agent_builder_assistants": 1000,

"enable_granular_usage_reporting": false

"connected_to_stripe": true,

"connected_to_metronome": true,

"payment_method": {"brand": "string",

"last4": "string",

"exp_month": 0,

"exp_year": 0,

"email": "string"

"has_cancelled": true,

"end_of_billing_period": "2019-08-24T14:15:22Z",

"current_plan": {"tier": "no_plan",

"started_on": "2019-08-24T14:15:22Z",

"ends_on": "2019-08-24T14:15:22Z"

"upcoming_plan": {"tier": "no_plan",

"reached_max_workspaces": false,

"permissions": [ ],

"marketplace_payouts_enabled": false,

"wallet": {"credit_balance_micros": 0,

"inflight_balance_micros": 0

## tag/orgs/operation/get_current_organization_info_api_v1_orgs_current_info_get Get Current Organization Info

##### Authorizations:

### Responses

get/api/v1/orgs/current/info

### Response samples

"disabled": false,

"member_disabled": false,

## tag/orgs/operation/update_current_organization_info_api_v1_orgs_current_info_patch Update Current Organization Info

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| display\_name | string (Display Name) non-empty ^\[a-zA-Z0-9\\-\_ \]+$ |
| public\_sharing\_disabled | Public Sharing Disabled (boolean) or Public Sharing Disabled (null) (Public Sharing Disabled) |
| pat\_creation\_disabled | Pat Creation Disabled (boolean) or Pat Creation Disabled (null) (Pat Creation Disabled) |
| unshare\_all | Unshare All (boolean) or Unshare All (null) (Unshare All) |
| jit\_provisioning\_enabled | Jit Provisioning Enabled (boolean) or Jit Provisioning Enabled (null) (Jit Provisioning Enabled) |
| workspace\_admin\_can\_invite\_to\_org | Workspace Admin Can Invite To Org (boolean) or Workspace Admin Can Invite To Org (null) (Workspace Admin Can Invite To Org) |
| invites\_enabled | Invites Enabled (boolean) or Invites Enabled (null) (Invites Enabled) |

### Responses

patch/api/v1/orgs/current/info

### Request samples

"public_sharing_disabled": true,

"pat_creation_disabled": true,

"unshare_all": true,

"workspace_admin_can_invite_to_org": true,

"invites_enabled": true

### Response samples

## tag/orgs/operation/get_organization_billing_info_api_v1_orgs_current_billing_get Get Organization Billing Info

##### Authorizations:

### Responses

get/api/v1/orgs/current/billing

### Response samples

## tag/orgs/operation/get_dashboard_api_v1_orgs_current_dashboard_get Get Dashboard

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/orgs/current/dashboard

### Response samples

`{"embeddable_url": "string"

## tag/orgs/operation/on_payment_method_created_api_v1_orgs_current_payment_method_post On Payment Method Created

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/orgs/current/payment-method

### Request samples

`{"billing_info": {"name": "string",

"address": {"line1": "string",

"line2": "string",

"city": "string",

"state": "string",

"postal_code": "string",

"country": "string"

"setup_intent": "string"

### Response samples

## tag/orgs/operation/get_company_info_api_v1_orgs_current_business_info_get Get Company Info

##### Authorizations:

### Responses

get/api/v1/orgs/current/business-info

### Response samples

`{"company_info": {"name": "string",

"tax_id": {"value": "string",

"type": "string"

"invoice_email": "string",

"is_business": false

## tag/orgs/operation/set_company_info_api_v1_orgs_current_business_info_post Set Company Info

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| company\_info | StripeBusinessBillingInfo (object) or null |
| tax\_id | StripeTaxId (object) or null |
| invoice\_email | Invoice Email (string) or Invoice Email (null) (Invoice Email) |

### Responses

post/api/v1/orgs/current/business-info

### Request samples

### Response samples

## tag/orgs/operation/change_payment_plan_api_v1_orgs_current_plan_post Change Payment Plan

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/orgs/current/plan

### Request samples

`{"tier": "disabled"

### Response samples

## tag/orgs/operation/list_organization_roles_api_v1_orgs_current_roles_get List Organization Roles

##### Authorizations:

### Responses

get/api/v1/orgs/current/roles

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"name": "string",\
\
"display_name": "string",\
\
"description": "string",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"permissions": ["string"\
\
],\
\
"access_scope": "organization"\
\
}\
\
]`

## tag/orgs/operation/create_organization_roles_api_v1_orgs_current_roles_post Create Organization Roles

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/orgs/current/roles

### Request samples

"permissions": ["string"\
\
]

### Response samples

"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",

"permissions": ["string"\
\
],

"access_scope": "organization"

## tag/orgs/operation/delete_organization_roles_api_v1_orgs_current_roles__role_id__delete Delete Organization Roles

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/orgs/current/roles/{role\_id}

### Response samples

## tag/orgs/operation/update_organization_roles_api_v1_orgs_current_roles__role_id__patch Update Organization Roles

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

patch/api/v1/orgs/current/roles/{role\_id}

### Request samples

### Response samples

## tag/orgs/operation/list_permissions_api_v1_orgs_permissions_get List Permissions

##### Authorizations:

### Responses

get/api/v1/orgs/permissions

### Response samples

`[{"name": "string",\
\
"description": "string",\
\
"access_scope": "organization"\
\
}\
\
]`

## tag/orgs/operation/list_pending_organization_invites_api_v1_orgs_pending_get List Pending Organization Invites

Get all pending orgs visible to this auth

##### Authorizations:

### Responses

get/api/v1/orgs/pending

### Response samples

## tag/orgs/operation/get_current_org_members_api_v1_orgs_current_members_get Get Current Org Members

##### Authorizations:

### Responses

get/api/v1/orgs/current/members

### Response samples

`{"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",

"members": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"ls_user_id": "e368397c-ab29-448f-907e-f04754df6859",\
\
"read_only": true,\
\
"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",\
\
"role_name": "string",\
\
"access_scope": "organization",\
\
"email": "string",\
\
"full_name": "string",\
\
"avatar_url": "string",\
\
"linked_login_methods": [ ],\
\
"display_name": "string",\
\
"is_disabled": false,\
\
"org_role_id": "ddedde81-fced-474e-bafa-6314571c3554",\
\
"org_role_name": "string",\
\
"tenant_ids": [ ]\
\
}\
\
],

"pending": [{"email": "string",\
\
"read_only": false,\
\
"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",\
\
"workspace_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"workspace_role_id": "a57b9738-8b78-46be-b756-c7376ec37998",\
\
"password": "string",\
\
"full_name": "string",\
\
"access_scope": "organization",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"role_name": "string",\
\
"org_role_id": "ddedde81-fced-474e-bafa-6314571c3554",\
\
"org_role_name": "string",\
\
"tenant_ids": [ ]\
\
}\
\
]

## tag/orgs/operation/add_member_to_current_org_api_v1_orgs_current_members_post Add Member To Current Org

##### Authorizations:

##### Request Body schema: application/json required

| role\_id | Role Id (string) or Role Id (null) (Role Id) |
| workspace\_ids | Array of Workspace Ids (strings) or Workspace Ids (null) (Workspace Ids) |
| workspace\_role\_id | Workspace Role Id (string) or Workspace Role Id (null) (Workspace Role Id) |
| password | Password (string) or Password (null) (Password) |
| full\_name | Full Name (string) or Full Name (null) (Full Name) |

### Responses

post/api/v1/orgs/current/members

### Request samples

`{"email": "string",

"read_only": false,

"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",

"workspace_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"workspace_role_id": "a57b9738-8b78-46be-b756-c7376ec37998",

"password": "string",

"full_name": "string"

### Response samples

"full_name": "string",

"access_scope": "organization",

"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",

"role_name": "string",

"org_role_id": "ddedde81-fced-474e-bafa-6314571c3554",

"org_role_name": "string"

## tag/orgs/operation/get_current_active_org_members_api_v1_orgs_current_members_active_get Get Current Active Org Members

##### Authorizations:

##### query Parameters

| is\_disabled | Is Disabled (boolean) or Is Disabled (null) (Is Disabled) |

### Responses

get/api/v1/orgs/current/members/active

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"ls_user_id": "e368397c-ab29-448f-907e-f04754df6859",\
\
"read_only": true,\
\
"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",\
\
"role_name": "string",\
\
"access_scope": "organization",\
\
"email": "string",\
\
"full_name": "string",\
\
"avatar_url": "string",\
\
"linked_login_methods": [ ],\
\
"display_name": "string",\
\
"is_disabled": false,\
\
"org_role_id": "ddedde81-fced-474e-bafa-6314571c3554",\
\
"org_role_name": "string",\
\
"tenant_ids": [ ]\
\
}\
\
]`

## tag/orgs/operation/get_current_pending_org_members_api_v1_orgs_current_members_pending_get Get Current Pending Org Members

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/orgs/current/members/pending

### Response samples

`[{"email": "string",\
\
"read_only": false,\
\
"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",\
\
"workspace_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"workspace_role_id": "a57b9738-8b78-46be-b756-c7376ec37998",\
\
"password": "string",\
\
"full_name": "string",\
\
"access_scope": "organization",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"role_name": "string",\
\
"org_role_id": "ddedde81-fced-474e-bafa-6314571c3554",\
\
"org_role_name": "string",\
\
"tenant_ids": [ ]\
\
}\
\
]`

## tag/orgs/operation/add_members_to_current_org_batch_api_v1_orgs_current_members_batch_post Add Members To Current Org Batch

Batch invite up to 500 users to the current org.

##### Authorizations:

##### Request Body schema: application/json required

Array

### Responses

post/api/v1/orgs/current/members/batch

### Request samples

`[{"email": "string",\
\
"read_only": false,\
\
"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",\
\
"workspace_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"workspace_role_id": "a57b9738-8b78-46be-b756-c7376ec37998",\
\
"password": "string",\
\
"full_name": "string"\
\
}\
\
]`

### Response samples

`[{"email": "string",\
\
"read_only": false,\
\
"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",\
\
"workspace_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"workspace_role_id": "a57b9738-8b78-46be-b756-c7376ec37998",\
\
"password": "string",\
\
"full_name": "string",\
\
"access_scope": "organization",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"role_name": "string",\
\
"org_role_id": "ddedde81-fced-474e-bafa-6314571c3554",\
\
"org_role_name": "string"\
\
}\
\
]`

## tag/orgs/operation/add_basic_auth_members_to_current_org_api_v1_orgs_current_members_basic_batch_post Add Basic Auth Members To Current Org

Batch add up to 500 users to the org and specified workspaces in basic auth mode.

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| user\_id | User Id (string) or User Id (null) (User Id) |
| ls\_user\_id | Ls User Id (string) or Ls User Id (null) (Ls User Id) |

| role\_id | Role Id (string) or Role Id (null) (Role Id) |
| password | Password (string) or Password (null) (Password) |
| full\_name | Full Name (string) or Full Name (null) (Full Name) |
| workspace\_role\_id | Workspace Role Id (string) or Workspace Role Id (null) (Workspace Role Id) |
| workspace\_ids | Array of Workspace Ids (strings) or Workspace Ids (null) (Workspace Ids) |

### Responses

post/api/v1/orgs/current/members/basic/batch

### Request samples

`[{"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"ls_user_id": "e368397c-ab29-448f-907e-f04754df6859",\
\
"email": "string",\
\
"read_only": true,\
\
"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",\
\
"password": "string",\
\
"full_name": "string",\
\
"workspace_role_id": "a57b9738-8b78-46be-b756-c7376ec37998",\
\
"workspace_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
}\
\
]`

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"ls_user_id": "e368397c-ab29-448f-907e-f04754df6859",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"email": "string",\
\
"full_name": "string",\
\
"avatar_url": "string",\
\
"password": "string"\
\
}\
\
]`

## tag/orgs/operation/delete_current_org_pending_member_api_v1_orgs_current_members__identity_id__pending_delete Delete Current Org Pending Member

When an admin deletes a pending member invite.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/orgs/current/members/{identity\_id}/pending

### Response samples

## tag/orgs/operation/delete_pending_organization_invite_api_v1_orgs_pending__organization_id__delete Delete Pending Organization Invite

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/orgs/pending/{organization\_id}

### Response samples

## tag/orgs/operation/claim_pending_organization_invite_api_v1_orgs_pending__organization_id__claim_post Claim Pending Organization Invite

##### Authorizations:

##### path Parameters

### Responses

post/api/v1/orgs/pending/{organization\_id}/claim

### Response samples

"ls_user_id": "e368397c-ab29-448f-907e-f04754df6859",

"read_only": true,

## tag/orgs/operation/remove_member_from_current_org_api_v1_orgs_current_members__identity_id__delete Remove Member From Current Org

Remove a user from the current organization.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/orgs/current/members/{identity\_id}

### Response samples

## tag/orgs/operation/update_current_org_member_api_v1_orgs_current_members__identity_id__patch Update Current Org Member

This is used for updating a user's role (all auth modes) or full\_name/password (basic auth)

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| password | Password (string) or Password (null) (Password) |
| full\_name | Full Name (string) or Full Name (null) (Full Name) |
| role\_id | Role Id (string) or Role Id (null) (Role Id) |

### Responses

patch/api/v1/orgs/current/members/{identity\_id}

### Request samples

`{"password": "string",

"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9"

### Response samples

## tag/orgs/operation/update_current_user_api_v1_orgs_members_basic_patch Update Current User

Update a user's full\_name/password (basic auth only)

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| password | Password (string) or Password (null) (Password) |
| full\_name | Full Name (string) or Full Name (null) (Full Name) |

### Responses

patch/api/v1/orgs/members/basic

### Request samples

### Response samples

## tag/orgs/operation/list_ttl_settings_api_v1_orgs_ttl_settings_get List Ttl Settings

List out the configured TTL settings for a given org (org-level and tenant-level).

##### Authorizations:

### Responses

get/api/v1/orgs/ttl-settings

### Response samples

`[{"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"default_trace_tier": "longlived",\
\
"apply_to_all_projects": false,\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"configured_by": "system"\
\
}\
\
]`

## tag/orgs/operation/upsert_ttl_settings_api_v1_orgs_ttl_settings_put Upsert Ttl Settings

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| tenant\_id | Tenant Id (string) or Tenant Id (null) (Tenant Id) |

### Responses

put/api/v1/orgs/ttl-settings

### Request samples

`{"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",

"default_trace_tier": "longlived",

"apply_to_all_projects": false

### Response samples

"apply_to_all_projects": false,

"updated_at": "2019-08-24T14:15:22Z",

"configured_by": "system"

## tag/orgs/operation/get_current_sso_settings_api_v1_orgs_current_sso_settings_get Get Current Sso Settings

Get SSO provider settings for the current organization.

##### Authorizations:

### Responses

get/api/v1/orgs/current/sso-settings

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"provider_id": "fe3d49af-4061-436b-ae60-f7044f252a44",\
\
"default_workspace_role_id": "ebbe3ddf-b126-46d7-af9d-624616bf9fa8",\
\
"default_workspace_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"metadata_url": "string",\
\
"metadata_xml": "string"\
\
}\
\
]`

## tag/orgs/operation/create_sso_settings_api_v1_orgs_current_sso_settings_post Create Sso Settings

Create SSO provider settings for the current organization.

##### Authorizations:

##### Request Body schema: application/json required

| metadata\_xml | Metadata Xml (string) or Metadata Xml (null) (Metadata Xml) |
| metadata\_url | Metadata Url (string) or Metadata Url (null) (Metadata Url) |
| attribute\_mapping | object (Attribute Mapping) |

### Responses

**201**

post/api/v1/orgs/current/sso-settings

### Request samples

`{"default_workspace_role_id": "ebbe3ddf-b126-46d7-af9d-624616bf9fa8",

"default_workspace_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"metadata_xml": "string",

"metadata_url": "string",

"attribute_mapping": {"property1": "string",

### Response samples

- 201
- 422

"provider_id": "fe3d49af-4061-436b-ae60-f7044f252a44",

"default_workspace_role_id": "ebbe3ddf-b126-46d7-af9d-624616bf9fa8",

"metadata_xml": "string"

## tag/orgs/operation/update_sso_settings_api_v1_orgs_current_sso_settings__id__patch Update Sso Settings

Update SSO provider settings defaults for the current organization.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| default\_workspace\_role\_id | Default Workspace Role Id (string) or Default Workspace Role Id (null) (Default Workspace Role Id) |
| default\_workspace\_ids | Array of Default Workspace Ids (strings) or Default Workspace Ids (null) (Default Workspace Ids) |
| metadata\_url | Metadata Url (string) or Metadata Url (null) (Metadata Url) |
| metadata\_xml | Metadata Xml (string) or Metadata Xml (null) (Metadata Xml) |

### Responses

patch/api/v1/orgs/current/sso-settings/{id}

### Request samples

### Response samples

## tag/orgs/operation/delete_sso_settings_api_v1_orgs_current_sso_settings__id__delete Delete Sso Settings

Delete SSO provider settings for the current organization.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/orgs/current/sso-settings/{id}

### Response samples

## tag/orgs/operation/update_allowed_login_methods_api_v1_orgs_current_login_methods_patch Update Allowed Login Methods

Update allowed login methods for the current organization.

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| sso\_only | Sso Only (boolean) or Sso Only (null) (Sso Only) |

### Responses

patch/api/v1/orgs/current/login-methods

### Request samples

`{"sso_only": true

### Response samples

`{ }`

## tag/orgs/operation/get_org_usage_api_v1_orgs_current_billing_usage_get Get Org Usage

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/orgs/current/billing/usage

### Response samples

`[{"customer_id": "string",\
\
"billable_metric_id": "string",\
\
"billable_metric_name": "string",\
\
"start_timestamp": "string",\
\
"end_timestamp": "string",\
\
"value": 0,\
\
"groups": {"property1": 0,\
\
"property2": 0\
\
}\
\
}\
\
]`

## tag/orgs/operation/get_granular_usage_api_v1_orgs_current_billing_granular_usage_get Get Granular Usage

Get granular usage data with flexible grouping.

workspace\_ids filters results to the specified workspaces. Only workspaces
the user has read access to will be included in the results.

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/orgs/current/billing/granular-usage

### Response samples

`{"stride": {"days": 0,

"hours": 0

"usage": [{"time_bucket": "2019-08-24T14:15:22Z",\
\
"dimensions": {"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"user_email": "string",\
\
"api_key_id": "b0dd218e-3bcf-4bdb-a1e3-0689d60a8afd",\
\
"api_key_short_key": "string",\
\
"project_id": "405d8375-3514-403b-8c43-83ae74cfe0e9",\
\
"project_name": "string",\
\
"workspace_id": "0967198e-ec7b-4c6b-b4d3-f71244cadbe9",\
\
"workspace_name": "string"\
\
},\
\
"traces": 0\
\
}\
\
]

## tag/orgs/operation/export_granular_usage_csv_api_v1_orgs_current_billing_granular_usage_export_get Export Granular Usage Csv

Export granular usage data as CSV.

Returns the same data as the granular-usage endpoint but formatted as a
downloadable CSV file. Only workspaces the user has read access to will
be included in the results.

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/orgs/current/billing/granular-usage/export

### Response samples

## tag/orgs/operation/get_current_user_login_methods_api_v1_orgs_current_user_login_methods_get Get Current User Login Methods

Get login methods for the current user.

##### Authorizations:

### Responses

get/api/v1/orgs/current/user/login-methods

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"provider": "email",\
\
"ls_user_id": "e368397c-ab29-448f-907e-f04754df6859",\
\
"saml_provider_id": "cb4996c0-5104-4604-9815-d20a0f207d98",\
\
"provider_user_id": "b85fc302-f5cf-47ae-9c42-4e8796aa59c3",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"email": "string",\
\
"full_name": "string",\
\
"first_name": "string",\
\
"last_name": "string",\
\
"username": "string",\
\
"is_disabled": true,\
\
"provisioning_method": "scim",\
\
"email_confirmed_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/orgs/operation/create_stripe_checkout_sessions_endpoint_api_v1_orgs_current_stripe_checkout_session_post Create Stripe Checkout Sessions Endpoint

Kick off a Stripe checkout session flow.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/orgs/current/stripe\_checkout\_session

### Request samples

`{"amount_cents": 0,

"success_path": "string"

### Response samples

## tag/orgs/operation/confirm_checkout_session_completion_endpoint_api_v1_orgs_current_confirm_checkout_session_completion_post Confirm Checkout Session Completion Endpoint

Complete a Stripe checkout session flow.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/orgs/current/confirm\_checkout\_session\_completion

### Request samples

`{"stripe_session_id": "string"

### Response samples

## tag/orgs/operation/create_stripe_account_links_endpoint_api_v1_orgs_current_stripe_account_links_post Create Stripe Account Links Endpoint

Kick off a Stripe account link flow.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/orgs/current/stripe\_account\_links

### Request samples

`{"success_path": "string"

### Response samples

## tag/orgs/operation/list_org_service_keys_api_v1_orgs_current_service_keys_get List Org Service Keys

##### Authorizations:

### Responses

get/api/v1/orgs/current/service-keys

### Response samples

`[{"created_at": "2019-08-24T14:15:22Z",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"short_key": "string",\
\
"description": "string",\
\
"read_only": false,\
\
"last_used_at": "2019-08-24T14:15:22Z",\
\
"expires_at": "2019-08-24T14:15:22Z",\
\
"workspace_names": ["string"\
\
]\
\
}\
\
]`

## tag/orgs/operation/create_org_service_key_api_v1_orgs_current_service_keys_post Create Org Service Key

Create org-scoped service key. If workspaces is None, key is org-wide.

##### Authorizations:

##### Request Body schema: application/json required

| expires\_at | Expires At (string) or Expires At (null) (Expires At) |
| workspaces | Array of Workspaces (strings) or Workspaces (null) (Workspaces) |
| role\_id | Role Id (string) or Role Id (null) (Role Id) |
| org\_role\_id | Org Role Id (string) or Org Role Id (null) (Org Role Id) |
| default\_workspace\_id | Default Workspace Id (string) or Default Workspace Id (null) (Default Workspace Id) |

### Responses

post/api/v1/orgs/current/service-keys

### Request samples

`{"description": "Default API key",

"expires_at": "2019-08-24T14:15:22Z",

"workspaces": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"default_workspace_id": "d3378579-d2d2-4013-844f-d2631508ee1f"

### Response samples

`{"created_at": "2019-08-24T14:15:22Z",

"short_key": "string",

"last_used_at": "2019-08-24T14:15:22Z",

"workspace_names": ["string"\
\
],

"key": "string"

## tag/orgs/operation/delete_org_service_key_api_v1_orgs_current_service_keys__api_key_id__delete Delete Org Service Key

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/orgs/current/service-keys/{api\_key\_id}

### Response samples

"workspace_names": ["string"\
\
]

## tag/orgs/operation/list_org_personal_access_tokens_api_v1_orgs_current_personal_access_tokens_get List Org Personal Access Tokens

##### Authorizations:

### Responses

get/api/v1/orgs/current/personal-access-tokens

### Response samples

## tag/orgs/operation/create_org_personal_access_token_api_v1_orgs_current_personal_access_tokens_post Create Org Personal Access Token

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/orgs/current/personal-access-tokens

### Request samples

### Response samples

## tag/orgs/operation/delete_org_personal_access_token_api_v1_orgs_current_personal_access_tokens__pat_id__delete Delete Org Personal Access Token

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/orgs/current/personal-access-tokens/{pat\_id}

### Response samples

## tag/orgs/operation/set_default_sso_provision_api_v1_orgs_current_set_default_sso_provision_post Set Default Sso Provision

Set the current organization as the default for SSO provisioning in self-hosted environments.

##### Authorizations:

### Responses

post/api/v1/orgs/current/set-default-sso-provision

### Response samples

## tag/auth auth

## tag/auth/operation/login_api_v1_login_post Login

### Responses

post/api/v1/login

### Response samples

`{"access_token": "string"

## tag/auth/operation/send_sso_email_confirmation_api_v1_sso_email_verification_send_post Send Sso Email Confirmation

Send an email to confirm the email address for an SSO user.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

**202**

post/api/v1/sso/email-verification/send

### Request samples

"saml_provider_id": "cb4996c0-5104-4604-9815-d20a0f207d98"

### Response samples

- 202
- 422

## tag/auth/operation/check_sso_email_verification_status_api_v1_sso_email_verification_status_post Check Sso Email Verification Status

Retrieve the email verification status of an SSO user.

##### Request Body schema: application/json required

### Responses

post/api/v1/sso/email-verification/status

### Request samples

### Response samples

`{"email_confirmed_at": "2019-08-24T14:15:22Z"

## tag/auth/operation/confirm_sso_user_email_api_v1_sso_email_verification_confirm_post Confirm Sso User Email

Confirm the email of an SSO user.

##### Request Body schema: application/json required

### Responses

post/api/v1/sso/email-verification/confirm

### Request samples

`{"token": "string"

### Response samples

## tag/auth/operation/get_sso_settings_api_v1_sso_settings__sso_login_slug__get Get Sso Settings

Get SSO provider settings from login slug.

##### path Parameters

### Responses

get/api/v1/sso/settings/{sso\_login\_slug}

### Response samples

`[{"provider_id": "fe3d49af-4061-436b-ae60-f7044f252a44",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"organization_display_name": "string"\
\
}\
\
]`

## tag/api-key api-key

## tag/api-key/operation/get_api_keys_api_v1_api_key_get Get Api Keys

Get the current tenant's API keys

##### Authorizations:

### Responses

get/api/v1/api-key

### Response samples

## tag/api-key/operation/generate_api_key_api_v1_api_key_post Generate Api Key

Generate an api key for the user

##### Authorizations:

##### Request Body schema: application/json

### Responses

post/api/v1/api-key

### Request samples

### Response samples

## tag/api-key/operation/delete_api_key_api_v1_api_key__api_key_id__delete Delete Api Key

Delete an api key for the user

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/api-key/{api\_key\_id}

### Response samples

## tag/api-key/operation/get_personal_access_tokens_api_v1_api_key_current_get Get Personal Access Tokens Deprecated

DEPRECATED: Use /orgs/current/personal-access-tokens instead

##### Authorizations:

### Responses

get/api/v1/api-key/current

### Response samples

## tag/api-key/operation/generate_personal_access_token_api_v1_api_key_current_post Generate Personal Access Token

##### Authorizations:

##### Request Body schema: application/json

### Responses

post/api/v1/api-key/current

### Request samples

### Response samples

## tag/api-key/operation/delete_personal_access_token_api_v1_api_key_current__pat_id__delete Delete Personal Access Token Deprecated

DEPRECATED: Use /orgs/current/personal-access-tokens/{pat\_id} instead

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/api-key/current/{pat\_id}

### Response samples

## tag/examples examples

## tag/examples/operation/count_examples_api_v1_examples_count_get Count Examples

Count all examples by query params

##### Authorizations:

##### query Parameters

| | |
| --- | --- |
| id | Array of Id (strings) or Id (null) (Id) |

| metadata | Metadata (string) or Metadata (null) (Metadata) |
| full\_text\_contains | Array of Full Text Contains (strings) or Full Text Contains (null) (Full Text Contains) |
| splits | Array of Splits (strings) or Splits (null) (Splits) |
| dataset | Dataset (string) or Dataset (null) (Dataset) |
| filter | Filter (string) or Filter (null) (Filter) |

### Responses

get/api/v1/examples/count

### Response samples

Copy
0

`0`

## tag/examples/operation/read_example_api_v1_examples__example_id__get Read Example

Get a specific example.

##### Authorizations:

##### path Parameters

##### query Parameters

| dataset | Dataset (string) or Dataset (null) (Dataset) |

### Responses

get/api/v1/examples/{example\_id}

### Response samples

`{"outputs": { },

"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0",

"source_run_id": "51116a97-0041-4601-85fc-eb599c2885fb",

"inputs": { },

"attachment_urls": { }

## tag/examples/operation/update_example_api_v1_examples__example_id__patch Update Example

Update a specific example.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| dataset\_id | Dataset Id (string) or Dataset Id (null) (Dataset Id) |
| inputs | Inputs (object) or Inputs (null) (Inputs) |
| outputs | Outputs (object) or Outputs (null) (Outputs) |
| attachments\_operations | AttachmentsOperations (object) or null |
| metadata | Metadata (object) or Metadata (null) (Metadata) |
| split | Array of Split (strings) or Split (string) or Split (null) (Split) |

### Responses

patch/api/v1/examples/{example\_id}

### Request samples

`{"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0",

"outputs": { },

"attachments_operations": {"rename": {"property1": "string",

"retain": ["string"\
\
]

"split": ["string"\
\
],

"overwrite": false

### Response samples

## tag/examples/operation/delete_example_api_v1_examples__example_id__delete Delete Example

Soft delete an example. Only deletes the example in the 'latest' version of the dataset.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/examples/{example\_id}

### Response samples

## tag/examples/operation/read_examples_api_v1_examples_get Read Examples

Get all examples by query params

##### Authorizations:

##### query Parameters

| metadata | Metadata (string) or Metadata (null) (Metadata) |
| full\_text\_contains | Array of Full Text Contains (strings) or Full Text Contains (null) (Full Text Contains) |
| splits | Array of Splits (strings) or Splits (null) (Splits) |
| dataset | Dataset (string) or Dataset (null) (Dataset) |

| random\_seed | Random Seed (number) or Random Seed (null) (Random Seed) |

| descending | Descending (boolean) or Descending (null) (Descending) |
| filter | Filter (string) or Filter (null) (Filter) |

### Responses

get/api/v1/examples

### Response samples

`[{"outputs": { },\
\
"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0",\
\
"source_run_id": "51116a97-0041-4601-85fc-eb599c2885fb",\
\
"metadata": { },\
\
"inputs": { },\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"name": "string",\
\
"modified_at": "2019-08-24T14:15:22Z",\
\
"attachment_urls": { }\
\
}\
\
]`

## tag/examples/operation/create_example_api_v1_examples_post Create Example

Create a new example.

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| outputs | Outputs (object) or Outputs (null) (Outputs) |

| source\_run\_id | Source Run Id (string) or Source Run Id (null) (Source Run Id) |
| metadata | Metadata (object) or Metadata (null) (Metadata) |
| inputs | Inputs (object) or Inputs (null) (Inputs) |

| id | Id (string) or Id (null) (Id) |

| created\_at | string (Created At) |

### Responses

post/api/v1/examples

### Request samples

"split": "base",

"use_source_run_io": false,

"use_source_run_attachments": [ ],

"use_legacy_message_format": false,

"created_at": "string"

### Response samples

## tag/examples/operation/delete_examples_api_v1_examples_delete Delete Examples

Soft delete examples. Only deletes the examples in the 'latest' version of the dataset.

##### Authorizations:

##### query Parameters

### Responses

delete/api/v1/examples

### Response samples

## tag/examples/operation/create_examples_api_v1_examples_bulk_post Create Examples

Create bulk examples.

##### Authorizations:

##### Request Body schema: application/json required

| created\_at | Created At (string) or Created At (null) (Created At) |

### Responses

post/api/v1/examples/bulk

### Request samples

`[{"outputs": { },\
\
"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0",\
\
"source_run_id": "51116a97-0041-4601-85fc-eb599c2885fb",\
\
"metadata": { },\
\
"inputs": { },\
\
"split": "base",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"use_source_run_io": false,\
\
"use_source_run_attachments": [ ],\
\
"use_legacy_message_format": false,\
\
"created_at": "string"\
\
}\
\
]`

### Response samples

## tag/examples/operation/legacy_update_examples_api_v1_examples_bulk_patch Legacy Update Examples

Legacy update examples in bulk. For update involving attachments, use PATCH /v1/platform/datasets/{dataset\_id}/examples instead.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

patch/api/v1/examples/bulk

### Request samples

`[{"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0",\
\
"inputs": { },\
\
"outputs": { },\
\
"attachments_operations": {"rename": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"retain": ["string"\
\
]\
\
},\
\
"metadata": { },\
\
"split": ["string"\
\
],\
\
"overwrite": false,\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
}\
\
]`

### Response samples

## tag/examples/operation/upload_examples_from_csv_api_v1_examples_upload__dataset_id__post Upload Examples From Csv

Upload examples from a CSV file.

Note: For non-csv upload, please use
the POST /v1/platform/datasets/{dataset\_id}/examples endpoint which provides more efficient upload.

##### Authorizations:

##### path Parameters

##### Request Body schema: multipart/form-data required

| output\_keys | Array of strings (Output Keys) |
| metadata\_keys | Array of strings (Metadata Keys) |

### Responses

post/api/v1/examples/upload/{dataset\_id}

### Response samples

## tag/examples/operation/validate_example_api_v1_examples_validate_post Validate Example

Validate an example.

##### Authorizations:

### Responses

post/api/v1/examples/validate

### Response samples

## tag/examples/operation/validate_examples_api_v1_examples_validate_bulk_post Validate Examples

##### Authorizations:

### Responses

post/api/v1/examples/validate/bulk

### Response samples

`[{"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0",\
\
"inputs": { },\
\
"outputs": { },\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"metadata": { },\
\
"source_run_id": "51116a97-0041-4601-85fc-eb599c2885fb",\
\
"split": "base",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"use_source_run_io": false,\
\
"overwrite": false\
\
}\
\
]`

## tag/examples/paths/~1v1~1platform~1datasets~1examples~1delete/post Hard Delete Examples

This endpoint hard deletes _all_ versions of a dataset example(s).
Deletion is performed by setting inputs, outputs, and metadata to null and deleting attachment files while keeping the example ID, dataset ID, and creation timestamp.
IMPORTANT: attachment files can take up to 7 days to be deleted. inputs, outputs and metadata are nullified immediately.

##### Request Body schema: application/json required

### Responses

OK

**400**

Bad Request

**403**

Forbidden

**404**

Not Found

Unprocessable Entity

post/v1/platform/datasets/examples/delete

### Request samples

`{"example_ids": ["string"\
\
],

"hard_delete": true

### Response samples

- 200
- 400
- 403
- 404
- 422

`{"count": 1,

"example_ids": ["[\"123e4567-e89b-12d3-a456-426614174000\"]"\
\
]

## tag/examples/paths/~1v1~1platform~1datasets~1{dataset_id}~1examples/post Upload Examples

This endpoint allows clients to upload examples to a specified dataset by sending a multipart/form-data POST request.
Each form part contains either JSON-encoded data or binary attachment files associated with an example.

##### path Parameters

##### Request Body schema: multipart/form-data required

### Responses

Created

**409**

Conflict

post/v1/platform/datasets/{dataset\_id}/examples

### Response samples

- 201
- 400
- 403
- 409
- 422

## tag/examples/paths/~1v1~1platform~1datasets~1{dataset_id}~1examples/patch Update Examples

This endpoint allows clients to update existing examples in a specified dataset by sending a multipart/form-data PATCH request.
Each form part contains either JSON-encoded data or binary attachment files to update an example.

##### path Parameters

##### Request Body schema: multipart/form-data required

### Responses

patch/v1/platform/datasets/{dataset\_id}/examples

### Response samples

- 201
- 400
- 403
- 404
- 409
- 422

## tag/datasets datasets

## tag/datasets/operation/read_datasets_api_v1_datasets_get Read Datasets

Get all datasets by query params and owner.

##### Authorizations:

##### query Parameters

| | |
| --- | --- |
| id | Array of Id (strings) or Id (null) (Id) |
| data\_type | Array of Data Type (strings) or DataType (string) or Data Type (null) (Data Type) |
| name | Name (string) or Name (null) (Name) |
| name\_contains | Name Contains (string) or Name Contains (null) (Name Contains) |
| metadata | Metadata (string) or Metadata (null) (Metadata) |

| exclude | Array of Exclude (strings) or Exclude (null) (Exclude) |

### Responses

get/api/v1/datasets

### Response samples

`[{"name": "string",\
\
"description": "string",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"inputs_schema_definition": { },\
\
"outputs_schema_definition": { },\
\
"externally_managed": false,\
\
"transformations": [{"path": ["string"\
\
],\
\
"transformation_type": "convert_to_openai_message"\
\
}\
\
],\
\
"data_type": "kv",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"example_count": 0,\
\
"session_count": 0,\
\
"modified_at": "2019-08-24T14:15:22Z",\
\
"last_session_start_time": "2019-08-24T14:15:22Z",\
\
"metadata": { }\
\
}\
\
]`

## tag/datasets/operation/create_dataset_api_v1_datasets_post Create Dataset

Create a new dataset.

##### Authorizations:

##### Request Body schema: application/json required

| inputs\_schema\_definition | Inputs Schema Definition (object) or Inputs Schema Definition (null) (Inputs Schema Definition) |
| outputs\_schema\_definition | Outputs Schema Definition (object) or Outputs Schema Definition (null) (Outputs Schema Definition) |

| transformations | Array of Transformations (objects) or Transformations (null) (Transformations) |
| id | Id (string) or Id (null) (Id) |
| extra | Extra (object) or Extra (null) (Extra) |

### Responses

post/api/v1/datasets

### Request samples

"inputs_schema_definition": { },

"outputs_schema_definition": { },

"externally_managed": false,

"transformations": [{"path": ["string"\
\
],\
\
"transformation_type": "convert_to_openai_message"\
\
}\
\
],

"data_type": "kv"

### Response samples

"data_type": "kv",

"example_count": 0,

"session_count": 0,

"last_session_start_time": "2019-08-24T14:15:22Z",

"metadata": { }

## tag/datasets/operation/read_datasets_stream_api_v1_datasets_stream_get Read Datasets Stream

Stream all datasets by query params and owner as JSON patches.

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/datasets/stream

### Response samples

## tag/datasets/operation/read_dataset_api_v1_datasets__dataset_id__get Read Dataset

Get a specific dataset.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/datasets/{dataset\_id}

### Response samples

## tag/datasets/operation/delete_dataset_api_v1_datasets__dataset_id__delete Delete Dataset

Delete a specific dataset.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/datasets/{dataset\_id}

### Response samples

## tag/datasets/operation/update_dataset_api_v1_datasets__dataset_id__patch Update Dataset

Update a specific dataset.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| patch\_examples | Patch Examples (object) or Patch Examples (null) (Patch Examples) |

### Responses

Dataset updated successfully

patch/api/v1/datasets/{dataset\_id}

### Request samples

`{"name": {"__missing__": "__missing__"

"description": {"__missing__": "__missing__"

"inputs_schema_definition": {"__missing__": "__missing__"

"outputs_schema_definition": {"__missing__": "__missing__"

"patch_examples": {"property1": {"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0",

"property2": {"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0",

"transformations": {"__missing__": "__missing__"

"metadata": {"__missing__": "__missing__"

### Response samples

"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0"

## tag/datasets/operation/upload_csv_dataset_api_v1_datasets_upload_post Upload Csv Dataset

Create a new dataset from a CSV file.

##### Authorizations:

##### Request Body schema: multipart/form-data required

| name | Name (string) or Name (null) (Name) |

| description | Description (string) or Description (null) (Description) |
| inputs\_schema\_definition | Inputs Schema Definition (string) or Inputs Schema Definition (null) (Inputs Schema Definition) |
| outputs\_schema\_definition | Outputs Schema Definition (string) or Outputs Schema Definition (null) (Outputs Schema Definition) |
| transformations | Transformations (string) or Transformations (null) (Transformations) |
| input\_key\_mappings | Input Key Mappings (string) or Input Key Mappings (null) (Input Key Mappings) |
| output\_key\_mappings | Output Key Mappings (string) or Output Key Mappings (null) (Output Key Mappings) |
| metadata\_key\_mappings | Metadata Key Mappings (string) or Metadata Key Mappings (null) (Metadata Key Mappings) |

### Responses

post/api/v1/datasets/upload

### Response samples

## tag/datasets/operation/upload_experiment_api_v1_datasets_upload_experiment_post Upload Experiment

Upload an experiment that has already been run.

##### Authorizations:

##### Request Body schema: application/json required

| experiment\_description | Experiment Description (string) or Experiment Description (null) (Experiment Description) |
| dataset\_id | Dataset Id (string) or Dataset Id (null) (Dataset Id) |
| dataset\_name | Dataset Name (string) or Dataset Name (null) (Dataset Name) |
| dataset\_description | Dataset Description (string) or Dataset Description (null) (Dataset Description) |
| summary\_experiment\_scores | Array of Summary Experiment Scores (objects) or Summary Experiment Scores (null) (Summary Experiment Scores) |

| experiment\_metadata | Experiment Metadata (object) or Experiment Metadata (null) (Experiment Metadata) |

### Responses

post/api/v1/datasets/upload-experiment

### Request samples

`{"experiment_name": "string",

"experiment_description": "string",

"dataset_name": "string",

"dataset_description": "string",

"summary_experiment_scores": [{"created_at": "2019-08-24T14:15:22Z",\
\
"modified_at": "2019-08-24T14:15:22Z",\
\
"key": "string",\
\
"score": 0,\
\
"value": 0,\
\
"comment": "string",\
\
"correction": { },\
\
"feedback_group_id": "4ef7406c-cdc6-4573-acb1-4f69f1775190",\
\
"comparative_experiment_id": "f54c3bfd-24c9-46c1-a7e1-f4aea1c1b10a",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"feedback_source": {"type": "app",\
\
"metadata": { }\
\
},\
\
"feedback_config": {"type": "continuous",\
\
"min": 0,\
\
"max": 0,\
\
"categories": [{"value": 0,\
\
"label": "string"\
\
}\
\
]\
\
},\
\
"extra": { }\
\
}\
\
],

"results": [{"row_id": "63f10953-3e9d-4dde-8a8d-8423a5ddfff4",\
\
"inputs": { },\
\
"expected_outputs": { },\
\
"actual_outputs": { },\
\
"evaluation_scores": [{"created_at": "2019-08-24T14:15:22Z",\
\
"modified_at": "2019-08-24T14:15:22Z",\
\
"key": "string",\
\
"score": 0,\
\
"value": 0,\
\
"comment": "string",\
\
"correction": { },\
\
"feedback_group_id": "4ef7406c-cdc6-4573-acb1-4f69f1775190",\
\
"comparative_experiment_id": "f54c3bfd-24c9-46c1-a7e1-f4aea1c1b10a",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"feedback_source": {"type": "app",\
\
"metadata": { }\
\
},\
\
"feedback_config": {"type": "continuous",\
\
"min": 0,\
\
"max": 0,\
\
"categories": [{"value": 0,\
\
"label": "string"\
\
}\
\
]\
\
},\
\
"extra": { }\
\
}\
\
],\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"run_name": "string",\
\
"error": "string",\
\
"run_metadata": { }\
\
}\
\
],

"experiment_start_time": "2019-08-24T14:15:22Z",

"experiment_end_time": "2019-08-24T14:15:22Z",

"experiment_metadata": { }

### Response samples

`{"dataset": {"name": "string",

"experiment": {"start_time": "2019-08-24T14:15:22Z",

## tag/datasets/operation/get_dataset_versions_api_v1_datasets__dataset_id__versions_get Get Dataset Versions

Get dataset versions.

##### Authorizations:

##### path Parameters

##### query Parameters

| | |
| --- | --- |
| search | Search (string) or Search (null) (Search) |
| example | Example (string) or Example (null) (Example) |

### Responses

get/api/v1/datasets/{dataset\_id}/versions

### Response samples

`[{"tags": ["string"\
\
],\
\
"as_of": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/datasets/operation/diff_dataset_versions_api_v1_datasets__dataset_id__versions_diff_get Diff Dataset Versions

Get diff between two dataset versions.

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/datasets/{dataset\_id}/versions/diff

### Response samples

`{"examples_modified": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"examples_added": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"examples_removed": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]

## tag/datasets/operation/get_dataset_version_api_v1_datasets__dataset_id__version_get Get Dataset Version

Get dataset version by as\_of or exact tag.

##### Authorizations:

##### path Parameters

##### query Parameters

| | |
| --- | --- |
| as\_of | As Of (string) or As Of (null) (As Of) |
| tag | Tag (string) or Tag (null) (Tag) |

### Responses

get/api/v1/datasets/{dataset\_id}/version

### Response samples

`{"tags": ["string"\
\
],

"as_of": "2019-08-24T14:15:22Z"

## tag/datasets/operation/update_dataset_version_api_v1_datasets__dataset_id__tags_put Update Dataset Version

Set a tag on a dataset version.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

put/api/v1/datasets/{dataset\_id}/tags

### Request samples

`{"as_of": "2019-08-24T14:15:22Z",

"tag": "string"

### Response samples

## tag/datasets/operation/download_dataset_openai_api_v1_datasets__dataset_id__openai_get Download Dataset Openai

Download a dataset as OpenAI Evals Jsonl format.

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/datasets/{dataset\_id}/openai

### Response samples

## tag/datasets/operation/download_dataset_openai_ft_api_v1_datasets__dataset_id__openai_ft_get Download Dataset Openai Ft

Download a dataset as OpenAI Jsonl format.

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/datasets/{dataset\_id}/openai\_ft

### Response samples

## tag/datasets/operation/download_dataset_csv_api_v1_datasets__dataset_id__csv_get Download Dataset Csv

Download a dataset as CSV format.

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/datasets/{dataset\_id}/csv

### Response samples

## tag/datasets/operation/download_dataset_jsonl_api_v1_datasets__dataset_id__jsonl_get Download Dataset Jsonl

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/datasets/{dataset\_id}/jsonl

### Response samples

## tag/datasets/operation/read_examples_with_runs_api_v1_datasets__dataset_id__runs_post Read Examples With Runs

Fetch examples for a dataset, and fetch the runs for each example if they are associated with the given session\_ids.

##### Authorizations:

##### path Parameters

##### query Parameters

##### Request Body schema: application/json required

| limit | Limit (integer) or Limit (null) (Limit) |

| comparative\_experiment\_id | Comparative Experiment Id (string) or Comparative Experiment Id (null) (Comparative Experiment Id) |
| sort\_params | SortParamsForRunsComparisonView (object) or null |
| filters | Filters (object) or Filters (null) (Filters) |

### Responses

post/api/v1/datasets/{dataset\_id}/runs

### Request samples

`{"session_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"offset": 0,

"limit": 1,

"preview": false,

"comparative_experiment_id": "f54c3bfd-24c9-46c1-a7e1-f4aea1c1b10a",

"sort_params": {"sort_by": "string",

"sort_order": "ASC"

"filters": {"property1": ["string"\
\
],

"stream": false

### Response samples

Example

ExamplesWithRunsResponse Read Examples With Runs Api V1 Datasets Dataset Id Runs PostExamplesWithRuns

`[ ]`

## tag/datasets/operation/read_examples_with_runs_grouped_api_v1_datasets__dataset_id__group_runs_post Read Examples With Runs Grouped

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| filters | Filters (object) or Filters (null) (Filters) |

### Responses

post/api/v1/datasets/{dataset\_id}/group/runs

### Request samples

"limit": 10,

"group_by": "run_metadata",

"metadata_key": "string",

"per_group_limit": 5,

### Response samples

`{"groups": [{"filter": "string",\
\
"count": 0,\
\
"total_tokens": 0,\
\
"total_cost": "string",\
\
"min_start_time": "2019-08-24T14:15:22Z",\
\
"max_start_time": "2019-08-24T14:15:22Z",\
\
"latency_p50": 0,\
\
"latency_p99": 0,\
\
"feedback_stats": { },\
\
"group_key": "string",\
\
"sessions": [{"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"extra": { },\
\
"name": "string",\
\
"description": "string",\
\
"default_dataset_id": "467c3141-9a68-490e-ab5c-4000a5a11fa8",\
\
"reference_dataset_id": "4abef5d4-9fac-44e9-b393-753a8de1db88",\
\
"trace_tier": "longlived",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"run_count": 0,\
\
"latency_p50": 0,\
\
"latency_p99": 0,\
\
"first_token_p50": 0,\
\
"first_token_p99": 0,\
\
"total_tokens": 0,\
\
"prompt_tokens": 0,\
\
"completion_tokens": 0,\
\
"total_cost": "string",\
\
"prompt_cost": "string",\
\
"completion_cost": "string",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"last_run_start_time": "2019-08-24T14:15:22Z",\
\
"last_run_start_time_live": "2019-08-24T14:15:22Z",\
\
"feedback_stats": { },\
\
"session_feedback_stats": { },\
\
"run_facets": [{ }\
\
],\
\
"error_rate": 0,\
\
"streaming_rate": 0,\
\
"test_run_number": 0,\
\
"example_count": 0,\
\
"filter": "string",\
\
"min_start_time": "2019-08-24T14:15:22Z",\
\
"max_start_time": "2019-08-24T14:15:22Z"\
\
}\
\
],\
\
"examples": [{"outputs": { },\
\
"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0",\
\
"source_run_id": "51116a97-0041-4601-85fc-eb599c2885fb",\
\
"metadata": { },\
\
"inputs": { },\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"name": "string",\
\
"modified_at": "2019-08-24T14:15:22Z",\
\
"attachment_urls": { },\
\
"runs": [{"name": "string",\
\
"inputs": { },\
\
"inputs_preview": "string",\
\
"run_type": "tool",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"extra": { },\
\
"error": "string",\
\
"execution_order": 1,\
\
"serialized": { },\
\
"outputs": { },\
\
"outputs_preview": "string",\
\
"parent_run_id": "f8faf8c1-9778-49a4-9004-628cdb0047e5",\
\
"manifest_id": "82825e8e-31fc-47d5-83ce-cd926068341e",\
\
"manifest_s3_id": "0454f93b-7eb6-4b9d-a203-f1261e686840",\
\
"events": [{ }\
\
],\
\
"tags": ["string"\
\
],\
\
"inputs_s3_urls": { },\
\
"outputs_s3_urls": { },\
\
"s3_urls": { },\
\
"trace_id": "df570c03-5a03-4cea-8df0-c162d05127ac",\
\
"dotted_order": "string",\
\
"trace_min_start_time": "2019-08-24T14:15:22Z",\
\
"trace_max_start_time": "2019-08-24T14:15:22Z",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",\
\
"reference_example_id": "9fb06aaa-105f-4c87-845f-47d62ffd7ee6",\
\
"total_tokens": 0,\
\
"prompt_tokens": 0,\
\
"completion_tokens": 0,\
\
"total_cost": "string",\
\
"prompt_cost": "string",\
\
"completion_cost": "string",\
\
"status": "string",\
\
"feedback_stats": {"property1": { },\
\
"property2": { }\
\
},\
\
"app_path": "string"\
\
}\
\
]\
\
}\
\
],\
\
"example_count": 0,\
\
"prompt_tokens": 0,\
\
"completion_tokens": 0,\
\
"prompt_cost": "string",\
\
"completion_cost": "string",\
\
"error_rate": 0\
\
}\
\
]

## tag/datasets/operation/read_delta_api_v1_datasets__dataset_id__runs_delta_post Read Delta

Fetch the number of regressions/improvements for each example in a dataset, between sessions\[0\] and sessions\[1\].

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| comparative\_experiment\_id | Comparative Experiment Id (string) or Comparative Experiment Id (null) (Comparative Experiment Id) |

### Responses

post/api/v1/datasets/{dataset\_id}/runs/delta

### Request samples

`{"baseline_session_id": "99dde862-3898-42f7-9a86-7af7817c182c",

"comparison_session_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"feedback_key": "string",

"limit": 100,

"comparative_experiment_id": "f54c3bfd-24c9-46c1-a7e1-f4aea1c1b10a"

### Response samples

`{"feedback_deltas": {"property1": {"improved_examples": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"regressed_examples": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]

"property2": {"improved_examples": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

## tag/datasets/operation/read_grouped_experiments_api_v1_datasets__dataset_id__experiments_grouped_post Read Grouped Experiments

Stream grouped and aggregated experiments.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| stats\_start\_time | Stats Start Time (string) or Stats Start Time (null) (Stats Start Time) |
| name\_contains | Name Contains (string) or Name Contains (null) (Name Contains) |
| tag\_value\_id | Array of Tag Value Id (strings) or Tag Value Id (null) (Tag Value Id) |
| dataset\_version | Dataset Version (string) or Dataset Version (null) (Dataset Version) |
| filter | Filter (string) or Filter (null) (Filter) |

### Responses

post/api/v1/datasets/{dataset\_id}/experiments/grouped

### Request samples

`{"stats_start_time": "2019-08-24T14:15:22Z",

"name_contains": "string",

"tag_value_id": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"dataset_version": "string",

"use_approx_stats": false,

"metadata_keys": ["string"\
\
],

"experiment_limit": 1000

### Response samples

## tag/datasets/operation/read_dataset_share_state_api_v1_datasets__dataset_id__share_get Read Dataset Share State

Get the state of sharing a dataset

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/datasets/{dataset\_id}/share

### Response samples

DatasetShareSchemaResponse Read Dataset Share State Api V1 Datasets Dataset Id Share GetDatasetShareSchema

"share_token": "d0430ac3-04a1-4e32-a7ea-57776ad22c1c"

## tag/datasets/operation/share_dataset_api_v1_datasets__dataset_id__share_put Share Dataset

Share a dataset.

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

put/api/v1/datasets/{dataset\_id}/share

### Response samples

## tag/datasets/operation/unshare_dataset_api_v1_datasets__dataset_id__share_delete Unshare Dataset

Unshare a dataset.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/datasets/{dataset\_id}/share

### Response samples

## tag/datasets/operation/read_comparative_experiments_api_v1_datasets__dataset_id__comparative_get Read Comparative Experiments

Get all comparative experiments for a given dataset.

##### Authorizations:

##### path Parameters

##### query Parameters

| | |
| --- | --- |
| name | Name (string) or Name (null) (Name) |
| name\_contains | Name Contains (string) or Name Contains (null) (Name Contains) |
| id | Array of Id (strings) or Id (null) (Id) |

### Responses

get/api/v1/datasets/{dataset\_id}/comparative

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"name": "string",\
\
"description": "string",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"modified_at": "2019-08-24T14:15:22Z",\
\
"reference_dataset_id": "4abef5d4-9fac-44e9-b393-753a8de1db88",\
\
"extra": { },\
\
"experiments_info": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"name": "string"\
\
}\
\
],\
\
"feedback_stats": { }\
\
}\
\
]`

## tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_post Create Comparative Experiment

Create a comparative experiment.

##### Authorizations:

##### Request Body schema: application/json required

| name | Name (string) or Name (null) (Name) |
| description | Description (string) or Description (null) (Description) |

| reference\_dataset\_id | Reference Dataset Id (string) or Reference Dataset Id (null) (Reference Dataset Id) |
| extra | Extra (object) or Extra (null) (Extra) |

### Responses

post/api/v1/datasets/comparative

### Request samples

"experiment_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"extra": { }

### Response samples

## tag/datasets/operation/delete_comparative_experiment_api_v1_datasets_comparative__comparative_experiment_id__delete Delete Comparative Experiment

Delete a specific comparative experiment.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/datasets/comparative/{comparative\_experiment\_id}

### Response samples

## tag/datasets/operation/clone_dataset_api_v1_datasets_clone_post Clone Dataset

Clone a dataset.

##### Authorizations:

##### Request Body schema: application/json required

| as\_of | (As Of (As Of (string) or As Of (string))) or As Of (null) (As Of) |

| split | Split (string) or Array of Split (strings) or Split (null) (Split) |

### Responses

post/api/v1/datasets/clone

### Request samples

`{"target_dataset_id": "f50bdcfa-e6ad-4d17-b63b-b3db72d9ce6a",

"source_dataset_id": "6427adb2-1d43-4594-8566-54612b009ee7",

"as_of": "2019-08-24T14:15:22Z",

"examples": [ ],

"split": "string"

### Response samples

`[{ }\
\
]`

## tag/datasets/operation/get_dataset_splits_api_v1_datasets__dataset_id__splits_get Get Dataset Splits

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/datasets/{dataset\_id}/splits

### Response samples

`["string"\
\
]`

## tag/datasets/operation/update_dataset_splits_api_v1_datasets__dataset_id__splits_put Update Dataset Splits

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

put/api/v1/datasets/{dataset\_id}/splits

### Request samples

`{"split_name": "string",

"examples": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"remove": false

### Response samples

`["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]`

## tag/datasets/operation/index_api_v1_datasets__dataset_id__index_post Index

Index a dataset.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/datasets/{dataset\_id}/index

### Request samples

`{"tag": "latest"

### Response samples

## tag/datasets/operation/remove_index_api_v1_datasets__dataset_id__index_delete Remove Index

Remove an index for a dataset.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/datasets/{dataset\_id}/index

### Response samples

## tag/datasets/operation/get_index_info_api_v1_datasets__dataset_id__index_get Get Index Info

Get index info.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/datasets/{dataset\_id}/index

### Response samples

"tag": "latest",

"last_updated_version": "2019-08-24T14:15:22Z"

## tag/datasets/operation/sync_index_api_v1_datasets__dataset_id__index_sync_post Sync Index

Sync an index for a dataset.

##### Authorizations:

##### path Parameters

### Responses

post/api/v1/datasets/{dataset\_id}/index/sync

### Response samples

## tag/datasets/operation/search_api_v1_datasets__dataset_id__search_post Search

Search a dataset.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/datasets/{dataset\_id}/search

### Request samples

`{"inputs": { },

"limit": 5,

"debug": false,

"filter": "string"

### Response samples

`{"examples": [{"inputs": { },\
\
"outputs": { },\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"debug_info": { }\
\
}\
\
]

## tag/datasets/operation/generate_api_v1_datasets__dataset_id__generate_post Generate

Generate synthetic examples for a dataset.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| example\_ids | Array of Example Ids (strings) or Example Ids (null) (Example Ids) |

### Responses

post/api/v1/datasets/{dataset\_id}/generate

### Request samples

`{"example_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"num_examples": 0

### Response samples

## tag/datasets/operation/dataset_handler_api_v1_datasets_playground_experiment_batch_post Dataset Handler

##### Authorizations:

##### Request Body schema: application/json required

| run\_id | Run Id (string) or Run Id (null) (Run Id) |
| repo\_id | Repo Id (string) or Repo Id (null) (Repo Id) |
| tools | Array of Tools (any) or Tools (null) (Tools) |
| tool\_choice | Tool Choice (string) or Tool Choice (null) (Tool Choice) |
| parallel\_tool\_calls | Parallel Tool Calls (boolean) or Parallel Tool Calls (null) (Parallel Tool Calls) |

| repo\_handle | Repo Handle (string) or Repo Handle (null) (Repo Handle) |
| owner | Owner (string) or Owner (null) (Owner) |
| commit | Commit (string) or Commit (null) (Commit) |
| evaluator\_rules | Array of Evaluator Rules (strings) or Evaluator Rules (null) (Evaluator Rules) |
| requests\_per\_second | Requests Per Second (integer) or Requests Per Second (null) (Requests Per Second) |

| metadata | Metadata (object) or Metadata (null) (Metadata) |
| batch\_size | Batch Size (integer) or Batch Size (null) (Batch Size) |

### Responses

post/api/v1/datasets/playground\_experiment/batch

### Request samples

`{"manifest": null,

"secrets": {"property1": "string",

"run_id": "string",

"repo_id": "string",

"tools": [null\
\
],

"tool_choice": "string",

"parallel_tool_calls": true,

"options": {"tags": ["string"\
\
],

"callbacks": [null\
\
],

"run_name": "string",

"max_concurrency": 0,

"recursion_limit": 0,

"configurable": { },

"run_id": "dded282c-8ebd-44cf-8ba5-9a234973d1ec"

"project_name": "string",

"repo_handle": "string",

"owner": "string",

"commit": "string",

"evaluator_rules": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"requests_per_second": 0,

"use_or_fallback_to_workspace_secrets": false,

"runner_context": "langsmith_ui",

"dataset_splits": ["string"\
\
],

"repetitions": 1,

"batch_size": 1

### Response samples

## tag/datasets/operation/stream_dataset_handler_api_v1_datasets_playground_experiment_stream_post Stream Dataset Handler

##### Authorizations:

##### Request Body schema: application/json required

| metadata | Metadata (object) or Metadata (null) (Metadata) |

### Responses

post/api/v1/datasets/playground\_experiment/stream

### Request samples

### Response samples

## tag/datasets/operation/studio_experiment_api_v1_datasets_studio_experiment_post Studio Experiment

##### Authorizations:

##### Request Body schema: application/json required

| evaluator\_rules | Array of Evaluator Rules (strings) or Evaluator Rules (null) (Evaluator Rules) |
| metadata | Metadata (object) or Metadata (null) (Metadata) |

### Responses

post/api/v1/datasets/studio\_experiment

### Request samples

`{"project_name": "string",

### Response samples

## tag/run run

## tag/run/operation/list_rules_api_v1_runs_rules_get List Rules

List all run rules.

##### Authorizations:

##### query Parameters

| | |
| --- | --- |
| dataset\_id | Dataset Id (string) or Dataset Id (null) (Dataset Id) |
| session\_id | Session Id (string) or Session Id (null) (Session Id) |
| type | Type (string) or Type (null) (Type) |
| name\_contains | Name Contains (string) or Name Contains (null) (Name Contains) |
| id | Array of Id (strings) or Id (null) (Id) |

### Responses

get/api/v1/runs/rules

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"is_enabled": true,\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",\
\
"session_name": "string",\
\
"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0",\
\
"dataset_name": "string",\
\
"display_name": "string",\
\
"sampling_rate": 0,\
\
"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"add_to_annotation_queue_id": "c91d216f-a5b3-4142-9b9f-0d1c00001187",\
\
"add_to_annotation_queue_name": "string",\
\
"add_to_dataset_id": "83d0b7c4-699b-4390-80e0-96ad44a8df85",\
\
"add_to_dataset_name": "string",\
\
"add_to_dataset_prefer_correction": false,\
\
"corrections_dataset_id": "c0e4c74b-5212-441e-8c9e-b1f0c1d70143",\
\
"use_corrections_dataset": false,\
\
"num_few_shot_examples": 0,\
\
"evaluators": [{"structured": {"hub_ref": "string",\
\
"prompt": [["string",\
\
"string"\
\
]\
\
],\
\
"template_format": "string",\
\
"schema": { },\
\
"variable_mapping": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"model": { }\
\
}\
\
}\
\
],\
\
"code_evaluators": [{"code": "string",\
\
"language": "python"\
\
}\
\
],\
\
"alerts": [{"type": "pagerduty",\
\
"routing_key": "string",\
\
"summary": "string",\
\
"severity": "warning"\
\
}\
\
],\
\
"webhooks": [{"url": "string",\
\
"headers": {"property1": "string",\
\
"property2": "string"\
\
}\
\
}\
\
],\
\
"extend_only": false,\
\
"include_extended_stats": false,\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"backfill_from": "2019-08-24T14:15:22Z",\
\
"transient": false,\
\
"evaluator_version": 0,\
\
"evaluator_id": "92c2c259-25b9-47e2-bc3b-bd161a87f750",\
\
"alignment_annotation_queue_id": "930cfabe-5e02-48be-8cf2-ad0d56952b8e",\
\
"group_by": "thread_id"\
\
}\
\
]`

## tag/run/operation/create_rule_api_v1_runs_rules_post Create Rule

Create a new run rule.

##### Authorizations:

##### Request Body schema: application/json required

| session\_id | Session Id (string) or Session Id (null) (Session Id) |

| dataset\_id | Dataset Id (string) or Dataset Id (null) (Dataset Id) |

| filter | Filter (string) or Filter (null) (Filter) |
| trace\_filter | Trace Filter (string) or Trace Filter (null) (Trace Filter) |
| tree\_filter | Tree Filter (string) or Tree Filter (null) (Tree Filter) |
| backfill\_from | Backfill From (string) or Backfill From (null) (Backfill From) |

| num\_few\_shot\_examples | Num Few Shot Examples (integer) or Num Few Shot Examples (null) (Num Few Shot Examples) |

| add\_to\_annotation\_queue\_id | Add To Annotation Queue Id (string) or Add To Annotation Queue Id (null) (Add To Annotation Queue Id) |
| add\_to\_dataset\_id | Add To Dataset Id (string) or Add To Dataset Id (null) (Add To Dataset Id) |

| evaluators | Array of Evaluators (objects) or Evaluators (null) (Evaluators) |
| code\_evaluators | Array of Code Evaluators (objects) or Code Evaluators (null) (Code Evaluators) |
| alerts | Array of Alerts (objects) or Alerts (null) (Alerts) |
| webhooks | Array of Webhooks (objects) or Webhooks (null) (Webhooks) |
| evaluator\_version | Evaluator Version (integer) or Evaluator Version (null) (Evaluator Version) |

| group\_by | "thread\_id" (string) or Group By (null) (Group By) |

### Responses

post/api/v1/runs/rules

### Request samples

"is_enabled": true,

"sampling_rate": 0,

"trace_filter": "string",

"tree_filter": "string",

"backfill_from": "2019-08-24T14:15:22Z",

"use_corrections_dataset": false,

"num_few_shot_examples": 0,

"extend_only": false,

"transient": false,

"add_to_annotation_queue_id": "c91d216f-a5b3-4142-9b9f-0d1c00001187",

"add_to_dataset_id": "83d0b7c4-699b-4390-80e0-96ad44a8df85",

"add_to_dataset_prefer_correction": false,

"evaluators": [{"structured": {"hub_ref": "string",\
\
"prompt": [["string",\
\
"string"\
\
]\
\
],\
\
"template_format": "string",\
\
"schema": { },\
\
"variable_mapping": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"model": { }\
\
}\
\
}\
\
],

"code_evaluators": [{"code": "string",\
\
"language": "python"\
\
}\
\
],

"alerts": [{"type": "pagerduty",\
\
"routing_key": "string",\
\
"summary": "string",\
\
"severity": "warning"\
\
}\
\
],

"webhooks": [{"url": "string",\
\
"headers": {"property1": "string",\
\
"property2": "string"\
\
}\
\
}\
\
],

"evaluator_version": 0,

"create_alignment_queue": false,

"include_extended_stats": false,

"group_by": "thread_id"

### Response samples

"session_name": "string",

"add_to_annotation_queue_name": "string",

"add_to_dataset_name": "string",

"corrections_dataset_id": "c0e4c74b-5212-441e-8c9e-b1f0c1d70143",

"evaluator_id": "92c2c259-25b9-47e2-bc3b-bd161a87f750",

"alignment_annotation_queue_id": "930cfabe-5e02-48be-8cf2-ad0d56952b8e",

## tag/run/operation/validate_rule_api_v1_runs_rules_validate_post Validate Rule

Validate a rule by executing it with test data without creating a saved rule.

This endpoint allows testing LLM-as-judge evaluators before saving them. It accepts
a rule configuration (same as rule creation) and test data, executes the evaluator,
and returns the evaluation results in the same format as batch\_invoke\_evaluator.

Only LLM-as-judge rules (evaluators) are supported. Code evaluators are not allowed.

The evaluator execution traces are written to the database (in the "evaluators"
project), which allows users to see the evaluator execution history.

##### Authorizations:

##### Request Body schema: application/json required

| group\_by | "thread\_id" (string) or Group By (null) (Group By) |
| test\_inputs | Test Inputs (object) or Test Inputs (null) (Test Inputs) |
| test\_outputs | Test Outputs (object) or Test Outputs (null) (Test Outputs) |
| test\_reference\_outputs | Test Reference Outputs (object) or Test Reference Outputs (null) (Test Reference Outputs) |

### Responses

post/api/v1/runs/rules/validate

### Request samples

"group_by": "thread_id",

"test_inputs": { },

"test_outputs": { },

"test_reference_outputs": { }

### Response samples

## tag/run/operation/update_rule_api_v1_runs_rules__rule_id__patch Update Rule

Update a run rule.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

patch/api/v1/runs/rules/{rule\_id}

### Request samples

"include_extended_stats": false

### Response samples

## tag/run/operation/delete_rule_api_v1_runs_rules__rule_id__delete Delete Rule

Delete a run rule.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/runs/rules/{rule\_id}

### Response samples

## tag/run/operation/thread_preview_api_v1_runs_threads__thread_id__get Thread Preview

Get preview of a thread.

##### Authorizations:

##### path Parameters

##### query Parameters

| select | Array of Select (strings) or Select (null) (Select) |
| variables | Array of Variables (strings) or Variables (null) (Variables) |

### Responses

get/api/v1/runs/threads/{thread\_id}

### Response samples

`{"thread_id": "string",

"previews": {"property1": "string",

## tag/run/operation/list_rule_logs_api_v1_runs_rules__rule_id__logs_get List Rule Logs

List logs for a particular rule

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/runs/rules/{rule\_id}/logs

### Response samples

`[{"rule_id": "728c1541-d6d1-4290-9a53-cdf01dd32d60",\
\
"run_id": "dded282c-8ebd-44cf-8ba5-9a234973d1ec",\
\
"run_name": "string",\
\
"run_type": "string",\
\
"run_session_id": "4b290cc7-5731-4ce8-8cd7-1bbbd88cfdb2",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"application_time": "2019-08-24T14:15:22Z",\
\
"add_to_annotation_queue": {"outcome": "success",\
\
"payload": { }\
\
},\
\
"add_to_dataset": {"outcome": "success",\
\
"payload": { }\
\
},\
\
"evaluators": {"outcome": "success",\
\
"payload": { }\
\
},\
\
"alerts": {"outcome": "success",\
\
"payload": { }\
\
},\
\
"webhooks": {"outcome": "success",\
\
"payload": { }\
\
},\
\
"thread_id": "string"\
\
}\
\
]`

## tag/run/operation/get_last_applied_rule_api_v1_runs_rules__rule_id__last_applied_get Get Last Applied Rule

Get the last applied rule.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/runs/rules/{rule\_id}/last\_applied

### Response samples

`{"rule_id": "728c1541-d6d1-4290-9a53-cdf01dd32d60",

"run_id": "dded282c-8ebd-44cf-8ba5-9a234973d1ec",

"run_type": "string",

"run_session_id": "4b290cc7-5731-4ce8-8cd7-1bbbd88cfdb2",

"application_time": "2019-08-24T14:15:22Z",

"add_to_annotation_queue": {"outcome": "success",

"payload": { }

"add_to_dataset": {"outcome": "success",

"evaluators": {"outcome": "success",

"alerts": {"outcome": "success",

"webhooks": {"outcome": "success",

"thread_id": "string"

## tag/run/operation/trigger_rule_api_v1_runs_rules__rule_id__trigger_post Trigger Rule

Trigger a run rule manually.

##### Authorizations:

##### path Parameters

### Responses

post/api/v1/runs/rules/{rule\_id}/trigger

### Response samples

## tag/run/operation/trigger_rules_api_v1_runs_rules_trigger_post Trigger Rules

Trigger an array of run rules manually.

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| rule\_ids | Array of Rule Ids (strings) or Rule Ids (null) (Rule Ids) |
| dataset\_id | Dataset Id (string) or Dataset Id (null) (Dataset Id) |

### Responses

post/api/v1/runs/rules/trigger

### Request samples

`{"rule_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0"

### Response samples

## tag/run/operation/read_run_api_v1_runs__run_id__get Read Run

Get a specific run.

##### Authorizations:

##### path Parameters

##### query Parameters

| | |
| --- | --- |
| session\_id | Session Id (string) or Session Id (null) (Session Id) |
| start\_time | Start Time (string) or Start Time (null) (Start Time) |

### Responses

get/api/v1/runs/{run\_id}

### Response samples

"inputs_preview": "string",

"run_type": "tool",

"execution_order": 1,

"serialized": { },

"outputs_preview": "string",

"parent_run_id": "f8faf8c1-9778-49a4-9004-628cdb0047e5",

"manifest_id": "82825e8e-31fc-47d5-83ce-cd926068341e",

"manifest_s3_id": "0454f93b-7eb6-4b9d-a203-f1261e686840",

"events": [{ }\
\
],

"tags": ["string"\
\
],

"inputs_s3_urls": { },

"outputs_s3_urls": { },

"s3_urls": { },

"trace_id": "df570c03-5a03-4cea-8df0-c162d05127ac",

"dotted_order": "string",

"trace_min_start_time": "2019-08-24T14:15:22Z",

"trace_max_start_time": "2019-08-24T14:15:22Z",

"child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"direct_child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"parent_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"feedback_stats": {"property1": { },

"reference_example_id": "9fb06aaa-105f-4c87-845f-47d62ffd7ee6",

"prompt_token_details": {"property1": 0,

"completion_token_details": {"property1": 0,

"prompt_cost_details": {"property1": "string",

"completion_cost_details": {"property1": "string",

"price_model_id": "0b5d9575-bec3-4256-b43a-05893b8b8440",

"first_token_time": "2019-08-24T14:15:22Z",

"messages": [{ }\
\
],

"app_path": "string",

"last_queued_at": "2019-08-24T14:15:22Z",

"in_dataset": true,

"share_token": "d0430ac3-04a1-4e32-a7ea-57776ad22c1c",

"trace_first_received_at": "2019-08-24T14:15:22Z",

"ttl_seconds": 0,

"trace_upgrade": false,

## tag/run/operation/update_run_api_v1_runs__run_id__patch Update Run

Update a run.

##### Authorizations:

##### path Parameters

### Responses

patch/api/v1/runs/{run\_id}

### Response samples

## tag/run/operation/read_run_share_state_api_v1_runs__run_id__share_get Read Run Share State

Get the state of sharing of a run.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/runs/{run\_id}/share

### Response samples

RunShareSchemaResponse Read Run Share State Api V1 Runs Run Id Share GetRunShareSchema

`{"run_id": "dded282c-8ebd-44cf-8ba5-9a234973d1ec",

## tag/run/operation/share_run_api_v1_runs__run_id__share_put Share Run

Share a run.

##### Authorizations:

##### path Parameters

### Responses

put/api/v1/runs/{run\_id}/share

### Response samples

## tag/run/operation/unshare_run_api_v1_runs__run_id__share_delete Unshare Run

Unshare a run.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/runs/{run\_id}/share

### Response samples

## tag/run/operation/query_runs_api_v1_runs_query_post Query Runs

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| id | Array of Id (strings) or Id (null) (Id) |
| trace | Trace (string) or Trace (null) (Trace) |
| parent\_run | Parent Run (string) or Parent Run (null) (Parent Run) |
| run\_type | RunTypeEnum (string) or null |
| session | Array of Session (strings) or Session (null) (Session) |
| reference\_example | Array of Reference Example (strings) or Reference Example (null) (Reference Example) |
| execution\_order | Execution Order (integer) or Execution Order (null) (Execution Order) |
| start\_time | Start Time (string) or Start Time (null) (Start Time) |
| end\_time | End Time (string) or End Time (null) (End Time) |
| error | Error (boolean) or Error (null) (Error) |
| query | Query (string) or Query (null) (Query) |
| filter | Filter (string) or Filter (null) (Filter) |
| trace\_filter | Trace Filter (string) or Trace Filter (null) (Trace Filter) |
| tree\_filter | Tree Filter (string) or Tree Filter (null) (Tree Filter) |
| is\_root | Is Root (boolean) or Is Root (null) (Is Root) |
| data\_source\_type | RunsFilterDataSourceTypeEnum (string) or null |
| skip\_pagination | Skip Pagination (boolean) or Skip Pagination (null) (Skip Pagination) |
| search\_filter | Search Filter (string) or Search Filter (null) (Search Filter) |

| cursor | Cursor (string) or Cursor (null) (Cursor) |

### Responses

post/api/v1/runs/query

### Request samples

`{"id": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"trace": "cd05b1fc-bc10-4ba6-9a9f-b73c503dbd77",

"parent_run": "26f1a1e5-ad05-4711-8359-5239cb3687c8",

"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"reference_example": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"error": true,

"query": "string",

"is_root": true,

"data_source_type": "current",

"skip_pagination": true,

"search_filter": "string",

"use_experimental_search": false,

"cursor": "string",

"select": ["id",\
\
"name",\
\
"run_type",\
\
"start_time",\
\
"end_time",\
\
"status",\
\
"error",\
\
"extra",\
\
"events",\
\
"inputs",\
\
"outputs",\
\
"parent_run_id",\
\
"manifest_id",\
\
"manifest_s3_id",\
\
"manifest",\
\
"session_id",\
\
"serialized",\
\
"reference_example_id",\
\
"reference_dataset_id",\
\
"total_tokens",\
\
"prompt_tokens",\
\
"prompt_token_details",\
\
"completion_tokens",\
\
"completion_token_details",\
\
"total_cost",\
\
"prompt_cost",\
\
"prompt_cost_details",\
\
"completion_cost",\
\
"completion_cost_details",\
\
"price_model_id",\
\
"first_token_time",\
\
"trace_id",\
\
"dotted_order",\
\
"last_queued_at",\
\
"feedback_stats",\
\
"parent_run_ids",\
\
"tags",\
\
"in_dataset",\
\
"app_path",\
\
"share_token",\
\
"trace_tier",\
\
"trace_first_received_at",\
\
"ttl_seconds",\
\
"trace_upgrade",\
\
"thread_id"\
\
],

"order": "asc",

"skip_prev_cursor": false

### Response samples

`{"runs": [{"name": "string",\
\
"inputs": { },\
\
"inputs_preview": "string",\
\
"run_type": "tool",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"extra": { },\
\
"error": "string",\
\
"execution_order": 1,\
\
"serialized": { },\
\
"outputs": { },\
\
"outputs_preview": "string",\
\
"parent_run_id": "f8faf8c1-9778-49a4-9004-628cdb0047e5",\
\
"manifest_id": "82825e8e-31fc-47d5-83ce-cd926068341e",\
\
"manifest_s3_id": "0454f93b-7eb6-4b9d-a203-f1261e686840",\
\
"events": [{ }\
\
],\
\
"tags": ["string"\
\
],\
\
"inputs_s3_urls": { },\
\
"outputs_s3_urls": { },\
\
"s3_urls": { },\
\
"trace_id": "df570c03-5a03-4cea-8df0-c162d05127ac",\
\
"dotted_order": "string",\
\
"trace_min_start_time": "2019-08-24T14:15:22Z",\
\
"trace_max_start_time": "2019-08-24T14:15:22Z",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"status": "string",\
\
"child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"direct_child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"parent_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"feedback_stats": {"property1": { },\
\
"property2": { }\
\
},\
\
"reference_example_id": "9fb06aaa-105f-4c87-845f-47d62ffd7ee6",\
\
"total_tokens": 0,\
\
"prompt_tokens": 0,\
\
"completion_tokens": 0,\
\
"prompt_token_details": {"property1": 0,\
\
"property2": 0\
\
},\
\
"completion_token_details": {"property1": 0,\
\
"property2": 0\
\
},\
\
"total_cost": "string",\
\
"prompt_cost": "string",\
\
"completion_cost": "string",\
\
"prompt_cost_details": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"completion_cost_details": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"price_model_id": "0b5d9575-bec3-4256-b43a-05893b8b8440",\
\
"first_token_time": "2019-08-24T14:15:22Z",\
\
"messages": [{ }\
\
],\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",\
\
"app_path": "string",\
\
"last_queued_at": "2019-08-24T14:15:22Z",\
\
"in_dataset": true,\
\
"share_token": "d0430ac3-04a1-4e32-a7ea-57776ad22c1c",\
\
"trace_tier": "longlived",\
\
"trace_first_received_at": "2019-08-24T14:15:22Z",\
\
"ttl_seconds": 0,\
\
"trace_upgrade": false,\
\
"reference_dataset_id": "4abef5d4-9fac-44e9-b393-753a8de1db88",\
\
"thread_id": "string"\
\
}\
\
],

"cursors": {"property1": "string",

"search_cursors": {"property1": { },

"parsed_query": "string"

## tag/run/operation/generate_query_for_runs_api_v1_runs_generate_query_post Generate Query For Runs

Get runs filter expression query for a given natural language query.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/runs/generate-query

### Request samples

`{"query": "string",

"feedback_keys": ["user_score"\
\
]

### Response samples

`{"filter": "string",

"feedback_urls": {"property1": "string",

## tag/run/operation/stats_runs_api_v1_runs_stats_post Stats Runs

Get all runs by query in body payload.

##### Authorizations:

##### Request Body schema: application/json required

| group\_by | RunStatsGroupBy (object) or null |
| groups | (Array of Groups (strings or null)) or Groups (null) (Groups) |
| select | Array of Select (strings) or Select (null) (Select) |

### Responses

post/api/v1/runs/stats

### Request samples

"groups": ["string"\
\
],

"select": ["run_count"\
\
]

### Response samples

RunStatsResponse Stats Runs Api V1 Runs Stats PostRunStats

`{"run_count": 0,

"median_tokens": 0,

"completion_tokens_p50": 0,

"prompt_tokens_p50": 0,

"tokens_p99": 0,

"completion_tokens_p99": 0,

"prompt_tokens_p99": 0,

"cost_p50": "string",

"cost_p99": "string",

"prompt_token_details": { },

"completion_token_details": { },

"prompt_cost_details": { },

"completion_cost_details": { }

## tag/run/operation/group_runs_api_v1_runs_group_post Group Runs

Get runs grouped by an expression

##### Authorizations:

##### header Parameters

##### Request Body schema: application/json required

| filter | Filter (string) or Filter (null) (Filter) |
| start\_time | Start Time (string) or Start Time (null) (Start Time) |
| end\_time | End Time (string) or End Time (null) (End Time) |

### Responses

post/api/v1/runs/group

### Request samples

`{"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",

"group_by": "conversation",

"limit": 10

### Response samples

## tag/run/operation/stats_group_runs_api_v1_runs_group_stats_post Stats Group Runs

Get stats for the grouped runs.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/runs/group/stats

### Request samples

### Response samples

"completion_cost_details": { },

"group_count": 0

## tag/run/operation/delete_runs_api_v1_runs_delete_post Delete Runs

Delete specific runs by trace IDs or metadata key-value pairs.

##### Authorizations:

##### Request Body schema: application/json

| | |
| --- | --- |
| session\_id | Session Id (string) or Session Id (null) (Session Id) |
| trace\_ids | Array of Trace Ids (strings) or Trace Ids (null) (Trace Ids) |
| metadata | Metadata (object) or Metadata (null) (Metadata) |

### Responses

post/api/v1/runs/delete

### Request samples

"trace_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"metadata": {"property1": "string",

### Response samples

## tag/feedback feedback

## tag/feedback/operation/create_feedback_formula_ep_api_v1_feedback_formulas_post Create Feedback Formula Ep

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| dataset\_id | Dataset Id (string) or Dataset Id (null) (Dataset Id) |
| session\_id | Session Id (string) or Session Id (null) (Session Id) |

### Responses

post/api/v1/feedback/formulas

### Request samples

"aggregation_type": "sum",

"formula_parts": [{"part_type": "weighted_key",\
\
"weight": 0,\
\
"key": "string"\
\
}\
\
]

### Response samples

"formula_parts": [{"part_type": "weighted_key",\
\
"weight": 0,\
\
"key": "string"\
\
}\
\
],

"modified_at": "2019-08-24T14:15:22Z"

## tag/feedback/operation/list_feedback_formula_ep_api_v1_feedback_formulas_get List Feedback Formula Ep

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/feedback/formulas

### Response samples

`[{"dataset_id": "8c4c51f1-f6f3-43bc-b65d-7415e8ef22c0",\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",\
\
"feedback_key": "string",\
\
"aggregation_type": "sum",\
\
"formula_parts": [{"part_type": "weighted_key",\
\
"weight": 0,\
\
"key": "string"\
\
}\
\
],\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"modified_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/feedback/operation/get_feedback_formula_ep_api_v1_feedback_formulas__feedback_formula_id__get Get Feedback Formula Ep

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/feedback/formulas/{feedback\_formula\_id}

### Response samples

## tag/feedback/operation/update_feedback_formula_ep_api_v1_feedback_formulas__feedback_formula_id__put Update Feedback Formula Ep

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

put/api/v1/feedback/formulas/{feedback\_formula\_id}

### Request samples

`{"feedback_key": "string",

### Response samples

## tag/feedback/operation/delete_feedback_formula_endpoint_api_v1_feedback_formulas__feedback_formula_id__delete Delete Feedback Formula Endpoint

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/feedback/formulas/{feedback\_formula\_id}

### Response samples

## tag/feedback/operation/read_feedback_api_v1_feedback__feedback_id__get Read Feedback

Get a specific feedback.

##### Authorizations:

##### path Parameters

##### query Parameters

| | |
| --- | --- |
| include\_user\_names | Include User Names (boolean) or Include User Names (null) (Include User Names) |

### Responses

get/api/v1/feedback/{feedback\_id}

### Response samples

"key": "string",

"score": 0,

"value": 0,

"comment": "string",

"correction": { },

"feedback_group_id": "4ef7406c-cdc6-4573-acb1-4f69f1775190",

"feedback_source": {"type": "string",

"user_name": "string"

"feedback_thread_id": "string"

## tag/feedback/operation/update_feedback_api_v1_feedback__feedback_id__patch Update Feedback

Replace an existing feedback entry with a new, modified entry.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| score | Score (number) or Score (integer) or Score (boolean) or Score (null) (Score) |
| value | Value (number) or Value (integer) or Value (boolean) or Value (string) or Value (object) or Value (null) (Value) |
| comment | Comment (string) or Comment (null) (Comment) |
| correction | Correction (object) or Correction (string) or Correction (null) (Correction) |
| feedback\_config | FeedbackConfig (object) or null |

### Responses

patch/api/v1/feedback/{feedback\_id}

### Request samples

`{"score": 0,

"feedback_config": {"type": "continuous",

"min": 0,

"max": 0,

"categories": [{"value": 0,\
\
"label": "string"\
\
}\
\
]

### Response samples

## tag/feedback/operation/delete_feedback_api_v1_feedback__feedback_id__delete Delete Feedback

Delete a feedback.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/feedback/{feedback\_id}

### Response samples

## tag/feedback/operation/read_feedbacks_api_v1_feedback_get Read Feedbacks

List all Feedback by query params.

##### Authorizations:

##### query Parameters

| | |
| --- | --- |
| run | Array of Run (strings) or Run (null) (Run) |
| key | Array of Key (strings) or Key (null) (Key) |
| session | Array of Session (strings) or Session (null) (Session) |
| source | Array of Source (strings) or Source (null) (Source) |

| user | Array of User (strings) or User (null) (User) |
| has\_comment | Has Comment (boolean) or Has Comment (null) (Has Comment) |
| has\_score | Has Score (boolean) or Has Score (null) (Has Score) |
| level | FeedbackLevel (string) or Level (null) (Level) |
| max\_created\_at | Max Created At (string) or Max Created At (null) (Max Created At) |
| min\_created\_at | Min Created At (string) or Min Created At (null) (Min Created At) |
| include\_user\_names | Include User Names (boolean) or Include User Names (null) (Include User Names) |
| comparative\_experiment\_id | Comparative Experiment Id (string) or Comparative Experiment Id (null) (Comparative Experiment Id) |

### Responses

get/api/v1/feedback

### Response samples

`[{"created_at": "2019-08-24T14:15:22Z",\
\
"modified_at": "2019-08-24T14:15:22Z",\
\
"key": "string",\
\
"score": 0,\
\
"value": 0,\
\
"comment": "string",\
\
"correction": { },\
\
"feedback_group_id": "4ef7406c-cdc6-4573-acb1-4f69f1775190",\
\
"comparative_experiment_id": "f54c3bfd-24c9-46c1-a7e1-f4aea1c1b10a",\
\
"run_id": "dded282c-8ebd-44cf-8ba5-9a234973d1ec",\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"trace_id": "df570c03-5a03-4cea-8df0-c162d05127ac",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"feedback_source": {"type": "string",\
\
"metadata": { },\
\
"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"ls_user_id": "e368397c-ab29-448f-907e-f04754df6859",\
\
"user_name": "string"\
\
},\
\
"extra": { },\
\
"feedback_thread_id": "string"\
\
}\
\
]`

## tag/feedback/operation/create_feedback_api_v1_feedback_post Create Feedback

Create a new feedback.

##### Authorizations:

##### Request Body schema: application/json required

| score | Score (number) or Score (integer) or Score (boolean) or Score (null) (Score) |
| value | Value (number) or Value (integer) or Value (boolean) or Value (string) or Value (object) or Value (null) (Value) |
| comment | Comment (string) or Comment (null) (Comment) |
| correction | Correction (object) or Correction (string) or Correction (null) (Correction) |
| feedback\_group\_id | Feedback Group Id (string) or Feedback Group Id (null) (Feedback Group Id) |
| comparative\_experiment\_id | Comparative Experiment Id (string) or Comparative Experiment Id (null) (Comparative Experiment Id) |
| run\_id | Run Id (string) or Run Id (null) (Run Id) |
| session\_id | Session Id (string) or Session Id (null) (Session Id) |
| trace\_id | Trace Id (string) or Trace Id (null) (Trace Id) |
| start\_time | Start Time (string) or Start Time (null) (Start Time) |

| feedback\_source | Feedback Source (any) or Feedback Source (null) (Feedback Source) |
| feedback\_config | FeedbackConfig (object) or null |
| error | Error (boolean) or Error (null) (Error) |

### Responses

post/api/v1/feedback

### Request samples

"feedback_source": {"type": "app",

"error": true

### Response samples

## tag/feedback/operation/eagerly_create_feedback_api_v1_feedback_eager_post Eagerly Create Feedback

This method is invoked under the assumption that the run
is already visible in the app, thus already present in DB

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/feedback/eager

### Request samples

### Response samples

## tag/feedback/operation/create_feedback_ingest_token_api_v1_feedback_tokens_post Create Feedback Ingest Token

Create a new feedback ingest token.

##### Authorizations:

##### Request Body schema: application/json required

Any of

FeedbackIngestTokenCreateSchemaFeedback Ingest Token

| | |
| --- | --- |
| expires\_in | TimedeltaInput (object) or null |
| expires\_at | Expires At (string) or Expires At (null) (Expires At) |

| feedback\_config | FeedbackConfig (object) or null |

### Responses

post/api/v1/feedback/tokens

### Request samples

FeedbackIngestTokenCreateSchemaFeedback Ingest TokenFeedbackIngestTokenCreateSchema

`{"expires_in": {"days": 0,

### Response samples

FeedbackIngestTokenSchemaResponse Create Feedback Ingest Token Api V1 Feedback Tokens PostFeedbackIngestTokenSchema

"url": "string",

"feedback_key": "string"

## tag/feedback/operation/list_feedback_ingest_tokens_api_v1_feedback_tokens_get List Feedback Ingest Tokens

List all feedback ingest tokens for a run.

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/feedback/tokens

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"url": "string",\
\
"expires_at": "2019-08-24T14:15:22Z",\
\
"feedback_key": "string"\
\
}\
\
]`

## tag/feedback/operation/create_feedback_with_token_get_api_v1_feedback_tokens__token__get Create Feedback With Token Get

Create a new feedback with a token.

##### path Parameters

##### query Parameters

| | |
| --- | --- |
| score | Score (number) or Score (integer) or Score (boolean) or Score (null) (Score) |
| value | Value (number) or Value (integer) or Value (boolean) or Value (string) or Value (null) (Value) |
| comment | Comment (string) or Comment (null) (Comment) |
| correction | Correction (string) or Correction (null) (Correction) |

### Responses

get/api/v1/feedback/tokens/{token}

### Response samples

## tag/feedback/operation/create_feedback_with_token_post_api_v1_feedback_tokens__token__post Create Feedback With Token Post

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| score | Score (number) or Score (integer) or Score (boolean) or Score (null) (Score) |
| value | Value (number) or Value (integer) or Value (boolean) or Value (string) or Value (null) (Value) |
| comment | Comment (string) or Comment (null) (Comment) |
| correction | Correction (object) or Correction (string) or Correction (null) (Correction) |
| metadata | Metadata (object) or Metadata (null) (Metadata) |

### Responses

post/api/v1/feedback/tokens/{token}

### Request samples

### Response samples

## tag/feedback/paths/~1feedback~1batch/post Ingest Feedback (Batch JSON)

Ingests a batch of feedback objects in a single JSON array payload.

##### Request Body schema: application/json required

| | |
| --- | --- |
| comment | string |
| comparative\_experiment\_id | string |
| correction | any |
| created\_at | string |
| error | boolean |
| feedback\_config | object (types.FeedbackConfig) |
| feedback\_group\_id | string |
| feedback\_source | object (feedback.FeedbackSource) |
| id | string |
| key | string |
| modified\_at | string |
| run\_id | string |
| score | any |
| session\_id | string |
| start\_time | string |
| trace\_id | string |
| value | any |

### Responses

Feedback batch ingested

**429**

Too Many Requests

post/feedback/batch

### Request samples

`[{"comment": "string",\
\
"comparative_experiment_id": "string",\
\
"correction": null,\
\
"created_at": "string",\
\
"error": true,\
\
"feedback_config": {"categories": [{"label": "string",\
\
"value": 0\
\
}\
\
],\
\
"max": 0,\
\
"min": 0,\
\
"type": "continuous"\
\
},\
\
"feedback_group_id": "string",\
\
"feedback_source": {"ls_user_id": "string",\
\
"metadata": { },\
\
"type": "string",\
\
"user_id": "string"\
\
},\
\
"id": "string",\
\
"key": "string",\
\
"modified_at": "string",\
\
"run_id": "string",\
\
"score": null,\
\
"session_id": "string",\
\
"start_time": "string",\
\
"trace_id": "string",\
\
"value": null\
\
}\
\
]`

### Response samples

- 202
- 400
- 403
- 409
- 422
- 429

`{"property1": {"message": "string"

"property2": {"message": "string"

## tag/public public

## tag/public/operation/get_shared_run_api_v1_public__share_token__run_get Get Shared Run

Get the shared run.

##### path Parameters

##### query Parameters

### Responses

get/api/v1/public/{share\_token}/run

### Response samples

"messages": [{ }\
\
]

## tag/public/operation/get_shared_run_by_id_api_v1_public__share_token__run__id__get Get Shared Run By Id

##### path Parameters

##### query Parameters

### Responses

get/api/v1/public/{share\_token}/run/{id}

### Response samples

## tag/public/operation/query_shared_runs_api_v1_public__share_token__runs_query_post Query Shared Runs

Get run by ids or the shared run if not specifed.

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/public/{share\_token}/runs/query

### Request samples

`{"id": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]

### Response samples

`{"runs": [{"name": "string",\
\
"inputs": { },\
\
"inputs_preview": "string",\
\
"run_type": "tool",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"extra": { },\
\
"error": "string",\
\
"execution_order": 1,\
\
"serialized": { },\
\
"outputs": { },\
\
"outputs_preview": "string",\
\
"parent_run_id": "f8faf8c1-9778-49a4-9004-628cdb0047e5",\
\
"manifest_id": "82825e8e-31fc-47d5-83ce-cd926068341e",\
\
"manifest_s3_id": "0454f93b-7eb6-4b9d-a203-f1261e686840",\
\
"events": [{ }\
\
],\
\
"tags": ["string"\
\
],\
\
"inputs_s3_urls": { },\
\
"outputs_s3_urls": { },\
\
"s3_urls": { },\
\
"trace_id": "df570c03-5a03-4cea-8df0-c162d05127ac",\
\
"dotted_order": "string",\
\
"trace_min_start_time": "2019-08-24T14:15:22Z",\
\
"trace_max_start_time": "2019-08-24T14:15:22Z",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"status": "string",\
\
"child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"direct_child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"parent_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"feedback_stats": {"property1": { },\
\
"property2": { }\
\
},\
\
"reference_example_id": "9fb06aaa-105f-4c87-845f-47d62ffd7ee6",\
\
"total_tokens": 0,\
\
"prompt_tokens": 0,\
\
"completion_tokens": 0,\
\
"prompt_token_details": {"property1": 0,\
\
"property2": 0\
\
},\
\
"completion_token_details": {"property1": 0,\
\
"property2": 0\
\
},\
\
"total_cost": "string",\
\
"prompt_cost": "string",\
\
"completion_cost": "string",\
\
"prompt_cost_details": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"completion_cost_details": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"price_model_id": "0b5d9575-bec3-4256-b43a-05893b8b8440",\
\
"first_token_time": "2019-08-24T14:15:22Z",\
\
"messages": [{ }\
\
]\
\
}\
\
],

## tag/public/operation/read_shared_feedbacks_api_v1_public__share_token__feedbacks_get Read Shared Feedbacks

##### path Parameters

##### query Parameters

| user | Array of User (strings) or User (null) (User) |
| has\_comment | Has Comment (boolean) or Has Comment (null) (Has Comment) |
| has\_score | Has Score (boolean) or Has Score (null) (Has Score) |
| level | FeedbackLevel (string) or Level (null) (Level) |

### Responses

get/api/v1/public/{share\_token}/feedbacks

### Response samples

## tag/public/operation/read_shared_dataset_api_v1_public__share_token__datasets_get Read Shared Dataset

Get dataset by ids or the shared dataset if not specifed.

##### path Parameters

##### query Parameters

### Responses

get/api/v1/public/{share\_token}/datasets

### Response samples

"example_count": 0

## tag/public/operation/count_shared_examples_api_v1_public__share_token__examples_count_get Count Shared Examples

##### path Parameters

##### query Parameters

| metadata | Metadata (string) or Metadata (null) (Metadata) |
| filter | Filter (string) or Filter (null) (Filter) |

### Responses

get/api/v1/public/{share\_token}/examples/count

### Response samples

## tag/public/operation/read_shared_examples_api_v1_public__share_token__examples_get Read Shared Examples

Get example by ids or the shared example if not specifed.

##### path Parameters

##### query Parameters

| metadata | Metadata (string) or Metadata (null) (Metadata) |

### Responses

get/api/v1/public/{share\_token}/examples

### Response samples

## tag/public/operation/read_shared_dataset_tracer_sessions_api_v1_public__share_token__datasets_sessions_get Read Shared Dataset Tracer Sessions

Get projects run on a dataset that has been shared.

##### path Parameters

##### query Parameters

| | |
| --- | --- |
| id | Array of Id (strings) or Id (null) (Id) |
| name | Name (string) or Name (null) (Name) |
| name\_contains | Name Contains (string) or Name Contains (null) (Name Contains) |
| dataset\_version | Dataset Version (string) or Dataset Version (null) (Dataset Version) |

| sort\_by\_feedback\_key | Sort By Feedback Key (string) or Sort By Feedback Key (null) (Sort By Feedback Key) |

##### header Parameters

### Responses

get/api/v1/public/{share\_token}/datasets/sessions

### Response samples

## tag/public/operation/read_shared_dataset_tracer_sessions_bulk_api_v1_public_datasets_sessions_bulk_get Read Shared Dataset Tracer Sessions Bulk

Get sessions from multiple datasets using share tokens.

##### query Parameters

### Responses

get/api/v1/public/datasets/sessions-bulk

### Response samples

## tag/public/operation/read_shared_dataset_examples_with_runs_api_v1_public__share_token__examples_runs_post Read Shared Dataset Examples With Runs

Get examples with associated runs from sessions in a dataset that has been shared.

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/public/{share\_token}/examples/runs

### Request samples

### Response samples

## tag/public/operation/read_shared_delta_api_v1_public__share_token__datasets_runs_delta_post Read Shared Delta

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/public/{share\_token}/datasets/runs/delta

### Request samples

### Response samples

## tag/public/operation/query_shared_dataset_runs_api_v1_public__share_token__datasets_runs_query_post Query Shared Dataset Runs

Get runs in projects run over a dataset that has been shared.

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/public/{share\_token}/datasets/runs/query

### Request samples

### Response samples

`{"runs": [{"name": "string",\
\
"inputs": { },\
\
"inputs_preview": "string",\
\
"run_type": "tool",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"extra": { },\
\
"error": "string",\
\
"execution_order": 1,\
\
"serialized": { },\
\
"outputs": { },\
\
"outputs_preview": "string",\
\
"parent_run_id": "f8faf8c1-9778-49a4-9004-628cdb0047e5",\
\
"manifest_id": "82825e8e-31fc-47d5-83ce-cd926068341e",\
\
"manifest_s3_id": "0454f93b-7eb6-4b9d-a203-f1261e686840",\
\
"events": [{ }\
\
],\
\
"tags": ["string"\
\
],\
\
"inputs_s3_urls": { },\
\
"outputs_s3_urls": { },\
\
"s3_urls": { },\
\
"trace_id": "df570c03-5a03-4cea-8df0-c162d05127ac",\
\
"dotted_order": "string",\
\
"trace_min_start_time": "2019-08-24T14:15:22Z",\
\
"trace_max_start_time": "2019-08-24T14:15:22Z",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"status": "string",\
\
"child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"direct_child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"parent_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"feedback_stats": {"property1": { },\
\
"property2": { }\
\
},\
\
"reference_example_id": "9fb06aaa-105f-4c87-845f-47d62ffd7ee6",\
\
"total_tokens": 0,\
\
"prompt_tokens": 0,\
\
"completion_tokens": 0,\
\
"prompt_token_details": {"property1": 0,\
\
"property2": 0\
\
},\
\
"completion_token_details": {"property1": 0,\
\
"property2": 0\
\
},\
\
"total_cost": "string",\
\
"prompt_cost": "string",\
\
"completion_cost": "string",\
\
"prompt_cost_details": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"completion_cost_details": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"price_model_id": "0b5d9575-bec3-4256-b43a-05893b8b8440",\
\
"first_token_time": "2019-08-24T14:15:22Z",\
\
"messages": [{ }\
\
],\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82"\
\
}\
\
],

## tag/public/operation/generate_query_for_shared_dataset_runs_api_v1_public__share_token__datasets_runs_generate_query_post Generate Query For Shared Dataset Runs

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/public/{share\_token}/datasets/runs/generate-query

### Request samples

### Response samples

## tag/public/operation/stats_shared_dataset_runs_api_v1_public__share_token__datasets_runs_stats_post Stats Shared Dataset Runs

Get run stats in projects run over a dataset that has been shared.

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/public/{share\_token}/datasets/runs/stats

### Request samples

### Response samples

## tag/public/operation/read_shared_dataset_run_api_v1_public__share_token__datasets_runs__run_id__get Read Shared Dataset Run

##### path Parameters

##### query Parameters

### Responses

get/api/v1/public/{share\_token}/datasets/runs/{run\_id}

### Response samples

"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82"

## tag/public/operation/read_shared_dataset_feedback_api_v1_public__share_token__datasets_feedback_get Read Shared Dataset Feedback

Get feedback for runs in projects run over a dataset that has been shared.

##### path Parameters

##### query Parameters

### Responses

get/api/v1/public/{share\_token}/datasets/feedback

### Response samples

## tag/public/operation/read_shared_comparative_experiments_api_v1_public__share_token__datasets_comparative_get Read Shared Comparative Experiments

##### path Parameters

##### query Parameters

| | |
| --- | --- |
| name | Name (string) or Name (null) (Name) |
| name\_contains | Name Contains (string) or Name Contains (null) (Name Contains) |

### Responses

get/api/v1/public/{share\_token}/datasets/comparative

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"name": "string",\
\
"description": "string",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"modified_at": "2019-08-24T14:15:22Z",\
\
"extra": { },\
\
"experiments_info": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"name": "string"\
\
}\
\
],\
\
"feedback_stats": { }\
\
}\
\
]`

## tag/public/operation/get_message_json_schema_api_v1_public_schemas__version__message_json_get Get Message Json Schema

##### path Parameters

### Responses

get/api/v1/public/schemas/{version}/message.json

### Response samples

## tag/public/operation/get_tool_def_json_schema_api_v1_public_schemas__version__tooldef_json_get Get Tool Def Json Schema

##### path Parameters

### Responses

get/api/v1/public/schemas/{version}/tooldef.json

### Response samples

## tag/annotation-queues annotation-queues

## tag/annotation-queues/operation/get_annotation_queues_api_v1_annotation_queues_get Get Annotation Queues

##### Authorizations:

##### query Parameters

| | |
| --- | --- |
| ids | Array of Ids (strings) or Ids (null) (Ids) |
| name | Name (string) or Name (null) (Name) |
| name\_contains | Name Contains (string) or Name Contains (null) (Name Contains) |

| tag\_value\_id | Array of Tag Value Id (strings) or Tag Value Id (null) (Tag Value Id) |
| dataset\_id | Dataset Id (string) or Dataset Id (null) (Dataset Id) |
| queue\_type | Queue Type (string) or Queue Type (null) (Queue Type) |

### Responses

get/api/v1/annotation-queues

### Response samples

`[{"description": "string",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"num_reviewers_per_item": 1,\
\
"enable_reservations": true,\
\
"reservation_minutes": 1,\
\
"name": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"source_rule_id": "09d16b35-06d8-44f8-b6da-d69d3150a762",\
\
"run_rule_id": "4e5253f3-54aa-4546-8683-2a9fb3d88383",\
\
"default_dataset": "1d728328-2c74-4eda-bc31-d15f02a4d2f2",\
\
"queue_type": "single",\
\
"metadata": { },\
\
"total_runs": 0\
\
}\
\
]`

## tag/annotation-queues/operation/create_annotation_queue_api_v1_annotation_queues_post Create Annotation Queue

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| description | Description (string) or Description (null) (Description) |

| default\_dataset | Default Dataset (string) or Default Dataset (null) (Default Dataset) |
| rubric\_items | Array of Rubric Items (objects) or Rubric Items (null) (Rubric Items) |
| rubric\_instructions | Rubric Instructions (string) or Rubric Instructions (null) (Rubric Instructions) |
| session\_ids | Array of Session Ids (strings) or Session Ids (null) (Session Ids) |
| metadata | Metadata (object) or Metadata (null) (Metadata) |

### Responses

post/api/v1/annotation-queues

### Request samples

`{"description": "string",

"num_reviewers_per_item": 1,

"enable_reservations": true,

"reservation_minutes": 1,

"default_dataset": "1d728328-2c74-4eda-bc31-d15f02a4d2f2",

"rubric_items": [{"feedback_key": "string",\
\
"description": "string",\
\
"value_descriptions": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"score_descriptions": {"property1": "string",\
\
"property2": "string"\
\
}\
\
}\
\
],

"rubric_instructions": "string",

"session_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

### Response samples

"source_rule_id": "09d16b35-06d8-44f8-b6da-d69d3150a762",

"run_rule_id": "4e5253f3-54aa-4546-8683-2a9fb3d88383",

"queue_type": "single",

## tag/annotation-queues/operation/populate_annotation_queue_api_v1_annotation_queues_populate_post Populate Annotation Queue

Populate annotation queue with runs from an experiment.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/annotation-queues/populate

### Request samples

`{"queue_id": "cefd6192-7a66-4699-a2fc-dbb7f43ad507",

"session_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]

### Response samples

## tag/annotation-queues/operation/delete_annotation_queue_api_v1_annotation_queues__queue_id__delete Delete Annotation Queue

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/annotation-queues/{queue\_id}

### Response samples

## tag/annotation-queues/operation/update_annotation_queue_api_v1_annotation_queues__queue_id__patch Update Annotation Queue

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| name | Name (string) or Name (null) (Name) |
| description | Description (string) or Description (null) (Description) |
| default\_dataset | Default Dataset (string) or Default Dataset (null) (Default Dataset) |

| reservation\_minutes | Reservation Minutes (integer) or Reservation Minutes (null) (Reservation Minutes) |
| rubric\_items | Array of Rubric Items (objects) or Rubric Items (null) (Rubric Items) |
| rubric\_instructions | Rubric Instructions (string) or Rubric Instructions (null) (Rubric Instructions) |

### Responses

patch/api/v1/annotation-queues/{queue\_id}

### Request samples

"reservation_minutes": 0,

### Response samples

## tag/annotation-queues/operation/get_annotation_queue_api_v1_annotation_queues__queue_id__get Get Annotation Queue

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/annotation-queues/{queue\_id}

### Response samples

"rubric_instructions": "string"

## tag/annotation-queues/operation/add_runs_to_annotation_queue_api_v1_annotation_queues__queue_id__runs_post Add Runs To Annotation Queue

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

RunsRuns

post/api/v1/annotation-queues/{queue\_id}/runs

### Request samples

### Response samples

`[{"run_id": "dded282c-8ebd-44cf-8ba5-9a234973d1ec",\
\
"queue_id": "cefd6192-7a66-4699-a2fc-dbb7f43ad507",\
\
"last_reviewed_time": "2019-08-24T14:15:22Z",\
\
"added_at": "2019-08-24T14:15:22Z",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
}\
\
]`

## tag/annotation-queues/operation/get_runs_from_annotation_queue_api_v1_annotation_queues__queue_id__runs_get Get Runs From Annotation Queue

##### Authorizations:

##### path Parameters

##### query Parameters

| archived | Archived (boolean) or Archived (null) (Archived) |
| include\_stats | Include Stats (boolean) or Include Stats (null) (Include Stats) |

### Responses

get/api/v1/annotation-queues/{queue\_id}/runs

### Response samples

`[{"name": "string",\
\
"inputs": { },\
\
"inputs_preview": "string",\
\
"run_type": "tool",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"extra": { },\
\
"error": "string",\
\
"execution_order": 1,\
\
"serialized": { },\
\
"outputs": { },\
\
"outputs_preview": "string",\
\
"parent_run_id": "f8faf8c1-9778-49a4-9004-628cdb0047e5",\
\
"manifest_id": "82825e8e-31fc-47d5-83ce-cd926068341e",\
\
"manifest_s3_id": "0454f93b-7eb6-4b9d-a203-f1261e686840",\
\
"events": [{ }\
\
],\
\
"tags": ["string"\
\
],\
\
"inputs_s3_urls": { },\
\
"outputs_s3_urls": { },\
\
"s3_urls": { },\
\
"trace_id": "df570c03-5a03-4cea-8df0-c162d05127ac",\
\
"dotted_order": "string",\
\
"trace_min_start_time": "2019-08-24T14:15:22Z",\
\
"trace_max_start_time": "2019-08-24T14:15:22Z",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"status": "string",\
\
"child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"direct_child_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"parent_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"feedback_stats": {"property1": { },\
\
"property2": { }\
\
},\
\
"reference_example_id": "9fb06aaa-105f-4c87-845f-47d62ffd7ee6",\
\
"total_tokens": 0,\
\
"prompt_tokens": 0,\
\
"completion_tokens": 0,\
\
"prompt_token_details": {"property1": 0,\
\
"property2": 0\
\
},\
\
"completion_token_details": {"property1": 0,\
\
"property2": 0\
\
},\
\
"total_cost": "string",\
\
"prompt_cost": "string",\
\
"completion_cost": "string",\
\
"prompt_cost_details": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"completion_cost_details": {"property1": "string",\
\
"property2": "string"\
\
},\
\
"price_model_id": "0b5d9575-bec3-4256-b43a-05893b8b8440",\
\
"first_token_time": "2019-08-24T14:15:22Z",\
\
"messages": [{ }\
\
],\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",\
\
"app_path": "string",\
\
"last_queued_at": "2019-08-24T14:15:22Z",\
\
"in_dataset": true,\
\
"share_token": "d0430ac3-04a1-4e32-a7ea-57776ad22c1c",\
\
"trace_tier": "longlived",\
\
"trace_first_received_at": "2019-08-24T14:15:22Z",\
\
"ttl_seconds": 0,\
\
"trace_upgrade": false,\
\
"reference_dataset_id": "4abef5d4-9fac-44e9-b393-753a8de1db88",\
\
"thread_id": "string",\
\
"queue_run_id": "3ffacc9a-0e54-4fab-a48a-6806dce49696",\
\
"last_reviewed_time": "2019-08-24T14:15:22Z",\
\
"added_at": "2019-08-24T14:15:22Z",\
\
"effective_added_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/annotation-queues/operation/export_annotation_queue_archived_runs_api_v1_annotation_queues__queue_id__export_post Export Annotation Queue Archived Runs

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| start\_time | Start Time (string) or Start Time (null) (Start Time) |
| end\_time | End Time (string) or End Time (null) (End Time) |

### Responses

post/api/v1/annotation-queues/{queue\_id}/export

### Request samples

"end_time": "2019-08-24T14:15:22Z"

### Response samples

## tag/annotation-queues/operation/get_run_from_annotation_queue_api_v1_annotation_queues__queue_id__run__index__get Get Run From Annotation Queue

Get a run from an annotation queue

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/annotation-queues/{queue\_id}/run/{index}

### Response samples

"thread_id": "string",

"queue_run_id": "3ffacc9a-0e54-4fab-a48a-6806dce49696",

"last_reviewed_time": "2019-08-24T14:15:22Z",

"added_at": "2019-08-24T14:15:22Z",

"effective_added_at": "2019-08-24T14:15:22Z"

## tag/annotation-queues/operation/get_annotation_queues_for_run_api_v1_annotation_queues__run_id__queues_get Get Annotation Queues For Run

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/annotation-queues/{run\_id}/queues

### Response samples

`[{"description": "string",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"num_reviewers_per_item": 1,\
\
"enable_reservations": true,\
\
"reservation_minutes": 1,\
\
"name": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"source_rule_id": "09d16b35-06d8-44f8-b6da-d69d3150a762",\
\
"run_rule_id": "4e5253f3-54aa-4546-8683-2a9fb3d88383",\
\
"default_dataset": "1d728328-2c74-4eda-bc31-d15f02a4d2f2",\
\
"queue_type": "single",\
\
"metadata": { }\
\
}\
\
]`

## tag/annotation-queues/operation/update_run_in_annotation_queue_api_v1_annotation_queues__queue_id__runs__queue_run_id__patch Update Run In Annotation Queue

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| last\_reviewed\_time | Last Reviewed Time (string) or Last Reviewed Time (null) (Last Reviewed Time) |
| added\_at | Added At (string) or Added At (null) (Added At) |

### Responses

patch/api/v1/annotation-queues/{queue\_id}/runs/{queue\_run\_id}

### Request samples

`{"last_reviewed_time": "2019-08-24T14:15:22Z",

"added_at": "2019-08-24T14:15:22Z"

### Response samples

## tag/annotation-queues/operation/delete_run_from_annotation_queue_api_v1_annotation_queues__queue_id__runs__queue_run_id__delete Delete Run From Annotation Queue

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/annotation-queues/{queue\_id}/runs/{queue\_run\_id}

### Response samples

## tag/annotation-queues/operation/delete_runs_from_annotation_queue_api_v1_annotation_queues__queue_id__runs_delete_post Delete Runs From Annotation Queue

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| run\_ids | Array of Run Ids (strings) or Run Ids (null) (Run Ids) |
| exclude\_run\_ids | Array of Exclude Run Ids (strings) or Exclude Run Ids (null) (Exclude Run Ids) |

### Responses

post/api/v1/annotation-queues/{queue\_id}/runs/delete

### Request samples

`{"delete_all": false,

"run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"exclude_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]

### Response samples

## tag/annotation-queues/operation/get_total_size_from_annotation_queue_api_v1_annotation_queues__queue_id__total_size_get Get Total Size From Annotation Queue

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/annotation-queues/{queue\_id}/total\_size

### Response samples

`{"size": 0

## tag/annotation-queues/operation/get_total_archived_from_annotation_queue_api_v1_annotation_queues__queue_id__total_archived_get Get Total Archived From Annotation Queue

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/annotation-queues/{queue\_id}/total\_archived

### Response samples

## tag/annotation-queues/operation/get_size_from_annotation_queue_api_v1_annotation_queues__queue_id__size_get Get Size From Annotation Queue

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/annotation-queues/{queue\_id}/size

### Response samples

## tag/annotation-queues/operation/create_identity_annotation_queue_run_status_api_v1_annotation_queues_status__annotation_queue_run_id__post Create Identity Annotation Queue Run Status

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| status | Status (string) or Status (null) (Status) |
| override\_added\_at | Override Added At (string) or Override Added At (null) (Override Added At) |

### Responses

post/api/v1/annotation-queues/status/{annotation\_queue\_run\_id}

### Request samples

`{"status": "string",

"override_added_at": "2019-08-24T14:15:22Z"

### Response samples

## tag/ace ace

## tag/ace/operation/execute_api_v1_ace_execute_post Execute

Execute some custom code for testing purposes.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/ace/execute

### Request samples

`{"args": [null\
\
],

"code": "string",

"language": "string"

### Response samples

## tag/bulk-exports bulk-exports

## tag/bulk-exports/operation/get_bulk_exports_api_v1_bulk_exports_get Get Bulk Exports

Get the current workspace's bulk exports

##### Authorizations:

### Responses

get/api/v1/bulk-exports

### Response samples

`[{"bulk_export_destination_id": "6706564e-1d1d-48bf-843f-8c9b9634fa47",\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"filter": "string",\
\
"format": "Parquet",\
\
"format_version": "v1",\
\
"compression": "none",\
\
"interval_hours": 0,\
\
"export_fields": ["string"\
\
],\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"status": "Cancelled",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"finished_at": "2019-08-24T14:15:22Z",\
\
"source_bulk_export_id": "58f8a0ff-3183-41fa-a0e6-d7178eec7b73"\
\
}\
\
]`

## tag/bulk-exports/operation/create_bulk_export_api_v1_bulk_exports_post Create Bulk Export

Create a new bulk export

##### Authorizations:

##### Request Body schema: application/json required

| end\_time | End Time (string) or End Time (null) (End Time) |
| filter | Filter (string) or Filter (null) (Filter) |

| interval\_hours | Interval Hours (integer) or Interval Hours (null) (Interval Hours) |
| export\_fields | Array of Export Fields (strings) or Export Fields (null) (Export Fields) |

### Responses

post/api/v1/bulk-exports

### Request samples

`{"bulk_export_destination_id": "6706564e-1d1d-48bf-843f-8c9b9634fa47",

"format": "Parquet",

"format_version": "v1",

"compression": "none",

"interval_hours": 0,

"export_fields": ["string"\
\
]

### Response samples

"export_fields": ["string"\
\
],

"status": "Cancelled",

"finished_at": "2019-08-24T14:15:22Z",

"source_bulk_export_id": "58f8a0ff-3183-41fa-a0e6-d7178eec7b73"

## tag/bulk-exports/operation/get_bulk_export_destinations_api_v1_bulk_exports_destinations_get Get Bulk Export Destinations

Get the current workspace's bulk export destinations

##### Authorizations:

### Responses

get/api/v1/bulk-exports/destinations

### Response samples

`[{"destination_type": "s3",\
\
"display_name": "string",\
\
"config": {"endpoint_url": "string",\
\
"prefix": "",\
\
"bucket_name": "string",\
\
"region": "string",\
\
"s3_additional_kwargs": { },\
\
"include_bucket_in_prefix": true\
\
},\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"credentials_keys": ["string"\
\
]\
\
}\
\
]`

## tag/bulk-exports/operation/create_bulk_export_destination_api_v1_bulk_exports_destinations_post Create Bulk Export Destination

Create a new bulk export destination

##### Authorizations:

##### Request Body schema: application/json required

| credentials | BulkExportDestinationS3Credentials (object) or null |

### Responses

post/api/v1/bulk-exports/destinations

### Request samples

`{"destination_type": "s3",

"config": {"endpoint_url": "string",

"prefix": "",

"bucket_name": "string",

"region": "string",

"s3_additional_kwargs": { },

"include_bucket_in_prefix": true

"credentials": {"access_key_id": "string",

"secret_access_key": "string",

"session_token": "string"

### Response samples

"credentials_keys": ["string"\
\
]

## tag/bulk-exports/operation/get_bulk_export_runs_filtered_api_v1_bulk_exports_runs_get Get Bulk Export Runs Filtered

Get all bulk export runs for exports that were created from a scheduled bulk export

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/bulk-exports/runs

### Response samples

`[{"bulk_export_id": "5b75bd73-24dc-4865-a939-6747d392e509",\
\
"metadata": {"prefix": "string",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"end_time": "2019-08-24T14:15:22Z",\
\
"result": {"rows_written": 0,\
\
"exported_files": ["string"\
\
],\
\
"export_path": "string",\
\
"latest_cursor": "string"\
\
}\
\
},\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"status": "Cancelled",\
\
"retry_number": 0,\
\
"errors": { },\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"finished_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/bulk-exports/operation/get_bulk_export_api_v1_bulk_exports__bulk_export_id__get Get Bulk Export

Get a single bulk export by ID

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/bulk-exports/{bulk\_export\_id}

### Response samples

## tag/bulk-exports/operation/cancel_bulk_export_api_v1_bulk_exports__bulk_export_id__patch Cancel Bulk Export

Cancel a bulk export by ID

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

patch/api/v1/bulk-exports/{bulk\_export\_id}

### Request samples

`{"status": "Cancelled"

### Response samples

## tag/bulk-exports/operation/get_bulk_export_destination_api_v1_bulk_exports_destinations__destination_id__get Get Bulk Export Destination

Get a single bulk export destination by ID

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/bulk-exports/destinations/{destination\_id}

### Response samples

## tag/bulk-exports/operation/get_bulk_export_runs_api_v1_bulk_exports__bulk_export_id__runs_get Get Bulk Export Runs

Get a bulk export's runs

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/bulk-exports/{bulk\_export\_id}/runs

### Response samples

## tag/bulk-exports/operation/get_bulk_export_run_api_v1_bulk_exports__bulk_export_id__runs__run_id__get Get Bulk Export Run

Get a single bulk export's run by ID

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/bulk-exports/{bulk\_export\_id}/runs/{run\_id}

### Response samples

`{"bulk_export_id": "5b75bd73-24dc-4865-a939-6747d392e509",

"metadata": {"prefix": "string",

"result": {"rows_written": 0,

"exported_files": ["string"\
\
],

"export_path": "string",

"latest_cursor": "string"

"retry_number": 0,

"errors": { },

"finished_at": "2019-08-24T14:15:22Z"

## tag/tenant tenant

## tag/tenant/operation/list_tenants_api_v1_tenants_get List Tenants

Get all tenants visible to this auth

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/tenants

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"display_name": "string",\
\
"is_personal": true,\
\
"is_deleted": true,\
\
"tenant_handle": "string",\
\
"read_only": false,\
\
"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",\
\
"role_name": "string",\
\
"permissions": ["string"\
\
]\
\
}\
\
]`

## tag/tenant/operation/create_tenant_api_v1_tenants_post Create Tenant

Create a new organization and corresponding workspace.

##### Authorizations:

##### Request Body schema: application/json required

| organization\_id | Organization Id (string) or Organization Id (null) (Organization Id) |

| tenant\_handle | Tenant Handle (string) or Tenant Handle (null) (Tenant Handle) |

### Responses

post/api/v1/tenants

### Request samples

"tenant_handle": "string",

"is_personal": false

### Response samples

"is_deleted": true,

"tenant_handle": "string"

## tag/info info

## tag/info/operation/get_server_info_api_v1_info_get Get Server Info

Get information about the current deployment of LangSmith.

### Responses

get/api/v1/info

### Response samples

`{"version": "string",

"license_expiration_time": "2019-08-24T14:15:22Z",

"batch_ingest_config": {"use_multipart_endpoint": true,

"scale_up_qsize_trigger": 1000,

"scale_up_nthreads_limit": 16,

"scale_down_nempty_trigger": 4,

"size_limit": 100,

"size_limit_bytes": 20971520

"instance_flags": { },

"customer_info": {"customer_id": "string",

"customer_name": "string"

## tag/info/operation/get_health_info_api_v1_info_health_get Get Health Info

Get health information about the current deployment of LangSmith.

### Responses

get/api/v1/info/health

### Response samples

`{"clickhouse_disk_free_pct": 0

## tag/feedback-configs feedback-configs

## tag/feedback-configs/operation/list_feedback_configs_endpoint_api_v1_feedback_configs_get List Feedback Configs Endpoint

##### Authorizations:

##### query Parameters

| | |
| --- | --- |
| key | Array of Key (strings) or Key (null) (Key) |

### Responses

get/api/v1/feedback-configs

### Response samples

`[{"feedback_key": "string",\
\
"feedback_config": {"type": "continuous",\
\
"min": 0,\
\
"max": 0,\
\
"categories": [{"value": 0,\
\
"label": "string"\
\
}\
\
]\
\
},\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"modified_at": "2019-08-24T14:15:22Z",\
\
"is_lower_score_better": true\
\
}\
\
]`

## tag/feedback-configs/operation/create_feedback_config_endpoint_api_v1_feedback_configs_post Create Feedback Config Endpoint

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/feedback-configs

### Request samples

"is_lower_score_better": false

### Response samples

"is_lower_score_better": true

## tag/feedback-configs/operation/update_feedback_config_endpoint_api_v1_feedback_configs_patch Update Feedback Config Endpoint

##### Authorizations:

##### Request Body schema: application/json required

| feedback\_config | FeedbackConfig (object) or null |
| is\_lower\_score\_better | Is Lower Score Better (boolean) or Is Lower Score Better (null) (Is Lower Score Better) |

### Responses

patch/api/v1/feedback-configs

### Request samples

### Response samples

## tag/feedback-configs/operation/delete_feedback_config_endpoint_api_v1_feedback_configs_delete Delete Feedback Config Endpoint

Soft delete a feedback config by marking it as deleted.

The config can be recreated later with the same key (simple reuse pattern).
Existing feedback records with this key will remain unchanged.

##### Authorizations:

##### query Parameters

### Responses

**204**

delete/api/v1/feedback-configs

### Response samples

- 422

`{"detail": [{"loc": ["string"\
\
],\
\
"msg": "string",\
\
"type": "string"\
\
}\
\
]

## tag/metrics metrics

## tag/metrics/operation/get_queue_metrics_api_v1_metrics_queue__queue_name__get Get Queue Metrics

Return cached SAQ queue counts for the requested queue.

##### path Parameters

### Responses

get/api/v1/metrics/queue/{queue\_name}

### Response samples

`{"queued": 0,

"active": 0,

"scheduled": 0

## tag/model-price-map model-price-map

## tag/model-price-map/operation/read_model_price_map_api_v1_model_price_map_get Read Model Price Map

##### Authorizations:

### Responses

get/api/v1/model-price-map

### Response samples

## tag/model-price-map/operation/create_new_model_price_api_v1_model_price_map_post Create New Model Price

##### Authorizations:

##### Request Body schema: application/json required

| start\_time | Start Time (string) or Start Time (null) (Start Time) |

| prompt\_cost\_details | Prompt Cost Details (object) or Prompt Cost Details (null) (Prompt Cost Details) |
| completion\_cost\_details | Completion Cost Details (object) or Completion Cost Details (null) (Completion Cost Details) |
| provider | Provider (string) or Provider (null) (Provider) |

### Responses

post/api/v1/model-price-map

### Request samples

"match_path": ["model",\
\
"model_name",\
\
"model_id",\
\
"model_path",\
\
"endpoint_name"\
\
],

"match_pattern": "string",

"prompt_cost": 0,

"completion_cost": 0,

"prompt_cost_details": {"property1": 0,

"completion_cost_details": {"property1": 0,

"provider": "string"

### Response samples

## tag/model-price-map/operation/update_model_price_api_v1_model_price_map__id__put Update Model Price

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

put/api/v1/model-price-map/{id}

### Request samples

### Response samples

## tag/model-price-map/operation/delete_model_price_api_v1_model_price_map__id__delete Delete Model Price

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/model-price-map/{id}

### Response samples

## tag/usage-limits usage-limits

## tag/usage-limits/operation/list_usage_limits_api_v1_usage_limits_get List Usage Limits

List out the configured usage limits for a given tenant.

##### Authorizations:

### Responses

get/api/v1/usage-limits

### Response samples

`[{"limit_type": "monthly_traces",\
\
"limit_value": 0,\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/usage-limits/operation/upsert_usage_limit_api_v1_usage_limits_put Upsert Usage Limit

Create a new usage limit.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

put/api/v1/usage-limits

### Request samples

`{"limit_type": "monthly_traces",

"limit_value": 0,

### Response samples

## tag/usage-limits/operation/list_org_usage_limits_api_v1_usage_limits_org_get List Org Usage Limits

List out the configured usage limits for a given organization.

##### Authorizations:

### Responses

get/api/v1/usage-limits/org

### Response samples

## tag/usage-limits/operation/delete_usage_limit_api_v1_usage_limits__usage_limit_id__delete Delete Usage Limit

Delete a specific usage limit.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/usage-limits/{usage\_limit\_id}

### Response samples

## tag/ttl-settings ttl-settings

## tag/ttl-settings/operation/list_ttl_settings_api_v1_ttl_settings_get List Ttl Settings

List out the configured TTL settings for a given tenant.

##### Authorizations:

### Responses

get/api/v1/ttl-settings

### Response samples

## tag/ttl-settings/operation/upsert_ttl_settings_api_v1_ttl_settings_put Upsert Ttl Settings

##### Authorizations:

##### Request Body schema: application/json required

### Responses

put/api/v1/ttl-settings

### Request samples

### Response samples

## tag/prompts prompts

## tag/prompts/operation/invoke_prompt_api_v1_prompts_invoke_prompt_post Invoke Prompt

##### Request Body schema: application/json required

### Responses

post/api/v1/prompts/invoke\_prompt

### Request samples

`{"messages": [["string",\
\
"string"\
\
]\
\
],

"template_format": "string",

"inputs": { }

### Response samples

## tag/prompts/operation/prompt_canvas_api_v1_prompts_canvas_post Prompt Canvas

##### Authorizations:

##### Request Body schema: application/json required

| highlighted | Highlight (object) or null |
| artifact | Artifact (object) or null |
| artifact\_length | Artifact Length (string) or Artifact Length (null) (Artifact Length) |
| reading\_level | Reading Level (string) or Reading Level (null) (Reading Level) |
| custom\_action | Custom Action (string) or Custom Action (null) (Custom Action) |

### Responses

post/api/v1/prompts/canvas

### Request samples

`{"messages": [{"content": "string",\
\
"additional_kwargs": { },\
\
"response_metadata": { },\
\
"type": "ai",\
\
"name": "string",\
\
"id": "string",\
\
"tool_calls": [ ],\
\
"invalid_tool_calls": [ ],\
\
"usage_metadata": {"input_tokens": 0,\
\
"output_tokens": 0,\
\
"total_tokens": 0,\
\
"input_token_details": {"audio": 0,\
\
"cache_creation": 0,\
\
"cache_read": 0\
\
},\
\
"output_token_details": {"audio": 0,\
\
"reasoning": 0\
\
}\
\
}\
\
}\
\
],

"highlighted": {"prompt_chunk_start_index": 0,

"prompt_chunk_end_index": 0,

"prompt_chunk": "string",

"highlight_text": "string"

"artifact": {"id": "string",

"contents": [{"index": 0,\
\
"content": "string"\
\
}\
\
],

"current_content_index": 0

"artifact_length": "shortest",

"reading_level": "child",

"custom_action": "string",

"template_format": "f-string",

### Response samples

## tag/prompt-webhooks prompt-webhooks

## tag/prompt-webhooks/operation/list_prompt_webhooks_api_v1_prompt_webhooks_get List Prompt Webhooks

List all prompt webhooks for the current tenant.

##### Authorizations:

### Responses

get/api/v1/prompt-webhooks

### Response samples

`[{"url": "http://example.com",\
\
"headers": { },\
\
"include_prompts": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"exclude_prompts": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"triggers": ["commit"\
\
],\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/prompt-webhooks/operation/create_prompt_webhook_api_v1_prompt_webhooks_post Create Prompt Webhook

Create a new prompt webhook.

##### Authorizations:

##### Request Body schema: application/json required

| headers | Headers (object) or Headers (null) (Headers) |
| include\_prompts | Array of Include Prompts (strings) or Include Prompts (null) (Include Prompts) |
| exclude\_prompts | Array of Exclude Prompts (strings) or Exclude Prompts (null) (Exclude Prompts) |

### Responses

post/api/v1/prompt-webhooks

### Request samples

`{"url": "http://example.com",

"headers": { },

"include_prompts": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"exclude_prompts": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"triggers": ["commit"\
\
],

### Response samples

## tag/prompt-webhooks/operation/get_prompt_webhook_api_v1_prompt_webhooks__webhook_id__get Get Prompt Webhook

Get a specific prompt webhook.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/prompt-webhooks/{webhook\_id}

### Response samples

## tag/prompt-webhooks/operation/update_prompt_webhook_api_v1_prompt_webhooks__webhook_id__patch Update Prompt Webhook

Update a specific prompt webhook.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| include\_prompts | Array of Include Prompts (strings) or Include Prompts (null) (Include Prompts) |
| exclude\_prompts | Array of Exclude Prompts (strings) or Exclude Prompts (null) (Exclude Prompts) |
| url | Url (string) or Url (null) (Url) |
| headers | Headers (object) or Headers (null) (Headers) |
| triggers | Array of Triggers (strings) or Triggers (null) (Triggers) |

### Responses

patch/api/v1/prompt-webhooks/{webhook\_id}

### Request samples

`{"include_prompts": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"url": "http://example.com",

"triggers": ["commit"\
\
]

### Response samples

## tag/prompt-webhooks/operation/delete_prompt_webhook_api_v1_prompt_webhooks__webhook_id__delete Delete Prompt Webhook

Delete a specific prompt webhook.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/prompt-webhooks/{webhook\_id}

### Response samples

## tag/prompt-webhooks/operation/test_prompt_webhook_api_v1_prompt_webhooks_test_post Test Prompt Webhook

Test a specific prompt webhook.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/prompt-webhooks/test

### Request samples

`{"webhook": {"url": "http://example.com",

"payload": {"prompt_id": "string",

"prompt_name": "string",

"manifest": { },

"commit_hash": "string",

"created_at": "string",

"created_by": "string",

"event": "commit",

"tag_name": "string"

### Response samples

`{"property1": "string",

## tag/workspaces workspaces

## tag/workspaces/operation/list_workspaces_api_v1_workspaces_get List Workspaces

Get all workspaces visible to this auth in the current org. Does not create a new workspace/org.

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/workspaces

### Response samples

## tag/workspaces/operation/create_workspace_api_v1_workspaces_post Create Workspace

Create a new workspace.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/workspaces

### Request samples

### Response samples

## tag/workspaces/operation/patch_workspace_api_v1_workspaces__workspace_id__patch Patch Workspace

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

patch/api/v1/workspaces/{workspace\_id}

### Request samples

`{"display_name": "string"

### Response samples

## tag/workspaces/operation/delete_workspace_api_v1_workspaces__workspace_id__delete Delete Workspace

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/workspaces/{workspace\_id}

### Response samples

## tag/workspaces/operation/list_pending_workspace_invites_api_v1_workspaces_pending_get List Pending Workspace Invites

Get all workspaces visible to this auth

##### Authorizations:

### Responses

get/api/v1/workspaces/pending

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"display_name": "string",\
\
"is_personal": true,\
\
"is_deleted": true,\
\
"tenant_handle": "string"\
\
}\
\
]`

## tag/workspaces/operation/delete_pending_workspace_invite_api_v1_workspaces_pending__id__delete Delete Pending Workspace Invite

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/workspaces/pending/{id}

### Response samples

## tag/workspaces/operation/claim_pending_workspace_invite_api_v1_workspaces_pending__workspace_id__claim_post Claim Pending Workspace Invite Deprecated

##### Authorizations:

##### path Parameters

### Responses

post/api/v1/workspaces/pending/{workspace\_id}/claim

### Response samples

## tag/workspaces/operation/get_current_workspace_stats_api_v1_workspaces_current_stats_get Get Current Workspace Stats

##### Authorizations:

##### query Parameters

| | |
| --- | --- |
| tag\_value\_id | Array of Tag Value Id (strings) or Tag Value Id (null) (Tag Value Id) |

### Responses

get/api/v1/workspaces/current/stats

### Response samples

"dataset_count": 0,

"tracer_session_count": 0,

"repo_count": 0,

"annotation_queue_count": 0,

"deployment_count": 0,

"dashboards_count": 0

## tag/workspaces/operation/get_current_workspace_members_api_v1_workspaces_current_members_get Get Current Workspace Members

##### Authorizations:

### Responses

get/api/v1/workspaces/current/members

### Response samples

"members": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"ls_user_id": "e368397c-ab29-448f-907e-f04754df6859",\
\
"read_only": true,\
\
"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",\
\
"role_name": "string",\
\
"access_scope": "organization",\
\
"email": "string",\
\
"full_name": "string",\
\
"avatar_url": "string",\
\
"linked_login_methods": [ ],\
\
"display_name": "string",\
\
"is_disabled": false,\
\
"org_role_id": "ddedde81-fced-474e-bafa-6314571c3554",\
\
"org_role_name": "string"\
\
}\
\
],

"pending": [{"email": "string",\
\
"read_only": false,\
\
"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",\
\
"workspace_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"workspace_role_id": "a57b9738-8b78-46be-b756-c7376ec37998",\
\
"password": "string",\
\
"full_name": "string",\
\
"access_scope": "organization",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"role_name": "string",\
\
"org_role_id": "ddedde81-fced-474e-bafa-6314571c3554",\
\
"org_role_name": "string"\
\
}\
\
]

## tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post Add Member To Current Workspace

Add an existing organization member to the current workspace.

##### Authorizations:

##### Request Body schema: application/json required

| org\_identity\_id | Org Identity Id (string) or Org Identity Id (null) (Org Identity Id) |
| ls\_user\_id | Ls User Id (string) or Ls User Id (null) (Ls User Id) |

| role\_id | Role Id (string) or Role Id (null) (Role Id) |

### Responses

post/api/v1/workspaces/current/members

### Request samples

`{"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",

"org_identity_id": "671f75b8-0e3c-4866-b9c9-da234874371c",

### Response samples

## tag/workspaces/operation/get_current_active_workspace_members_api_v1_workspaces_current_members_active_get Get Current Active Workspace Members

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/workspaces/current/members/active

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"user_id": "a169451c-8525-4352-b8ca-070dd449a1a5",\
\
"ls_user_id": "e368397c-ab29-448f-907e-f04754df6859",\
\
"read_only": true,\
\
"role_id": "ac4e70c8-d5be-48af-93eb-760f58fc91a9",\
\
"role_name": "string",\
\
"access_scope": "organization",\
\
"email": "string",\
\
"full_name": "string",\
\
"avatar_url": "string",\
\
"linked_login_methods": [ ],\
\
"display_name": "string",\
\
"is_disabled": false,\
\
"org_role_id": "ddedde81-fced-474e-bafa-6314571c3554",\
\
"org_role_name": "string"\
\
}\
\
]`

## tag/workspaces/operation/get_current_pending_workspace_members_api_v1_workspaces_current_members_pending_get Get Current Pending Workspace Members

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/workspaces/current/members/pending

### Response samples

## tag/workspaces/operation/add_members_to_current_workspace_batch_api_v1_workspaces_current_members_batch_post Add Members To Current Workspace Batch

Batch invite up to 500 users to the current workspace and organization.

##### Authorizations:

_API Key__Tenant ID__Bearer Auth__Organization ID_

##### Request Body schema: application/json required

### Responses

post/api/v1/workspaces/current/members/batch

### Request samples

### Response samples

## tag/workspaces/operation/get_shared_tokens_api_v1_workspaces_current_shared_get Get Shared Tokens

List all shared entities and their tokens by the workspace.

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/workspaces/current/shared

### Response samples

`{"entities": [{"type": "run",\
\
"share_token": "string",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"run_id": "dded282c-8ebd-44cf-8ba5-9a234973d1ec",\
\
"run_name": "string",\
\
"run_type": "string",\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",\
\
"session_name": "string"\
\
}\
\
]

## tag/workspaces/operation/bulk_unshare_entities_api_v1_workspaces_current_shared_delete Bulk Unshare Entities

Bulk unshare entities by share tokens for the workspace.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

delete/api/v1/workspaces/current/shared

### Request samples

`{"share_tokens": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]

### Response samples

## tag/workspaces/operation/delete_current_workspace_member_api_v1_workspaces_current_members__identity_id__delete Delete Current Workspace Member

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/workspaces/current/members/{identity\_id}

### Response samples

## tag/workspaces/operation/patch_current_workspace_member_api_v1_workspaces_current_members__identity_id__patch Patch Current Workspace Member

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

patch/api/v1/workspaces/current/members/{identity\_id}

### Request samples

`{"read_only": true,

### Response samples

## tag/workspaces/operation/delete_current_workspace_pending_member_api_v1_workspaces_current_members__identity_id__pending_delete Delete Current Workspace Pending Member

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/workspaces/current/members/{identity\_id}/pending

### Response samples

## tag/workspaces/operation/get_current_workspace_usage_limits_info_api_v1_workspaces_current_usage_limits_get Get Current Workspace Usage Limits Info

##### Authorizations:

### Responses

get/api/v1/workspaces/current/usage\_limits

### Response samples

`{"in_reject_set": true,

"usage_limit_type": "payload_size",

"tenant_limit": 0

## tag/workspaces/operation/list_current_workspace_secrets_api_v1_workspaces_current_secrets_get List Current Workspace Secrets

##### Authorizations:

### Responses

get/api/v1/workspaces/current/secrets

### Response samples

`[{"key": "string"\
\
}\
\
]`

## tag/workspaces/operation/upsert_current_workspace_secrets_api_v1_workspaces_current_secrets_post Upsert Current Workspace Secrets

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/workspaces/current/secrets

### Request samples

`[{"key": "string",\
\
"value": "string"\
\
}\
\
]`

### Response samples

## tag/workspaces/operation/get_current_workspace_encrypted_secrets_api_v1_workspaces_current_secrets_encrypted_get Get Current Workspace Encrypted Secrets

Get encrypted workspace secrets for use with Agent Builder and external services.

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/workspaces/current/secrets/encrypted

### Response samples

`{"encrypted_secrets": "string"

## tag/workspaces/operation/list_tag_keys_api_v1_workspaces_current_tag_keys_get List Tag Keys

##### Authorizations:

### Responses

get/api/v1/workspaces/current/tag-keys

### Response samples

`[{"key": "string",\
\
"description": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/workspaces/operation/create_tag_key_api_v1_workspaces_current_tag_keys_post Create Tag Key

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/workspaces/current/tag-keys

### Request samples

`{"key": "string",

### Response samples

## tag/workspaces/operation/update_tag_key_api_v1_workspaces_current_tag_keys__tag_key_id__patch Update Tag Key

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| key | Key (string) or Key (null) (Key) |
| description | Description (string) or Description (null) (Description) |

### Responses

patch/api/v1/workspaces/current/tag-keys/{tag\_key\_id}

### Request samples

### Response samples

## tag/workspaces/operation/get_tag_key_api_v1_workspaces_current_tag_keys__tag_key_id__get Get Tag Key

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/workspaces/current/tag-keys/{tag\_key\_id}

### Response samples

## tag/workspaces/operation/delete_tag_key_api_v1_workspaces_current_tag_keys__tag_key_id__delete Delete Tag Key

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/workspaces/current/tag-keys/{tag\_key\_id}

### Response samples

## tag/workspaces/operation/create_tag_value_api_v1_workspaces_current_tag_keys__tag_key_id__tag_values_post Create Tag Value

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/workspaces/current/tag-keys/{tag\_key\_id}/tag-values

### Request samples

`{"value": "string",

### Response samples

"tag_key_id": "3dc0073f-ec89-4cf9-b2d6-fd87ae159962",

## tag/workspaces/operation/list_tag_values_api_v1_workspaces_current_tag_keys__tag_key_id__tag_values_get List Tag Values

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/workspaces/current/tag-keys/{tag\_key\_id}/tag-values

### Response samples

`[{"value": "string",\
\
"description": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tag_key_id": "3dc0073f-ec89-4cf9-b2d6-fd87ae159962",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/workspaces/operation/get_tag_value_api_v1_workspaces_current_tag_keys__tag_key_id__tag_values__tag_value_id__get Get Tag Value

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/workspaces/current/tag-keys/{tag\_key\_id}/tag-values/{tag\_value\_id}

### Response samples

## tag/workspaces/operation/update_tag_value_api_v1_workspaces_current_tag_keys__tag_key_id__tag_values__tag_value_id__patch Update Tag Value

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| value | Value (string) or Value (null) (Value) |
| description | Description (string) or Description (null) (Description) |

### Responses

patch/api/v1/workspaces/current/tag-keys/{tag\_key\_id}/tag-values/{tag\_value\_id}

### Request samples

### Response samples

## tag/workspaces/operation/delete_tag_value_api_v1_workspaces_current_tag_keys__tag_key_id__tag_values__tag_value_id__delete Delete Tag Value

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/workspaces/current/tag-keys/{tag\_key\_id}/tag-values/{tag\_value\_id}

### Response samples

## tag/workspaces/operation/create_tagging_api_v1_workspaces_current_taggings_post Create Tagging

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/workspaces/current/taggings

### Request samples

`{"tag_value_id": "8b2a1297-7a06-4ea0-a72d-8b92f428ef81",

"resource_type": "prompt",

"resource_id": "4d5215ed-38bb-48ed-879a-fdb9ca58522f"

### Response samples

"resource_id": "4d5215ed-38bb-48ed-879a-fdb9ca58522f",

## tag/workspaces/operation/list_taggings_api_v1_workspaces_current_taggings_get List Taggings

##### Authorizations:

##### query Parameters

| | |
| --- | --- |
| tag\_value\_id | Tag Value Id (string) or Tag Value Id (null) (Tag Value Id) |

### Responses

get/api/v1/workspaces/current/taggings

### Response samples

`[{"tag_key": "string",\
\
"tag_key_id": "3dc0073f-ec89-4cf9-b2d6-fd87ae159962",\
\
"tag_value": "string",\
\
"tag_value_id": "8b2a1297-7a06-4ea0-a72d-8b92f428ef81",\
\
"resources": {"prompts": [ ],\
\
"projects": [ ],\
\
"queues": [ ],\
\
"deployments": [ ],\
\
"experiments": [ ],\
\
"datasets": [ ],\
\
"dashboards": [ ]\
\
}\
\
}\
\
]`

## tag/workspaces/operation/delete_tagging_api_v1_workspaces_current_taggings__tagging_id__delete Delete Tagging

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/workspaces/current/taggings/{tagging\_id}

### Response samples

## tag/workspaces/operation/list_tags_api_v1_workspaces_current_tags_get List Tags

##### Authorizations:

### Responses

get/api/v1/workspaces/current/tags

### Response samples

`[{"key": "string",\
\
"description": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"values": [{"value": "string",\
\
"description": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tag_key_id": "3dc0073f-ec89-4cf9-b2d6-fd87ae159962",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z"\
\
}\
\
]\
\
}\
\
]`

## tag/workspaces/operation/list_tags_for_resource_api_v1_workspaces_current_tags_resource_get List Tags For Resource

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/workspaces/current/tags/resource

### Response samples

`[{"key": "string",\
\
"description": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"values": [{"value": "string",\
\
"description": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tag_key_id": "3dc0073f-ec89-4cf9-b2d6-fd87ae159962",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"taggings": [{"tag_value_id": "8b2a1297-7a06-4ea0-a72d-8b92f428ef81",\
\
"resource_type": "prompt",\
\
"resource_id": "4d5215ed-38bb-48ed-879a-fdb9ca58522f",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"created_at": "2019-08-24T14:15:22Z"\
\
}\
\
]\
\
}\
\
]\
\
}\
\
]`

## tag/workspaces/operation/list_tags_for_resources_api_v1_workspaces_current_tags_resources_post List Tags For Resources

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/workspaces/current/tags/resources

### Request samples

`[{"resource_id": "4d5215ed-38bb-48ed-879a-fdb9ca58522f",\
\
"resource_type": "prompt"\
\
}\
\
]`

### Response samples

`{"property1": [{"key": "string",\
\
"description": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"values": [{"value": "string",\
\
"description": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tag_key_id": "3dc0073f-ec89-4cf9-b2d6-fd87ae159962",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"taggings": [{"tag_value_id": "8b2a1297-7a06-4ea0-a72d-8b92f428ef81",\
\
"resource_type": "prompt",\
\
"resource_id": "4d5215ed-38bb-48ed-879a-fdb9ca58522f",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"created_at": "2019-08-24T14:15:22Z"\
\
}\
\
]\
\
}\
\
]\
\
}\
\
],

"property2": [{"key": "string",\
\
"description": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"values": [{"value": "string",\
\
"description": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tag_key_id": "3dc0073f-ec89-4cf9-b2d6-fd87ae159962",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"taggings": [{"tag_value_id": "8b2a1297-7a06-4ea0-a72d-8b92f428ef81",\
\
"resource_type": "prompt",\
\
"resource_id": "4d5215ed-38bb-48ed-879a-fdb9ca58522f",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"created_at": "2019-08-24T14:15:22Z"\
\
}\
\
]\
\
}\
\
]\
\
}\
\
]

## tag/playground-settings playground-settings

## tag/playground-settings/operation/list_playground_settings_api_v1_playground_settings_get List Playground Settings

Get all playground settings for this tenant id.

##### Authorizations:

### Responses

get/api/v1/playground-settings

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"settings": { },\
\
"options": {"requests_per_second": 0\
\
},\
\
"name": "string",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"description": "string"\
\
}\
\
]`

## tag/playground-settings/operation/create_playground_settings_api_v1_playground_settings_post Create Playground Settings

Create playground settings.

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| name | Name (string) or Name (null) (Name) |
| description | Description (string) or Description (null) (Description) |

| options | PlaygroundSavedOptions (object) or null |

### Responses

post/api/v1/playground-settings

### Request samples

"settings": { },

"options": {"requests_per_second": 0

### Response samples

## tag/playground-settings/operation/update_playground_settings_api_v1_playground_settings__playground_settings_id__patch Update Playground Settings

Update playground settings.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| name | Name (string) or Name (null) (Name) |
| description | Description (string) or Description (null) (Description) |
| settings | Settings (object) or Settings (null) (Settings) |
| options | PlaygroundSavedOptions (object) or null |

### Responses

patch/api/v1/playground-settings/{playground\_settings\_id}

### Request samples

### Response samples

## tag/playground-settings/operation/delete_playground_settings_api_v1_playground_settings__playground_settings_id__delete Delete Playground Settings

Delete playground settings.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/playground-settings/{playground\_settings\_id}

### Response samples

## tag/me me

## tag/me/operation/get_onboarding_state_api_v1_me_onboarding_state_get Get Onboarding State

Get onboarding state for the current user.

##### Authorizations:

### Responses

get/api/v1/me/onboarding\_state

### Response samples

"tracing_completed_at": "2019-08-24T14:15:22Z",

"lgstudio_completed_at": "2019-08-24T14:15:22Z",

"playground_completed_at": "2019-08-24T14:15:22Z",

"evaluation_completed_at": "2019-08-24T14:15:22Z",

"success_viewed_at": "2019-08-24T14:15:22Z",

## tag/me/operation/create_onboarding_state_api_v1_me_onboarding_state_post Create Onboarding State

Initialize onboarding state for the current user.

##### Authorizations:

### Responses

post/api/v1/me/onboarding\_state

### Response samples

## tag/me/operation/update_onboarding_state_field_api_v1_me_onboarding_state__field__put Update Onboarding State Field

Update a specific onboarding completion field for the current user.

Valid fields:

- tracing\_completed\_at
- lgstudio\_completed\_at
- playground\_completed\_at
- evaluation\_completed\_at
- success\_viewed\_at

##### Authorizations:

##### path Parameters

### Responses

put/api/v1/me/onboarding\_state/{field}

### Response samples

## tag/me/operation/get_ls_user_id_api_v1_me_ls_user_id_get Get Ls User Id

Get the LangSmith user ID for the current user.

##### Authorizations:

### Responses

get/api/v1/me/ls\_user\_id

### Response samples

`"string"`

## tag/service-accounts service-accounts

## tag/service-accounts/operation/get_service_accounts_api_v1_service_accounts_get Get Service Accounts

Get the current organization's service accounts.

##### Authorizations:

### Responses

get/api/v1/service-accounts

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"name": "string",\
\
"organization_id": "7c60d51f-b44e-4682-87d6-449835ea4de6",\
\
"default_workspace_id": "d3378579-d2d2-4013-844f-d2631508ee1f"\
\
}\
\
]`

## tag/service-accounts/operation/create_service_account_api_v1_service_accounts_post Create Service Account

Create a service account

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/service-accounts

### Request samples

"workspaces": [ ]

### Response samples

"default_workspace_id": "d3378579-d2d2-4013-844f-d2631508ee1f",

"organization_identity_id": "8c51c8f2-cb04-42f5-8de0-4bfeba7239d5"

## tag/service-accounts/operation/delete_service_account_api_v1_service_accounts__service_account_id__delete Delete Service Account

Delete a service account

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/service-accounts/{service\_account\_id}

### Response samples

## tag/charts charts

## tag/charts/operation/clone_section_api_v1_charts_section_clone_post Clone Section

Clone a dashboard.

##### Authorizations:

##### Request Body schema: application/json required

| | |
| --- | --- |
| section\_id | Section Id (string) or Section Id (null) (Section Id) |
| session\_id | Session Id (string) or Session Id (null) (Session Id) |

### Responses

post/api/v1/charts/section/clone

### Request samples

`{"section_id": "f5e5c4dc-7d2e-40f1-a895-9bb2f8703fd8",

### Response samples

"chart_count": 0,

## tag/charts/operation/read_sections_api_v1_charts_section_get Read Sections

Get all sections for the tenant.

##### Authorizations:

##### query Parameters

| title\_contains | Title Contains (string) or Title Contains (null) (Title Contains) |
| ids | Array of Ids (strings) or Ids (null) (Ids) |

### Responses

get/api/v1/charts/section

### Response samples

`[{"title": "string",\
\
"description": "string",\
\
"index": 0,\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"chart_count": 0,\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"modified_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/charts/operation/create_section_api_v1_charts_section_post Create Section

Create a new section.

##### Authorizations:

##### Request Body schema: application/json required

| description | Description (string) or Description (null) (Description) |
| index | Index (integer) or Index (null) (Index) |

### Responses

post/api/v1/charts/section

### Request samples

"index": 0

### Response samples

## tag/charts/operation/read_charts_api_v1_charts_post Read Charts

Get all charts for the tenant.

##### Authorizations:

##### Request Body schema: application/json required

| after\_index | After Index (integer) or After Index (null) (After Index) |
| tag\_value\_id | Array of Tag Value Id (strings) or Tag Value Id (null) (Tag Value Id) |

### Responses

post/api/v1/charts

### Request samples

"after_index": 0,

"tag_value_id": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]

### Response samples

`{"sections": [{"title": "string",\
\
"description": "string",\
\
"index": 0,\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",\
\
"charts": [{"data": [{"series_id": "string",\
\
"timestamp": "2019-08-24T14:15:22Z",\
\
"value": 0,\
\
"group": "string"\
\
}\
\
],\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"title": "string",\
\
"description": "string",\
\
"metadata": { },\
\
"index": 0,\
\
"chart_type": "line",\
\
"series": [{"name": "string",\
\
"filters": {"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
},\
\
"metric": "run_count",\
\
"feedback_key": "string",\
\
"workspace_id": "0967198e-ec7b-4c6b-b4d3-f71244cadbe9",\
\
"project_metric": "memory_usage",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"group_by": {"attribute": "name",\
\
"path": "string",\
\
"max_groups": 5,\
\
"set_by": "section"\
\
}\
\
}\
\
],\
\
"common_filters": {"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
}\
\
}\
\
],\
\
"sub_sections": [{"title": "string",\
\
"description": "string",\
\
"index": 0,\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"charts": [{"data": [{"series_id": "string",\
\
"timestamp": "2019-08-24T14:15:22Z",\
\
"value": 0,\
\
"group": "string"\
\
}\
\
],\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"title": "string",\
\
"description": "string",\
\
"metadata": { },\
\
"index": 0,\
\
"chart_type": "line",\
\
"series": [{"name": "string",\
\
"filters": {"filter": null,\
\
"trace_filter": null,\
\
"tree_filter": null,\
\
"session": null\
\
},\
\
"metric": "run_count",\
\
"feedback_key": "string",\
\
"workspace_id": "0967198e-ec7b-4c6b-b4d3-f71244cadbe9",\
\
"project_metric": "memory_usage",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"group_by": {"attribute": null,\
\
"path": null,\
\
"max_groups": null,\
\
"set_by": null\
\
}\
\
}\
\
],\
\
"common_filters": {"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
}\
\
}\
\
]\
\
}\
\
]\
\
}\
\
]

## tag/charts/operation/read_chart_preview_api_v1_charts_preview_post Read Chart Preview

Get a preview for a chart without actually creating it.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/charts/preview

### Request samples

`{"bucket_info": {"timezone": "UTC",

"omit_data": false

"chart": {"series": [{"name": "string",\
\
"filters": {"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
},\
\
"metric": "run_count",\
\
"feedback_key": "string",\
\
"workspace_id": "0967198e-ec7b-4c6b-b4d3-f71244cadbe9",\
\
"project_metric": "memory_usage",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"group_by": {"attribute": "name",\
\
"path": "string",\
\
"max_groups": 5,\
\
"set_by": "section"\
\
}\
\
}\
\
],

"common_filters": {"filter": "string",

"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]

### Response samples

`{"data": [{"series_id": "string",\
\
"timestamp": "2019-08-24T14:15:22Z",\
\
"value": 0,\
\
"group": "string"\
\
}\
\
]

## tag/charts/operation/create_chart_api_v1_charts_create_post Create Chart

Create a new chart.

##### Authorizations:

##### Request Body schema: application/json required

| section\_id | Section Id (string) or Section Id (null) (Section Id) |
| metadata | Metadata (object) or Metadata (null) (Metadata) |
| common\_filters | CustomChartSeriesFilters (object) or null |

### Responses

post/api/v1/charts/create

### Request samples

"index": 100,

"chart_type": "line",

"series": [{"name": "string",\
\
"filters": {"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
},\
\
"metric": "run_count",\
\
"feedback_key": "string",\
\
"workspace_id": "0967198e-ec7b-4c6b-b4d3-f71244cadbe9",\
\
"project_metric": "memory_usage",\
\
"group_by": {"attribute": "name",\
\
"path": "string",\
\
"max_groups": 5\
\
}\
\
}\
\
],

"section_id": "f5e5c4dc-7d2e-40f1-a895-9bb2f8703fd8",

### Response samples

"series": [{"name": "string",\
\
"filters": {"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
},\
\
"metric": "run_count",\
\
"feedback_key": "string",\
\
"workspace_id": "0967198e-ec7b-4c6b-b4d3-f71244cadbe9",\
\
"project_metric": "memory_usage",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"group_by": {"attribute": "name",\
\
"path": "string",\
\
"max_groups": 5,\
\
"set_by": "section"\
\
}\
\
}\
\
]

## tag/charts/operation/read_single_chart_api_v1_charts__chart_id__post Read Single Chart

Get a single chart by ID.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/charts/{chart\_id}

### Request samples

### Response samples

`{"data": [{"series_id": "string",\
\
"timestamp": "2019-08-24T14:15:22Z",\
\
"value": 0,\
\
"group": "string"\
\
}\
\
],

"series": [{"name": "string",\
\
"filters": {"filter": "string",\
\
"trace_filter": "string",\
\
"tree_filter": "string",\
\
"session": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
]\
\
},\
\
"metric": "run_count",\
\
"feedback_key": "string",\
\
"workspace_id": "0967198e-ec7b-4c6b-b4d3-f71244cadbe9",\
\
"project_metric": "memory_usage",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"group_by": {"attribute": "name",\
\
"path": "string",\
\
"max_groups": 5,\
\
"set_by": "section"\
\
}\
\
}\
\
],

## tag/charts/operation/update_chart_api_v1_charts__chart_id__patch Update Chart

Update a chart.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

patch/api/v1/charts/{chart\_id}

### Request samples

`{"title": {"__missing__": "__missing__"

"index": {"__missing__": "__missing__"

"chart_type": {"__missing__": "__missing__"

"series": {"__missing__": "__missing__"

"section_id": {"__missing__": "__missing__"

"common_filters": {"__missing__": "__missing__"

### Response samples

## tag/charts/operation/delete_chart_api_v1_charts__chart_id__delete Delete Chart

Delete a chart.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/charts/{chart\_id}

### Response samples

## tag/charts/operation/read_single_section_api_v1_charts_section__section_id__post Read Single Section

Get a single section by ID.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/charts/section/{section\_id}

### Request samples

### Response samples

## tag/charts/operation/update_section_api_v1_charts_section__section_id__patch Update Section

Update a section.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

patch/api/v1/charts/section/{section\_id}

### Request samples

### Response samples

## tag/charts/operation/delete_section_api_v1_charts_section__section_id__delete Delete Section

Delete a section.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/charts/section/{section\_id}

### Response samples

## tag/charts/operation/org_read_sections_api_v1_org_charts_section_get Org Read Sections

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/org-charts/section

### Response samples

## tag/charts/operation/org_create_section_api_v1_org_charts_section_post Org Create Section

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/org-charts/section

### Request samples

### Response samples

## tag/charts/operation/org_read_charts_api_v1_org_charts_post Org Read Charts

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/org-charts

### Request samples

### Response samples

## tag/charts/operation/org_read_chart_preview_api_v1_org_charts_preview_post Org Read Chart Preview

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/org-charts/preview

### Request samples

### Response samples

## tag/charts/operation/org_create_chart_api_v1_org_charts_create_post Org Create Chart

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/org-charts/create

### Request samples

### Response samples

## tag/charts/operation/org_read_single_chart_api_v1_org_charts__chart_id__post Org Read Single Chart

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/org-charts/{chart\_id}

### Request samples

### Response samples

## tag/charts/operation/org_update_chart_api_v1_org_charts__chart_id__patch Org Update Chart

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

patch/api/v1/org-charts/{chart\_id}

### Request samples

### Response samples

## tag/charts/operation/org_delete_chart_api_v1_org_charts__chart_id__delete Org Delete Chart

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/org-charts/{chart\_id}

### Response samples

## tag/charts/operation/org_read_single_section_api_v1_org_charts_section__section_id__post Org Read Single Section

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/org-charts/section/{section\_id}

### Request samples

### Response samples

## tag/charts/operation/org_update_section_api_v1_org_charts_section__section_id__patch Org Update Section

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

patch/api/v1/org-charts/section/{section\_id}

### Request samples

### Response samples

## tag/charts/operation/org_delete_section_api_v1_org_charts_section__section_id__delete Org Delete Section

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/org-charts/section/{section\_id}

### Response samples

## tag/mcp mcp

## tag/mcp/operation/proxy_get_api_v1_mcp_proxy_get Proxy Get

##### Authorizations:

##### query Parameters

### Responses

get/api/v1/mcp/proxy

### Response samples

## tag/mcp/operation/proxy_api_v1_mcp_proxy_post Proxy

##### Authorizations:

##### Request Body schema: application/json required

| body | Body (any) or Body (null) (Body) |

### Responses

post/api/v1/mcp/proxy

### Request samples

`{"url": "string",

"method": "GET",

"timeout": 120,

"body": { }

### Response samples

## operation/ok_api_v1_ok_get Ok

### Responses

get/api/v1/ok

### Response samples

## tag/repos repos

## tag/repos/operation/list_repos_api_v1_repos_get List Repos

Get all repos.

##### Authorizations:

##### query Parameters

| tenant\_handle | Tenant Handle (string) or Tenant Handle (null) (Tenant Handle) |
| tenant\_id | Tenant Id (string) or Tenant Id (null) (Tenant Id) |
| query | Query (string) or Query (null) (Query) |
| has\_commits | Has Commits (boolean) or Has Commits (null) (Has Commits) |
| tags | Array of Tags (strings) or Tags (null) (Tags) |
| is\_archived | Is Archived (string) or Is Archived (null) (Is Archived) |
| is\_public | TrueFalseLiteral (string) or Is Public (null) (Is Public) |
| upstream\_repo\_owner | Upstream Repo Owner (string) or Upstream Repo Owner (null) (Upstream Repo Owner) |
| upstream\_repo\_handle | Upstream Repo Handle (string) or Upstream Repo Handle (null) (Upstream Repo Handle) |
| tag\_value\_id | Array of Tag Value Id (strings) or Tag Value Id (null) (Tag Value Id) |
| sort\_field | Sort Field (string) or Sort Field (null) (Sort Field) |
| sort\_direction | "asc" (string) or "desc" (string) or Sort Direction (null) (Sort Direction) |

### Responses

get/api/v1/repos

### Response samples

`{"repos": [{"repo_handle": "string",\
\
"description": "string",\
\
"readme": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"is_public": true,\
\
"is_archived": true,\
\
"tags": ["string"\
\
],\
\
"original_repo_id": "6109e386-7a89-4f9c-aca6-45f908b791d8",\
\
"upstream_repo_id": "d9a0848f-cf61-4b25-97e5-f0bdab70250b",\
\
"owner": "string",\
\
"full_name": "string",\
\
"num_likes": 0,\
\
"num_downloads": 0,\
\
"num_views": 0,\
\
"liked_by_auth_user": true,\
\
"last_commit_hash": "string",\
\
"num_commits": 0,\
\
"created_by": "string",\
\
"original_repo_full_name": "string",\
\
"upstream_repo_full_name": "string",\
\
"latest_commit_manifest": {"commit_hash": "string",\
\
"manifest": { },\
\
"examples": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"inputs": { },\
\
"outputs": { },\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82"\
\
}\
\
]\
\
}\
\
}\
\
],

"total": 0

## tag/repos/operation/create_repo_api_v1_repos_post Create Repo

Create a repo.

##### Authorizations:

##### Request Body schema: application/json required

| description | Description (string) or Description (null) (Description) |
| readme | Readme (string) or Readme (null) (Readme) |

| tags | Array of Tags (strings) or Tags (null) (Tags) |

### Responses

post/api/v1/repos

### Request samples

`{"repo_handle": "string",

"readme": "string",

"is_public": true,

"tags": ["string"\
\
]

### Response samples

`{"repo": {"repo_handle": "string",

"is_archived": true,

"original_repo_id": "6109e386-7a89-4f9c-aca6-45f908b791d8",

"upstream_repo_id": "d9a0848f-cf61-4b25-97e5-f0bdab70250b",

"num_likes": 0,

"num_downloads": 0,

"num_views": 0,

"liked_by_auth_user": true,

"last_commit_hash": "string",

"num_commits": 0,

"original_repo_full_name": "string",

"upstream_repo_full_name": "string",

"latest_commit_manifest": {"commit_hash": "string",

"examples": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"start_time": "2019-08-24T14:15:22Z",\
\
"inputs": { },\
\
"outputs": { },\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82"\
\
}\
\
]

## tag/repos/operation/get_repo_api_v1_repos__owner___repo__get Get Repo

Get a repo.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/repos/{owner}/{repo}

### Response samples

## tag/repos/operation/update_repo_api_v1_repos__owner___repo__patch Update Repo

Update a repo.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| description | Description (string) or Description (null) (Description) |
| readme | Readme (string) or Readme (null) (Readme) |
| tags | Array of Tags (strings) or Tags (null) (Tags) |
| is\_public | Is Public (boolean) or Is Public (null) (Is Public) |
| is\_archived | Is Archived (boolean) or Is Archived (null) (Is Archived) |

### Responses

patch/api/v1/repos/{owner}/{repo}

### Request samples

"is_archived": true

### Response samples

## tag/repos/operation/delete_repo_api_v1_repos__owner___repo__delete Delete Repo

Delete a repo.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/repos/{owner}/{repo}

### Response samples

## tag/repos/operation/fork_repo_api_v1_repos__owner___repo__fork_post Fork Repo

Fork a repo.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| readme | Readme (string) or Readme (null) (Readme) |
| description | Description (string) or Description (null) (Description) |
| tags | Array of Tags (strings) or Tags (null) (Tags) |
| is\_public | Is Public (boolean) or Is Public (null) (Is Public) |

### Responses

post/api/v1/repos/{owner}/{repo}/fork

### Request samples

"is_public": true

### Response samples

## tag/repos/operation/list_repo_tags_api_v1_repos_tags_get List Repo Tags

Get all repo tags.

##### Authorizations:

##### query Parameters

| tenant\_handle | Tenant Handle (string) or Tenant Handle (null) (Tenant Handle) |
| tenant\_id | Tenant Id (string) or Tenant Id (null) (Tenant Id) |
| query | Query (string) or Query (null) (Query) |
| has\_commits | Has Commits (boolean) or Has Commits (null) (Has Commits) |
| tags | Array of Tags (strings) or Tags (null) (Tags) |
| is\_archived | Is Archived (string) or Is Archived (null) (Is Archived) |
| is\_public | TrueFalseLiteral (string) or Is Public (null) (Is Public) |
| upstream\_repo\_owner | Upstream Repo Owner (string) or Upstream Repo Owner (null) (Upstream Repo Owner) |
| upstream\_repo\_handle | Upstream Repo Handle (string) or Upstream Repo Handle (null) (Upstream Repo Handle) |
| tag\_value\_id | Array of Tag Value Id (strings) or Tag Value Id (null) (Tag Value Id) |

### Responses

get/api/v1/repos/tags

### Response samples

`{"tags": [{"tag": "string",\
\
"count": 0\
\
}\
\
]

## tag/repos/operation/optimize_prompt_job_api_v1_repos_optimize_job_post Optimize Prompt Job

Optimize prompt

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/repos/optimize-job

### Request samples

`{"algorithm": "promptim",

"config": {"message_index": 0,

"task_description": "string",

"train_split": "string",

"dev_split": "string",

"test_split": "string",

"evaluators": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"num_epochs": 0,

"auto_commit": true

"prompt_name": "string"

### Response samples

`{"optimization_job_id": "59cba767-c328-4a82-bb9c-532e0d78d366"

## tag/likes likes

## tag/likes/operation/like_repo_api_v1_likes__owner___repo__post Like Repo

Like a repo.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/likes/{owner}/{repo}

### Request samples

`{"like": true

### Response samples

`{"likes": 0

## tag/settings settings

## tag/settings/operation/get_settings_api_v1_settings_get Get Settings

Get settings.

##### Authorizations:

### Responses

get/api/v1/settings

### Response samples

## tag/settings/operation/set_tenant_handle_api_v1_settings_handle_post Set Tenant Handle

Set tenant handle.

##### Authorizations:

##### Request Body schema: application/json required

### Responses

post/api/v1/settings/handle

### Request samples

`{"tenant_handle": "string"

### Response samples

## tag/comments comments

## tag/comments/operation/create_comment_api_v1_comments__owner___repo__post Create Comment

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/comments/{owner}/{repo}

### Request samples

`{"content": "string"

### Response samples

## tag/comments/operation/get_comments_api_v1_comments__owner___repo__get Get Comments

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/comments/{owner}/{repo}

### Response samples

`{"comments": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"comment_by": "487bedc4-4229-4ebc-9701-67a6e05ee31e",\
\
"comment_on": "00dd208d-29cf-4f95-a07d-219836949e0f",\
\
"parent_id": "1c6ca187-e61f-4301-8dcb-0e9749e89eef",\
\
"content": "string",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z",\
\
"comment_by_name": "string",\
\
"num_sub_comments": 0,\
\
"num_likes": 0,\
\
"liked_by_auth_user": true\
\
}\
\
],

## tag/comments/operation/get_sub_comments_api_v1_comments__owner___repo___parent_comment_id__get Get Sub Comments

##### Authorizations:

##### path Parameters

##### query Parameters

### Responses

get/api/v1/comments/{owner}/{repo}/{parent\_comment\_id}

### Response samples

## tag/comments/operation/create_sub_comment_api_v1_comments__owner___repo___parent_comment_id__post Create Sub Comment

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/comments/{owner}/{repo}/{parent\_comment\_id}

### Request samples

### Response samples

"comment_by": "487bedc4-4229-4ebc-9701-67a6e05ee31e",

"comment_on": "00dd208d-29cf-4f95-a07d-219836949e0f",

"content": "string",

"comment_by_name": "string",

"num_sub_comments": 0,

"liked_by_auth_user": true

## tag/comments/operation/like_comment_api_v1_comments__owner___repo___parent_comment_id__like_post Like Comment

##### Authorizations:

##### path Parameters

### Responses

post/api/v1/comments/{owner}/{repo}/{parent\_comment\_id}/like

### Response samples

## tag/comments/operation/unlike_comment_api_v1_comments__owner___repo___parent_comment_id__like_delete Unlike Comment

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/comments/{owner}/{repo}/{parent\_comment\_id}/like

### Response samples

## tag/tags tags

## tag/tags/operation/get_tags_api_v1_repos__owner___repo__tags_get Get Tags

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/repos/{owner}/{repo}/tags

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"repo_id": "59e69ce7-60be-4408-9333-ec12132fc070",\
\
"commit_id": "7be25ca4-a873-457e-be69-1432462588dc",\
\
"commit_hash": "string",\
\
"tag_name": "string",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/tags/operation/create_tag_api_v1_repos__owner___repo__tags_post Create Tag

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/repos/{owner}/{repo}/tags

### Request samples

`{"tag_name": "string",

"commit_id": "7be25ca4-a873-457e-be69-1432462588dc",

"skip_webhooks": false

### Response samples

"repo_id": "59e69ce7-60be-4408-9333-ec12132fc070",

"tag_name": "string",

## tag/tags/operation/get_tag_api_v1_repos__owner___repo__tags__tag_name__get Get Tag

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/repos/{owner}/{repo}/tags/{tag\_name}

### Response samples

## tag/tags/operation/update_tag_api_v1_repos__owner___repo__tags__tag_name__patch Update Tag

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

patch/api/v1/repos/{owner}/{repo}/tags/{tag\_name}

### Request samples

`{"commit_id": "7be25ca4-a873-457e-be69-1432462588dc",

### Response samples

## tag/tags/operation/delete_tag_api_v1_repos__owner___repo__tags__tag_name__delete Delete Tag

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/repos/{owner}/{repo}/tags/{tag\_name}

### Response samples

## tag/optimization-jobs optimization-jobs

## tag/optimization-jobs/operation/list_jobs_api_v1_repos__owner___repo__optimization_jobs_get List Jobs

List all prompt optimization jobs.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/repos/{owner}/{repo}/optimization-jobs

### Response samples

`[{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"repo_id": "59e69ce7-60be-4408-9333-ec12132fc070",\
\
"status": "created",\
\
"tenant_id": "34f5c98e-f430-457b-a812-92637d0c6fd0",\
\
"algorithm": "promptim",\
\
"config": {"message_index": 0,\
\
"task_description": "string",\
\
"dataset_name": "string",\
\
"train_split": "string",\
\
"dev_split": "string",\
\
"test_split": "string",\
\
"evaluators": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"num_epochs": 0,\
\
"auto_commit": true\
\
},\
\
"results": [{"timestamp": "2019-08-24T14:15:22Z",\
\
"x": 0,\
\
"y": 0\
\
}\
\
],\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"updated_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/optimization-jobs/operation/create_job_api_v1_repos__owner___repo__optimization_jobs_post Create Job

Create a new prompt optimization job.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

### Responses

post/api/v1/repos/{owner}/{repo}/optimization-jobs

### Request samples

### Response samples

"status": "created",

"algorithm": "promptim",

"results": [{"timestamp": "2019-08-24T14:15:22Z",\
\
"x": 0,\
\
"y": 0\
\
}\
\
],

## tag/optimization-jobs/operation/get_job_api_v1_repos__owner___repo__optimization_jobs__job_id__get Get Job

Get a specific optimization job.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/repos/{owner}/{repo}/optimization-jobs/{job\_id}

### Response samples

"logs": [{"log_type": "info",\
\
"message": "string",\
\
"data": { },\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"job_id": "453bd7d7-5355-4d6d-a38e-d9e7eb218c3f",\
\
"created_at": "2019-08-24T14:15:22Z"\
\
}\
\
]

## tag/optimization-jobs/operation/update_job_api_v1_repos__owner___repo__optimization_jobs__job_id__patch Update Job

Replace an existing prompt optimization job with a new, modified job.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| status | EPromptOptimizationJobStatus (string) or null |
| result | PromptOptimizationResult (object) or null |

### Responses

patch/api/v1/repos/{owner}/{repo}/optimization-jobs/{job\_id}

### Request samples

`{"status": "created",

"result": {"timestamp": "2019-08-24T14:15:22Z",

"x": 0,

"y": 0

### Response samples

## tag/optimization-jobs/operation/delete_job_api_v1_repos__owner___repo__optimization_jobs__job_id__delete Delete Job

Delete a prompt optimization job.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/repos/{owner}/{repo}/optimization-jobs/{job\_id}

### Response samples

## tag/optimization-jobs/operation/list_job_logs_api_v1_repos__owner___repo__optimization_jobs__job_id__logs_get List Job Logs

List all logs for a specific prompt optimization job.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/repos/{owner}/{repo}/optimization-jobs/{job\_id}/logs

### Response samples

`[{"log_type": "info",\
\
"message": "string",\
\
"data": { },\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"job_id": "453bd7d7-5355-4d6d-a38e-d9e7eb218c3f",\
\
"created_at": "2019-08-24T14:15:22Z"\
\
}\
\
]`

## tag/optimization-jobs/operation/create_log_api_v1_repos__owner___repo__optimization_jobs__job_id__logs_post Create Log

Create a new log entry for a prompt optimization job.

##### Authorizations:

##### path Parameters

##### Request Body schema: application/json required

| data | Data (object) or Data (null) (Data) |

### Responses

post/api/v1/repos/{owner}/{repo}/optimization-jobs/{job\_id}/logs

### Request samples

`{"log_type": "info",

"message": "string",

"data": { }

### Response samples

"data": { },

"job_id": "453bd7d7-5355-4d6d-a38e-d9e7eb218c3f",

## tag/optimization-jobs/operation/get_log_api_v1_repos__owner___repo__optimization_jobs__job_id__logs__log_id__get Get Log

Get a specific prompt optimization job log.

##### Authorizations:

##### path Parameters

### Responses

get/api/v1/repos/{owner}/{repo}/optimization-jobs/{job\_id}/logs/{log\_id}

### Response samples

## tag/optimization-jobs/operation/delete_log_api_v1_repos__owner___repo__optimization_jobs__job_id__logs__log_id__delete Delete Log

Delete a prompt optimization job log.

##### Authorizations:

##### path Parameters

### Responses

delete/api/v1/repos/{owner}/{repo}/optimization-jobs/{job\_id}/logs/{log\_id}

### Response samples

## tag/commits commits

## tag/commits/paths/~1commits~1{owner}~1{repo}/get List commits

Lists all commits for a repository with pagination support.
This endpoint supports both authenticated and unauthenticated access.
Authenticated users can access private repos, while unauthenticated users can only access public repos.
The include\_stats parameter controls whether download and view statistics are computed (defaults to true).

##### path Parameters

##### query Parameters

### Responses

**500**

Internal Server Error

get/commits/{owner}/{repo}

### Response samples

- 200
- 400
- 404
- 500

`{"commits": [{"commit_hash": "string",\
\
"created_at": "2019-08-24T14:15:22Z",\
\
"example_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],\
\
"full_name": "string",\
\
"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"manifest": { },\
\
"manifest_sha": [0\
\
],\
\
"num_downloads": 0,\
\
"num_views": 0,\
\
"parent_commit_hash": "string",\
\
"parent_id": "1c6ca187-e61f-4301-8dcb-0e9749e89eef",\
\
"repo_id": "59e69ce7-60be-4408-9333-ec12132fc070",\
\
"updated_at": "2019-08-24T14:15:22Z"\
\
}\
\
],

## tag/commits/paths/~1commits~1{owner}~1{repo}/post Create a commit

Creates a new commit in a repository.
Requires authentication and write access to the repository.

##### path Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| manifest | object |
| parent\_commit | string |

### Responses

post/commits/{owner}/{repo}

### Request samples

`{"manifest": { },

"parent_commit": "string",

"skip_webhooks": null

### Response samples

- 201
- 400
- 403
- 404
- 500

`{"commit": {"commit_hash": "string",

"example_run_ids": ["497f6eca-6276-4993-bfeb-53cbbbba6f08"\
\
],

"manifest_sha": [0\
\
],

"parent_commit_hash": "string",

## tag/commits/paths/~1commits~1{owner}~1{repo}~1{commit}/get Get a commit

Retrieves a specific commit by hash, tag, or "latest" for a repository.
This endpoint supports both authenticated and unauthenticated access.
Authenticated users can access private repos, while unauthenticated users can only access public repos.
Commit resolution logic:

- "latest" or empty: Get the most recent commit
- Less than 8 characters: Only check for tags
- 8 or more characters: Prioritize commit hash over tag, check both

##### path Parameters

##### query Parameters

### Responses

get/commits/{owner}/{repo}/{commit}

### Response samples

`{"commit_hash": "string",

"examples": [{"id": "497f6eca-6276-4993-bfeb-53cbbbba6f08",\
\
"inputs": { },\
\
"outputs": { },\
\
"session_id": "1ffd059c-17ea-40a8-8aef-70fd0307db82",\
\
"start_time": "string"\
\
}\
\
],

"manifest": { }

## tag/experiment-view-overrides experiment-view-overrides

## tag/experiment-view-overrides/paths/~1datasets~1{dataset_id}~1experiment-view-overrides/get Get experiment view override configurations for a dataset

Retrieves all experiment view override configurations for a specific dataset.
This endpoint returns column display overrides including color gradients,
precision settings, and column visibility configurations that customize how
experiment results are displayed in the UI.

The response includes all column overrides with their display settings:

- Column identifiers (must start with inputs, outputs, reference\_outputs, feedback, metrics, attachments, or metadata)
- Color gradients for numeric data visualization
- Precision settings for numeric columns (1-6 decimal places)
- Hide flags to control column visibility

##### path Parameters

### Responses

Successfully retrieved experiment view override configurations

Invalid dataset ID format" example({"error":"invalid dataset ID format"})

**401**

Unauthorized access" example({"error":"Unauthorized"})

Dataset not found or not accessible" example({"error":"dataset not found or not accessible"})

Internal server error" example({"error":"internal server error"})

get/datasets/{dataset\_id}/experiment-view-overrides

### Response samples

- 200
- 400
- 401
- 404
- 500

`[{"column_overrides": [{"color_gradient": [[null\
\
]\
\
],\
\
"color_map": { },\
\
"column": "string",\
\
"disable_colors": true,\
\
"hide": true,\
\
"precision": 1\
\
}\
\
],\
\
"created_at": "string",\
\
"dataset_id": "string",\
\
"id": "string",\
\
"modified_at": "string"\
\
}\
\
]`

## tag/experiment-view-overrides/paths/~1datasets~1{dataset_id}~1experiment-view-overrides/post Create new experiment view override configuration for a dataset

Creates a new experiment view override configuration for a dataset with column display settings.
This endpoint allows you to customize how experiment results are displayed by configuring
column-specific overrides including colors, precision, and visibility.

The request must include a 'column\_overrides' array with at least one override configuration.
Each column override can specify:

- column: Required field name (must start with inputs, outputs, reference\_outputs, feedback, metrics, attachments, or metadata)
- color\_gradient: Optional array of \[number, color\] tuples for numeric data visualization
- precision: Optional number (1-6) for decimal places in numeric columns
- hide: Optional boolean to control column visibility

Example request body:
{
"column\_overrides": \[\
{\
"column": "outputs.accuracy",\
"color\_gradient": \[\[0.0, "#ff0000"\], \[0.5, "#ffff00"\], \[1.0, "#00ff00"\]\],\
"precision": 3\
},\
{\
"column": "inputs.model\_type",\
"hide": false\
}\
\]
}

This operation fails if an override already exists for the dataset (use PATCH to update).

##### path Parameters

##### Request Body schema: application/json required

### Responses

Successfully created experiment view override

Invalid request data" example({"error":"column\_overrides field is required"})

Dataset not found" example({"error":"dataset not found or not accessible"})

Override already exists" example({"error":"experiment view override already exists"})

Validation error" example({"error":"column name at index 0 must start with one of: inputs, outputs, reference\_outputs, feedback, metrics, attachments, metadata"})

post/datasets/{dataset\_id}/experiment-view-overrides

### Request samples

`{"column_overrides": [{"color_gradient": [[null\
\
]\
\
],\
\
"color_map": { },\
\
"column": "string",\
\
"disable_colors": true,\
\
"hide": true,\
\
"precision": 1\
\
}\
\
]

### Response samples

- 201
- 400
- 401
- 404
- 409
- 422
- 500

`{"column_overrides": [{"color_gradient": [[null\
\
]\
\
],\
\
"color_map": { },\
\
"column": "string",\
\
"disable_colors": true,\
\
"hide": true,\
\
"precision": 1\
\
}\
\
],

"dataset_id": "string",

"id": "string",

"modified_at": "string"

## tag/experiment-view-overrides/paths/~1datasets~1{dataset_id}~1experiment-view-overrides~1{id}/get Get experiment view override configuration by specific ID

Retrieves a specific experiment view override configuration using both dataset ID and override ID.
This endpoint provides more precise access to experiment view overrides when you have
the specific override ID, useful for direct links or cached references.

The response includes the same column override information as the dataset-level endpoint:

- Column identifiers with validation prefixes
- Color gradient settings for numeric data visualization
- Numeric precision configurations
- Column visibility controls

Both the dataset and override must exist and be accessible by the authenticated user.

##### path Parameters

### Responses

Successfully retrieved experiment view override configuration

Invalid ID format" example({"error":"invalid experiment view override ID format"})

Override not found" example({"error":"experiment view override not found"})

get/datasets/{dataset\_id}/experiment-view-overrides/{id}

### Response samples

## tag/experiment-view-overrides/paths/~1datasets~1{dataset_id}~1experiment-view-overrides~1{id}/delete Delete experiment view override configuration

Permanently deletes an experiment view override configuration for a dataset.
This operation removes all column override settings including color gradients,
precision configurations, and visibility settings.

After deletion, the experiment view will revert to default column display settings.
This action cannot be undone - you will need to recreate the override configuration
if you want to restore custom column settings.

Both the dataset and override must exist and be accessible by the authenticated user.
The operation will fail if the override doesn't exist or if the user doesn't have
appropriate permissions for the dataset.

##### path Parameters

### Responses

Successfully deleted experiment view override (no content returned)

delete/datasets/{dataset\_id}/experiment-view-overrides/{id}

### Response samples

- 400
- 401
- 404
- 500

## tag/experiment-view-overrides/paths/~1datasets~1{dataset_id}~1experiment-view-overrides~1{id}/patch Update existing experiment view override configuration

Updates an existing experiment view override configuration by completely replacing
the column overrides for the specified dataset and override ID.

This endpoint performs a complete replacement of the column overrides configuration.
All existing column overrides will be replaced with the new configuration provided
in the request body. To add or modify individual columns, include the complete
desired configuration in the request.

The request format is identical to the create endpoint:

- column\_overrides: Required array with at least one override configuration
- Each override can specify color gradients, precision, and visibility

Example request body:
{
"column\_overrides": \[\
{\
"column": "metrics.f1\_score",\
"color\_gradient": \[\[0.0, "#ff4444"\], \[0.8, "#44ff44"\]\],\
"precision": 4\
},\
{\
"column": "feedback.rating",\
"hide": false\
}\
\]
}

##### path Parameters

##### Request Body schema: application/json required

### Responses

Successfully updated experiment view override

Invalid request data" example({"error":"invalid experiment view override ID format"})

Validation error" example({"error":"'precision' must be between 1 and 6 for column at index 0"})

patch/datasets/{dataset\_id}/experiment-view-overrides/{id}

### Request samples

### Response samples

- 200
- 400
- 401
- 404
- 422
- 500

## tag/runs runs

## tag/runs/paths/~1runs/post Create a Run

Queues a single run for ingestion. The request body must be a JSON-encoded run object that follows the Run schema.

##### Request Body schema: application/json required

| | |
| --- | --- |
| dotted\_order | string |
| end\_time | string |
| error | string |
| events | Array of objects |
| extra | object |
| id | string |
| input\_attachments | object |
| inputs | object |
| name | string |
| output\_attachments | object |
| outputs | object |
| parent\_run\_id | string |
| reference\_example\_id | string |

| serialized | object |
| session\_id | string |
| session\_name | string |
| start\_time | string |
| status | string |
| tags | Array of strings |
| trace\_id | string |

### Responses

Run created

post/runs

### Request samples

`{"dotted_order": "string",

"end_time": "string",

"input_attachments": { },

"output_attachments": { },

"parent_run_id": "string",

"reference_example_id": "string",

"session_id": "string",

"start_time": "string",

"trace_id": "string"

### Response samples

## tag/runs/paths/~1runs~1batch/post Ingest Runs (Batch JSON)

Ingests a batch of runs in a single JSON payload. The payload must have `post` and/or `patch` arrays containing run objects.
Prefer this endpoint over single‑run ingestion when submitting hundreds of runs, but `/runs/multipart` offers better handling for very large fields and attachments.

##### Request Body schema: application/json required

| | |
| --- | --- |
| patch | Array of objects (runs.Run) |
| post | Array of objects (runs.Run) |

### Responses

Runs batch ingested

post/runs/batch

### Request samples

`{"patch": [{"dotted_order": "string",\
\
"end_time": "string",\
\
"error": "string",\
\
"events": [{ }\
\
],\
\
"extra": { },\
\
"id": "string",\
\
"input_attachments": { },\
\
"inputs": { },\
\
"name": "string",\
\
"output_attachments": { },\
\
"outputs": { },\
\
"parent_run_id": "string",\
\
"reference_example_id": "string",\
\
"run_type": "tool",\
\
"serialized": { },\
\
"session_id": "string",\
\
"session_name": "string",\
\
"start_time": "string",\
\
"status": "string",\
\
"tags": ["string"\
\
],\
\
"trace_id": "string"\
\
}\
\
],

"post": [{"dotted_order": "string",\
\
"end_time": "string",\
\
"error": "string",\
\
"events": [{ }\
\
],\
\
"extra": { },\
\
"id": "string",\
\
"input_attachments": { },\
\
"inputs": { },\
\
"name": "string",\
\
"output_attachments": { },\
\
"outputs": { },\
\
"parent_run_id": "string",\
\
"reference_example_id": "string",\
\
"run_type": "tool",\
\
"serialized": { },\
\
"session_id": "string",\
\
"session_name": "string",\
\
"start_time": "string",\
\
"status": "string",\
\
"tags": ["string"\
\
],\
\
"trace_id": "string"\
\
}\
\
]

### Response samples

## tag/runs/paths/~1runs~1multipart/post Ingest Runs (Multipart)

Ingests multiple runs, feedback objects, and binary attachments in a single `multipart/form-data` request.

**Headers**: every part must set `Content-Type` **and** either a `Content-Length` header or `length` parameter. Per‑part `Content-Encoding` is **not** allowed; the top‑level request may be `Content-Encoding: gzip` or `Content-Encoding: zstd`.
**Best performance** for high‑volume ingestion.

##### Request Body schema: multipart/form-data required

### Responses

Accepted

post/runs/multipart

### Response samples

## tag/runs/paths/~1runs~1{run_id}/patch Update a Run

Updates a run identified by its ID. The body should contain only the fields to be changed; unknown fields are ignored.

##### path Parameters

##### Request Body schema: application/json required

### Responses

Run updated

patch/runs/{run\_id}

### Request samples

### Response samples

- 202
- 400
- 403
- 404
- 409
- 422
- 429

## tag/alert_rules alert\_rules

## tag/alert_rules/paths/~1v1~1platform~1alerts~1{session_id}/post Create an alert rule

Creates a new alert rule. The request body must be a JSON-encoded alert rule object that follows the CreateAlertRuleRequest schema.

##### path Parameters

##### header Parameters

##### Request Body schema: application/json required

### Responses

Alert rule created

Bad request

Alert Rule Limit Reached

Internal server error

**503**

Service unavailable

post/v1/platform/alerts/{session\_id}

### Request samples

`{"actions": [{"alert_rule_id": "string",\
\
"config": { },\
\
"id": "string",\
\
"target": "pagerduty"\
\
}\
\
],

"rule": {"aggregation": "avg",

"attribute": "latency",

"denominator_filter": "string",

"operator": "gte",

"threshold": 0,

"threshold_multiplier": 0,

"threshold_window_minutes": 60,

"type": "threshold",

"window_minutes": 15

### Response samples

- 201
- 400
- 403
- 429
- 500
- 503

`{"actions": [{"alert_rule_id": "string",\
\
"config": { },\
\
"created_at": "string",\
\
"id": "string",\
\
"target": "pagerduty",\
\
"updated_at": "string"\
\
}\
\
],

"updated_at": "string",

## tag/alert_rules/paths/~1v1~1platform~1alerts~1{session_id}~1test/post Test an alert action to determine if configuration is valid

Tests an alert action which will fire a notification to all configured recipients if the configuration is valid.

##### path Parameters

##### header Parameters

### Responses

Alert action fired successfully

post/v1/platform/alerts/{session\_id}/test

### Response samples

- 200
- 400
- 403
- 500
- 503

## tag/alert_rules/paths/~1v1~1platform~1alerts~1{session_id}~1{alert_rule_id}/get Get an alert rule

Gets an alert rule.

##### path Parameters

##### header Parameters

### Responses

Alert rule

Not found

get/v1/platform/alerts/{session\_id}/{alert\_rule\_id}

### Response samples

- 200
- 400
- 403
- 404
- 500
- 503

## tag/alert_rules/paths/~1v1~1platform~1alerts~1{session_id}~1{alert_rule_id}/delete Delete an alert rule

Deletes an alert rule

##### path Parameters

##### header Parameters

### Responses

Alert rule deleted

delete/v1/platform/alerts/{session\_id}/{alert\_rule\_id}

### Response samples

## tag/alert_rules/paths/~1v1~1platform~1alerts~1{session_id}~1{alert_rule_id}/patch Update an alert rule

Updates an alert rule.

##### path Parameters

##### header Parameters

##### Request Body schema: application/json required

### Responses

Alert rule updated

patch/v1/platform/alerts/{session\_id}/{alert\_rule\_id}

### Request samples

### Response samples

## tag/access_policies access\_policies

## tag/access_policies/paths/~1v1~1platform~1orgs~1current~1access-policies/get List access policies

Lists all access policies for the organization.

##### header Parameters

### Responses

List of access policies

get/v1/platform/orgs/current/access-policies

### Response samples

- 200
- 403
- 500
- 503

`{"access_policies": [{"condition_groups": [{"conditions": [{"attribute_key": "string",\
\
"attribute_name": "resource_tag_key",\
\
"attribute_value": "string",\
\
"operator": "equals"\
\
}\
\
],\
\
"permission": "annotation-queues:create",\
\
"resource_type": "string"\
\
}\
\
],\
\
"created_at": "string",\
\
"description": "string",\
\
"effect": "string",\
\
"id": "string",\
\
"name": "string",\
\
"role_ids": ["string"\
\
],\
\
"updated_at": "string"\
\
}\
\
]

## tag/access_policies/paths/~1v1~1platform~1orgs~1current~1access-policies/post Create an access policy

Creates a new access policy. The request body must be a JSON-encoded access policy object that follows the CreateAccessPolicyRequest schema.

##### header Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| payload | object (authz\_internal.CreateAccessPolicyPayload) |

### Responses

Access policy created

Role not found

post/v1/platform/orgs/current/access-policies

### Request samples

`{"payload": {"condition_groups": [{"conditions": [{"attribute_key": "string",\
\
"attribute_name": "resource_tag_key",\
\
"attribute_value": "string",\
\
"operator": "equals"\
\
}\
\
],\
\
"permission": "annotation-queues:create",\
\
"resource_type": "string"\
\
}\
\
],

"effect": "string",

"role_ids": ["string"\
\
]

### Response samples

- 201
- 400
- 403
- 404
- 500
- 503

`{"id": "string"

## tag/access_policies/paths/~1v1~1platform~1orgs~1current~1access-policies~1roles~1{role_id}~1access-policies/post Attach access policies to a role

Attaches one or more access policies to a specific role. The request body must contain an array of access policy IDs.

##### path Parameters

##### header Parameters

##### Request Body schema: application/json required

| | |
| --- | --- |
| payload | object (authz\_internal.AttachAccessPoliciesPayload) |

### Responses

Access policies attached successfully

post/v1/platform/orgs/current/access-policies/roles/{role\_id}/access-policies

### Request samples

`{"payload": {"access_policy_ids": ["string"\
\
]

### Response samples

- 400
- 403
- 404
- 500
- 503

`{"error": "Invalid request: missing required fields"

## tag/access_policies/paths/~1v1~1platform~1orgs~1current~1access-policies~1{access_policy_id}/get Get an access policy

Gets a specific access policy by ID.

##### path Parameters

##### header Parameters

### Responses

Access policy details

Access policy not found

get/v1/platform/orgs/current/access-policies/{access\_policy\_id}

### Response samples

`{"condition_groups": [{"conditions": [{"attribute_key": "string",\
\
"attribute_name": "resource_tag_key",\
\
"attribute_value": "string",\
\
"operator": "equals"\
\
}\
\
],\
\
"permission": "annotation-queues:create",\
\
"resource_type": "string"\
\
}\
\
],

"role_ids": ["string"\
\
],

"updated_at": "string"

## tag/access_policies/paths/~1v1~1platform~1orgs~1current~1access-policies~1{access_policy_id}/delete Delete an access policy

Deletes a specific access policy by ID.

##### path Parameters

##### header Parameters

### Responses

Access policy deleted successfully

delete/v1/platform/orgs/current/access-policies/{access\_policy\_id}

### Response samples

- 400
- 403
- 500
- 503

---

# https://docs.langchain.com/langsmith/server-api-ref

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Agent Server API

Agent Server API reference for LangSmith Deployment

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Overview
- Assistants

- Threads

- Thread Runs

- Crons (Plus tier)

- Stateless Runs

- Store

- A2A

- MCP

- System
- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

On this page

- Authentication

The Agent Server API reference is available within each deployment at the `/docs` endpoint (e.g. `http://localhost:8124/docs`).Browse the full API reference in the **Agent Server API** section in the sidebar, or see the endpoint groups below:

- Assistants \- Configured instances of a graph
- Threads \- Accumulated outputs of a group of runs
- Thread Runs \- Invocations of a graph/assistant on a thread
- Stateless Runs \- Invocations with no state persistence
- Crons \- Periodic runs on a schedule
- Store \- Persistent key-value store for long-term memory
- A2A \- Agent-to-Agent Protocol endpoints
- MCP \- Model Context Protocol endpoints
- System \- Health checks and server info

## ​ Authentication

For deployments to LangSmith, authentication is required. Pass the `X-Api-Key` header with each request to the Agent Server. The value of the header should be set to a valid LangSmith API key for the organization where the Agent Server is deployed.Example `curl` command:

Copy

curl --request POST \
--url \
--header 'Content-Type: application/json' \
--header 'X-Api-Key: LANGSMITH_API_KEY' \
--data '{
"metadata": {},
"limit": 10,
"offset": 0
}'

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

LangSmith API reference\\
\\
Previous Create Assistant\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/env-var)



---

# https://docs.langchain.com/langsmith/agent-server-changelog)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

404

# Page not found

We couldn’t find the page you were looking for.

Agent Server changelog Configure LangSmith Agent Server for scale Self-hosted LangSmith changelog

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/self-hosted-changelog)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

404

# Page not found

We couldn’t find the page you were looking for.

Self-hosted LangSmith changelog Self-hosted LangSmith Self-host LangSmith with Docker

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/release-versions)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith docs Beta LangSmith Collector-Proxy Self-hosted LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/smith-python-sdk)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Python SDK LangSmith reference LangSmith Deployment components

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/smith-js-ts-sdk)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith JS/TS SDK LangGraph JS/TS SDK Trace with the Vercel AI SDK (JS/TS only)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/langgraph-python-sdk)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith reference LangGraph Python SDK LangSmith Python SDK

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/langgraph-js-ts-sdk)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

404

# Page not found

We couldn’t find the page you were looking for.

LangGraph JS/TS SDK LangSmith JS/TS SDK LangSmith reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/smith-api-ref)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

404

# Page not found

We couldn’t find the page you were looking for.

Control plane API reference for LangSmith Deployment Agent Server API reference for LangSmith Deployment LangSmith API reference

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/server-api-ref)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- LangSmith Python SDK

- LangSmith JS/TS SDK

- LangGraph Python SDK

- LangGraph JS/TS SDK

- LangSmith API

##### LangSmith Deployment

- Agent Server API

- Control Plane API

- LangGraph CLI
- RemoteGraph
- Agent Server environment variables

##### Releases

- Agent Server changelog
- Self-hosted changelog
- Release versions

404

# Page not found

We couldn’t find the page you were looking for.

Agent Server API reference for LangSmith Deployment Control plane API reference for LangSmith Deployment LangSmith Deployment components

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/add-metadata-tags

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Data & privacy

Add metadata and tags to traces

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Add metadata and tags to traces
- Prevent logging of sensitive data in traces
- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

LangSmith supports sending arbitrary metadata and tags along with traces.Tags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace.Both are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID. For more information on tags and metadata, see the Concepts page. For information on how to query traces and runs by metadata and tags, see the Filter traces in the application page.

Python

TypeScript

Copy

import openai
import langsmith as ls
from langsmith.wrappers import wrap_openai

client = openai.Client()
messages = [\
{"role": "system", "content": "You are a helpful assistant."},\
{"role": "user", "content": "Hello!"}\
]

# You can set metadata & tags **statically** when decorating a function
# Use the @traceable decorator with tags and metadata
# Ensure that the LANGSMITH_TRACING environment variables are set for @traceable to work
@ls.traceable(
run_type="llm",
name="OpenAI Call Decorator",
tags=["my-tag"],
metadata={"my-key": "my-value"}
)
def call_openai(
messages: list[dict], model: str = "gpt-4o-mini"

# You can also dynamically set metadata on the parent run:
rt = ls.get_current_run_tree()
rt.metadata["some-conditional-key"] = "some-val"
rt.tags.extend(["another-tag"])
return client.chat.completions.create(
model=model,
messages=messages,
).choices[0].message.content

call_openai(
messages,
# To add at **invocation time**, when calling the function.
# via the langsmith_extra parameter
langsmith_extra={"tags": ["my-other-tag"], "metadata": {"my-other-key": "my-value"}}
)

# Alternatively, you can use the context manager
with ls.trace(
name="OpenAI Call Trace",
run_type="llm",
inputs={"messages": messages},
tags=["my-tag"],
metadata={"my-key": "my-value"},
) as rt:
chat_completion = client.chat.completions.create(
model="gpt-4o-mini",
messages=messages,
)
rt.metadata["some-conditional-key"] = "some-val"
rt.end(outputs={"output": chat_completion})

# You can use the same techniques with the wrapped client
patched_client = wrap_openai(
client, tracing_extra={"metadata": {"my-key": "my-value"}, "tags": ["a-tag"]}
)
chat_completion = patched_client.chat.completions.create(
model="gpt-4o-mini",
messages=messages,
langsmith_extra={
"tags": ["my-other-tag"],
"metadata": {"my-other-key": "my-value"},
},
)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Trace generator functions\\
\\
Previous Prevent logging of sensitive data in traces\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/upload-files-with-traces

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Manual instrumentation

Upload files with traces

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Custom instrumentation
- Trace with API
- Log LLM calls
- Log retriever traces
- Metadata parameters
- Upload files with traces
- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Python
- TypeScript

Before diving into this content, it would be helpful to read the following guides:

- Trace with LangSmith using the traceable decorator or wrapper

The following features are available in the following SDK versions:

LangSmith supports uploading binary files (such as images, audio, videos, PDFs, and CSVs) with your traces. This is particularly useful when working with LLM pipelines using multimodal inputs or outputs.In both the Python and TypeScript SDKs, attachments can be added to your traces by specifying the MIME type and binary content of each file. This guide explains how to define and trace attachments using the `Attachment` type in Python and `Uint8Array` / `ArrayBuffer` in TypeScript.

### ​ Python

In the Python SDK, you can use the `Attachment` type to add files to your traces. Each `Attachment` requires:

- `mime_type` (str): The MIME type of the file (e.g., `"image/png"`).
- `data` (bytes \| Path): The binary content of the file, or the file path.

You can also define an attachment with a tuple tuple of the form `(mime_type, data)` for convenience.Simply decorate a function with `@traceable` and include your `Attachment` instances as arguments. Note that to use the file path instead of the raw bytes, you need to set the `dangerously_allow_filesystem` flag to `True` in your traceable decorator.

Python

Copy

from langsmith import traceable
from langsmith.schemas import Attachment
from pathlib import Path
import os

# Must set dangerously_allow_filesystem to True if you want to use file paths
@traceable(dangerously_allow_filesystem=True)
def trace_with_attachments(
val: int,
text: str,
image: Attachment,
audio: Attachment,
video: Attachment,
pdf: Attachment,
csv: Attachment,
):
return f"Processed: {val}, {text}, {len(image.data)}, {len(audio.data)}, {len(video.data)}, {len(pdf.data), {len(csv.data)}}"

# Helper function to load files as bytes

with open(file_path, "rb") as f:
return f.read()

# Load files and create attachments
image_data = load_file("my_image.png")
audio_data = load_file("my_mp3.mp3")
video_data = load_file("my_video.mp4")
pdf_data = load_file("my_document.pdf")

image_attachment = Attachment(mime_type="image/png", data=image_data)
audio_attachment = Attachment(mime_type="audio/mpeg", data=audio_data)
video_attachment = Attachment(mime_type="video/mp4", data=video_data)
pdf_attachment = ("application/pdf", pdf_data) # Can just define as tuple of (mime_type, data)
csv_attachment = Attachment(mime_type="text/csv", data=Path(os.getcwd()) / "my_csv.csv")

# Define other parameters
val = 42
text = "Hello, world!"

# Call the function with traced attachments
result = trace_with_attachments(
val=val,
text=text,
image=image_attachment,
audio=audio_attachment,
video=video_attachment,
pdf=pdf_attachment,
csv=csv_attachment,
)

### ​ TypeScript

In the TypeScript SDK, you can add attachments to traces by using `Uint8Array` or `ArrayBuffer` as data types. Each attachment’s MIME type is specified within `extractAttachments`:

- `Uint8Array`: Useful for handling binary data directly.
- `ArrayBuffer`: Represents fixed-length binary data, which can be converted to `Uint8Array` as needed.

Wrap your function with `traceable` and include your attachments within the `extractAttachments` option.In the TypeScript SDK, the `extractAttachments` function is an optional parameter in the `traceable` configuration. When the traceable-wrapped function is invoked, it extracts binary data (e.g., images, audio files) from your inputs and logs them alongside other trace data, specifying their MIME types.Note that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.

TypeScript

type AttachmentData = Uint8Array | ArrayBuffer;

extractAttachments?: (

import { traceable } from "langsmith/traceable";

const traceableWithAttachments = traceable(
(
val: number,
text: string,
attachment: Uint8Array,
attachment2: ArrayBuffer,
attachment3: Uint8Array,
attachment4: ArrayBuffer,
attachment5: Uint8Array,

{
name: "traceWithAttachments",
extractAttachments: (
val: number,
text: string,
attachment: Uint8Array,
attachment2: ArrayBuffer,
attachment3: Uint8Array,
attachment4: ArrayBuffer,
attachment5: Uint8Array,

{\
"image inputs": ["image/png", attachment],\
"mp3 inputs": ["audio/mpeg", new Uint8Array(attachment2)],\
"video inputs": ["video/mp4", attachment3],\
"pdf inputs": ["application/pdf", new Uint8Array(attachment4)],\
"csv inputs": ["text/csv", new Uint8Array(attachment5)],\
},\
{ val, text },\
],
}
);

const fs = Deno // or Node.js fs module
const image = await fs.readFile("my_image.png"); // Uint8Array
const mp3Buffer = await fs.readFile("my_mp3.mp3");
const mp3ArrayBuffer = mp3Buffer.buffer; // Convert to ArrayBuffer
const video = await fs.readFile("my_video.mp4"); // Uint8Array
const pdfBuffer = await fs.readFile("my_document.pdf");
const pdfArrayBuffer = pdfBuffer.buffer; // Convert to ArrayBuffer
const csv = await fs.readFile("test-vals.csv"); // Uint8Array

// Define example parameters
const val = 42;
const text = "Hello, world!";

// Call traceableWithAttachments with the files
const result = await traceableWithAttachments(
val, text, image, mp3ArrayBuffer, video, pdfArrayBuffer, csv
);

Here is how the above would look in the LangSmith UI. You can expand each attachment to view its contents.!Trace with attachments

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Metadata parameters reference\\
\\
Previous Configure threads\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/log-traces-to-project

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Project & environment settings

Log traces to a specific project

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Log traces to a specific project
- Trace without env vars
- Set a sampling rate for traces
- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Set the destination project statically
- Set the destination project dynamically

You can change the destination project of your traces both statically through environment variables and dynamically at runtime.

## ​ Set the destination project statically

As mentioned in the Tracing Concepts section, LangSmith uses the concept of a `Project` to group traces. If left unspecified, the project is set to `default`. You can set the `LANGSMITH_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your application.

Copy

export LANGSMITH_PROJECT=my-custom-project

If the project specified does not exist, it will be created automatically when the first trace is ingested.

## ​ Set the destination project dynamically

You can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application.

Setting the project name dynamically using one of the below methods overrides the project name set by the `LANGSMITH_PROJECT` environment variable.

Python

TypeScript

import openai
from langsmith import traceable
from langsmith.run_trees import RunTree

client = openai.Client()
messages = [\
{"role": "system", "content": "You are a helpful assistant."},\
{"role": "user", "content": "Hello!"}\
]

# Use the @traceable decorator with the 'project_name' parameter to log traces to LangSmith
# Ensure that the LANGSMITH_TRACING environment variables is set for @traceable to work
@traceable(
run_type="llm",
name="OpenAI Call Decorator",
project_name="My Project"
)
def call_openai(
messages: list[dict], model: str = "gpt-4o-mini"

return client.chat.completions.create(
model=model,
messages=messages,
).choices[0].message.content

# Call the decorated function
call_openai(messages)

# You can also specify the Project via the project_name parameter
# This will override the project_name specified in the @traceable decorator
call_openai(
messages,
langsmith_extra={"project_name": "My Overridden Project"},
)

# The wrapped OpenAI client accepts all the same langsmith_extra parameters
# as @traceable decorated functions, and logs traces to LangSmith automatically.
# Ensure that the LANGSMITH_TRACING environment variables is set for the wrapper to work.
from langsmith import wrappers
wrapped_client = wrappers.wrap_openai(client)
wrapped_client.chat.completions.create(
model="gpt-4o-mini",
messages=messages,
langsmith_extra={"project_name": "My Project"},
)

# Alternatively, create a RunTree object
# You can set the project name using the project_name parameter
rt = RunTree(
run_type="llm",
name="OpenAI Call RunTree",
inputs={"messages": messages},
project_name="My Project"
)
chat_completion = client.chat.completions.create(
model="gpt-4o-mini",
messages=messages,
)
# End and submit the run
rt.end(outputs=chat_completion)
rt.post()

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Configure threads\\
\\
Previous Trace without setting environment variables\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/add-metadata-tags)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Add metadata and tags to traces Trace with Semantic Kernel Self-hosted LangSmith changelog

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/upload-files-with-traces)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Upload files with traces LangSmith Observability LangSmith Fetch

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/log-traces-to-project)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Log multimodal traces LangSmith Observability Log traces to a specific project

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/threads).!

Skip to main content.!#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/annotation-queues).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

Use annotation queues LangSmith Observability Annotate traces and runs inline

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/evaluate-llm-application).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Evaluation approaches

##### Datasets

- Create a dataset

- Manage datasets
- Custom output rendering

##### Set up evaluations

- Run an evaluation

- Evaluation types

- Frameworks & integrations

- Evaluation techniques

- Improve evaluators

- Tutorials

##### Analyze experiment results

- Analyze an experiment
- Compare experiment results
- Filter experiments in the UI
- Fetch performance metrics for an experiment
- Upload experiments run outside of LangSmith

##### Annotation & human feedback

- Use annotation queues
- Set up feedback criteria
- Annotate traces and runs inline
- Audit evaluator scores

##### Common data types

- Example data format
- Dataset prebuilt JSON schema types
- Dataset transformations

404

# Page not found

We couldn’t find the page you were looking for.

How to evaluate an LLM application LangSmith docs Evaluate a RAG application

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/online-evaluations).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Evaluation Evaluation types Set up online evaluators

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/feedback-data-format).!

Skip to main content.!#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/add-metadata-tags).!

Skip to main content.!#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/trace-openai

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Integrations

Trace with OpenAI

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Overview
- LangChain
- LangGraph
- Anthropic (Python only)
- OpenAI
- AutoGen
- Claude Agent SDK
- Claude Code
- CrewAI
- Google ADK
- Instructor (Python only)
- OpenAI Agents SDK
- OpenTelemetry
- PydanticAI
- Semantic Kernel
- Vercel AI SDK
- LiveKit
- Pipecat
- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

The `wrap_openai`/`wrapOpenAI` methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces — no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. Also note that the wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.

The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_openai` or `wrapOpenAI`. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see Setup for more information).If your LangSmith API key is linked to multiple workspaces, set the `LANGSMITH_WORKSPACE_ID` environment variable to specify which workspace to use.By default, the traces will be logged to a project named `default`. To log traces to a different project, see this section.

Python

TypeScript

Copy

import openai
from langsmith import traceable
from langsmith.wrappers import wrap_openai

client = wrap_openai(openai.Client())

@traceable(run_type="tool", name="Retrieve Context")

return "During this morning's meeting, we solved all world conflict."

@traceable(name="Chat Pipeline")
def chat_pipeline(question: str):
context = my_tool(question)
messages = [\
{ "role": "system", "content": "You are a helpful assistant. Please respond to the user's request only based on the given context." },\
{ "role": "user", "content": f"Question: {question}\nContext: {context}"}\
]
chat_completion = client.chat.completions.create(
model="gpt-4o-mini", messages=messages
)
return chat_completion.choices[0].message.content

chat_pipeline("Can you summarize this morning's meetings?")

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Trace with Anthropic\\
\\
Previous Trace with AutoGen\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/trace-anthropic

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Integrations

Trace with Anthropic

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Overview
- LangChain
- LangGraph
- Anthropic (Python only)
- OpenAI
- AutoGen
- Claude Agent SDK
- Claude Code
- CrewAI
- Google ADK
- Instructor (Python only)
- OpenAI Agents SDK
- OpenTelemetry
- PydanticAI
- Semantic Kernel
- Vercel AI SDK
- LiveKit
- Pipecat
- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

The `wrap_anthropic` methods in Python allows you to wrap your Anthropic client in order to automatically log traces — no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. The wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.

The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_anthropic`. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see Setup for more information).If your LangSmith API key is linked to multiple workspaces, set the `LANGSMITH_WORKSPACE_ID` environment variable to specify which workspace to use.By default, the traces will be logged to a project named `default`. To log traces to a different project, see this section.

Copy

import anthropic
from langsmith import traceable
from langsmith.wrappers import wrap_anthropic

client = wrap_anthropic(anthropic.Anthropic())

# You can also wrap the async client as well
# async_client = wrap_anthropic(anthropic.AsyncAnthropic())

@traceable(run_type="tool", name="Retrieve Context")

return "During this morning's meeting, we solved all world conflict."

@traceable(name="Chat Pipeline")
def chat_pipeline(question: str):
context = my_tool(question)
messages = [\
{ "role": "user", "content": f"Question: {question}\nContext: {context}"}\
]
messages = client.messages.create(
model="claude-sonnet-4-5-20250929",
messages=messages,
max_tokens=1024,
system="You are a helpful assistant. Please respond to the user's request only based on the given context."
)
return messages

chat_pipeline("Can you summarize this morning's meetings?")

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Trace with LangGraph\\
\\
Previous Trace with OpenAI\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/log-llm-trace

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Manual instrumentation

Log LLM calls

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Custom instrumentation
- Trace with API
- Log LLM calls
- Log retriever traces
- Metadata parameters
- Upload files with traces
- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Messages Format
- Examples
- Converting custom I/O formats into LangSmith compatible formats
- Identifying a custom model in traces
- Provide token and cost information
- Time-to-first-token

This guide will cover how to log LLM calls to LangSmith when you are using a custom model or a custom input/output format. To make the most of LangSmith’s LLM trace processing, you should log your LLM traces in one of the specified formats.LangSmith offers the following benefits for LLM traces:

- Rich, structured rendering of message lists
- Token and cost tracking per LLM call, per trace and across traces over time

If you don’t log your LLM traces in the suggested formats, you will still be able to log the data to LangSmith, but it may not be processed or rendered in expected ways.If you are using LangChain OSS to call language models or LangSmith wrappers ( OpenAI, Anthropic), these approaches will automatically log traces in the correct format.

The examples on this page use the `traceable` decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the RunTree or API directly.

## ​ Messages Format

When tracing a custom model or a custom input/output format, it must either follow the LangChain format, OpenAI completions format or Anthropic messages format. For more details, refer to the OpenAI Chat Completions or Anthropic Messages documentation. The LangChain format is:

Show LangChain format

​

messages

array

required

A list of messages containing the content of the conversation.

role

string

Identifies the message type. One of: `system` \| `reasoning` \| `user` \| `assistant` \| `tool`

content

Content of the message. List of typed dictionaries.

Show Content options

type

One of: `text` \| `image` \| `file` \| `audio` \| `video` \| `tool_call` \| `server_tool_call` \| `server_tool_result`.

Show text

literal('text')

text

Text content.

annotations

object\[\]

List of annotations for the text

extras

object

Additional provider-specific data.

Show reasoning

literal('reasoning')

Show image

literal('image')

url

URL pointing to the image location.

base64

Base64-encoded image data.

id

Reference ID to an externally stored image (e.g., in a provider’s file system or in a bucket).

mime\_type

Image MIME type (e.g., `image/jpeg`, `image/png`).

Show file (e.g., PDFs)

literal('file')

URL pointing to the file.

Base64-encoded file data.

Reference ID to an externally stored file (e.g., in a provider’s file system or in a bucket).

File MIME type (e.g., `application/pdf`).

Show audio

literal('audio')

URL pointing to the audio file.

Base64-encoded audio data.

Reference ID to an externally stored audio file (e.g., in a provider’s file system or in a bucket).

Audio MIME type (e.g., `audio/mpeg`, `audio/wav`).

Show video

literal('video')

URL pointing to the video file.

Base64-encoded video data.

Reference ID to an externally stored video file (e.g., in a provider’s file system or in a bucket).

Video MIME type (e.g., `video/mp4`, `video/webm`).

Show tool\_call

literal('tool\_call')

name

args

Arguments to pass to the tool.

Unique identifier for this tool call.

Show server\_tool\_call

literal('server\_tool\_call')

The name of the tool to be called.

Show server\_tool\_result

literal('server\_tool\_result')

tool\_call\_id

Identifier of the corresponding server tool call.

status

Execution status of the server-side tool. One of: `success` \| `error`.

output

Output of the executed tool.

Must match the `id` of a prior `assistant` message’s `tool_calls[i]` entry. Only valid when `role` is `tool`.

usage\_metadata

Use this field to send token counts and/or costs with your model’s output. See this guide for more details.

### ​ Examples

Text and reasoning

Tool calls

Multimodal

Server-side tool calls

Copy

inputs = {
"messages": [\
{\
"role": "user",\
"content": [\
{\
"type": "text",\
"text": "Hi, can you tell me the capital of France?"\
}\
]\
}\
]
}

outputs = {
"messages": [\
{\
"role": "assistant",\
"content": [\
{\
"type": "text",\
"text": "The capital of France is Paris."\
},\
{\
"type": "reasoning",\
"text": "The user is asking about..."\
}\
]\
}\
]
}

## ​ Converting custom I/O formats into LangSmith compatible formats

If you’re using a custom input or output format, you can convert it to a LangSmith compatible format using `process_inputs`/`processInputs` and `process_outputs`/`processOutputs` functions on the `@traceable` decorator (Python) or `traceable` function (TS).`process_inputs`/`processInputs` and `process_outputs`/`processOutputs` accept functions that allow you to transform the inputs and outputs of a specific trace before they are logged to LangSmith. They have access to the trace’s inputs and outputs, and can return a new dictionary with the processed data.Here’s a boilerplate example of how to use `process_inputs` and `process_outputs` to convert a custom I/O format into a LangSmith compatible format:

Show the code

Python

class OriginalInputs(BaseModel):
"""Your app's custom request shape"""

class OriginalOutputs(BaseModel):
"""Your app's custom response shape."""

class LangSmithInputs(BaseModel):
"""The input format LangSmith expects."""

class LangSmithOutputs(BaseModel):
"""The output format LangSmith expects."""

@traceable(run_type="llm", process_inputs=process_inputs, process_outputs=process_outputs)

"""
Your app's model call. Keeps your custom I/O shape.
The decorators call process_* to log LangSmith-compatible format.
"""

## ​ Identifying a custom model in traces

When using a custom model, it is recommended to also provide the following `metadata` fields to identify the model when viewing traces and when filtering.

- `ls_provider`: The provider of the model, e.g. “openai”, “anthropic”, etc.
- `ls_model_name`: The name of the model, e.g. “gpt-4o-mini”, “claude-3-opus-20240229”, etc.

TypeScript

from langsmith import traceable

inputs = [\
{"role": "system", "content": "You are a helpful assistant."},\
{"role": "user", "content": "I'd like to book a table for two."},\
]
output = {
"choices": [\
{\
"message": {\
"role": "assistant",\
"content": "Sure, what time would you like to book the table for?"\
}\
}\
]
}

@traceable(
run_type="llm",
metadata={"ls_provider": "my_provider", "ls_model_name": "my_model"}
)
def chat_model(messages: list):
return output

chat_model(inputs)

This code will log the following trace:

If you implement a custom streaming chat\_model, you can “reduce” the outputs into the same format as the non-streaming version. This is currently only supported in Python.

def _reduce_chunks(chunks: list):
all_text = "".join([chunk["choices"][0]["message"]["content"] for chunk in chunks])
return {"choices": [{"message": {"content": all_text, "role": "assistant"}}]}

@traceable(
run_type="llm",
reduce_fn=_reduce_chunks,
metadata={"ls_provider": "my_provider", "ls_model_name": "my_model"}
)
def my_streaming_chat_model(messages: list):
for chunk in ["Hello, " + messages[1]["content"]]:
yield {
"choices": [\
{\
"message": {\
"content": chunk,\
"role": "assistant",\
}\
}\
]
}

list(
my_streaming_chat_model(
[\
{"role": "system", "content": "You are a helpful assistant. Please greet the user."},\
{"role": "user", "content": "polly the parrot"},\
],
)
)

If `ls_model_name` is not present in `extra.metadata`, other fields might be used from the `extra.metadata` for estimating token counts. The following fields are used in the order of precedence:

1. `metadata.ls_model_name`
2. `inputs.model`
3. `inputs.model_name`

To learn more about how to use the `metadata` fields, refer to the Add metadata and tags guide.

## ​ Provide token and cost information

LangSmith calculates costs derived from token counts and model prices automatically. Learn about how to provide tokens and/or costs in a run and viewing costs in the LangSmith UI.

## ​ Time-to-first-token

If you are using `traceable` or one of our SDK wrappers, LangSmith will automatically populate time-to-first-token for streaming LLM runs.
However, if you are using the `RunTree` API directly, you will need to add a `new_token` event to the run tree in order to properly populate time-to-first-token.Here’s an example:

from langsmith.run_trees import RunTree
run_tree = RunTree(
name="CustomChatModel",
run_type="llm",
inputs={ ... }
)
run_tree.post()
llm_stream = ...
first_token = None
for token in llm_stream:
if first_token is None:
first_token = token
run_tree.add_event({
"name": "new_token"
})
run_tree.end(outputs={ ... })
run_tree.patch()

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Trace with API\\
\\
Previous Log retriever traces\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/sample-traces

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Project & environment settings

Set a sampling rate for traces

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Log traces to a specific project
- Trace without env vars
- Set a sampling rate for traces
- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Set a global sampling rate
- Set different sampling rates per client

When working with high-volume applications, you may not want to log every trace to LangSmith. Sampling rates allow you to control what percentage of traces are logged, helping you balance observability needs with cost considerations.

## ​ Set a global sampling rate

This section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API.

By default, all traces are logged to LangSmith. To down-sample the number of traces logged to LangSmith, set the `LANGSMITH_TRACING_SAMPLING_RATE` environment variable to any float between `0` (no traces) and `1` (all traces). For instance, setting the following environment variable will log 75% of the traces.

Copy

export LANGSMITH_TRACING_SAMPLING_RATE=0.75

This works for the `traceable` decorator and `RunTree` objects.

## ​ Set different sampling rates per client

You can also set sampling rates on specific `Client` instances and use the `tracing_context` context manager:

from langsmith import Client, tracing_context

# Create clients with different sampling rates
client_1 = Client(tracing_sampling_rate=0.5) # 50% sampling
client_2 = Client(tracing_sampling_rate=0.25) # 25% sampling
client_no_trace = Client(tracing_sampling_rate=0.0) # No tracing

# Use different sampling rates for different operations
with tracing_context(client=client_1):
# Your code here - will be traced with 50% sampling rate
agent_1.invoke(...)

with tracing_context(client=client_2):
# Your code here - will be traced with 25% sampling rate

with tracing_context(client=client_no_trace):
# Your code here - will not be traced

This allows you to control sampling rates at the operation level.

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Trace without setting environment variables\\
\\
Previous Cost tracking\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/distributed-tracing

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Advanced tracing techniques

Implement distributed tracing

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Implement distributed tracing
- Trace JS functions in serverless environments
- Log multimodal traces
- Trace generator functions
- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- Distributed tracing in Python
- Distributed tracing in TypeScript

Sometimes, you need to trace a request across multiple services.LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (`langsmith-trace` and optional `baggage` for metadata/tags).Example client-server setup:

- Trace starts on client
- Continues on server

## ​ Distributed tracing in Python

Copy

# client.py
from langsmith.run_helpers import get_current_run_tree, traceable
import httpx

@traceable
async def my_client_function():
headers = {}
async with httpx.AsyncClient(base_url="...") as client:
if run_tree := get_current_run_tree():
# add langsmith-id to headers
headers.update(run_tree.to_headers())
return await client.post("/my-route", headers=headers)

Then the server (or other service) can continue the trace by handling the headers appropriately. If you are using an asgi app Starlette or FastAPI, you can connect the distributed trace using LangSmith’s `TracingMiddleware`.

The `TracingMiddleware` class was added in `langsmith==0.1.133`.

Example using FastAPI:

from langsmith import traceable
from langsmith.middleware import TracingMiddleware
from fastapi import FastAPI, Request

app = FastAPI() # Or Flask, Django, or any other framework
app.add_middleware(TracingMiddleware)

@traceable
async def some_function():
...

@app.post("/my-route")
async def fake_route(request: Request):
return await some_function()

Or in Starlette:

from starlette.applications import Starlette
from starlette.middleware import Middleware
from langsmith.middleware import TracingMiddleware

routes = ...
middleware = [\
Middleware(TracingMiddleware),\
]
app = Starlette(..., middleware=middleware)

If you are using other server frameworks, you can always “receive” the distributed trace by passing the headers in through `langsmith_extra`:

# server.py
import langsmith as ls
from fastapi import FastAPI, Request

@ls.traceable
async def my_application():
...

app = FastAPI() # Or Flask, Django, or any other framework

@app.post("/my-route")
async def fake_route(request: Request):
# request.headers: {"langsmith-trace": "..."}
# as well as optional metadata/tags in `baggage`
with ls.tracing_context(parent=request.headers):
return await my_application()

The example above uses the `tracing_context` context manager. You can also directly specify the parent run context in the `langsmith_extra` parameter of a method wrapped with `@traceable`.

# ... same as above

@app.post("/my-route")
async def fake_route(request: Request):
my_application(langsmith_extra={"parent": request.headers})

## ​ Distributed tracing in TypeScript

First, we obtain the current run tree from the client and convert it to `langsmith-trace` and `baggage` header values, which we can pass to the server:

// client.mts
import { getCurrentRunTree, traceable } from "langsmith/traceable";

const client = traceable(

const runTree = getCurrentRunTree();
return await fetch("...", {
method: "POST",
headers: runTree.toHeaders(),

},
{ name: "client" }
);

await client();

Then, the server converts the headers or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Cost tracking\\
\\
Previous Trace JS functions in serverless environments\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/trace-openai)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Trace with OpenAI Agents SDK Trace with OpenAI Trace with CrewAI

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/trace-anthropic)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Trace with Anthropic LangSmith Observability Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/trace-anthropic).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Trace with Anthropic LangSmith Observability Trace with LangChain (Python and JS/TS)

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/sample-traces)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Observability Set a sampling rate for traces

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/distributed-tracing)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Implement distributed tracing Distributed tracing with Agent Server LangSmith Observability

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/integrations

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Integrations

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Overview
- LangChain
- LangGraph
- Anthropic (Python only)
- OpenAI
- AutoGen
- Claude Agent SDK
- Claude Code
- CrewAI
- Google ADK
- Instructor (Python only)
- OpenAI Agents SDK
- OpenTelemetry
- PydanticAI
- Semantic Kernel
- Vercel AI SDK
- LiveKit
- Pipecat
- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

LangSmith provides support for LangChain and LangGraph as well as integrations with a growing set of popular LLM providers and agent frameworks. For setup and usage, refer to the guide pages in the navigation bar.

## ​ LangChain open source frameworks

**LangChain** **LangGraph**

## ​ LLM providers

![https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=eb1007c5cd89bedb14c30571c78b596d\\
\\
**OpenAI**](https://docs.langchain.com/langsmith/trace-openai) ![https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/anthropic-icon.svg?fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=72154d71964e6774d27d4594c9f8d5df\\
\\
**Anthropic**](https://docs.langchain.com/langsmith/trace-anthropic)

## Google Gemini

## Amazon Bedrock

## DeepSeek

## Mistral

## ​ Agent frameworks

![https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/ag-icon.svg?fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=d26f92a1a92dfa32c5a6a808fcd53437\\
\\
**AutoGen**](https://docs.langchain.com/langsmith/trace-with-autogen) ![https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=8b7cf6ad6a521b6db5193db084dc3d4c\\
\\
**Claude Agent SDK**](https://docs.langchain.com/langsmith/trace-claude-agent-sdk) ![https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/crewai-icon.svg?fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=b55086b8158f6986b44a1d47fc4ca412\\
\\
**CrewAI**](https://docs.langchain.com/langsmith/trace-with-crewai) ![https://mintcdn.com/langchain-5e9cc07a/NMCyHPS_X1n3SCfv/langsmith/images/agent-development-kit.png?fit=max&auto=format&n=NMCyHPS_X1n3SCfv&q=85&s=afc5c9f25aa944f4592e7aebb04b467d\\
\\
**Google ADK**](https://docs.langchain.com/langsmith/trace-with-google-adk) ![https://mintcdn.com/langchain-5e9cc07a/DtQtEdiVyCTfcVOD/langsmith/images/OpenAI-black-monoblossom.svg?fit=max&auto=format&n=DtQtEdiVyCTfcVOD&q=85&s=eb1007c5cd89bedb14c30571c78b596d\\
\\
**OpenAI Agents**](https://docs.langchain.com/langsmith/trace-with-openai-agents-sdk) ![https://mintcdn.com/langchain-5e9cc07a/kL_P9zpLIoeJVEMB/langsmith/images/opentelemetry-icon.svg?fit=max&auto=format&n=kL_P9zpLIoeJVEMB&q=85&s=b5ed93d3d8f091346beab9ff553258ee\\
\\
**OpenTelemetry**](https://docs.langchain.com/langsmith/trace-with-opentelemetry) ![https://mintcdn.com/langchain-5e9cc07a/IQij97c1iT4J1DZy/langsmith/images/pydantic-icon.png?fit=max&auto=format&n=IQij97c1iT4J1DZy&q=85&s=b91351878ba0ab1bc4120828e0f40499\\
\\
**PydanticAI**](https://docs.langchain.com/langsmith/trace-with-pydantic-ai) ![https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/microsoft-icon.svg?fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=f72a7081983314666065154f0b36f984\\
\\
**Semantic Kernel**](https://docs.langchain.com/langsmith/trace-with-semantic-kernel) ![https://mintcdn.com/langchain-5e9cc07a/igkac3bTlN5hpgNy/langsmith/images/vercel-icon-light.svg?fit=max&auto=format&n=igkac3bTlN5hpgNy&q=85&s=ae7683dcafe139ae42a32a1be055a3e9\\
\\
**Vercel AI SDK**](https://docs.langchain.com/langsmith/trace-with-vercel-ai-sdk)

## ​ Voice AI frameworks

![https://mintcdn.com/langchain-5e9cc07a/C_d0VuYopkV_gLkg/langsmith/images/pipecat-icon.png?fit=max&auto=format&n=C_d0VuYopkV_gLkg&q=85&s=f983bb047370fa7b25253d3e83ae477a\\
\\
**Pipecat**](https://docs.langchain.com/langsmith/trace-with-pipecat) ![https://mintcdn.com/langchain-5e9cc07a/znUi0d7O0eUHbRzU/langsmith/images/livekit-icon.png?fit=max&auto=format&n=znUi0d7O0eUHbRzU&q=85&s=e5f3749ba5b4d0e6bda3582951521e5e\\
\\
**Livekit**](https://docs.langchain.com/langsmith/trace-with-livekit)

## ​ Other

![https://mintcdn.com/langchain-5e9cc07a/5R5TUrRCLnd2oAV2/langsmith/images/instructor-icon.svg?fit=max&auto=format&n=5R5TUrRCLnd2oAV2&q=85&s=6efff2ac45f2641e6bc4ea7572eb4b9b\\
\\
**Instructor**](https://docs.langchain.com/langsmith/trace-with-instructor) ![https://mintcdn.com/langchain-5e9cc07a/VTUyw-EOhx2PnFCY/langsmith/images/claude.svg?fit=max&auto=format&n=VTUyw-EOhx2PnFCY&q=85&s=8b7cf6ad6a521b6db5193db084dc3d4c\\
\\
**Claude Code**](https://docs.langchain.com/langsmith/trace-claude-code)

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Trace a RAG application tutorial\\
\\
Previous Trace with LangChain (Python and JS/TS)\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/integrations)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith docs Self-hosted LangSmith changelog Self-hosted LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/run-data-format).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith data plane LangSmith Fetch Self-hosted LangSmith

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/threads).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

LangSmith Fetch LangSmith Polly LangSmith docs

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/collector-proxy

Skip to main content

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Troubleshooting guides

Beta LangSmith Collector-Proxy

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

- Troubleshoot trace nesting
- Troubleshoot variable caching
- Beta LangSmith Collector-Proxy

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

On this page

- When to Use the Collector-Proxy
- Key Features
- Configuration
- Project Configuration
- Authentication
- Deployment (Docker)
- Usage
- Health & Scaling
- Horizontal Scaling
- Fork & Extend

This is a beta feature. The API may change in future releases.

The LangSmith Collector-Proxy is a lightweight, high-performance proxy server that sits between your application and the LangSmith backend. It batches and compresses trace data before sending it to LangSmith, reducing network overhead and improving performance.

## ​ When to Use the Collector-Proxy

The Collector-Proxy is particularly valuable when:

- You’re running multiple instances of your application in parallel and need to efficiently aggregate traces
- You want more efficient tracing than direct OTEL API calls to LangSmith (the collector optimizes batching and compression)
- You’re using a language that doesn’t have a native LangSmith SDK

## ​ Key Features

- **Efficient Data Transfer** Batches multiple spans into fewer, larger uploads.
- **Compression** Uses zstd to minimize payload size.
- **OTLP Support** Accepts OTLP JSON and Protobuf over HTTP POST.
- **Semantic Translation** Maps GenAI/OpenInference conventions to the LangSmith Run model.
- **Flexible Batching** Flush by span count or time interval.

## ​ Configuration

Configure via environment variables:

| Variable | Description | Default |
| --- | --- | --- |
| `HTTP_PORT` | Port to run the proxy server | `4318` |
| `LANGSMITH_ENDPOINT` | LangSmith backend URL | `https://api.smith.langchain.com` |
| `LANGSMITH_API_KEY` | API key for LangSmith | **Required** (env var or header) |
| `LANGSMITH_PROJECT` | Default tracing project | Default project if not specified |
| `BATCH_SIZE` | Spans per upload batch | `100` |
| `FLUSH_INTERVAL_MS` | Flush interval in milliseconds | `1000` |
| `MAX_BUFFER_BYTES` | Max uncompressed buffer size | `10485760` (10 MB) |
| `MAX_BODY_BYTES` | Max incoming request body size | `209715200` (200 MB) |
| `MAX_RETRIES` | Retry attempts for failed uploads | `3` |
| `RETRY_BACKOFF_MS` | Initial backoff in milliseconds | `100` |

### ​ Project Configuration

The Collector-Proxy supports LangSmith project configuration with the following priority:

1. If a project is specified in the request headers (`Langsmith-Project`), that project will be used
2. If no project is specified in headers, it will use the project set in the `LANGSMITH_PROJECT` environment variable
3. If neither is set, it will trace to the `default` project.

### ​ Authentication

The API key can be provided either:

- As an environment variable (`LANGSMITH_API_KEY`)
- In the request headers (`X-API-Key`)

## ​ Deployment (Docker)

You can deploy the Collector-Proxy with Docker:

1. **Build the image**

Copy

docker build \
-t langsmith-collector-proxy:beta .

2. **Run the container**

docker run -d \
-p 4318:4318 \

langsmith-collector-proxy:beta

## ​ Usage

Point any OTLP-compatible client or the OpenTelemetry Collector exporter at:

Send a test trace:

curl -X POST \
-H "Content-Type: application/json" \
--data '{
"resourceSpans": [\
{\
"resource": {\
"attributes": [\
{\
"key": "service.name",\
"value": { "stringValue": "test-service" }\
}\
]\
},\
"scopeSpans": [\
{\
"scope": {\
"name": "example/instrumentation",\
"version": "1.0.0"\
},\
"spans": [\
{\
"traceId": "T6nh/mMkIONaoHewS9UWIw==",\
"spanId": "0tEqJwCpvU0=",\
"name": "parent-span",\
"kind": "SPAN_KIND_INTERNAL",\
"startTimeUnixNano": 1747675155185223936,\
"endTimeUnixNano": 1747675156185223936,\
"attributes": [\
{\
"key": "gen_ai.prompt",\
"value": {\
"stringValue": "{\"text\":\"Hello, world!\"}"\
}\
},\
{\
"key": "gen_ai.usage.input_tokens",\
"value": {\
"intValue": "5"\
}\
},\
{\
"key": "gen_ai.completion",\
"value": {\
"stringValue": "{\"text\":\"Hi there!\"}"\
}\
},\
{\
"key": "gen_ai.usage.output_tokens",\
"value": {\
"intValue": "3"\
}\
}\
],\
"droppedAttributesCount": 0,\
"events": [],\
"links": [],\
"status": {}\
}\
]\
}\
]\
}\
]
}'

## ​ Health & Scaling

- **Liveness**: `GET /live` → 200
- **Readiness**: `GET /ready` → 200

## ​ Horizontal Scaling

To ensure full traces are batched correctly, route spans with the same trace ID to the same instance (e.g., via consistent hashing).

## ​ Fork & Extend

Fork the Collector-Proxy repo on GitHub and implement your own converter:

- Create a custom `GenAiConverter` or modify the existing one in `internal/translator/otel_converter.go`
- Register the custom converter in `internal/translator/translator.go`

* * *

Edit this page on GitHub or file an issue.

Connect these docs to Claude, VSCode, and more via MCP for real-time answers.

Was this page helpful?

YesNo

Troubleshoot variable caching\\
\\
Previous Filter traces\\
\\
Next

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/add-metadata-tags).

Skip to main content.#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Add metadata and tags to traces Trace with Semantic Kernel Self-hosted LangSmith changelog

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

# https://docs.langchain.com/langsmith/collector-proxy)

Skip to main content#content-area)

Docs by LangChain home page![light logo!dark logo](https://docs.langchain.com/)

LangSmith

Search...

Ctrl K

Navigation

Page Not Found

Get started Observability Evaluation Prompt engineering Deployment Agent Builder Platform setup Reference

- Overview

- Quickstart

- Concepts

- Trace a RAG application

##### Tracing setup

- Integrations

- Manual instrumentation

- Threads

##### Configuration & troubleshooting

- Project & environment settings

- Cost tracking
- Advanced tracing techniques

- Data & privacy

- Troubleshooting guides

##### Viewing & managing traces

- Filter traces
- Query traces (SDK)
- Compare traces
- Share or unshare a trace publicly
- Retrieve traces via CLI
- View server logs for a trace
- Bulk export trace data

##### Automations

- Set up automation rules
- Configure webhook notifications for rules

##### Feedback & evaluation

- Log user feedback using the SDK
- Set up online evaluators

##### Monitoring & alerting

- Monitor projects with dashboards
- Alerts
- Configure webhook notifications for alerts
- Insights

##### Data type reference

- Run (span) data format
- Feedback data format
- Trace query syntax

404

# Page not found

We couldn’t find the page you were looking for.

Beta LangSmith Collector-Proxy Configure your collector for LangSmith telemetry Export LangSmith telemetry to your observability backend

Ctrl+I

Assistant

Responses are generated using AI and may contain mistakes.

---

